[["kata-pengantar.html", "Analisis Resiko Kata Pengantar Deskripsi Buku Ucapan Terima Kasih Kontributor Kritik &amp; Saran", " Analisis Resiko Bakti Siregar, M.Sc 2023-06-13 Kata Pengantar Deskripsi Buku Analisa Resiko adalah buku yang interaktif, online, dan tersedia secara gratis. Versi online berisi banyak objek interaktif (kuis, demonstrasi komputer, grafik interaktif, video, dan sejenisnya) yang dapat dipergunakan untuk menunjang pembelajaran lebih baik. Sebagian besar isi dari buku ini tersedia untuk dibaca offline dalam format pdf dan EPUB. Direncanakan akan tersedia dalam berbagai bahasa. Petunjuk Penggunaan Buku ini dapat dipergunakan dalam pembelajaran kurikulum aktuaria di seluruh dunia. Adapun cakupan pembelajarannya adalah analisa data kerugian dari berbagai organisasi aktuaria ternama didunia. Sehingga, buku ini cocok digunakan ditingkat universitas maupun pembelajar mandiri yang ingin lulus ujian aktuaria profesional. Selain itu, buku juga akan sangat berguna dalam pengembangan profesional berkelanjutan bagi para aktuaris maupun profesional lainnya di bidang asuransi dan industri terkait manajemen risiko keuangan. Manfaat Salah satu manfaat penting dari buku online ini adalah pemerataan akses pengetahuan, sehingga memungkinkan masyarakat yang lebih luas untuk belajar tentang profesi aktuaria. Selain itu, setiap orang memiliki kapasitas untuk melibatkan banyak pihak melalui pembelajaran aktif yang memperdalam proses pembelajaran, menghasilkan analis terbaik dalam melakukan pekerjaan aktuaria yang solid. Sekarang, pertanyaan besarnya adalah “Mengapa buku ini baik untuk mahasiswa dan dosen serta orang lain yang terlibat dalam proses pembelajaran?” Biaya adalah salah satu faktor yang sering disebut sebagai kendala utama bagi mahasiswa dan dosen dalam pemilihan buku teks. Selain itu, Mahasiswa sekarang ini lebih menyukai buku yang dapat dibawa secara secara elektronik (online). Mengapa Analisa Resiko? Tujuannya adalah agar buku ini pada akhirnya akan dapat dikembangkan secara serius kurikulum aktuaria. Mengingat perubahan era digital seperti sekarang ini akhirnya mendorong para aktuaris dalam melakukan analisa bergantung pada data yang dimiliki. Ide di balik nama Analisa Resiko adalah untuk mengintegrasikan model data kerugian klasik dari probabilitas yang diterapkan dengan alat analitik modern. Secara khusus, penulis menyadari bahwa big data (termasuk media sosial dan asuransi berbasis penggunaan) akan terus berkembang dan komputasi berkecepatan tinggi sudah tersedia. Ucapan Terima Kasih Kami juga ingin mengucapkan terima kasih yang sebesar-sebesar pada semua pihak yang terlibat dalam pengembangan buku ini, yakni; mahasiswa-i, dosen, dan Universitas Matana atas dukungan dalam upaya bersama kami untuk menyediakan konten pendidikan dalam bidang aktuaria. Kontributor Sebagian besar dari isi buku ini diadopsi dari Loss Data Analytics. Berikut ini adalah nama-nama dan biografi singkat para penulis: Bakti Siregar, M.Sc adalah Kepala Program Studi dan Dosen di Jurusan Statistika Universitas Matana. Beliau juga seorang dosen yang juga bekerja sebagai ilmuwan data lepas yang memiliki antusiasme untuk analitik data besar, pembelajaran mesin, Pemodelan, dan pemecahan masalah. Orang menganggap saya programmer Matematika karena saya memiliki kemampuan yang kuat dalam program Statistik seperti R Studio, dan Python, dan juga akrab dengan alat basis data seperti MySQL dan sistem data besar baik Spark maupun Hadoop. Selain itu, saya dapat mengoperasikan salah satu perangkat lunak analitik bisnis yang paling kuat seperti Tableau. Yosia adalah salah satu mahasiwa terbaik di jurusan Statistika Universitas Matana. Dia juga memiliki minat dalam pembelajaran sains data dan akuturia khususnya melakukan komputasi dengan menggunakan R dan Python. Yosia bercita-cita suatu saat nanti akan menjadi seseorang yang ahli dibidang aktuaria maupun sain data. Yosia adalah mahasiswi jurusan Statistik di Universitas Matana. Dia memiliki minat teoretis yang luas serta minat dalam komputasi, ia juga sudah pernah terlibat dalam menerbitkan di jurnal Pengabdian Kepada Masyarakat (PKM). Dia juga aktif dalam berbagai aktifitas organisasi kampus. Clara Della adalah mahasiswi jurusan Statistik di Universitas Matana. Dia memiliki minat teoretis yang luas serta minat dalam komputasi, ia juga sudah pernah terlibat dalam menerbitkan di jurnal Pengabdian Kepada Masyarakat (PKM). Dia juga aktif dalam berbagai aktifitas organisasi kampus. adalah mahasiswi jurusan Statistik di Universitas Matana. Dia memiliki minat teoretis yang luas serta minat dalam komputasi, ia juga sudah pernah terlibat dalam menerbitkan di jurnal Pengabdian Kepada Masyarakat (PKM). Dia juga aktif dalam berbagai aktifitas organisasi kampus. Karen adalah mahasiswi jurusan Statistik di Universitas Matana yang memiliki keahlian penelitiannya dengan menggunakan teori pemodelan, manajemen risiko, dan optimasi. adalah mahasiswi jurusan Statistik di Universitas Matana. Dia memiliki minat teoretis yang luas serta minat dalam komputasi, ia juga sudah pernah terlibat dalam menerbitkan di jurnal Pengabdian Kepada Masyarakat (PKM). Dia juga aktif dalam berbagai aktifitas organisasi kampus. Brigita adalah dosen senior di Macquarie University di Australia, di mana ia menjabat sebagai direktur program sarjana aktuaria sejak 2018. Ia memperoleh gelar Ph.D. pada tahun 2015 dari Nanyang Technological University di Singapura. Dia adalah seorang aktuaris yang berkualifikasi penuh, memegang kredensial dari US Society of Actuaries dan Australian Actuaries Institute. Minat penelitian utamanya adalah pemodelan kematian, manajemen risiko umur panjang, dan sistem bonus-malus. Naufal adalah seorang profesor di Universitas Matana. Dia memiliki gelar di bidang Matematika dan Ph.D. dalam Sains: Matematika, diperoleh di University of Antwerp. Selama Ph.D., ia berhasil mengambil Magister Asuransi dan Magister Teknik Keuangan dan Aktuaria, keduanya di KU Leuven. Penelitiannya berfokus pada adaptasi dan penerapan metode statistik yang kuat untuk data asuransi dan keuangan. adalah mahasiswi jurusan Statistik di Universitas Matana. Dia memiliki minat teoretis yang luas serta minat dalam komputasi, ia juga sudah pernah terlibat dalam menerbitkan di jurnal Pengabdian Kepada Masyarakat (PKM). Dia juga aktif dalam berbagai aktifitas organisasi kampus. Garry adalah Associate Professor di Departemen Manajemen Risiko, Asuransi, dan Kesehatan di Fox School of Business, Temple University? Dia adalah Associate dari Society of Actuaries. Dia mengajar mata kuliah Ilmu Aktuaria dan Manajemen Risiko di tingkat sarjana dan pascasarjana. Minat penelitiannya meliputi tata kelola perusahaan asuransi, manajemen modal, dan analisis sentimen. Dia menerima gelar Ph.D. dari The Wharton School of the University of Pennsylvania. Kritik &amp; Saran Buku teks interaktif yang tersedia secara gratis mewakili usaha baru dalam pendidikan aktuaria dan kami membutuhkan masukan Anda. Meskipun banyak upaya telah dilakukan untuk pengembangan, kami mengharapkan cegukan. Harap beri tahu instruktur Anda tentang peluang untuk peningkatan, hubungi kami melalui situs proyek kami, atau hubungi kontributor bab secara langsung dengan saran peningkatan. Berikut ini dilampirkan beberapa peninjau atau pembaca yang telah memberikan saran dan pendapat mengenai pengembangan buku ini, adalah: mahasiswa 1 mahasiswa 2 mahasiswa 3 mahasiswa 4 mahasiswa 5 mahasiswa 6 "],["pengantar-analitika-data-kerugian.html", "Bab 1 Pengantar Analitika Data Kerugian 1.1 Relevansi Analitika dalam Aktivitas Asuransi 1.2 Operasi Perusahaan Asuransi", " Bab 1 Pengantar Analitika Data Kerugian Preview Bab. Buku ini memperkenalkan pada metode analisis data asuransi. Bagian 1.1 dimulai dengan pembahasan mengapa penggunaan data itu penting dalam industri asuransi. Bagian 1.2 memberikan gambaran umum tentang tujuan analisis data asuransi yang diperkuat dalam studi kasus Bagian 1.3. Secara alami, ada kesenjangan besar antara tujuan umum yang dirangkum dalam gambaran dan aplikasi studi kasus. Kesenjangan ini dibahas melalui metode dan teknik analisis data yang tercakup dalam penjelasan berikutnya. 1.1 Relevansi Analitika dalam Aktivitas Asuransi yang akan dipelajari dalam bab ini yaitu: Meringkas pentingnya asuransi bagi konsumen dan ekonomi Menggambarkan analitika Mengidentifikasi peristiwa penghasil data yang terkait dengan jangka waktu kontrak asuransi yang umum 1.1.1 Sifat dan Relevansi Asuransi Buku ini memperkenalkan proses penggunaan data untuk mengambil keputusan dalam konteks asuransi. Buku ini tidak berasumsi bahwa pembaca sudah familiar dengan asuransi, tetapi memperkenalkan konsep-konsep asuransi sesuai kebutuhan. Jika baru mengenal asuransi, mungkin yang paling mudah adalah memikirkan sebuah polis asuransi yang mencakup isi apartemen atau rumah yang dapat di sewa (dikenal sebagai asuransi penyewa) atau isi dan properti dari bangunan yang dimiliki pribadi atau seorang teman (dikenal sebagai asuransi pemilik rumah). Contoh umum lainnya adalah asuransi mobil. Dalam kejadian kecelakaan, polis ini dapat mencakup kerusakan pada kendaraan pribadi, kerusakan pada kendaraan lain dalam kecelakaan tersebut, serta biaya medis bagi mereka yang terluka dalam kecelakaan. Salah satu cara untuk memahami sifat asuransi adalah dengan melihat siapa yang membelinya. Asuransi penyewa, pemilik rumah, dan asuransi mobil adalah contoh asuransi personal, karena polis-polis ini diterbitkan untuk individu. Bisnis juga membeli asuransi, seperti perlindungan atas properti mereka, dan ini dikenal sebagai asuransi komersial. Penjualnya, perusahaan asuransi, juga dikenal sebagai penanggung. Bahkan perusahaan asuransi pun membutuhkan asuransi. Hal ini dikenal sebagai reasuransi. Cara lain untuk memahami sifat asuransi adalah dengan jenis risiko yang dicakup. Di Amerika Serikat, kebijakan seperti asuransi penyewa dan pemilik rumah dikenal sebagai asuransi properti, sedangkan kebijakan seperti asuransi mobil yang mencakup kerusakan medis pada orang disebut asuransi kecelakaan. Di negara lain, keduanya dikenal sebagai asuransi non-hidup atau umum, untuk membedakannya dari asuransi jiwa. Baik asuransi jiwa maupun asuransi non-hidup adalah komponen penting dalam ekonomi dunia. Institut Informasi Asuransi (2016) memperkirakan premi asuransi langsung di dunia untuk tahun 2014 sebesar 2.654.549 juta dolar AS untuk asuransi jiwa dan 2.123.699 juta dolar AS untuk asuransi non-hidup; angka-angka ini dalam jutaan dolar AS. Total tersebut mewakili 6,2% dari produk domestik bruto (PDB) dunia. Dengan kata lain, asuransi jiwa menyumbang 55,5% dari premi asuransi dan 3,4% dari PDB dunia, sedangkan asuransi non-hidup menyumbang 44,5% dari premi asuransi dan 2,8% dari PDB dunia. Baik asuransi jiwa maupun asuransi non-hidup merupakan kegiatan ekonomi penting. Asuransi mungkin tidak semenarik industri olahraga, tetapi asuransi memengaruhi kehidupan keuangan banyak orang. Dalam segala ukuran, asuransi merupakan kegiatan ekonomi yang besar. Seperti yang telah disebutkan sebelumnya, secara global, premi asuransi mencakup sekitar 6,2% dari PDB dunia pada tahun 2014 (Institut Informasi Asuransi 2016). Sebagai contoh, premi asuransi menyumbang 18,9% dari PDB di Taiwan (yang tertinggi dalam studi ini) dan mewakili 7,3% dari PDB di Amerika Serikat. Pada tingkat pribadi, hampir setiap orang yang memiliki rumah memiliki asuransi untuk melindungi diri mereka dalam kejadian kebakaran, badai es, atau peristiwa bencana lainnya. Hampir setiap negara mengharuskan asuransi bagi mereka yang mengemudikan mobil. Secara keseluruhan, meskipun tidak terlalu menghibur, asuransi memainkan peran penting dalam perekonomian negara-negara dan kehidupan individu. 1.1.2 Apa itu Analitika? Asuransi merupakan industri yang mengandalkan data. Seperti perusahaan-perusahaan besar dan organisasi lainnya, perusahaan asuransi menggunakan data ketika mencoba untuk menentukan berapa banyak yang harus dibayarkan kepada karyawan, berapa banyak karyawan yang harus dipertahankan, bagaimana cara memasarkan layanan dan produk mereka, bagaimana meramalkan tren keuangan, dan sebagainya. Hal ini mewakili bidang-bidang aktivitas umum yang tidak spesifik hanya untuk industri asuransi. Meskipun setiap industri memiliki nuansa dan kebutuhan data yang berbeda, pengumpulan, analisis, dan penggunaan data merupakan kegiatan yang dibagikan oleh semua, mulai dari raksasa internet hingga bisnis kecil, oleh organisasi publik dan pemerintah, dan tidak spesifik hanya untuk industri asuransi. Anda akan menemukan bahwa metode dan alat pengumpulan dan analisis data yang diperkenalkan dalam teks ini relevan untuk semua industri. Dalam industri yang mengandalkan data, analitika merupakan kunci untuk mendapatkan dan mengekstraksi informasi dari data. Namun, apa itu analitika? Pengambilan keputusan bisnis yang didasarkan pada data telah dijelaskan sebagai analitika bisnis, inteligensi bisnis, dan ilmu data. Istilah-istilah ini, antara lain, kadang-kadang digunakan secara bergantian dan kadang-kadang mengacu pada aplikasi yang berbeda. Inteligensi bisnis mungkin fokus pada proses pengumpulan data, sering kali melalui basis data dan gudang data, sedangkan analitika bisnis menggunakan alat dan metode untuk analisis statistik data. Berbeda dengan dua istilah tersebut yang menekankan aplikasi bisnis, istilah ilmu data dapat mencakup aplikasi data yang lebih luas dalam berbagai domain ilmiah. Untuk tujuan kami, kami menggunakan istilah analitika untuk merujuk pada proses penggunaan data dalam pengambilan keputusan. Proses ini melibatkan pengumpulan data, pemahaman konsep dan model ketidakpastian, membuat inferensi umum, dan mengkomunikasikan hasil. Ketika memperkenalkan metode data dalam teks ini, kami fokus pada kerugian yang timbul dari, atau terkait dengan, kewajiban dalam kontrak asuransi. Hal ini bisa berupa jumlah kerusakan pada apartemen seseorang dalam perjanjian asuransi penyewa, jumlah yang dibutuhkan untuk mengganti rugi seseorang yang Anda cedera dalam kecelakaan berkendara, dan sejenisnya. Kami menyebut jenis kewajiban ini sebagai klaim asuransi. Dengan fokus ini, kami dapat memperkenalkan dan langsung menggunakan alat dan teknik statistik yang umumnya berlaku. 1.1.3 Proses Asuransi Cara lain untuk memahami sifat asuransi adalah melalui durasi kontrak asuransi, yang dikenal sebagai masa berlaku. Teks ini akan berfokus pada kontrak asuransi jangka pendek. Dalam konteks jangka pendek, kontrak asuransi umumnya memberikan perlindungan selama satu tahun atau enam bulan. Sebagian besar kontrak komersial dan personal berlaku selama satu tahun, sehingga itu adalah durasi default kami. Namun, terdapat pengecualian penting seperti kebijakan asuransi mobil di Amerika Serikat yang sering kali berlaku selama enam bulan. Sebaliknya, biasanya kita menganggap asuransi jiwa sebagai kontrak jangka panjang di mana durasi default adalah beberapa tahun. Sebagai contoh, jika seseorang berusia 25 tahun membeli polis asuransi jiwa yang memberikan pembayaran saat meninggalnya tertanggung dan orang tersebut tidak meninggal sampai usia 100 tahun, maka kontrak tersebut berlaku selama 75 tahun. Terdapat perbedaan penting lainnya antara produk asuransi jiwa dan non-jiwa. Dalam asuransi jiwa, jumlah manfaat sering ditetapkan dalam ketentuan kontrak. Sebaliknya, sebagian besar kontrak non-jiwa memberikan kompensasi atas kerugian tertanggung yang tidak diketahui sebelum terjadinya kecelakaan. (Biasanya terdapat batasan jumlah kompensasi yang ditetapkan.) Dalam kontrak asuransi jiwa yang berlaku selama bertahun-tahun, nilai waktu uang memainkan peran penting. Dalam kontrak non-jiwa, jumlah kompensasi yang acak menjadi prioritas. Baik dalam asuransi jiwa maupun asuransi non-jiwa, frekuensi klaim sangat penting. Untuk banyak kontrak asuransi jiwa, peristiwa yang diasuransikan (seperti kematian) hanya terjadi sekali. Sebaliknya, dalam asuransi non-jiwa seperti asuransi mobil, umum bagi individu (terutama pengemudi pria muda) untuk mengalami lebih dari satu kecelakaan dalam setahun. Oleh karena itu, model-model kita perlu mencerminkan pengamatan ini; kami memperkenalkan berbagai model frekuensi yang juga mungkin Anda temui saat mempelajari asuransi jiwa. Untuk asuransi jangka pendek, kerangka model probabilistiknya sederhana. Hanya mempertimbangkan model satu periode (panjang periode, misalnya satu tahun, akan ditentukan dalam situasi tersebut). Pada awal periode, tertanggung membayar premi yang diketahui kepada perusahaan asuransi sesuai kesepakatan antara kedua belah pihak dalam kontrak. Pada akhir periode, perusahaan asuransi mengganti rugi tertanggung atas kerugian (mungkin multivariat) yang acak. Kerangka kerja ini akan dikembangkan seiring berjalannya waktu, tetapi yang pertama fokus pada mengintegrasikan kerangka kerja ini dengan kekhawatiran tentang bagaimana data dapat muncul. Dari sudut pandang perusahaan asuransi, kontrak mungkin hanya berlaku selama satu tahun tetapi cenderung diperpanjang. Selain itu, pembayaran yang timbul dari klaim selama setahun dapat meluas jauh melebihi satu tahun. Salah satu cara untuk menggambarkan data yang muncul dari operasional perusahaan asuransi adalah dengan menggunakan pendekatan granular dengan garis waktu. Pendekatan proses memberikan gambaran keseluruhan tentang peristiwa yang terjadi selama masa berlaku kontrak asuransi, dan sifatnya - acak atau direncanakan, peristiwa kerugian (klaim) dan peristiwa perubahan kontrak, dan sebagainya. Dalam pandangan mikro ini, kita dapat memikirkan apa yang terjadi pada suatu kontrak pada berbagai tahap keberadaannya. Gambar 1.1 menggambarkan garis waktu dari suatu kontrak asuransi yang khas. Sepanjang masa berlakunya kontrak, perusahaan secara rutin memproses peristiwa seperti pengumpulan premi dan penilaian, yang dijelaskan dalam Bagian 1.2; peristiwa-peristiwa ini ditandai dengan tanda x pada garis waktu. Peristiwa-peristiwa yang tidak teratur dan tak terduga juga terjadi. Sebagai contoh, \\(t_2\\) dan \\(t_4\\) menandai peristiwa klaim asuransi (beberapa kontrak, seperti asuransi jiwa, mungkin hanya memiliki satu klaim). Waktu \\(t_3\\) dan \\(t_5\\) menandai peristiwa ketika pemegang polis ingin mengubah fitur-fitur tertentu dalam kontrak, seperti pilihan klaim dan jumlah perlindungan. Dari perspektif perusahaan, kita bahkan dapat mempertimbangkan inisiasi kontrak (kedatangan, waktu \\(t_1\\)) dan terminasi kontrak (kepergian, waktu \\(t_6\\)) sebagai peristiwa yang tidak pasti. (Sebagai alternatif, untuk beberapa tujuan, Anda dapat mengkondisikan peristiwa-peristiwa ini dan memperlakukannya sebagai pasti.) ( Gambar 1) 1.2 Operasi Perusahaan Asuransi pada bab ini yang akan dipelajari adalah: Menggambarkan lima area operasional utama perusahaan asuransi. Mengidentifikasi peran data dan peluang analitik dalam setiap area operasional. Berdasarkan data asuransi, tujuan akhirnya adalah menggunakan data untuk mengambil keputusan. Kita akan mempelajari lebih lanjut tentang metode analisis dan ekstrapolasi data di bab-bab berikutnya. Untuk memulainya, mari kita pikirkan mengapa kita ingin melakukan analisis. Kita mengambil sudut pandang perusahaan asuransi (bukan orang yang diasuransikan) dan memperkenalkan cara untuk mendapatkan uang, membayarnya, mengelola biaya, dan memastikan bahwa kita memiliki cukup uang untuk memenuhi kewajiban. Penekanannya adalah pada operasi khusus asuransi daripada pada kegiatan bisnis umum seperti periklanan, pemasaran, dan manajemen sumber daya manusia. Secara khusus, dalam banyak perusahaan asuransi, umumnya praktik untuk menggabungkan proses asuransi yang terperinci ke dalam unit operasional yang lebih besar; banyak perusahaan menggunakan area fungsional ini untuk memisahkan aktivitas karyawan dan tanggung jawab area tertentu. Aktuaris, analis keuangan lainnya, dan regulator asuransi bekerja dalam unit-unit ini dan menggunakan data untuk kegiatan-kegiatan berikut ini: 1. Memulai Asuransi. Pada tahap ini, perusahaan membuat keputusan untuk menerima atau menolak risiko (tahap underwriting) dan menentukan premi yang sesuai (atau tarif). Analitik asuransi memiliki akar aktuaria dalam pembuatan tarif, di mana para analis berusaha untuk menentukan harga yang tepat untuk risiko yang tepat. 2. Memperbaharui Asuransi. Banyak kontrak, terutama dalam asuransi umum, memiliki durasi yang relatif pendek seperti 6 bulan atau 1 tahun. Meskipun ada harapan implisit bahwa kontrak-kontrak tersebut akan diperbaharui, perusahaan asuransi memiliki kesempatan untuk menolak jaminan dan menyesuaikan premi. Analitik juga digunakan pada tahap perpanjangan polis ini di mana tujuannya adalah untuk mempertahankan pelanggan yang menguntungkan. 3. Manajemen Klaim. Analitik telah lama digunakan dalam (1) mendeteksi dan mencegah penipuan klaim, (2) mengelola biaya klaim, termasuk mengidentifikasi dukungan yang tepat untuk biaya penanganan klaim, serta (3) memahami lapisan kelebihan untuk reasuransi dan retensi. 4. Reserving Kerugian. Alat analitik digunakan untuk memberikan perkiraan yang tepat kepada manajemen mengenai kewajiban di masa depan dan untuk mengkuantifikasi ketidakpastian perkiraan tersebut. 5. Kesolvenan dan Alokasi Modal. Menentukan jumlah modal yang diperlukan dan cara mengalokasikan modal di antara investasi-alternatif juga merupakan kegiatan analitik penting. Perusahaan harus memahami berapa banyak modal yang dibutuhkan agar mereka memiliki aliran kas yang cukup tersedia untuk memenuhi kewajiban mereka pada saat yang diharapkan (kesolvenan). Ini adalah pertanyaan penting yang tidak hanya menyangkut manajer perusahaan tetapi juga pelanggan, pemegang saham perusahaan, otoritas pengawas, serta masyarakat secara umum. Terkait dengan isu berapa banyak modal adalah pertanyaan tentang bagaimana mengalokasikan modal ke proyek keuangan yang berbeda, biasanya untuk memaksimalkan pengembalian investor. Meskipun pertanyaan ini dapat muncul pada beberapa tingkat, perusahaan asuransi umumnya tertarik dengan cara mengalokasikan modal ke berbagai lini bisnis dalam perusahaan dan ke anak perusahaan dari perusahaan induk. Meskipun data merupakan komponen penting dari kesolvenan dan alokasi modal, komponen lain termasuk kerangka ekonomi lokal dan global, lingkungan investasi keuangan, dan persyaratan yang sangat spesifik sesuai dengan lingkungan regulasi saat ini, juga penting. Karena latar belakang yang diperlukan untuk mengatasi komponen-komponen ini, kami tidak membahas isu-isu kesolvenan, alokasi modal, dan regulasi dalam teks ini. Namun demikian, untuk semua fungsi operasional, kami menekankan bahwa analitik dalam industri asuransi bukanlah latihan yang dapat dilakukan oleh sekelompok kecil analis sendiri. Hal ini membutuhkan perusahaan asuransi untuk melakukan investasi yang signifikan dalam teknologi informasi, pemasaran, underwriting, dan aktuaria. Karena area-area ini merupakan tujuan akhir utama dari analisis data, penjelasan tambahan tentang masing-masing unit operasional diberikan dalam subseksi berikutnya. 1.2.1 Memulai Asuransi Menentukan harga produk asuransi bisa menjadi masalah yang membingungkan. Hal ini berbeda dengan industri lain seperti manufaktur di mana biaya suatu produk (relatif) diketahui dan menjadi acuan untuk menilai harga permintaan pasar. Demikian pula, dalam bidang layanan keuangan lainnya, harga pasar tersedia dan menjadi dasar untuk struktur penetapan harga yang sesuai dengan pasar produk. Namun, untuk banyak jenis asuransi, biaya suatu produk tidak pasti dan harga pasar tidak tersedia. Harapan atas biaya acak adalah tempat yang wajar untuk memulai penetapan harga. (Jika Anda telah mempelajari keuangan, maka Anda akan mengingat bahwa harapan adalah harga optimal untuk perusahaan asuransi yang netral terhadap risiko.) Dalam penetapan harga asuransi, sudah menjadi tradisi untuk memulai dengan biaya yang diharapkan. Perusahaan asuransi kemudian menambahkan margin untuk memperhitungkan tingkat risiko produk, biaya yang dikeluarkan dalam pelayanan produk, dan alokasi keuntungan/surplus perusahaan. Penggunaan biaya yang diharapkan sebagai dasar penetapan harga umum terjadi dalam beberapa jenis bisnis asuransi. Ini termasuk asuransi mobil dan asuransi pemilik rumah. Dalam jenis asuransi ini, analitik telah membantu meningkatkan ketepatan perhitungan biaya yang diharapkan dari produk. Ketersediaan internet yang semakin luas bagi konsumen juga mendorong transparansi dalam penetapan harga; di pasar saat ini, konsumen memiliki akses mudah ke kutipan kompetitif dari sejumlah perusahaan asuransi. Perusahaan asuransi berusaha meningkatkan pangsa pasarnya dengan menyempurnakan sistem klasifikasi risiko mereka, sehingga mencapai perkiraan yang lebih baik mengenai harga produk dan memungkinkan strategi underwriting yang selektif (“cream-skimming” adalah istilah yang digunakan ketika perusahaan asuransi hanya mengasuransikan risiko terbaik). Survei (misalnya, Earnix (2013)) menunjukkan bahwa penetapan harga adalah penggunaan analitik yang paling umum di kalangan perusahaan asuransi. Underwriting, yaitu proses mengklasifikasikan risiko ke dalam kategori homogen dan mengalokasikan pemegang polis ke dalam kategori-kategori tersebut, merupakan inti dari penetapan tarif. Pemegang polis dalam satu kelas (kategori) memiliki profil risiko yang serupa sehingga dikenakan tarif asuransi yang sama. Ini adalah konsep premi yang adil secara aktuaria; adil untuk mengenakan tarif yang berbeda kepada pemegang polis hanya jika mereka dapat dibedakan berdasarkan faktor risiko yang dapat diidentifikasi. Sebuah artikel awal, Two Studies in Automobile Insurance Ratemaking (Bailey dan LeRoy 1960), memberikan dorongan bagi penerimaan metode analitik dalam industri asuransi. Makalah ini membahas masalah klasifikasi penetapan tarif asuransi mobil. Artikel tersebut menggambarkan contoh asuransi mobil yang memiliki lima kelas penggunaan yang saling berkaitan dengan empat kelas rating prestasi. Pada saat itu, kontribusi premi untuk kelas penggunaan dan rating prestasi ditentukan secara independen satu sama lain. Memikirkan efek interaksi dari variabel klasifikasi yang berbeda merupakan masalah yang lebih rumit. Saat risiko pertama kali diperoleh, kewajiban perusahaan asuransi dapat dikelola dengan menerapkan parameter kontrak yang mengubah pembayaran kontrak. Bab 3 menjelaskan modifikasi umum termasuk co-insurance, deduktibel, dan batas atas kebijakan. 1.2.2 Memperbarui Asuransi Asuransi merupakan jenis layanan keuangan dan, seperti banyak kontrak layanan lainnya, cakupan asuransi sering kali disepakati untuk jangka waktu terbatas di mana komitmen cakupan diselesaikan. Terutama untuk asuransi umum, kebutuhan akan cakupan terus berlanjut, dan oleh karena itu upaya dilakukan untuk mengeluarkan kontrak baru yang memberikan cakupan serupa ketika kontrak yang ada mencapai akhir masa berlakunya. Ini disebut perpanjangan polis. Isu perpanjangan juga dapat muncul dalam asuransi jiwa, misalnya, asuransi jiwa berjangka (sementara). Pada saat yang sama, kontrak lain seperti pensiun dini berakhir pada saat kematian tertanggung, sehingga masalah perpanjangan tidak relevan. Dalam ketiadaan pembatasan hukum, saat perpanjangan polis, perusahaan asuransi memiliki kesempatan untuk: menerima atau menolak menerbitkan risiko tersebut; dan menentukan premi baru, mungkin dengan melakukan klasifikasi ulang terhadap risiko tersebut. Klasifikasi risiko dan penilaian saat perpanjangan didasarkan pada dua jenis informasi. Pertama, pada tahap awal, perusahaan asuransi memiliki banyak variabel penilaian yang dapat digunakan untuk pengambilan keputusan. Banyak variabel yang kemungkinan tidak akan berubah, misalnya jenis kelamin, sedangkan yang lain kemungkinan akan berubah, misalnya usia, dan yang lainnya mungkin berubah atau tidak, misalnya skor kredit. Kedua, berbeda dengan tahap awal, saat perpanjangan, perusahaan asuransi memiliki riwayat pengalaman kerugian dari pemegang polis, dan riwayat ini dapat memberikan wawasan tentang pemegang polis yang tidak tersedia dari variabel penilaian. Modifikasi premi berdasarkan riwayat klaim dikenal sebagai penilaian berdasarkan pengalaman, juga kadang-kadang disebut sebagai penilaian berdasarkan prestasi. Metode penilaian berdasarkan pengalaman dapat diterapkan secara retrospektif atau prospektif. Dengan metode retrospektif, sebagian premi dikembalikan kepada pemegang polis dalam kejadian pengalaman yang menguntungkan bagi perusahaan asuransi. Premi retrospektif umum dalam perjanjian asuransi jiwa (di mana pemegang polis memperoleh dividen di Amerika Serikat, bonus di Inggris, dan pembagian keuntungan dalam cakupan asuransi jiwa sementara di Israel). Pada umumnya, metode prospektif lebih umum digunakan dalam asuransi umum, di mana pengalaman yang menguntungkan bagi pemegang polis akan dihargai melalui premi perpanjangan yang lebih rendah. Riwayat klaim dapat memberikan informasi tentang minat risiko pemegang polis. Sebagai contoh, dalam asuransi personal, umumnya digunakan variabel untuk menunjukkan apakah klaim telah terjadi dalam tiga tahun terakhir atau tidak. Sebagai contoh lain, dalam asuransi komersial seperti asuransi kecelakaan kerja, dapat dilihat frekuensi atau tingkat keparahan klaim rata-rata pemegang polis selama tiga tahun terakhir. Riwayat klaim dapat mengungkapkan informasi yang sebaliknya tersembunyi (bagi perusahaan asuransi) tentang pemegang polis. 1.2.3 Pengelolaan Klaim dan Produk Dalam beberapa jenis asuransi, proses pembayaran klaim untuk peristiwa yang diasuransikan relatif sederhana. Misalnya, dalam asuransi jiwa, sertifikat kematian sederhana sudah cukup untuk membayarkan jumlah manfaat sesuai dengan kontrak. Namun, dalam bidang asuransi properti dan kecelakaan, prosesnya bisa jauh lebih kompleks. Bayangkan peristiwa yang diasuransikan yang relatif sederhana seperti kecelakaan mobil. Di sini, seringkali diperlukan untuk menentukan pihak yang bertanggung jawab dan kemudian perlu mengevaluasi kerusakan pada semua kendaraan dan orang yang terlibat dalam insiden, baik yang diasuransikan maupun yang tidak diasuransikan. Selain itu, biaya yang dikeluarkan dalam mengevaluasi kerusakan juga harus dinilai, dan sebagainya. Proses menentukan cakupan, tanggung jawab hukum, dan penyelesaian klaim dikenal sebagai penyesuaian klaim. Manajer asuransi terkadang menggunakan istilah “kebocoran klaim” untuk merujuk pada dolar yang hilang melalui ketidakefisienan pengelolaan klaim. Ada banyak cara di mana analitik dapat membantu mengelola proses klaim, lihat Gorman dan Swenson (2013). Secara historis, yang paling penting adalah pendeteksian penipuan. Proses penyesuaian klaim melibatkan mengurangi asimetri informasi (klaiman mengetahui apa yang terjadi; perusahaan mengetahui sebagian dari apa yang terjadi). Mengurangi penipuan adalah bagian penting dari proses pengelolaan klaim. Deteksi penipuan hanyalah satu aspek dari pengelolaan klaim. Lebih luas lagi, kita dapat mempertimbangkan pengelolaan klaim terdiri dari komponen-komponen berikut: - Pemilahan klaim. Seperti dalam dunia medis, identifikasi dini dan penanganan yang tepat terhadap klaim dengan biaya tinggi (pasien, dalam dunia medis) dapat menghasilkan penghematan yang dramatis. Misalnya, dalam asuransi kecelakaan kerja, perusahaan asuransi berupaya untuk mengidentifikasi secara dini klaim-klaim yang berisiko mengakibatkan biaya medis tinggi dan periode pembayaran yang lama. Intervensi dini dalam kasus-kasus ini dapat memberikan perusahaan asuransi lebih banyak kendali atas penanganan klaim, perawatan medis, dan biaya secara keseluruhan dengan kembalinya pekerja lebih awal. - Pengolahan klaim. Tujuannya adalah menggunakan analitik untuk mengidentifikasi situasi rutin yang diperkirakan memiliki pembayaran kecil. Situasi yang lebih kompleks mungkin memerlukan penyesuaian oleh penyelesaian klaim yang berpengalaman dan bantuan hukum untuk menangani klaim dengan potensi pembayaran besar. - Keputusan penyesuaian. Setelah klaim kompleks diidentifikasi dan ditugaskan kepada penyelesaian klaim, rutinitas yang didorong oleh analitik dapat dibentuk untuk membantu proses pengambilan keputusan berikutnya. Proses tersebut juga dapat membantu penyelesaian klaim dalam mengembangkan cadangan kasus, perkiraan kewajiban finansial perusahaan asuransi di masa depan. Ini adalah masukan penting untuk cadangan kerugian perusahaan asuransi, yang dijelaskan dalam Bagian 1.2.4. Selain penggantian kerugian kepada tertanggung, perusahaan asuransi juga perlu memperhatikan sumber lain yang mengalirkan pendapatan, yaitu biaya. Biaya penyesuaian kerugian merupakan bagian dari biaya pengelolaan klaim oleh perusahaan asuransi. Analitik dapat digunakan untuk mengurangi biaya yang terkait langsung dengan penanganan klaim (dialokasikan) serta waktu staf umum untuk mengawasi proses klaim (tidak dialokasikan). Industri asuransi memiliki biaya operasional yang tinggi dibandingkan dengan sektor jasa keuangan lainnya. Selain pembayaran klaim, terdapat banyak cara lain di mana perusahaan asuransi menggunakan data untuk mengelola produk mereka. Kami telah membahas kebutuhan analitik dalam penjaminan, yaitu klasifikasi risiko pada tahap akuisisi awal dan tahap perpanjangan. Perusahaan asuransi juga tertarik untuk mengetahui pemegang polis mana yang memilih untuk memperpanjang kontrak mereka dan, seperti halnya dengan produk lainnya, memantau loyalitas pelanggan. Analitik juga dapat digunakan untuk mengelola portofolio atau kumpulan risiko yang telah diperoleh oleh perusahaan asuransi. Seperti yang dijelaskan dalam Bab 10, setelah kontrak disepakati dengan tertanggung, perusahaan asuransi masih dapat mengubah kewajibannya bersih dengan melakukan perjanjian reasuransi. Jenis perjanjian ini melibatkan perusahaan reasuransi, yaitu perusahaan asuransi bagi perusahaan asuransi lainnya. Umum bagi perusahaan asuransi untuk membeli asuransi atas portofolio risiko mereka guna mendapatkan perlindungan dari peristiwa yang tidak biasa, sama seperti individu dan perusahaan lainnya. 1.2.4 Penyisihan Klaim Fitur penting yang membedakan asuransi dari sektor lain dalam ekonomi adalah waktu pertukaran pertimbangan. Dalam industri manufaktur, pembayaran untuk barang umumnya dilakukan pada saat transaksi. Sebaliknya, dalam asuransi, uang yang diterima dari pelanggan terjadi sebelum manfaat atau layanan diberikan; ini diberikan pada tanggal yang lebih kemudian jika terjadi peristiwa tertanggung. Hal ini mengakibatkan kebutuhan untuk memiliki cadangan kekayaan guna memenuhi kewajiban di masa depan sehubungan dengan kewajiban yang dibuat, dan untuk memperoleh kepercayaan dari tertanggung bahwa perusahaan akan mampu memenuhi komitmennya. Besarannya cadangan kekayaan ini, dan pentingnya memastikan kecukupannya, merupakan perhatian utama bagi industri asuransi. Mengalokasikan uang untuk klaim yang belum dibayar dikenal sebagai penyisihan kerugian; di beberapa yurisdiksi, cadangan ini juga dikenal sebagai ketentuan teknis. Seperti yang terlihat pada Gambar 1.1 beberapa kali di mana perusahaan merangkum posisi keuangannya; waktu-waktu ini dikenal sebagai tanggal penilaian. Klaim yang muncul sebelum tanggal penilaian telah dibayarkan, sedang dalam proses pembayaran, atau akan segera dibayarkan; klaim di masa depan setelah tanggal penilaian ini tidak diketahui. Perusahaan harus memperkirakan kewajiban yang belum diselesaikan ini ketika menentukan kekuatan keuangannya. Menentukan dengan tepat penyisihan kerugian penting bagi perusahaan asuransi atas banyak alasan. Penyisihan kerugian merupakan klaim yang diantisipasi yang perusahaan asuransi harus bayarkan kepada pelanggannya. Ketidakcukupan penyisihan dapat mengakibatkan ketidakmampuan untuk memenuhi kewajiban klaim. Sebaliknya, perusahaan asuransi dengan penyisihan berlebihan dapat menunjukkan perkiraan kelebihan cadangan yang konservatif dan dengan demikian menggambarkan posisi keuangan yang lebih lemah daripada yang sebenarnya. Penyisihan memberikan perkiraan untuk biaya asuransi yang belum dibayar yang dapat digunakan untuk penetapan harga kontrak. Penyisihan kerugian ini diperlukan oleh hukum dan peraturan. Masyarakat memiliki kepentingan yang kuat terhadap kekuatan keuangan dan solvabilitas perusahaan asuransi. Selain regulator, pemangku kepentingan lain seperti manajemen perusahaan asuransi, investor, dan pelanggan membuat keputusan yang bergantung pada penyisihan kerugian perusahaan. Sementara regulator dan pelanggan menghargai perkiraan yang konservatif terkait klaim yang belum dibayar, manajer dan investor mencari perkiraan yang lebih objektif untuk mewakili kesehatan keuangan yang sebenarnya dari perusahaan. Penyisihan kerugian adalah topik di mana terdapat perbedaan yang signifikan antara asuransi jiwa dan asuransi umum (juga dikenal sebagai asuransi harta benda dan kecelakaan, atau asuransi non-jiwa). Dalam asuransi jiwa, tingkat keparahan (besaran kerugian) seringkali bukan sumber ketidakpastian karena pembayaran telah ditentukan dalam kontrak. Tingkat kejadian, yang dipengaruhi oleh kematian tertanggung, menjadi perhatian. Namun, karena waktu yang lama untuk penyelesaian kontrak asuransi jiwa, ketidakpastian nilai waktu uang yang diukur dari penerbitan hingga tanggal pembayaran dapat mendominasi perhatian tingkat kejadian. Sebagai contoh, bagi seorang tertanggung yang membeli kontrak asuransi jiwa pada usia 20 tahun, tidak jarang kontrak masih berlaku setelah 60 tahun, ketika tertanggung merayakan ulang tahun ke-80. Lihat, misalnya, Bowers et al. (1986) atau Dickson, Hardy, dan Waters (2013) untuk pengenalan terhadap penyisihan kerugian dalam asuransi jiwa. Sebaliknya, untuk sebagian besar jenis bisnis asuransi non-jiwa, tingkat keparahan adalah sumber ketidakpastian utama dan durasi kontrak cenderung lebih singkat. "],["frequency-modeling.html", "Bab 2 Frequency Modeling 2.1 Frequency Distributions 2.2 Basic Frequency Distributions 2.3 The \\((a,b,0)\\) Class 2.4 Mengestimasi Distibusi Frekuensi 2.5 Distribusi Campuran 2.6 Goodnes of Fit", " Bab 2 Frequency Modeling 2.1 Frequency Distributions Pada Dasarnya frekuensi distribusi yang kita ketahui adalah menjelaskan tentang jumlah pengamatan untuk setiap nilai dari sebuah variabel yang digambarkan menggunakan grafik dan tabel frekuensi atau dengan kata lain, tampilan visual yang menyajikan jumlah frekuensi di setiap rentang atau persentase penhgamatan sehingga informasi dapat diartikan lebih mudah. 2.1.1 Bagaimana Frekuensi menambah infromasi pada tingkat keparahan suatu kejadian 2.1.1.1 Basic Terminology Loss menunjukkan jumlah kerugian finansial yang diderita oleh tertanggung dimana klaim digunakan untik menunjukkan ganti rugi atas terjadinya peristiwwa yang diasuransikan sehingga jumlah yahg dibayarkan oleh perusahaan asuransi. Frekuensi mewakili seberapa sering peristiwa yang diasuransikan terjadi, biasanya dalam kontrak polis(menghitung variabel acak yang mewakili jumlah klaim, yaitu seberapa sering suatu peristiwa terjadi). Serevity menunjukkan jumlah, atau ukuran, dari setiap pembayaran untuk kejadian yang diasuransikan. 2.1.1.2 Pentingnya Frekuensi Frekuensi disini dijelaskan bahwa setiap biaya yang diharapkan untuk asuransi dapat ditentukan sebagai jumlah klaim yuang diharapkan dikalikan jumlah per klaim, artinya adalah frekuensi x tingkat keparahan. dalam asuransi, penetapan harga dimulai dengan biaya yang diharapkan kemudian memperhitungkan keberesikoan produk, biaya yang dikeluarkan untuk melayani produk dan tunjangan surplus untuk perusahaan asuransi. Jadi frekuensi difokuskan pada perhitungan klaim yang memungkinkan penanggung untuk mempertimbangkan faktor-faktor yang secara langsung mempengaruhi terjadinya kerugian, sehingga berpotensi menimbulkan klaim. 2.1.1.3 Mengapa perlu memeriksa informasi frekuensi? Kontraktual Dalam kontrak asuransi, deductible tertentu dan batasan polis biasanya dicantumkan dan digunakan untuk setiap kejadian yang diasuransikan.Data jumlah klaim yang dihasilkan akan menunjukkan jumlah klaim yang memenuhi kriteria tersebut, menunjukkan ukuran frekuensi klaim. Jadi model total kerugian yang diasuransikan perlu memperhitungkan deductible dan batasan polis untuk setiap kejadian yang diasuransikan. Perilaku Dalam mempertimbangkan faktor-faktor yang memengaruhi frekuensi kerugian, perilaku pengambilan risiko dan pengurangan risiko individu dan perusahaan harus dipertimbangkan. Misalnya dalam perawatan kesehatan, keputusan untuk menggunakan perawatan kesehatan oleh individu, dan meminimalkan penggunaan perawatan kesehatan tersebut melalui perawatan preventif dan tindakan kesehatan, terutama terkait dengan karakteristik pribadinya. Jadi perhatian dapat difokuskan dari frekuensi kunjungan perawatan kesehatan dan keparahan biaya perawatan kesehatan. Database Penanggung dapat menyimpan file data terpisah yang menyarankan pengembangan model frekuensi dan keparahan terpisah. Misalnya, file pemegang polis dibuat saat kebijakan ditulis (berisi informasi penjamin tentang tertanggung). Proses pencatatan ini kemudian dapat diperluas ke perusahaan asuransi yang memodelkan frekuensi dan keparahan sebagai proses terpisah. Regulasi dan Administratif regulator secara rutin mewajibkan pelaporan nomor dan jumlah klaim. Pemantauan seperti melihat potensi kesalahan saat melaporkan nomor klaim berkurang membantu memastikan stabilitas keuangan perusahaan asuransi ini. 2.2 Basic Frequency Distributions Variabel acak jumlah klaim dilambangkan dengan \\(N\\), digunakan untuk mengasumsikan nilai bilangan bulat non-negatif \\({0,1,...}\\) 2.2.1 Formula \\(N\\) adalah variabel acak diskrit yang memiliki nilai {0,1,…}. Deskripsi yang paling dasar dari distribusinya adalah spesifikasi probabilitas yang diasumsikan dengan masing-masing nilai bilangan bulat non-negatif, ini adalah konsep probability mass function (pmf) yaitu fungsi yang memberikan probabilitas bahwa variabel acak diskrit sama persis dengan suatu nilai yang dialmbangkan sebagai \\(P_N(.)\\) \\[ P_N(k)=Pr(N=k), k=0,1,... \\] Dari formula diatas adalah deskripsi lengkap alternatif dari distribusi \\(N\\), misal fungsi distribusi (peluang bahwa variabel acak kurang dari atau sama dengan x) dari \\(N\\) didefinisikan oleh \\(F_N(x) = \\Pr(N \\le x)\\) yang dideterminasikan sebagai: \\[F_N(x)=\\begin{cases} \\sum\\limits_{k=0}^{\\lfloor x \\rfloor } \\Pr(N=k), &amp;x\\geq 0;\\\\ 0, &amp; \\hbox{otherwise}. \\end{cases}\\] [.] menunjukkan fungsi dasar, [x] menunjukkan bilangan bulat terbesar kurang dari satu sama dengan \\(X\\). ini menunjkkan fungsi distribusi kumulatif deskriptor yaitu alternatif yang digunakan untuk menyatakann fungsi distribusi. Survival function dari \\(N\\) dilambangkan sebagai \\(S_N(.)\\) yaitu pelengkap satuan dari \\(F_N(.)\\) yaitu \\(S_N(\\cdot)=1-F_N(\\cdot)\\). ada banyak ukuran yang berbeda yang biasanya digunakan untuk mengukurnya, dari jumlah tersebut average maksud dari \\(N\\) dilambangkan dengan \\(\\mu_N\\) didefinisikan: \\[\\mu_N=\\sum_{k=0}^\\infty k~p_N(k).\\] \\(\\mu_N\\) adalah nilai yang diharapkan dari variabel acak \\(N\\) yaitu \\(\\mu N=E[N]\\) yang mengarah pada momen distribusi (nilai rata-rata dari variabel acak yang dipangkatkan ke-r). \\(r\\)-th adalah \\(N\\) dimana \\(r&gt;0\\) didefinisikan sebagai \\(E[N^r]\\) dan dilambangkan dengan \\(\\mu&#39;_N(r)\\). ” ’ ” tidak menunjukkan diferensiasi. 2.2.1.1 Fungsi Pembangkit Momen dan Probabilitas Teorema 1 \\(N\\) menjadi alat hitung variabel acak sehingga \\(E[e^{t^*N}]\\) terbatas untuk beberapa \\(t^*&gt;0\\): semua momen \\(N\\) terbatas: \\[ \\mathrm{E}{[N^r]}&lt;\\infty, \\quad r &gt; 0. \\] mgf dapat digunakan untuk: \\[ \\left.\\frac{{\\rm d}^m}{{\\rm d}t^m} M_N(t)\\right\\vert_{t=0}=\\mathrm{E}[N^m], \\quad m\\geq 1. \\] c. mgf \\(M_N(.)\\) mencirikan distribusi mgf sangat berguna untuk dua variabel acak independen x dan y karena keduanya ada disekitar 0. Teorema 2 \\(N\\) adalah alat hitung variabel acak sehingga \\(E(s*)^N\\) sehingga \\(s*&gt;1\\) memiliki: semua momen \\(N\\), yaitu: \\[ \\mathrm{E}~{N^r}&lt;\\infty, \\quad r\\geq 0. \\] \\(pmf\\) dari \\(N\\) bisa diturunkan dari \\(pgf\\) sebagai betikut: \\[ p_N(m)=\\begin{cases} P_N(0), &amp;m=0;\\cr &amp;\\cr \\left(\\frac{1}{m!}\\right) \\left.\\frac{{\\rm d}^m}{{\\rm d}s^m} P_N(s)\\right\\vert_{s=0}\\;, &amp;m\\geq 1.\\cr \\end{cases} \\] momen faktorial dari \\(N\\) dapat diturunkan sebagai berikut: \\[ \\left.\\frac{{\\rm d}^m}{{\\rm d}s^m} P_N(s)\\right\\vert_{s=1}=\\mathrm{E}~{\\prod\\limits_{i=0}^{m-1} (N-i)}, \\quad m\\geq 1. \\] d. \\(pgf P_N(.)\\) mencirikan distribusi. 2.2.1.2 Binomial Distribusi Misal, eksperimen pelemparan koin (bias atau tidak bias) dengan hasil berupa kepala atau ekor. Jadi jika \\(N\\) menunjukkan jumlah kepala dalam urutan \\(M\\) eksperimen pelemparan koin independen dengan koin identik yang menghasilkan probabilitas \\(Q\\) maka distribusi dari \\(N\\) disebut distribusi binomial dengan parameter \\((m,q)\\) dengan \\(M\\) bilangan bulat positif dan \\(Q\\in [0,1]\\). ketika \\(Q=0\\) maka distribusinya merosot dengan \\(N=0\\) dengan probabilitas = 1. dengan pmf diberikan: \\[ p_k= {m \\choose k} q^k (1-q)^{m-k}, \\quad k=0,\\ldots,m. \\] dimana \\[ {m \\choose k} = \\frac{m!}{k!(m-k)!} \\] Alasan pmf adalah karena mengambil nilai di antara istilah -istilah yang muncul dari perluasan binomial \\((q+(1-q))^m\\). Realisasi ini kemudian mengarah pada ekspresi berikut untuk PGF dari distribusi binomial: \\[ \\begin{array}{ll} P_N(z) &amp;= \\sum_{k=0}^m z^k {m \\choose k} q^k (1-q)^{m-k} \\\\ &amp;= \\sum_{k=0}^m {m \\choose k} (zq)^k (1-q)^{m-k} \\\\ &amp;= (qz+(1-q))^m = (1+q(z-1))^m. \\end{array} \\] Perhatikan bahwa ekspresi di atas untuk PGF mengkonfirmasi fakta bahwa distribusi binomial adalah konvolusi M dari distribusi Bernoulli, yang merupakan distribusi binomial dengan \\(m=1\\) dan pgf \\((1+q(z-1))\\) ekspetasi dari binomial distribusi: \\[ \\mathrm{E}[{N}]=\\mathrm{E}\\left[{\\sum_{i=1}^m N_i}\\right] = \\sum_{i=1}^m ~\\mathrm{E}[N_i] = mq. \\] Varians dari jumlah variabel acak independen adalah jumlah variannya: \\[ \\mathrm{Var}[{N}]=\\mathrm{Var}~\\left[{\\sum_{i=1}^m N_i}\\right]=\\sum_{i=1}^m \\mathrm{Var}[{N_i}] = mq(1-q). \\] 2.2.1.3 Poisson Distribution Distribusi Poisson diparametrikan dengan parameter tunggal yang biasanya dilamnbangkan dengan \\(\\lambda\\) yang memasukkan nilai \\((0, \\infty)\\) dimana pmf: \\[ p_k = \\frac{e^{-\\lambda}\\lambda^k}{k!}, k=0,1,\\ldots \\] formula diatas untuk setiap sukunya jelas tidak negatif dan jumlahnya menjadi satu emngikuti perluasan deret taylor tak terbatas dari \\(e^\\lambda\\). pgf dapat diturunkan dengan \\(P_N(.)\\) sebagai berikut: \\[ P_N(z)= \\sum_{k=0}^\\infty p_k z^k = \\sum_{k=0}^\\infty \\frac{e^{-\\lambda}\\lambda^kz^k}{k!} = e^{-\\lambda} e^{\\lambda z} = e^{\\lambda(z-1)}, \\forall z\\in\\mathbb{R}. \\] dari formula diatas didapatkan mgf nya: \\(M_N(t)=P_N(e^t)=e^{\\lambda(e^t-1)}, t\\in \\mathbb{R}\\) jadi penurunan rata-rata untuk distribusi poisson nya adalah: \\[ kp_k=\\begin{cases} 0, &amp;k=0 \\cr \\lambda~p_{k-1}, &amp;k\\geq1 . \\end{cases} \\] bentuk ekspetasi dari poisson: \\[ \\mathrm{E}[{N}]=\\sum_{k\\geq 0} k~p_k =\\lambda\\sum_{k\\geq 1} p_{k-1} = \\lambda\\sum_{j\\geq 0} p_{j} =\\lambda. \\] dengan menggunakan teorema 1 kita dapat melihat bahwa: \\[ \\mathrm{E}{\\prod\\limits_{i=0}^{m-1} (N-i)}=\\left.\\frac{{\\rm d}^m}{{\\rm d}s^m} P_N(s)\\right\\vert_{s=1}=\\lambda^m, \\quad m\\geq 1. \\] jadi, varians nya sebagai berikut: \\[ \\mathrm{Var}[{N}]=\\mathrm{E}[{N^2}]-[\\mathrm{E}({N})]^2 = \\mathrm{E}~[N(N-1)]+\\mathrm{E}[N]-(\\mathrm{E}[{N]})^2=\\lambda^2+\\lambda-\\lambda^2=\\lambda. \\] 2.2.1.4 distribusi binomial negatif distribusi binomial muncul sebagai jumlah keberhasilan dalam \\(M\\) pengulangan independen dari percobaan dengan hasil biner, Jika ingin mempertimbangkan jumlah keberhasilan sampai kita mengamati \\(R\\) adalah kegagalan dalam pengulangan independen dari percobaan dengan hasil biner, maka distribusinya adalah distribusi binomial negatif. Bentuk binomial adalah, bentuk koefisien binomial umum yaitu: \\[ (1+x)^s= 1 + s x + \\frac{s(s-1)}{2!}x^2 + \\ldots..., \\quad s\\in\\mathbb{R}; \\vert x \\vert&lt;1. \\] jadi, \\[ (1+x)^s= \\sum_{k=0}^{\\infty} {s \\choose k} x^k, \\quad s\\in\\mathbb{R}; \\vert x \\vert&lt;1. \\] jika \\(s=-r\\) maka: \\[ (1-x)^{-r}= 1 + r x + \\frac{(r+1)r}{2!}x^2 + \\ldots...= \\sum_{k=0}^\\infty {r+k-1 \\choose k} x^k, \\quad r\\in\\mathbb{R}; \\vert x \\vert&lt;1. \\] jika kita ingin mendefinisikan \\(P_k\\) sebagai \\[ p_k = {r+k-1 \\choose k} \\left(\\frac{1}{1+\\beta}\\right)^r \\left(\\frac{\\beta}{1+\\beta}\\right)^k, \\quad k=0,1,\\ldots \\] untuk \\(r&gt;0\\) dan \\(\\beta &gt;-0\\) lalu mendefinisikan pmf yang valid. distribusi yang ditentukan seperti diatas disebut distribusi binomial negatif dengan parameter \\((r,\\beta)\\) dengan \\(r&gt;0\\) dan \\(\\beta &gt;-0\\) 2.3 The \\((a,b,0)\\) Class Bagian ini, kita akan belajar bagaimana; Menetukan \\((a,b,0)\\) tingkat distribusi frequency. Diskusi pentingnya yaitu hubungan rekursif yang mendasari tingkat distribusi ini. Mengidentifikasi kondisi umum tingkat distribusi ini direduksi menjadi masing-masing distribusi binomial, Poisson, dan binomial negatif. Pada bagian sebelumnya kita mempelajari tiga distribusi, yaitu distribusi binomial, Poisson, dan binomial negatif. Dalam kasus Poisson, untuk mendapatkan rata-ratanya, kami menggunakan fakta bahwa \\[kp_k = \\lambda p_{k-1},k\\geq 1,\\] yang dapat dinyatakan setara sebagai \\[\\frac{p_k} {p_{k-1}} = \\frac{\\lambda} {k}, k \\geq 1 ,\\] Menariknya, kita juga dapat menunjukkan bahwa untuk distribusi binomial \\[\\frac{p_k} {p_{k-1}} = \\frac{-q}{1-q} + (\\frac {(m+1)q}{1-q}) \\frac{1}{k},k=1,..,m,\\] dan itu untuk distribusi binomial negatif \\[\\frac{p_k} {p_{k-1}} = \\frac{\\beta}{1+\\beta} + (\\frac {(r+1)\\beta}{1+\\beta}) \\frac{1}{k}, k\\geq 1.\\] Hubungan di atas semuanya berbentuk \\[\\frac{p_k} {p_{k-1}} = a + \\frac{b}{k}, k\\geq1;\\] Ini menimbulkan pertanyaan apakah ada distribusi lain yang memenuhi hubungan pengulangan yang tampaknya umum ini. Perhatikan bahwa rasio di sebelah kiri, rasio dua probabilitas, adalah non-negatif. ketiga distribusi ini secara kolektif disebut dalam literatur aktuaria sebagai \\((a,b,0)\\) kelas distribusi, dengan \\(0\\) mengacu pada titik awal pengulangan. Perhatikan bahwa nilai \\(p_0\\) tersirat oleh \\((a,b)\\) karena probabilitas harus dijumlahkan menjadi satu. kita akan melihat bahwa ia melakukannya bahkan dalam kasus distribusi majemuk dengan distribusi frekuensi milik \\((a,b,0)\\) kelas - fakta ini adalah alasan motivasi yang lebih penting untuk mempelajari ketiga distribusi ini dari sudut pandang ini. Contoh Distribusi probabilitas diskrit memiliki properti berikut \\[p_k=c(1+\\frac {2}{k})p_{k-1} \\space k=1,2,3...\\] \\[p_1= \\frac {9}{256}\\] Tentukan nilai yang diharapkan dari variabel acak diskrit ini. Solution Karena pmf memenuhi \\((a,b,0)\\) hubungan pengulangan kita tahu bahwa distribusi yang mendasarinya adalah satu di antara distribusi binomial, Poisson, dan binomial negatif. Karena rasio parameter \\((mis. \\space b/a)\\) sama dengan \\(2\\), kita tahu bahwa itu adalah binomial negatif dan \\(r=3\\). Selain itu, karena untuk binomial negatif \\(p_1 =r(1+\\beta)^{-(r+1)}\\beta\\), maka \\[\\begin{align*} \\frac {9}{256} &amp; = 3 \\frac{\\beta}{(1+\\beta)^4}\\\\ \\frac {3}{(1+3)^4}&amp; = \\frac{\\beta}{(1+\\beta)^4}\\\\ \\beta&amp;=3. \\end{align*}\\] Akhirnya, karena rata-rata binomial negatif adalah \\(rβ\\) kita memiliki rata-rata distribusi yang diberikan sama dengan 9. 2.4 Mengestimasi Distibusi Frekuensi Pada bagian ini, Anda akan mempelajari bagaimana caranya: Menentukan kemungkinan atau probability suatu sampel pengamatan dari distribusi diskrit Menentukan penaksir kemungkinan maksimum untuk sampel acak pengamatan dari distribusi diskrit Menghitung penaksir kemungkinan maksimum untuk distribusi binomial, distribusi Poisson, dan distribusi binomial negative 2.4.1 Estimasi Parameter Pada Bagian 2.2 kita telah mempelajari tiga distribusi yang penting dalam memodelkan berbagai jenis data perhitungan yang timbul dari asuransi. Sekarang mari kita anggap bahwa kita memiliki sekumpulan data perhitungan yang ingin kita sesuaikan dengan sebuah distribusi, dan kita telah menentukan bahwa salah satu dari distribusi ini \\((a,b,0)\\) distribusi ini lebih tepat daripada yang lain. Karena masing-masing membentuk kelas distribusi jika kita mengizinkan parameter atau nilainya untuk mengambil nilai apa pun yang diizinkan, masih ada tugas untuk menentukan nilai terbaik dari parameter untuk data yang ada. Ini adalah masalah estimasi titik statistik, dan dalam masalah inferensi parametrik, paradigma inferensi statistik kemungkinan maksimum biasanya menghasilkan perkiraan yang efisien. Pada bagian ini kita akan menjelaskan paradigma ini dan menurunkan perkiraan kemungkinan maksimum. Misalkan kita mengamati bahwa kita mengamati independen dan terdistribusi secara identik, iid, variabel acak \\(X_1,X_2,…,X_n\\) dari distribusi dengan Prabability Mass Function/pmf \\(pθ\\), dimana \\(θ\\) adalah vektor parameter dan nilai yang tidak diketahui dalam ruang parameter \\(Θ⊆\\mathbb{R}^d\\). Sebagai contoh, dalam kasus distribusi Poisson, ada satu parameter sehingga \\(d=1\\) dan \\[\\begin{align*} p_\\theta(x)=e^{-\\theta}\\frac{\\theta^x}{x!}, \\quad x=0,1,\\ldots, \\end{align*}\\] dengan \\(θ&gt;0\\). Dalam kasus distribusi binomial, kita memiliki \\[\\begin{align*} p_\\theta(x)= {m \\choose x} q^x(1-q)^{m-x}, \\quad x=0,1,\\ldots,m. \\end{align*}\\] Untuk beberapa aplikasi, kita dapat melihat m sebagai sebuah parameter dan dengan demikian ambil \\(d = 2\\) sehingga \\(θ = (m,q) ∈{0,1,2,...}× [0,1]\\). Misalkan pengamatannya adalah \\(x_1,...,x_n\\) nilai pengamatan dari sampel acak \\(X_1,X_2,...,X_n\\) yang disajikan sebelumnya. Dalam kasus ini, probabilitas pengamatan sampel ini dari \\(pθ\\) sama dengan \\[\\begin{align*} \\prod_{i=1}^n p_\\theta(x_i). \\end{align*}\\] Hal di atas, dilambangkan dengan \\(L(θ)\\) yang dipandang sebagai fungsi dari \\(θ\\) disebut dengan kemungkinan. Perhatikan bahwa kita telah menekan ketergantungannya pada data, untuk menekankan bahwa kita melihatnya sebagai fungsi dari vektor parameter. Sebagai contoh, dalam kasus distribusi Poisson, kita memiliki \\[\\begin{align*} L(\\lambda)=e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n x_i} \\left(\\prod_{i=1}^n x_i!\\right)^{-1}. \\end{align*}\\] Dalam kasus distribusi binomial, kita memiliki \\[\\begin{align*} L(m,q)=\\left(\\prod_{i=1}^n {m \\choose x_i}\\right) q^{\\sum_{i=1}^n x_i} (1-q)^{nm-\\sum_{i=1}^n x_i} . \\end{align*}\\] Penaksir kemungkinan maksimum (mle) untuk \\(θ\\) adalah pemaksimum dari kemungkinan; dalam artian, mle memilih himpunan nilai parameter yang paling baik menjelaskan pengamatan yang diamati. Kasus Khusus: Tiga Hasil Bernoulli. Sebagai ilustrasi, pertimbangkan sebuah sampel dengan ukuran \\(n=3\\) dari distribusi Bernoulli (binomial dengan \\(m = 1\\)) dengan nilai \\(0,1,0\\). Peluang dalam kasus ini dengan mudah diperiksa sama dengan \\[\\begin{align*} L(q)=q(1-q)^2, \\end{align*}\\] dan plot kemungkinan diberikan pada Gambar 2.1. Seperti yang ditunjukkan pada plot, nilai maksimum kemungkinan sama dengan \\(4/27\\) dan dicapai pada \\(q = 1/3\\), dan karenanya estimasi kemungkinan maksimum untuk \\(q\\) adalah \\(1/3\\) untuk sampel yang diberikan. Dalam hal ini, kita dapat menggunakan aljabar untuk menunjukkan bahwa \\[\\begin{align*} q(1-q)^2=\\left(q-\\frac{1}{3}\\right)^2\\left(q-\\frac{4}{3}\\right)+\\frac{4}{27}, \\end{align*}\\] dan menyimpulkan bahwa nilai maksimumnya sama dengan \\(4/27\\), dan dicapai pada \\(q = 1/3\\) (menggunakan fakta bahwa suku pertama tidak positif dalam interval \\([0,1]\\)). Tetapi seperti yang terlihat, cara menurunkan mle menggunakan aljabar ini tidak berlaku umum. Secara umum, seseorang menggunakan kalkulus untuk menurunkan mle - perhatikan bahwa untuk beberapa kemungkinan, seseorang mungkin harus menggunakan metode optimasi lainnya, terutama ketika kemungkinan tersebut memiliki banyak ekstrema lokal. Sudah menjadi kebiasaan untuk memaksimalkan secara ekuivalen logaritma dari kemungkinan \\(L(⋅)\\), dilambangkan dengan \\(l(⋅\\)), dan melihat himpunan nol dari turunan pertamanya$ l′(⋅)$. Dalam kasus kemungkinan di atas, \\(l(q) = log(q) + 2log(1-q)\\), dan \\[\\begin{align*} l&#39;(q)=\\frac{\\rm d}{{\\rm d}q}l(q)=\\frac{1}{q}-\\frac{2}{1-q}. \\end{align*}\\] Angka nol unik dari \\(l′(⋅)\\) sama dengan \\(1/3\\) , dan karena \\(l′′(⋅)\\) adalah negatif, kita memiliki \\(1/3\\) adalah pemaksimum unik dari kemungkinan dan karenanya merupakan estimasi kemungkinan maksimum. 2.4.2 Distribusi Frekuensi MLE Berikut ini, kami menurunkan penaksir kemungkinan maksimum, mle(Maximum likelihood estimator), untuk tiga anggota kelas \\((a,b,0)\\). Kita mulai dengan meringkas pembahasan di atas. Dalam situasi mengamati iid, independen dan berdistribusi identik, variabel acak \\(X_1, X_2,...,X_n\\) dari sebuah distribusi dengan pmf \\(p_θ\\), di mana \\(θ\\) memiliki nilai yang tidak diketahui dalam \\(Θ⊆R^d\\) , kemungkinan \\(L(⋅)\\), sebuah fungsi pada \\(Θ\\) didefinisikan sebagai \\[\\begin{align*} L(\\theta)=\\prod_{i=1}^n p_\\theta(x_i), \\end{align*}\\] di mana \\(x_1,...,x_n\\) adalah nilai yang diamati. MLE dari \\(θ\\), dinotasikan sebagai \\(\\hat{θ}_{MLE}\\), adalah sebuah fungsi yang memetakan observasi ke sebuah elemen dari himpunan pemaksimum \\(L(⋅)\\), yaitu \\[\\begin{align*} \\{\\theta \\vert L(\\theta)=\\max_{\\eta\\in\\Theta}L(\\eta)\\}. \\end{align*}\\] Perhatikan bahwa himpunan di atas merupakan fungsi dari pengamatan, meskipun ketergantungan ini tidak dibuat secara eksplisit. Dalam kasus tiga distribusi yang kita pelajari, dan secara umum, himpunan di atas adalah himpunan tunggal dengan probabilitas yang cenderung mendekati satu (dengan meningkatnya ukuran sampel). Dengan kata lain, untuk banyak distribusi yang umum digunakan dan ketika ukuran sampel besar, estimasi kemungkinan didefinisikan secara unik dengan probabilitas yang tinggi. Berikut ini, kita asumsikan bahwa kita telah mengamati n variabel acak ke-i \\(X_1,X_2,...,X_n\\) dari distribusi yang dipertimbangkan, meskipun nilai parametriknya tidak diketahui. Selain itu, \\(x_1,x_2,...,x_n\\) akan menunjukkan nilai yang diamati. Kami mencatat bahwa dalam kasus data cacahan, dan data dari distribusi diskrit secara umum, kemungkinannya dapat direpresentasikan sebagai \\[\\begin{align*} L(\\theta)=\\prod_{k\\geq 0} \\left(p_\\theta(k)\\right)^{m_k}, \\end{align*}\\] dimana \\(m_k\\) adalah jumlah observasi yang sama dengan \\(k\\) . Secara matematis, kita memiliki \\[\\begin{align*} m_k= \\left\\vert \\{i\\vert x_i=k, 1\\leq i \\leq n\\} \\right\\vert=\\sum_{i= 1}^n I(x_i=k), \\quad k\\geq 0. \\end{align*}\\] Perhatikan bahwa transformasi ini mempertahankan semua data, menyusunnya dengan cara yang efisien. Untuk \\(n\\) yang besar hal ini menyebabkan kompresi data dalam arti kecukupan. Di bawah ini, kami menyajikan ekspresi untuk mle dalam hal \\((m_k)_{k≥1}\\) juga. Kasus Khusus: Distribusi Poisson. Dalam kasus ini, seperti yang disebutkan di atas, kemungkinan diberikan oleh \\[\\begin{align*} L(\\lambda)=\\left(\\prod_{i=1}^n x_i!\\right)^{-1}e^{-n\\lambda}\\lambda^{\\sum_{i=1}^n x_i} . \\end{align*}\\] Dengan menggunakan logaritma, log-kemungkinan adalah \\[\\begin{align*} l(\\lambda)= -\\sum_{i=1}^n \\log(x_i!) -n\\lambda +\\log(\\lambda) \\cdot \\sum_{i=1}^n x_i . \\end{align*}\\] Mengambil turunannya, kami memiliki \\[\\begin{align*} l&#39;(\\lambda)= -n +\\frac{1}{\\lambda}\\sum_{i=1}^n x_i. \\end{align*}\\] Dalam mengevaluasi \\(l′′(λ)\\), ketika \\(∑^n_{i=1}xi&gt;0\\), \\(l′′&lt;0\\). Akibatnya, maksimum dicapai pada rata-rata sampel, \\(\\bar{x}\\) yang disajikan di bawah ini. Ketika \\(∑^n_{i=1}xi=0\\) maka kemungkinan adalah fungsi menurun dan karenanya maksimum dicapai pada nilai parameter yang paling kecil; hal ini mengakibatkan estimasi kemungkinan maksimum menjadi nol. Oleh karena itu, kita memiliki \\[\\begin{align*} \\overline{x} = \\hat{\\lambda}_{\\rm MLE} = \\frac{1}{n}\\sum_{i=1}^n x_i. \\end{align*}\\] Perhatikan bahwa rata-rata sampel juga dapat dihitung sebagai \\[\\begin{align*} \\overline{x} = \\frac{1}{n} \\sum_{k\\geq 1} k \\cdot m_k ~. \\end{align*}\\] Patut dicatat bahwa dalam kasus Poisson, distribusi yang tepat dari \\(λ_{MLE}\\) tersedia dalam bentuk tertutup - ini adalah Poisson berskala - ketika distribusi yang mendasarinya adalah Poisson. Hal ini dikarenakan jumlah variabel acak Poisson independen adalah Poisson juga. Tentu saja, untuk ukuran sampel yang besar, seseorang dapat menggunakan Teorema Batas Tengah/Central Limit Theorem (CLT) biasa untuk mendapatkan perkiraan normal. Perhatikan bahwa perkiraan yang terakhir berlaku bahkan jika distribusi yang mendasari adalah distribusi apa pun dengan momen kedua yang terbatas. Kasus Khusus: Distribusi Binomial. Tidak seperti kasus distribusi Poisson, ruang parameter dalam kasus binomial adalah \\(2\\) dimensi. Oleh karena itu, masalah optimasi sedikit lebih menantang. Kita mulai dengan mengamati bahwa kemungkinan diberikan oleh \\[\\begin{align*} L(m,q)= \\left(\\prod_{i=1}^n {m \\choose x_i}\\right) q^{\\sum_{i=1}^n x_i} (1-q)^{nm-\\sum_{i=1}^n x_i} . \\end{align*}\\] Dengan menggunakan logaritma, log-kemungkinan adalah \\[\\begin{align*} \\begin{array}{ll} l(m,q) &amp;= \\sum_{i=1}^n \\log\\left({m \\choose x_i}\\right) + \\left({\\sum_{i=1}^n x_i}\\right)\\log(q) \\\\ &amp; \\ \\ \\ + \\left({nm-\\sum_{i=1}^n x_i}\\right)\\log(1-q) \\\\ &amp;= \\sum_{i=1}^n \\log\\left({m \\choose x_i}\\right) + n \\overline{x}\\log(q) + n\\left({m- \\overline{x}}\\right)\\log(1-q) , \\end{array} \\end{align*}\\] di mana \\(\\bar{x} = n^{-1}∑^n_{i=1}xi\\). Perhatikan bahwa karena m hanya mengambil nilai bilangan bulat non-negatif, kita tidak dapat menggunakan kalkulus multivariat untuk menemukan nilai optimal. Namun demikian, kita dapat menggunakan kalkulus variabel tunggal untuk menunjukkan bahwa \\[\\begin{equation} \\hat{q}_{MLE}\\times \\hat{m}_{MLE} = \\overline{x}. \\tag{2.2} \\end{equation}\\] verifikasi persamaan 2.2 Terhadap hal ini, kami mencatat bahwa untuk nilai \\(m\\) yang tetap \\[\\begin{align*} \\frac{\\delta}{\\delta q} l(m,q) = \\frac{n \\overline{x}}{q}- \\frac{n\\left({m- \\overline{x}}\\right)}{1-q}, \\end{align*}\\] dan itu \\[\\begin{align*} \\frac{\\delta^2}{\\delta q^2} l(m,q)= -\\frac{n \\overline{x}}{q^2}+ \\frac{n\\left({m- \\overline{x}}\\right)}{(1-q)^2} \\le 0. \\end{align*}\\] Hal di atas mengimplikasikan bahwa untuk setiap nilai \\(m\\) yang tetap yang tetap, nilai maksimum dari \\(q\\) memenuhi \\[\\begin{align*} mq=\\overline{x}, \\end{align*}\\] dan karenanya kita membuat persamaan (2.2) Dengan persamaan (2.2), hal di atas mereduksi tugas menjadi pencarian \\(\\hat{m}_{MLE}\\) yang merupakan pemaksimum dari \\[\\begin{equation} L\\left(m,\\frac{\\overline{x}}{m} \\right). \\tag{2.3} \\end{equation}\\] Perhatikan bahwa kemungkinan akan menjadi nol untuk nilai \\(m\\) yang lebih kecil dari \\(max_{1≤i≤n}xi\\), dan karenanya \\(\\hat{m}_{MLE}≥max_{1≤i≤n}xi\\). Catatan Teknis tentang Perkiraan Poisson untuk Binomial Dalam menentukan algoritma untuk menghitung \\(\\hat{m}_{MLE}\\), pertama-tama kami tunjukkan bahwa untuk beberapa set data, \\(\\hat{m}_{MLE}\\) dapat sama dengan \\(∞\\), yang mengindikasikan bahwa distribusi Poisson akan memberikan kecocokan yang lebih baik dibandingkan distribusi binomial mana pun. Hal ini terjadi karena distribusi binomial dengan parameter \\((m, \\bar{x}/m)\\) mendekati distribusi Poisson dengan parameter \\(\\bar{x}\\) dengan \\(m\\) mendekati tak terhingga. Fakta bahwa beberapa set data lebih memilih distribusi Poisson seharusnya tidak mengherankan karena dalam pengertian di atas, himpunan distribusi Poisson berada pada batas himpunan distribusi binomial. Menariknya, dalam Olkin, Petkau, dan Zidek (1981) mereka menunjukkan bahwa jika rata-rata sampel kurang dari atau sama dengan varians sampel maka \\(\\hat{m}_{MLE} = ∞\\) jika tidak, maka terdapat sebuah \\(m\\) berhingga yang memaksimumkan persamaan (2.3). Pada Gambar 2.2 di bawah ini, kami menampilkan plot \\(L(m, \\bar{x}/m)\\) untuk tiga sampel yang berbeda dengan ukuran \\(5\\); mereka hanya berbeda dalam nilai maksimum sampel. Sampel pertama dari \\((2,2,2,4,5)\\) memiliki rasio rata-rata sampel terhadap varians sampel yang lebih besar dari \\(1\\) \\((1.875)\\), sampel kedua dari \\((2,2,2,4,6)\\) memiliki rasio sebesar \\(1.25\\) yang lebih mendekati \\(1\\), dan sampel ketiga dari \\((2,2,2,4,6)\\) memiliki rasio sebesar \\(1.25\\) yang lebih mendekati \\(1\\), dan sampel keempat dari \\((2,2,2,4,6)\\) memiliki rasio sebesar \\(1.25\\) yang lebih mendekati \\(1\\). dan sampel ketiga \\((2,2,2,4,7)\\) memiliki rasio lebih kecil dari \\(1\\) \\((0.885)\\). Untuk ketiga sampel ini, seperti yang ditunjukkan pada Gambar 2.2, \\(\\bar{m}_{MLE}\\) sama dengan \\(7\\), \\(18\\) dan \\(∞\\) masing-masing. Perhatikan bahwa nilai pembatas dari \\(L(m, \\bar{x}/m)\\) sebagai \\(m\\) mendekati tak terhingga sama dengan \\[\\begin{equation} \\left(\\prod_{i=1}^n x_i! \\right)^{-1} \\exp\\left(-n \\overline{x}~\\right) \\left(~\\overline{x}~\\right)^{n\\overline{x}}. \\tag{2.4} \\end{equation}\\] Juga, perhatikan bahwa Gambar 2.2 menunjukkan bahwa mle dari \\(m\\) tidak kuat, yaitu perubahan pada sebagian kecil set data dapat menyebabkan perubahan besar pada penaksir. Diskusi di atas menyarankan algoritma sederhana berikut ini: Langkah 1. Jika rata-rata sampel kurang dari atau sama dengan varians sampel, maka tetapkan \\(\\hat{m}_{MLE}=∞\\). Distribusi yang disarankan oleh MLE adalah distribusi Poisson dengan \\(\\hat{λ}=\\bar{x}\\). Langkah 2. Jika rata-rata sampel lebih besar dari varians sampel, maka hitunglah \\(L(m,\\bar{x}/m)\\) untuk \\(m\\) yang lebih besar atau sama dengan maksimum sampel sampai \\(L(m,\\bar{x}/m)\\) mendekati nilai kemungkinan Poisson yang diberikan dalam (2.4). Nilai \\(m\\) yang sesuai dengan nilai maksimum \\(L(m,\\bar{x}/m)\\) di antara yang dihitung sama dengan \\(\\hat{m}_{MLE}\\). Kami mencatat bahwa jika distribusi yang mendasari adalah distribusi binomial dengan parameter \\((m,q)\\) (dengan \\(q&gt;0\\)) maka \\(\\hat{m}_{MLE}\\) sama dengan \\(m\\) untuk ukuran sampel yang besar. Juga, \\(\\hat{q}_{MLE}\\) akan memiliki distribusi normal asimtotik dan konvergen dengan probabilitas satu untuk \\(q\\) . Kasus Khusus: Distribusi Binomial Negatif. Kasus distribusi binomial negatif mirip dengan distribusi binomial dalam arti kita memiliki dua parameter dan mles tidak tersedia dalam bentuk tertutup. Perbedaan di antara keduanya adalah bahwa tidak seperti parameter binomial \\(m\\) yang mengambil nilai bilangan bulat positif, parameter \\(r\\) dari binomial negatif dapat mengambil nilai real positif. Hal ini membuat masalah optimasi menjadi sedikit lebih kompleks. Kita mulai dengan mengamati bahwa kemungkinan dapat dinyatakan dalam bentuk berikut: \\[\\begin{align*} L(r,\\beta)=\\left(\\prod_{i=1}^n {r+x_i-1 \\choose x_i}{r+x_i-1 \\choose x_i}\\right) (1+\\beta)^{-n(r+\\overline{x})} \\beta^{n\\overline{x}}. \\end{align*}\\] Hal di atas menyiratkan bahwa log-kemungkinan diberikan oleh \\[\\begin{align*} l(r,\\beta)=\\sum_{i=1}^n \\log{r+x_i-1 \\choose x_i} -n(r+\\overline{x}) \\log(1+\\beta) +n\\overline{x}\\log\\beta, \\end{align*}\\] dan karenanya \\[\\begin{align*} \\frac{\\delta}{\\delta\\beta} l(r,\\beta) = -\\frac{n(r+\\overline{x})}{1+\\beta} + \\frac{n\\overline{x}}{\\beta}. \\end{align*}\\] Dengan menyamakan nilai di atas dengan nol, kita mendapatkan \\[\\begin{align*} \\frac{\\delta}{\\delta\\beta} l(r,\\beta) = -\\frac{n(r+\\overline{x})}{1+\\beta} + \\frac{n\\overline{x}}{\\beta}. \\end{align*}\\] Hal di atas mereduksi masalah optimasi dua dimensi menjadi masalah satu dimensi - kita perlu memaksimalkan \\[\\begin{align*} \\hat{r}_{MLE}\\times \\hat{\\beta}_{MLE} = \\overline{x}. \\end{align*}\\] terhadap \\(r\\), dengan \\(r\\) yang memaksimumkan adalah mle dan \\(\\hat{β}_{MLE}=\\bar{x}/\\hat{r}_{MLE}\\). Dalam Levin, Reeds, dan kawan-kawan (1977) ditunjukkan bahwa jika varians sampel lebih besar daripada rata-rata sampel maka terdapat unik \\(r &gt;0\\) yang memaksimalkan \\(l(r, \\bar{x}/r)\\) dan karenanya merupakan mle yang unik untuk \\(r\\) dan \\(β\\). Selain itu, mereka juga menunjukkan bahwa jika \\(\\hat{σ}^2≤\\bar{x}\\) , maka kemungkinan binomial negatif akan didominasi oleh kemungkinan Poisson dengan \\(\\hat{λ} = \\bar{x}\\) . Dengan kata lain, distribusi Poisson memberikan kecocokan yang lebih baik terhadap data. Jaminan dalam kasus \\(\\hat{σ}^2&gt;\\hat{μ}\\) memungkinkan kita untuk menggunakan beberapa algoritma untuk memaksimalkan \\(l(r,\\bar{x}/r)\\) . Untuk metode alternatif dalam menghitung kemungkinan, kita perhatikan bahwa \\[\\begin{array}{ll} l(r,\\overline{x}/r)&amp;=\\sum_{i=1}^n \\sum_{j=1}^{x_i}\\log(r-1+j) - \\sum_{i=1}^n\\log(x_i!) \\\\ &amp; \\ \\ \\ - n(r+\\overline{x}) \\log(r+\\overline{x}) + nr\\log(r) + n\\overline{x}\\log(\\overline{x}), \\end{array}\\] yang menghasilkan \\[\\begin{align*} \\left(\\frac{1}{n}\\right)\\frac{\\delta}{\\delta r}l(r,\\overline{x}/r)=\\frac{1}{n}\\sum_{i=1}^n \\sum_{j=1}^{x_i}\\frac{1}{r-1+j} - \\log(r+\\overline{x}) + \\log(r). \\end{align*}\\] Kita perhatikan bahwa, pada ekspresi di atas untuk suku-suku yang melibatkan penjumlahan ganda, jumlah bagian dalam sama dengan nol jika \\(x_i = 0\\). Estimasi kemungkinan maksimum untuk \\(r\\) adalah akar dari ekspresi terakhir dan kita dapat menggunakan algoritma pencarian akar untuk menghitungnya. Selain itu, kita juga memiliki \\[\\begin{align*} \\left(\\frac{1}{n}\\right)\\frac{\\delta^2}{\\delta r^2}l(r,\\overline{x}/r)=\\frac{\\overline{x}}{r(r+\\overline{x})}-\\frac{1}{n}\\sum_{i=1}^n \\sum_{j=1}^{x_i}\\frac{1}{(r-1+j)^2}. \\end{align*}\\] Algoritma pencarian akar berulang yang sederhana namun cepat konvergen adalah metode Newton, yang secara kebetulan diyakini telah digunakan oleh orang Babilonia untuk menghitung akar kuadrat. Dalam metode ini, sebuah perkiraan awal dipilih untuk akar dan perkiraan baru untuk akar tersebut dihasilkan secara berurutan sampai konvergen. Menerapkan metode Newton pada masalah kita akan menghasilkan algoritma berikut: Langkah i. Pilih sebuah solusi hampiran, katakanlah \\(r_0\\) . Tetapkan \\(k\\) ke \\(0\\). Langkah ii. Tetapkan \\(r_{k+1}\\) sebagai \\[\\begin{align*} r_{k+1}= r_k - \\frac{\\frac{1}{n}\\sum_{i=1}^n \\sum_{j=1}^{x_i}\\frac{1}{r_k-1+j} - \\log(r_k+\\overline{x})+\\log(r_k)}{\\frac{\\overline{x}}{r_k(r_k+\\overline{x})}-\\frac{1}{n}\\sum_{i=1}^n\\sum_{j=1}^{x_i}\\frac{1}{(r_k-1+j)^2}} \\end{align*}\\] Langkah iii. Jika \\(r_{k+1} ∼ r_k\\) maka laporkan \\(r_{k+1}\\) sebagai estimasi maksimum kemungkinan; jika tidak, naikkan \\(k\\) sebesar \\(1\\) dan ulangi Langkah ii. Sebagai contoh, kami mensimulasikan \\(5\\) sampel pengamatan \\(41,49,40,27,23\\) dari binomial negatif dengan parameter \\(r = 10\\) dan \\(β = 5\\). Memilih nilai awal dari \\(r\\) sedemikian sehingga \\[\\begin{align*} r\\beta=\\hat{\\mu} \\quad \\hbox{and} \\quad r\\beta(1+\\beta)=\\hat{\\sigma}^2 \\end{align*}\\] di mana \\(\\hat{μ}\\) merupakan estimasi rata-rata dan \\(\\hat{σ}^{2}\\) adalah estimasi varians. Hal ini menghasilkan nilai awal untuk \\(r\\) sebesar \\(23,14286\\). Iterasi dari \\(r\\) dari metode Newton adalah \\[\\begin{align*} 21.39627, 21.60287, 21.60647, 21.60647; \\end{align*}\\] konvergensi cepat yang terlihat di atas adalah tipikal dari metode Newton. Oleh karena itu, dalam contoh ini, \\(\\hat{r}_{MLE}∼21.60647\\) dan \\(\\hat{β}_{MLE} = 1.66616\\). Newton&lt;-function(x,abserr){ mu&lt;-mean(x); sigma2&lt;-mean(x^2)-mu^2; r&lt;-mu^2/(sigma2-mu); b&lt;-TRUE; iter&lt;-0; while (b) { tr&lt;-r; m1&lt;-mean(c(x[x==0],sapply(x[x&gt;0],function(z){sum(1/(tr:(tr-1+z)))}))); m2&lt;-mean(c(x[x==0],sapply(x[x&gt;0],function(z){sum(1/(tr:(tr-1+z))^2)}))); r&lt;-tr-(m1-log(1+mu/tr))/(mu/(tr*(tr+mu))-m2); b&lt;-!(abs(tr-r)&lt;abserr); iter&lt;-iter+1; } c(r,iter) } Untuk meringkas pembahasan kita tentang MLE untuk kelas distribusi \\((a,b,0)\\), pada Gambar 2.3 di bawah ini kita memplot nilai maksimum dari peluang Poisson , \\(L(m,\\bar{x}/m)\\) untuk binomial, dan \\(L(r,\\bar{x}/r)\\) untuk binomial negatif, untuk tiga sampel berukuran 5 yang diberikan pada Tabel 2.1. Data tersebut dibuat untuk mencakup tiga urutan dari rata-rata dan varians sampel. Seperti yang ditunjukkan pada Gambar 2.3, dan didukung oleh teori, jika (\\(\\hat{μ}\\)≤\\(\\hat{σ}^2\\) ) maka binomial negatif menghasilkan nilai kemungkinan maksimum yang lebih tinggi; jika \\(\\hat{μ}\\)=\\(\\hat{σ}^2\\) maka Poisson memiliki nilai kemungkinan tertinggi; dan terakhir dalam kasus \\(\\hat{μ}\\)&gt;\\(\\hat{σ}^2\\) binomial memberikan kecocokan yang lebih baik daripada yang lain. Jadi sebelum mencocokkan data frekuensi dengan distribusi \\((a,b,0)\\) \\((a,b,0)\\), yang terbaik adalah memulai dengan memeriksa urutan \\(\\hat{μ}\\) dan \\(\\hat{σ}^2\\) . Sekali lagi kami tekankan bahwa Poisson berada pada batas distribusi binomial negatif dan binomial. Jadi, dalam kasus \\(\\hat{μ}\\)≥\\(\\hat{σ}^2\\) (\\(\\hat{μ}\\)≤\\(\\hat{σ}^2\\)), Poisson menghasilkan kecocokan yang lebih baik daripada binomial negatif (binomial, resp.), yang ditunjukkan oleh \\(\\hat{r}\\) = ∞ (\\(\\hat{m}\\)=∞ masing-masing). \\[ \\small{ \\begin{array}{c|c|c} \\hline \\text{Data} &amp; \\text{Mean }(\\hat{\\mu}) &amp; \\text{Variance }(\\hat{\\sigma}^2) \\\\ \\hline (2,3,6,8,9) &amp; 5.60 &amp; 7.44 \\\\ (2,5,6,8,9) &amp; 6 &amp; 6\\\\ (4,7,8,10,11) &amp; 8 &amp; 6\\\\\\hline \\end{array} } \\] ## Other Frequency Distributions Sub topik: Mendefinisikan kelas distribusi frekuensi (a,b,1) dan mendiskusikan pentingnya hubungan rekursif yang mendasari kelas distribusi ini Menginterpretasikan versi terpotong nol dan versi modifikasi dari distribusi binomial, Poisson, dan binomial negatif Menghitung probabilitas menggunakan hubungan rekursif 2.4.3 Zero Truncation or Modification Contoh: Polis asuransi mobil yang muncul dalam database klaim mobil yang dibuat dalam periode tertentu. Jika ingin mempelajari jumlah klaim yang telah dibuat oleh polis-polis tersebut selama periode ini, maka jelas distribusi harus menetapkan probabilitas nol pada variabel hitungan dengan mengasumsikan nilainya nol. Dengan kata lain, membatasi perhatian pada data hitungan dari polis dalam database klaim, yang telah memotong nol data hitungan semua polis. Pada lini personal (seperti mobil), pemegang polis mungkin tidak ingin melaporkan klaim pertama karena khawatir hal itu akan meningkatkan tarif asuransi di masa depan - perilaku ini meningkatkan proporsi jumlah nol. Contoh seperti yang terakhir memodifikasi proporsi jumlah nol. Berikut merupakan bentuk modifikasi probabilitas yang diberikan pada jumlah nol dengan kelas (a,b,0) dengan tetap mempertahankan probabilitas relatif yang diberikan pada jumlah yang tidak nol - modifikasi nol. Perhatikan bahwa karena (a,b,0) memenuhi, mempertahankan probabilitas relatif dari jumlah yang tidak nol mengimplikasikan bahwa terpenuhi untuk \\(k≥2\\). Hal ini mengarah pada definisi dari kelas distribusi berikut ini. Definisi: Sebuah distribusi hitungan adalah anggota dari kelas (a,b,1) jika untuk beberapa konstanta a dan b probabilitas \\(p_k\\) memenuhi \\[ \\begin{equation} \\frac{p_k}{p_{k-1}}=a+\\frac{b}{k},\\quad k\\geq 2. \\tag{2.5} \\end{equation} \\] Nilai k diatas berawal pada \\(p_1\\) dan bukan \\(p_0\\) dengan distribusi-distribusi ini dengan (a,b,1).Setiap pasangan nilai yang valid untuk a dan b dari kelas (a,b,0) berhubungan dengan sebuah vektor unik dari probabilitas \\({p_k}_{(k≥0)}\\). Jika vektor probabilitas \\((\\bar{p}_k)_{k≥0}\\) yang diberikan oleh \\[ \\tilde{p}_k= \\frac{1-\\tilde{p}_0}{1-p_0}\\cdot p_k, \\quad k\\geq 1, \\] dimana \\((\\bar{p}_0)∈(0,1)\\) dipilih secara sembarang, maka karena probabilitas relatif untuk nilai positif menurut \\({p_k}_{k≥0}\\) dan \\((\\bar{p}_k)_{k≥0}\\) adalah sama, sehingga memiliki \\((\\bar{p}_k)_{k≥0}\\) memenuhi. Hal ini, secara khusus, menunjukkan bahwa kelas (a,b,1) sangat lebih luas daripada kelas (a,b,0). Dengan mulai pada sepasang nilai untuk a dan b yang menghasilkan distribusi (a,b,0) yang valid, dan kemudian melihat distribusi (a,b,1) yang sesuai dengan distribusi (a,b,0) ini. Sekarang berargumen bahwa kelas (a,b,1) memungkinkan untuk sebuah himpunan yang lebih besar dari distribusi-distribusi yang diizinkan untuk a dan b daripada kelas (a,b,0). Kesimpulan yang sama dapat dengan mudah ditarik untuk pasangan-pasangan dengan \\(a=0\\). Dalam kasus dimana \\(a&gt;0\\), alih-alih batasan \\(a+b&gt;0\\) untuk kelas (a,b,0) sehingga memiliki batasan yang lebih lemah dari \\(a+b/2&gt;0\\) untuk kelas (a,b,1) . Dengan parameterisasi \\(b = (r-1)a\\) seperti yang digunakan pada Bagian 2.3, sebagai ganti dari \\(r&gt;0\\) sehingga memiliki batasan yang lebih lemah dari \\(r&gt;-1\\). Secara khusus, melihat bahwa ketika nol memodifikasi distribusi (a,b,0) menghasilkan distribusi dalam kelas (a,b,1) kesimpulannya tidak berlaku untuk arah yang lain. Modifikasi nol dari distribusi hitungan \\(F\\) sedemikian sehingga memberikan probabilitas nol pada hitungan nol disebut pemotongan nol dari \\(F\\) . Oleh karena itu, versi terpotong nol dari probabilitas \\({pk}_{k≥0}\\) diberikan oleh \\[ \\tilde{p}_k= \\frac{1-\\tilde{p}_0}{1-p_0}\\cdot p_k, \\quad k\\geq 1, \\] Secara khusus, sehingga memiliki modifikasi nol dari distribusi count \\(({p^T_k})_{k≥0}\\) , dinotasikan dengan \\(({p^M_k})_{k≥0}\\) dapat dituliskan sebagai kombinasi cembung dari distribusi yang merosot di 0 dan pemotongan nol dari \\(({p_k})_{k≥0}\\) yang dinotasikan dengan \\(({p^T_k})_{k≥0}\\) sehingga memiliki \\[ p^M_k= p^M_0 \\cdot \\delta_{0}(k) + (1-p^M_0) \\cdot p^T_k, \\quad k\\geq 0. \\] 2.5 Distribusi Campuran Pada bagian ini, akan mempelajari cara : Menentukan distribusi campuran ketika komponen pencampuran didasarkan pada jumlah sub-grup yang terbatas. Menghitung probabilitas distribusi campuran dari proporsi pencampuran dan pengetahuan tentang distribusi masing-masing sub-grup. Menentukan distribusi campuran ketika komponen pencampuran kontinu. Dalam banyak aplikasi, populasi dasar terdiri dari sub-grup yang ditentukan secara alami dengan beberapa homogenitas dalam setiap sub-grup. Dalam kasus seperti itu, lebih mudah untuk memodelkan masing-masing sub-grup, dan dengan cara dasar memodelkan seluruh populasi. Seperti yang akan dijelaskan di bawah, di luar daya tarik estetika dari pendekatan,ini juga memperluas jangkauan aplikasi yang dapat dipenuhi oleh distribusi parametrik standar. Apabila \\(k\\) menunjukkan jumlah sub-grup yang ditentukan dalam suatu populasi, dan \\(F_i\\) menunjukkan distribusi pengamatan yang diambil dari sub-grup \\(i\\). Jika kita biarkan \\(α_i\\) menunjukkan proporsi populasi di subgrup \\(i\\), dengan \\(\\sum_{i=1}^{k} α_i = 1\\), maka distribusi pengamata yang dipilih secara acak dari populasi dilambangkan dengan \\(F\\). Maka didapatkan \\(F(x) = \\sum_{i=1}^{k} α_i . F_i(x)\\) Pada rumus di atas dapat dilihat sebagai penerapan langsung dari Hukum Probabilitas Total. Sebagai contoh, terdapat populasi pengemudi yang terbagi menjadi dua sub-grup. Mereka dibedakan dengan pengalaman mengemudi paling lama lima tahun dan yang memiliki pengalaman lebih dari lima tahun. Apabila \\(a\\) menunjukkan proporsipengemudi dengan pengalaman kurang dari 5 tahun, dan \\(F≤5\\) dan \\(F&gt;5\\) menunjukkan distribusi jumlah klaim dalam satu tahun untuk pengemudi dimasing-masing kelompok. Kemudian distribusi jumlah klaim pengemudi yang dipilih secara acak sehingga didapatkan \\(α⋅F_{≤5}(x)+(1−α)_{F&gt;5}(x)\\). Definisi alternatif dari distribusi campuran adalah sebagai berikut. Biarkan \\(N_i\\) menjadi variabel acak dengan distribusi distribusi \\(F_i , i=1,…,k\\) . Biarkan \\(I\\) menjadi variabel acak mengambil nilai \\(1,2,…,k\\) dengan probabilitas \\(α_1,…,α_k\\) , masing-masing. Kemudian variabel acak \\(N_I\\) memiliki distribusi yang diberikan oleh persamaan (2.6). Pada (2.6) kita melihat bahwa fungsi distribusi merupakan kombinasi konveks dari fungsi distribusi komponen. Hasil ini dengan mudah meluas ke fungsi massa probabilitas, fungsi survival, the raw moments, dan ekspektasi karena ini semua adalah pemetaan linier dari fungsi distribusi. Hal ini mencatat bahwa ini tidak berlaku untuk momen sentral seperti varians, dan tindakan bersyarat seperti fungsi hazard rate. Dalam kasus varians, dapat dilihat sebagai \\(Var[N_I]=E[Var[N_I|I]]+Var[E[N_I|I]]=\\sum_{i=1}^{k}α_iVar[N_i]+Var[E[N_I|I]].\\) 2.5.1 Contoh Soal Ujian Aktuaria Di kota tertentu jumlah flu biasa yang akan diderita seseorang dalam setahun mengikuti distribusi Poisson yang bergantung pada usia dan status merokok individu tersebut. Distribusi penduduk dan jumlah rata-rata pilek adalah sebagai berikut: \\[ \\small{ \\begin{array}{l|c|c} \\hline &amp; \\text{Proportion of population} &amp; \\text{Mean number of colds}\\\\\\hline \\text{Children} &amp; 0.3 &amp; 3\\\\ \\text{Adult Non-Smokers} &amp; 0.6 &amp; 1\\\\ \\text{Adult Smokers} &amp; 0.1 &amp; 4\\\\\\hline \\end{array} } \\] 2.5.1.1 Jawaban 1 Dengan menggunakan Law of Total Probability, kita dapat menuliskan probabilitas yang diperlukan sebagai \\(Pr(N_I=3)\\) , dengan \\(I\\) menunjukkan kelompok individu yang dipilih secara acak dengan 1,2 dan 3 menandakan kelompok Anak-anak, Dewasa Bukan Perokok, dan Perokok Dewasa, masing-masing. Sekarang dengan pengkondisian kita dapatkan \\(Pr(N_I=3)=0.3⋅Pr(N_1=3)+0.6⋅Pr(_N2=3)+0.1⋅Pr(N_3=3)\\) dengan \\(N_1\\),\\(N_2\\) dan \\(N_3\\) mengikuti distribusi Poisson dengan rata-rata 3,1 , dan 4 . Menggunakan di atas, kita mendapatkan \\(Pr(N_I=3)∼0.1235\\). 2.5.1.2 Jawaban 2 Probabilitas bersyarat dari peristiwa A diberikan peristiwa B, \\(Pr(A|B) = \\frac{(Pr⁡(A,B))}{(Pr⁡(B)})\\). Probabilitas bersyarat yang diperlukan dalam soal ini kemudian dapat ditulis sebagai \\(Pr(I=3|N_I=3)\\) , yang sama dengan \\(Pr(I=3|N_I=3)= \\frac{Pr(I=3,N_3=3)}{Pr(N_I=3)}∼\\frac{0.1×0.1954}{0.1235}∼0.1581\\) Dalam contoh di atas, jumlah sub-grup \\(k\\) sama dengan tiga. Secara umum, \\(k\\) dapat berupa bilangan asli apa pun, tetapi ketika \\(k\\) besar, ini adalah sedikit dari sudut pandang pemodelan untuk mengambil pendekatan subgrup tak terhingga. Untuk memotivasi pendekatan ini, misalkan subgrup \\(i\\) sedemikian rupa sehingga distribusi komponennya \\(F_i\\) diberikan oleh \\(G_\\bar{θ_{{i}}}\\) , di mana G adalah bagian dari distribusi parametrik dengan ruang parameter \\(Θ⊆R^d\\) . Dengan asumsi ini, fungsi distribusi \\(F\\) dari pengamatan yang diambil secara acak dari populasi maka didaptkan \\(F(x)=\\sum_{i=1}^k = α_iG_\\bar{θ_{{i}}}(x),∀x∈R\\) mirip dengan persamaan (2.6). Bergantian, dapat ditulis sebagai \\(F(x)=E[G_\\bar{θ_{{i}}}(x)],∀x∈R\\) di mana \\(\\barϑ\\) mengambil nilai \\(\\barθ_i\\) dengan probabilitas \\(α_i\\) , untuk \\(i=1,…,k\\) . Hal di atas memperjelas bahwa ketika \\(k\\) besar, seseorang dapat memodelkan di atas dengan memperlakukan \\(\\barϑ\\) sebagai variabel acak kontinu. Untuk mengilustrasikan pendekatan ini, misalkan kita memiliki populasi pengemudi dengan distribusi klaim untuk pengemudi individu yang didistribusikan sebagai Poisson. Setiap orang memiliki jumlah klaim yang diharapkan (pribadi) mereka sendiri \\(λ\\) - nilai yang lebih kecil untuk pengemudi yang baik, dan nilai yang lebih besar untuk orang lain. Ada distribusi \\(λ\\) dalam populasi; pilihan populer dan nyaman untuk memodelkan distribusi ini adalah distribusi gamma dengan parameter \\((α,θ)\\). Dengan spesifikasi tersebut ternyata distribusi yang dihasilkan \\(N\\) , klaim driver yang dipilih secara acak, adalah binomial negatif dengan parameter \\((r=α,β=θ)\\) . Ini dapat ditunjukkan dalam banyak cara, tetapi argumen langsungnya adalah sebagai berikut: \\[ \\begin{array}{ll} \\Pr(N=k)&amp;= \\int_0^\\infty \\frac{e^{-\\lambda}\\lambda^k}{k!} \\frac{\\lambda^{\\alpha-1}e^{-\\lambda/\\theta}}{\\Gamma{(\\alpha)}\\theta^{\\alpha}} d\\lambda = \\frac{1}{k!\\Gamma(\\alpha)\\theta^\\alpha}\\int_0^\\infty \\lambda^{\\alpha+k-1}e^{-\\lambda(1+1/\\theta)}~d\\lambda \\\\ &amp;=\\frac{\\Gamma{(\\alpha+k)}}{k!\\Gamma(\\alpha)\\theta^\\alpha(1+1/\\theta)^{\\alpha+k}} \\\\ &amp;={\\alpha+k-1 \\choose k}\\left(\\frac{1}{1+\\theta}\\right)^\\alpha\\left(\\frac{\\theta}{1+\\theta}\\right)^k, \\quad k=0,1,\\ldots \\end{array} \\] Perhatikan bahwa derivasi di atas secara implisit menggunakan yang berikut ini: \\(f_{N|Λ=λ}(N=k)=\\frac{e^{−λ}λ^k}{k!},k≥0;andfΛ(λ)=\\frac{λ^{α−1}e^{−λ/θ}}{Γ(α)θ^α},λ&gt;0\\) Dengan mempertimbangkan campuran dari kelas distribusi parametrik, kita meningkatkan kekayaan kelas tersebut. Perluasan distribusi ini menghasilkan kelas campuran yang mampu melayani lebih banyak aplikasi daripada kelas parametrik yang kita gunakan sebelumnya. Pemodelan campuran adalah teknik pemodelan yang penting dalam aplikasi asuransi dan bab-bab berikutnya akan membahas lebih banyak aspek dari teknik pemodelan ini. 2.6 Goodnes of Fit Dalam Materi ini akan mempelajari: - Perhitungan Statisik dengan menggunakan Goodness of fit dalam membandingkan distribusi distrik yang dihipotesiskan dengan sample pengamatan diskrit - Perbandingan Statistik dengan Distribusi referensi dalam menilai kecukupan atau keseluruhan dari fit tersebut. Sebelumnya kita telah membahas 3 Distribusi Frekuensi Dasar dengan beserta perluasan melalui Pemotongan dan modifikasi nol. Tetapi, pada kelas tersebut masih tetap parametrik dan karenanya pada dasarnya merupakan bagian kecil dari kelas dari semua distribusi frekuensi yang mungkin (himpunan distribusi pada bilangan bulat non-negatif). Maka dari itu meskipun metode untuk mengestimasi parameter yang tidak diketahui, distribusi yang cocok tidak menjadi representasi yang baik dari distribusi yang mendasari jika yang terakhir jauh dari kelas distribusi yang digunakan untuk pemodelan. Karena dapat dibuktikan dan ditunjukkan bahwa penaksir kemungkinan maksimum konvergen ke suatu nilai sehingga distribusi yang sesuai adalah proyeksi Kullback-Leibler dari distribusi yang mendasari pada kelas distribusi yang digunakan untuk pemodelan.Dalam metode pengujian statistik yang digunakan adalha chi-kuadrat Pearson dalam untuk memeriksa kecocokan dari distribusi yang cocok. Pada tahun 1993 sebuah portofolio yang terdiri dari n = 7.483 polis asuransi mobil dari sebuah perusahaan asuransi besar di Singapura memiliki distribusi kecelakaan mobil per pemegang polis seperti yang diberikan pada Tabel 2.4. \\[ \\small{ \\begin{array}{l|c|c|c|c|c|c} \\hline \\text{Count }(k) &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; \\text{Total}\\\\ \\hline \\text{No. of Policies with }k\\text{ accidents }(m_k) &amp; 6,996 &amp; 455 &amp; 28 &amp; 4 &amp; 0 &amp; 7,483\\\\ \\hline \\end{array} } \\] Dengan menggunakan Distribusi Poisson maka maximum likelihood estimator (mle) for λ dengan rata rata Poisson merupakan rata rata sampel yang diberikan : \\[ \\begin{align} \\bar{N}=\\frac{0 ⋅ 6996 +1 ⋅ 455 + 2⋅28+3⋅4+4⋅0}{7483}=0.06989 \\end{align} \\] Jika menggunakan Poisson (\\(\\hat{λ}MLE\\)) sebagai distribusi yang cocok, maka perbandingan tabel dari jumlah yang cocok dan jumlah yang diamati diberikan oleh Tabel 2.5 dengan \\(\\hat{p}_k\\) mewakili estimasi probabilitas di bawah distribusi Poisson yang sesuai. \\[ \\small{ \\begin{array}{c|r|r} \\hline \\text{Count} &amp; \\text{Observed} &amp; \\text{Fitted Counts}\\\\ (k) &amp; (m_k) &amp; \\text{Using Poisson }(n\\hat{p}_k)\\\\ \\hline 0 &amp; 6,996 &amp; 6,977.86 \\\\ 1 &amp; 455 &amp; 487.70 \\\\ 2 &amp; 28 &amp; 17.04 \\\\ 3 &amp; 4 &amp; 0.40 \\\\ \\geq 4 &amp; 0 &amp; 0.01\\\\ \\hline \\text{Total} &amp; 7,483 &amp; 7,483.00\\\\ \\hline \\end{array} } \\] perbandingan tabel tidak cukup untuk menguji hipotesis secara statistik bahwa distribusi yang mendasarinya memang Poisson. Statistik chi-kuadrat Pearson adalah ukuran statistik kecocokan yang dapat digunakan. Untuk menjelaskan statistik ini,dapat dimisalkan sebuah set data berukuran \\(n\\) yang dikelompokkan menjadi \\(k\\) dengan \\(m_k/n\\) dan \\(\\hat{p}_k\\) untuk \\(k = 1...,K\\) masing-masing merupakan probabilitas observasi dan estimasi dari sebuah observasi yang termasuk dalam sel ke-k masing-masing. Statistik uji chi-square Pearson kemudian diberikan oleh \\[ \\begin{align} \\sum_{k=1}^{K} = \\frac{(m_k−n\\hat{p}_k)^2}{n\\hat{p}_k} \\end{align} \\] Motivasi untuk statistik di atas berasal dari fakta bahwa \\[ \\begin{align} \\sum_{k=1}^{K} = \\frac{(m_k−np_k)^2}{np_k} \\end{align} \\] Dimana memiliki distribusi chi-kuadrat pembatas dengan \\(K-1\\) derajat kebebasan jika \\(p_k\\) dengan \\(k = 1,...,K\\) yang merupakan probabilitas sel yang sebenarnya. Selanjutnya menganggap bahwa hanya data yang dirangkum yang diwakili oleh \\(m_k\\) dengan \\(k = 1,...,K\\) yang tersedia. Selanjutnya, jika \\((p_k)&#39;s\\) merupakan fungsi dari s parameter-parameter, maka dapat mengganti \\((p_k)&#39;s\\) dengan probabilitas yang diestimasi secara efisien \\((\\hat{p}_k)&#39;s\\) yang akan menghasilkan statistik yang tetap memiliki distribusi chi-square yang membatasi tetapi dengan derajat kebebasan yang diberikan oleh \\(K-1-s\\) Estimasi yang efisien tersebut dapat diturunkan misalnya dengan menggunakan metode mle (dengan multinomial likelihood) atau dengan menaksir parameter \\(s\\) yang meminimumkan statistik chi-square Pearson di atas. Sebagai contoh, kode R di bawah ini menghitung estimasi untuk \\(λ\\) dengan cara yang terakhir dan menghasilkan estimasi 0.06623153. m = c(6996,455,28,4,0) op = m/sum(m) g&lt;-function(lam){sum((op-c(dpois(0:3,lam),1-ppois(3,lam)))^2)}; optim(sum(op*(0:4)),g,method=&quot;Brent&quot;,lower=0,upper=10)$par ## [1] 0.06623153 Ketika seseorang menggunakan data lengkap untuk mengestimasi probabilitas, distribusi asimtotik berada di antara distribusi chi-kuadrat dengan parameter \\(K-1\\) dan \\(K-1-s\\). Dalam praktiknya, hal yang umum untuk mengabaikan kehalusan ini dan mengasumsikan chi-kuadrat pembatas memiliki \\(K-1-s\\) derajat kebebasan. Menariknya, jalan pintas praktis ini bekerja dengan cukup baik dalam kasus distribusi Poisson. Untuk data otomotif Singapura, statistik chi-kuadrat Pearson sama dengan 41,98 dengan menggunakan \\(mle\\) data lengkap untuk \\(λ\\).Dengan menggunakan distribusi pembatas chi-kuadrat dengan \\(5-1-1=3\\) derajat kebebasan, kita melihat bahwa nilai 41,98 berada jauh di bagian ekor (persentil ke-99 persentil ke-99 berada di bawah 12). Oleh karena itu, kita dapat menyimpulkan bahwa distribusi Poisson memberikan kecocokan yang tidak memadai untuk data tersebut. Di atas, kita mulai dengan sel seperti yang diberikan dalam ringkasan tabel di atas. Dalam praktiknya, pertanyaan yang relevan adalah bagaimana mendefinisikan sel sehingga distribusi chi-kuadrat merupakan perkiraan yang baik untuk distribusi sampel terbatas dari statistik. Aturan praktisnya adalah mendefinisikan sel sedemikian rupa sehingga memiliki setidaknya 80%, jika tidak semua sel, jika tidak semua, sel memiliki jumlah yang diharapkan lebih besar dari 5. Selain itu, jelas bahwa jumlah sel yang lebih besar menghasilkan kekuatan yang lebih tinggi dari pengujian, dan karenanya aturan praktis yang sederhana adalah memaksimalkan jumlah sel sedemikian rupa sehingga setiap sel memiliki setidaknya 5 pengamatan. "],["modeling-loss-severity.html", "Bab 3 Modeling Loss Severity 3.1 Basic Distributional Quantities 3.2 Distribusi Kontinu untuk Memodelkan Tingkat Keparahan dari Kerugian 3.3 Methods of Creating New Distributions 3.4 modifikasi pertanggungan 3.5 Maximum Likelihood Estimation", " Bab 3 Modeling Loss Severity 3.1 Basic Distributional Quantities 3.1.1 Moments Dengan memisalkan X merupakan suatu variabel acak kontinu dengan probability density function (pdf) \\(f_X(x)\\) dan fungsi ditribusi \\(F_X(x)\\). Momen baku ke-k dari X yang dinotasikan dengan \\(μ&#39;_k\\) adalah nilai ekspektasi dari pangkat ke-k dari X yang diharapkan, asalkan mempunyai nilai. Momen pertama \\(μ&#39;_1\\) adalah nilai tengah (mean) dari X yang biasanya dilambangkan dengan μ . Rumus untuk \\(μ&#39;_k\\) adalah sebagai berikut : \\[ \\begin{align} μ&#39;_k = E(X^k) = \\int_{0}^{∞}x^kf_X(x)dx \\end{align} \\] Untuk mendukung dari variabel acak X dapat diasumsikan sebagai nonnegatif dikarenakan pada kenyataannya jarang sekali bernilai negatif. Salah satu contohnya yang menunjukkan bahwa raw moments untuk variabel nonnegatif dapat dihitung menggunakan rumus : \\[ \\begin{align} μ&#39;_k = \\int_{0}^{∞}kx^{k-1}[1-F_X(x)]dx, \\end{align} \\] yang didasarkan pada fungsi survival \\(S_X(x)=1−F_X(x)\\). Pada rumus ini berguna disaat k=1. Central moment ke -k dari X yang dinotasikan dengan \\(μ_k\\) yang merupakan nilai yang diharapkan dari pangkat ke-k dari deviasi x dan dari mean μ. Maka untuk rumus \\(μ_k\\) adalah \\[ \\begin{align} μ_k = E[(X-μ)^k] = \\int_{0}^{∞}(x-μ)^kf_X(x)dx \\end{align} \\] Momen pusat kedua \\(μ_2\\) mendefinisikan varians dari X yang dinotasikan dengan \\(σ^2\\). Akar kuadrat dari varians adalah simpangan baku \\(σ\\) . Rasio momen sentral ketiga terhadap pangkat tiga dari deviasi standar \\((μ_3/σ^3)\\) yang mendefinisikan koefisien kemiringan yang merupakan ukuran simetri. Koefisien kemencengan yang positif menunjukkan bahwa distribusi condong ke kanan (condong ke kanan). Rasio momen sentral keempat dengan pangkat empat dari deviasi standar \\((μ_4/σ^4)\\) mendefinisikan koefisien kurtosis. Distribusi normal memiliki koefisien kurtosis 3. Distribusi dengan koefisien kurtosis lebih besar dari 3 memiliki ekor yang lebih berat daripada distribusi normal, sedangkan distribusi dengan koefisien kurtosis kurang dari 3 memiliki ekor yang lebih ringan dan lebih datar. 3.1.1.1 Example Mengasumsikan bahwa variabel acak X memiliki distribusi gamma dengan rata-rata 9 dan skewness 1. Maka dicari perhitungan varians dari X. pdf dari X adalah : \\[ \\begin{align} f_{X}\\left( x \\right) = \\frac{\\left( x / \\theta \\right)^{\\alpha}}{x ~\\Gamma\\left( \\alpha \\right)} e^{- x / \\theta} \\end{align} \\] Untuk \\(x&gt;0\\), dan \\(α&gt;0\\), maka momen baku ke-k adalah \\[ \\begin{align} \\mu_{k}^{\\prime} = \\mathrm{E}\\left( X^{k} \\right) = \\int_{0}^{\\infty}{\\frac{1}{\\Gamma\\left( \\alpha \\right)\\theta^{\\alpha}}x^{k + \\alpha - 1}e^{- x / \\theta} dx} = \\frac{\\Gamma\\left( k + \\alpha \\right)}{\\Gamma\\left( \\alpha \\right)}\\theta^{k} \\end{align} \\] Dengan memberikan \\(\\Gamma\\left( r + 1 \\right) = r\\Gamma\\left( r \\right)\\) dan \\(\\Gamma\\left( 1 \\right) = 1\\). Maka \\(\\mu_{1}^{\\prime} = \\mathrm{E}\\left( X \\right) = \\alpha\\theta\\) $$ \\[\\begin{align} \\mu_{2}^{\\prime} &amp;= \\mathrm{E}\\left( X^{2} \\right) = \\left( \\alpha + 1 \\right)\\alpha\\theta^{2} = \\mu_{3}^{\\prime} = \\mathrm{E}\\left( X^{3} \\right) = \\left( \\alpha + 2 \\right)\\left( \\alpha + 1 \\right)\\alpha\\theta^{3}\\\\ \\mathrm{Var}\\left( X \\right) &amp;= (\\alpha + 1)\\alpha\\theta^2 - (\\alpha\\theta)^2 = \\alpha\\theta^{2}\\\\ \\end{align}\\] \\[ \\] \\[\\begin{array}{ll} \\text{Skewness} &amp;= \\frac{\\mathrm{E}\\left\\lbrack {(X - \\mu_{1}^{\\prime})}^{3} \\right\\rbrack}{{\\left( \\mathrm{Var}X \\right)}^{3/2}} = \\frac{\\mu_{3}^{\\prime} - 3\\mu_{2}^{\\prime}\\mu_{1}^{\\prime} + 2{\\mu_{1}^{\\prime}}^{3}}{{\\left(\\mathrm{Var} X \\right)}^{3/2}} \\\\ &amp;= \\frac{\\left( \\alpha + 2 \\right)\\left( \\alpha + 1 \\right)\\alpha\\theta^{3} - 3\\left( \\alpha + 1 \\right)\\alpha^{2}\\theta^{3} + 2\\alpha^{3}\\theta^{3}}{\\left( \\alpha\\theta^{2} \\right)^{3/2}} \\\\ &amp;= \\frac{2}{\\alpha^{1/2}} = 1. \\end{array}\\] $$ Maka didapatkan hasil \\[ \\begin{align} α&amp;=4\\\\ E(X)&amp;=αθ=8\\\\ θ&amp;=2\\\\ Var(X)&amp;=αθ^2=16 \\end{align} \\] 3.1.2 Quantiles Kuantil dapat digunakan dalam menggambarkan karakteristik distribusi X. Ketika distribusi X kontinu, untuk suatu pecahan tertentu 0≤p≤1 kuantil yang sesuai adalah solusi dari persamaan \\[ \\begin{align} F_X(π_p)=p \\end{align} \\] Sebagai contoh, titik tengah distribusi, \\(π_{0.5}\\) adalah median. Persentil adalah jenis kuantil; persentil \\(100p\\) yang merupakan angka sedemikian rupa sehingga \\(100×p\\) persen dari data berada di bawahnya. 3.1.3 Moment Generating Function Fungsi pembangkit momen (mgf)dilambangkan dengan MX(t) secara unik mencirikan distribusi dari X . Meskipun ada kemungkinan dua distribusi yang berbeda memiliki momen yang sama namun tetap berbeda, tidak demikian halnya dengan fungsi pembangkit momen. Artinya, jika dua variabel acak memiliki fungsi pembangkit momen yang sama, maka keduanya memiliki distribusi yang sama. Fungsi pembangkit momen diberikan oleh untuk semua nilai t yang memiliki nilai ekspektasi. \\[ \\begin{align} M_X(t) = E(e^{tX}) = \\int_{0}^{∞}e^{tX}f_X(x)dx \\end{align} \\] MGF adalah fungsi real yang turunan ke-k pada nol sama dengan momen mentah ke-k dari X . Dalam simbol, ini adalah \\[ \\begin{align} \\frac{d^k}{dt^k}MX(t)\\Bigr|_{t=0} = E(X^{k}) \\end{align} \\] 3.1.3.1 Example Variabel acak X memiliki distribusi eksponensial dengan mean \\(1/b\\). Maka dapat menacari b jika \\(M_{X}\\left( - b^{2} \\right) = 0.2\\). \\[ \\begin{align} M_{X}(t) = \\mathrm{E}\\left( e^{tX} \\right) = \\int_{0}^{\\infty}{e^{\\text{tx}}be^{- bx} dx} = \\int_{0}^{\\infty}{be^{- x\\left( b - t \\right)} dx} = \\frac{b}{\\left( b - t \\right)}. \\end{align} \\] Maka \\[ \\begin{align} M_{X}\\left( - b^{2} \\right) = \\frac{b}{\\left( b + b^{2} \\right)} = \\frac{1}{\\left( 1 + b \\right)} = 0.2 \\end{align} \\] Maka akan didapatkan \\[ \\begin{align} \\frac{1}{\\left( 1 + b \\right)} &amp;= 0.2\\\\ 1&amp;=0.2(1+b)\\\\ 1&amp;=0.2+0.2b\\\\ 0.8&amp;=0.2b\\\\ 4&amp;=b\\\\ \\end{align} \\] Kita juga dapat menggunakan fungsi pembangkit momen untuk menghitung fungsi pembangkit probabilitas dengan \\[ \\begin{align} P_X(z)= E(z^X)=M_X(logz) \\end{align} \\] 3.2 Distribusi Kontinu untuk Memodelkan Tingkat Keparahan dari Kerugian Metode yang akan dibahas: Gamma Pareto Weibull Generalized beta distribution of the second kind 3.2.1 Gamma Distribution Pendekatan konvensional dalam memodelkan kerugian adalah dengan membuat model terpisah untuk frekuensi dan tingkat keparahan klaim. Ketika frekuensi dan tingkat keparahan dimodelkan secara terpisah, biasanya para aktuaris menggunakan distribusi Poisson untuk jumlah klaim dan distribusi gamma untuk memodelkan tingkat keparahan dari kerugian itu sendiri. Namun, dengan perkembangan, menjadi popule metode dengan membuat model tunggal untuk premi murni (biaya klaim secara rata-rata). Rumus gamma density function: \\[\\begin{equation} f_{X}\\left( x \\right) = \\frac{\\left( x/ \\theta \\right)^{\\alpha}}{x~ \\Gamma\\left( \\alpha \\right)}\\exp \\left( -x/ \\theta \\right) \\ \\ \\ \\text{for } x &gt; 0 . \\end{equation}\\] Dengan nilai \\(a&gt;0\\) dan \\(θ&gt;0\\) juga. Variabel kontinu \\(X\\) yang merupakan fungsi adalah variable yang mewakilkan distribusi gamma dengan shape parameter \\(a\\) dan scale parameter \\(θ\\). Perubahan dari perubahan scale dan shape parameter untuk gamma density function tertuang dalam grafik: Apabila \\(a=1\\) gamma akan mereduksi menjadi distribusi eksponensial, apabila \\(a=\\frac{n}2\\) dan \\(θ=2\\) maka gamma akan mereduksi menjadi distribusi chi-square dengan nilai n sebagai derajat kebebesan pada distribusi.Fungsi distribusi dari model gamma merupakan bentuk tidak sempurna atau lengkap dari fungsi gamma, dan dianotasikan sebagai \\(\\Gamma\\left(\\alpha; \\frac{x}{\\theta} \\right)\\),, dan bentuk persamaannya adalah: \\[\\begin{equation} F_{X}\\left( x \\right) = \\Gamma\\left( \\alpha; \\frac{x}{\\theta} \\right) = \\frac{1}{\\Gamma\\left( \\alpha \\right)}\\int_{0}^{x /\\theta}t^{\\alpha - 1}e^{- t}~dt , \\end{equation}\\] dengan \\(\\alpha &gt; 0,\\ \\theta &gt; 0\\) adalah integer untul \\(\\alpha\\), sehingga dapat dituliskan sebagai \\(\\Gamma\\left( \\alpha; \\frac{x}{\\theta} \\right) = 1 - e^{-x/\\theta}\\sum_{k = 0}^{\\alpha-1}\\frac{(x/\\theta)^k}{k!}\\) Sedangkan, untuk momen pada suatu \\(K\\) dari variable acak berdistribusi gamma untuk \\(k\\) positif dituliskan dengan persamaan: \\[\\begin{equation} \\mathrm{E}\\left( X^{k} \\right) = \\theta^{k} \\frac{\\Gamma\\left( \\alpha + k \\right)}{\\Gamma\\left( \\alpha \\right)} . \\end{equation}\\] rata-rata dan variansi yang diberikan oleh \\(\\mathrm{E}\\left( X \\right) = \\alpha\\theta\\) dan \\(\\mathrm{Var}\\left( X \\right) = \\alpha\\theta^{2}\\), secara berturut-turut Karena semua momen ada untuk setiap \\(K\\) positif distribusi gamma dianggap sebagai distribusi berekor ringan, yang mungkin tidak cocok untuk memodelkan aset berisiko karena tidak akan memberikan penilaian yang realistis tentang kemungkinan kerugian yang parah. 3.2.2 Distribusi Pareto Distribusi Pareto, yang dinamai menurut nama ekonom Italia Vilfredo Pareto (1843-1923), yang memiliki banyak kontribusi pada aplikasi ekonomi dan keuangan. Distribusi ini memiliki kemiringan positif dan heavy tailed yang membuatnya cocok untuk memodelkan pendapatan, klaim asuransi berisiko tinggi, dan tingkat keparahan kerugian korban yang besar. Fungsi survival dari distribusi Pareto yang meluruh perlahan-lahan menuju nol pertama kali digunakan untuk menggambarkan distribusi pendapatan di mana sebagian kecil dari populasi memiliki proporsi yang besar dari total kekayaan. Untuk klaim asuransi yang ekstrim, ekor dari distribusi keparahan (kerugian yang melebihi ambang batas) dapat dimodelkan dengan menggunakan distribusi Pareto yang digeneralisasi. Variabel kontinu \\(X\\) dikatakan memiliki dua parameter pada distribusi pareto, dimana ada shape parameter \\(a\\) dan scale parameter \\(θ\\), yang persamaannya adalah \\[\\begin{equation} f_{X}\\left( x \\right) = \\frac{\\alpha\\theta^{\\alpha}}{\\left( x + \\theta \\right)^{\\alpha + 1}} \\ \\ \\ x &gt; 0, \\ \\alpha &gt; 0, \\ \\theta &gt; 0. \\tag{3.1} \\end{equation}\\] Grafis di bawah menggambarkan efek dari perubahan scale dan shape parameter pada pareto density function. Fungsi distribusi dari distribusi pareto adalah: \\[\\begin{equation} F_{X}\\left( x \\right) = 1 - \\left( \\frac{\\theta}{x + \\theta} \\right)^{\\alpha} \\ \\ \\ x &gt; 0,\\ \\alpha &gt; 0,\\ \\theta &gt; 0. \\end{equation}\\] Hazard function dari distribusi pareto adalah fungsi penurunan dari fungsi \\(x\\), indikasi ini menunjukkan bahwa distribusi pareto heavy tailed menggunakan analogi dari pendapatan populasi, apabila hazard function menurun seiring waktu, populasi akan habis pada suatu waktu penurunan yang menghasilkan heavier tail pada distribusi. Hazard function juga memberikan informasi tentang tail distribution yang digunakan untuk distribusi model data pada analisis survival. Hazard function ini juga dapat diartikan sebagai potensi sesaat bahwa suatu event yang menarik dapat terjadi dalam jangka waktu yang sangat sempit. Model \\(K\\) pada distribusi parretto dalam suatu keadaan random variable, jika \\(a\\)&gt;\\(k\\), yang dapat ditulis sebagai: \\[\\begin{equation} \\mathrm{E}\\left( X^{k} \\right) = \\frac{\\theta^{k}~ k!}{\\left( \\alpha - 1 \\right)\\cdots\\left( \\alpha - k \\right)} \\ \\ \\ \\alpha &gt; k. \\end{equation}\\] rata-rata dan variansi yang diberikan oleh \\[\\begin{equation} \\mathrm{E}\\left( X \\right) = \\frac{\\theta}{\\alpha - 1} \\ \\ \\ \\text{for } \\alpha &gt; 1 \\end{equation}\\] dan \\[\\begin{equation} \\mathrm{Var}\\left( X \\right) = \\frac{\\alpha\\theta^{2}}{\\left( \\alpha - 1 \\right)^{2}\\left( \\alpha - 2 \\right)} \\ \\ \\ \\text{for } \\alpha &gt; 2, \\end{equation}\\] secara berturut-turut 3.2.3 Distribusi Weibull Distribusi ini diambil dari nama pemiliknya yang merupakan fisikawan Swedish Waloddi Weibull (1887 – 1979) yang secara luas digunakan dalam keandalan, analisis data kehidupan, prakiraan cuaca, dan klaim asuransi umum. Data terpotong sering muncul dalam studi asuransi. Distribusi Weibull telah digunakan untuk memodelkan kelebihan perjanjian kerugian atas asuransi mobil serta waktu antar kedatangan gempa bumi. Variabel kontinu \\(X\\) dikatakan memiliki distribusi weibul dengan syarat memiliki 2 parameter yang digunakan, yaitu scale parameter \\(a\\) dan shape parameter \\(θ\\). Yang persamaannay turun dari: $$\\[\\begin{equation} \\end{equation}\\]$$ Pada grafik menunjukkan efek dari scale dan shape parameter pada perubahan weibul density function. Fungsi distribusi dari weibul distribusi diberikan sebagai: \\[\\begin{equation} F_{X}\\left( x \\right) = 1 - \\exp\\left(- \\left( \\frac{x}{\\theta} \\right)^{\\alpha}~\\right) \\ \\ \\ x &gt; 0,\\ \\alpha &gt; 0,\\ \\theta &gt; 0. \\end{equation}\\] Dari rumus dapat ditarik kesimpulan bahwa \\(a\\) mendeskripsikan bentuk dari hazard function dari weibul distribution. Dimana hazard function akan menjadi fungsi penurunan apabila \\(a\\)&lt;1 (heavy tailed distribution), akan konstan Ketika \\(a\\)=1 dan akan menjadi fungsi naik Ketika \\(a\\)&gt;1 (light tailed distribution). Sifat dari hazard function ini membuat weibul distribusi cocok untuk digunakan pada model yang variety yang luas, contohnya fenomena alam, forecast cuaca, Teknik industry, model asuransi, dan analisis resiko financial. Momen \\(K\\) pada weibul dianotasikan dalam : \\[\\begin{equation} \\mathrm{E}\\left( X^{k} \\right) = \\theta^{k}~\\Gamma\\left( 1 + \\frac{k}{\\alpha} \\right) . \\end{equation}\\] rata-rata dan variansi yang diberikan oleh \\[\\begin{equation} \\mathrm{E}\\left( X \\right) = \\theta~\\Gamma\\left( 1 + \\frac{1}{\\alpha} \\right) \\end{equation}\\] dan \\[\\begin{equation} \\mathrm{Var}(X)= \\theta^{2}\\left( \\Gamma\\left( 1 + \\frac{2}{\\alpha} \\right) - \\left\\lbrack \\Gamma\\left( 1 + \\frac{1}{\\alpha} \\right) \\right\\rbrack ^{2}\\right), \\end{equation}\\] secara berturut-turut 3.2.3.1 Contoh Soal Misalkan distribusi probabilitas masa hidup penderita AIDS (dalam bulan) dari saat diagnosis digambarkan oleh distribusi Weibull dengan parameter bentuk 1.2 dan parameter skala 33.33. Temukan probabilitas bahwa orang yang dipilih secara acak dari populasi ini bertahan setidaknya 12 bulan. Sebuah sampel acak dari 10 pasien akan dipilih dari populasi ini. Berapa peluang bahwa paling banyak dua orang akan meninggal dalam waktu satu tahun setelah diagnosis. Temukan persentil ke-99 dari distribusi masa hidup. 3.2.3.2 Solusi Biarkan X menjadi seumur hidup pasien AIDS (dalam bulan) memiliki distribusi Weibull dengan parameter \\((1.2,33.33)\\). Kita punya, \\(\\Pr \\left( X \\geq 12 \\right) = S_{X} \\left( 12 \\right) = e^{- \\left( \\frac{12}{33.33} \\right)^{1.2}} = 0.746.\\) Biarkan \\(Y\\) adalah jumlah pasien yang meninggal dalam waktu satu tahun diagnosis. Lalu, \\(Y\\)∼ \\(Bin(10, 0,254)\\) dan \\(Pr(Y≤2)=0,514\\). Misalkan \\(π0,99\\) menunjukkan persentil ke-99 dari distribusi ini. Kemudian, \\(S_{X}\\left( \\pi_{0.99} \\right) = \\exp\\left\\{- \\left( \\frac{\\pi_{0.99}}{33.33} \\right)^{1.2}\\right\\} = 0.01.\\) Memecahkan untuk \\(π_{0,99}\\) kita mendapatkan \\(π_{0,99}=118,99\\). 3.2.4 Distribusi Beta Umum Jenis Kedua Generalized Beta Distribution of the Second Kind (GB2) diperkenalkan oleh Venter (1983) dalam konteks pemodelan kerugian asuransi dan oleh McDonald (1984) sebagai distribusi pendapatan dan kekayaan. Ini adalah distribusi empat parameter, sangat fleksibel, yang dapat memodelkan distribusi miring positif dan negatif. Variabel kontinu \\(X\\) dikatakan memiliki distribusi GB2 dengan parameter \\(σ\\), \\(θ\\), \\(a_1\\) dan \\(a_2\\) jika pdf-nya diberikan oleh : \\[\\begin{equation} f_{X}\\left( x \\right) = \\frac{(x/\\theta)^{\\alpha_2/\\sigma}}{x \\sigma~\\mathrm{B}\\left( \\alpha_1,\\alpha_2\\right)\\left\\lbrack 1 + \\left( x/\\theta \\right)^{1/\\sigma} \\right\\rbrack^{\\alpha_1 + \\alpha_2}} \\ \\ \\ \\text{for } x &gt; 0, \\tag{3.2} \\end{equation}\\] \\(\\sigma,\\theta,\\alpha_1,\\alpha_2 &gt; 0\\) dan dimana fungsi Beta \\(\\mathrm{B}\\left( \\alpha_1,\\alpha_2 \\right)\\) didefinisikan sebagai: \\[\\begin{equation} \\mathrm{B}\\left( \\alpha_1,\\alpha_2\\right) = \\int_{0}^{1}{t^{\\alpha_1 - 1}\\left( 1 - t \\right)^{\\alpha_2 - 1}}~ dt. \\end{equation}\\] GB2 menyediakan model untuk data berekor berat dan ringan. Ini termasuk eksponensial, gamma, Weibull, Burr, Lomax, F, chi-square, Rayleigh, lognormal dan log-logistik sebagai kasus khusus atau terbatas. Misalnya dengan mengatur parameter \\(σ=α_1=α_2=1\\), GB2 direduksi menjadi distribusi logistik log. Ketika \\(σ=1\\) dan \\(α_2→∞\\), ini direduksi menjadi distribusi gamma, dan ketika \\(α=1\\) dan \\(α_2→∞\\) itu direduksi menjadi distribusi Weibull. Variabel acak GB2 dapat dibangun sebagai berikut. Misalkan \\(G_1\\) dan \\(G_2\\) adalah variabel acak independen di mana \\(G_i\\) memiliki distribusi gamma dengan parameter bentuk \\(α_i\\) dan parameter skala 1. Kemudian, dapat ditunjukkan bahwa variabel acak \\(X=θ(\\frac{G_1}{G_2})^σ\\) memiliki distribusi GB2. Hasil teoritis ini memiliki beberapa implikasi. Sebagai contoh, ketika momen-momen itu ada, dapat ditunjukkan bahwa \\(k\\) momen mentah ke-th dari variabel acak terdistribusi GB2 diberikan oleh : \\(\\mathrm{E}\\left( X^{k} \\right) = \\frac{\\theta^{k}~\\mathrm{B}\\left( \\alpha_1 +k \\sigma,\\alpha_2 - k \\sigma \\right)}{\\mathrm{B}\\left( \\alpha_1,\\alpha_2 \\right)}, \\ \\ \\ k &gt; 0.\\) Seperti yang sudah dijelaskan, GB2 juga terkait dengan \\(F\\)-distribusi, hasil yang dapat berguna dalam simulasi dan analisis residual. Aplikasi GB2 sebelumnya adalah pada data pendapatan dan baru-baru ini telah digunakan untuk memodelkan data klaim berekor panjang. GB2 telah digunakan untuk memodelkan berbagai jenis klaim asuransi mobil, tingkat kerugian akibat kebakaran, serta data klaim asuransi kesehatan. 3.3 Methods of Creating New Distributions 3.3.1 Functions of Random Variables and their Distributions Sub bab ini membahas mengenai cara-cara untuk membuat distribusi probabilitas parametrik baru dari distribusi yang sudah ada. Secara khusus, misalkan X sebuah variabel acak kontinu dengan pdf (probability distribution function) yang diketahui \\(fX(x)\\) dan fungsi distribusi \\(FX(x)\\). Kemudian distribusi \\(Y = g(X)\\) , di mana \\(g(X)\\) adalah transformasi satu-ke-satu yang mendefinisikan variabel acak baru \\(Y\\) . Dengan demikian pada sub bab ini menerapkan teknik-teknik berikut untuk membuat keluarga distribusi baru: perkalian dengan sebuah konstanta pemangkatan, eksponensial, dan pencampuran. 3.3.2 Multiplication by a Constant Jika data klaim menunjukkan perubahan dari waktu ke waktu, maka transformasi tersebut dapat berguna untuk menyesuaikan inflasi. Jika tingkat inflasi positif maka biaya klaim meningkat, dan jika negatif maka biaya menurun. Untuk menyesuaikan dengan inflasi, maka mengalikan biaya X dengan 1+ tingkat inflasi (inflasi negatif adalah deflasi). Untuk memperhitungkan dampak mata uang terhadap biaya klaim, maka menggunakan transformasi untuk menerapkan konversi mata uang dari mata uang dasar ke mata uang lawan. Pertimbangkan transformasi \\(Y = c_X\\) , dimana \\(c&gt;0\\) , maka fungsi distribusi dari \\(Y\\) diberikan oleh \\[\\begin{equation} F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( cX \\leq y \\right) = \\Pr\\left( X \\leq \\frac{y}{c} \\right) = F_{X}\\left( \\frac{y}{c} \\right). \\end{equation}\\] Dengan menggunakan aturan rantai untuk diferensiasi, pdf bunga \\(f_Y(y)\\) dapat ditulis sebagai \\[\\begin{equation} f_{Y}\\left( y \\right) = \\frac{1}{c}f_{X}\\left( \\frac{y}{c} \\right). \\end{equation}\\] Misalkan X termasuk dalam himpunan distribusi parametrik tertentu dan mendefinisikan versi yang diskalakan \\(Y = c_X , c &gt; 0\\) . Jika \\(Y\\) berada dalam himpunan distribusi yang sama maka distribusi tersebut dikatakan sebagai distribusi skala. Ketika sebuah anggota dari distribusi skala dikalikan dengan sebuah konstanta \\(c\\) \\(( c&gt;0 )\\), parameter skala untuk distribusi skala ini memenuhi dua kondisi: Parameter diubah dengan mengalikan dengan c Semua parameter lainnya tetap tidak berubah Contoh 3.3.1. Pertanyaan Ujian Aktuaria. Kerugian Asuransi Mobil Eiffel dilambangkan dalam mata uang Euro dan mengikuti distribusi lognormal dengan \\(μ = 8\\) dan \\(σ = 2\\) . Mengingat bahwa 1 euro = 1,3 dolar, tentukan himpunan parameter lognormal yang menggambarkan distribusi kerugian Eiffel dalam dolar. solusi: Misalkan \\(X\\) dan \\(Y\\) menunjukkan total kerugian Eiffel Auto Insurance dalam mata uang euro dan dolar secara berturut-turut. Karena \\(Y = 1.3X\\), kita memiliki: \\[\\begin{equation} F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( 1.3X \\leq y \\right) = \\Pr\\left( X \\leq \\frac{y}{1.3} \\right) = F_{X}\\left( \\frac{y}{1.3} \\right). \\end{equation}\\] \\(X\\) mengikuti distribusi lognormal dengan parameter \\(μ=8\\) dan \\(σ=2\\). Fungsi kepadatan probabilitas (\\(pdf\\)) dari \\(X\\) diberikan oleh: \\[\\begin{equation} f_{X}\\left( x \\right) = \\frac{1}{x \\sigma \\sqrt{2\\pi}}\\exp \\left\\{- \\frac{1}{2}\\left( \\frac{\\log x - \\mu}{\\sigma} \\right)^{2}\\right\\} \\ \\ \\ \\text{for } x &gt; 0. \\end{equation}\\] Karena \\(\\left| \\frac{dx}{dy} \\right| = \\frac{1}{1.3}\\), \\(PDF\\) yang diinginkan \\(f_{Y}(y)\\) adalah: \\[\\begin{array}{ll} f_{Y}\\left( y \\right) &amp; = \\frac{1}{1.3}f_{X}\\left( \\frac{y}{1.3} \\right) \\\\ &amp;= \\frac{1}{1.3}\\frac{1.3}{y \\sigma \\sqrt{2\\pi}}\\exp \\left\\{- \\frac{1}{2}\\left( \\frac{\\log\\left( y/1.3 \\right) - \\mu}{\\sigma} \\right)^{2}\\right\\} \\\\ &amp;= \\frac{1}{y \\sigma\\sqrt{2\\pi}}\\exp \\left\\{- \\frac{1}{2}\\left( \\frac{\\log y - \\left( \\log 1.3 + \\mu \\right)}{\\sigma} \\right)^{2}\\right\\}. \\end{array}\\] Maka Y mengikuti distribusi lognormal dengan parameter \\(4log1.3+μ=8.26\\) dan \\(σ = 2.00\\). Jika \\(μ = log(m)\\), dengan mudah dapat dilihat bahwa \\(m = e^μ\\) adalah parameter skala yang dikalikan dengan 1,3 sedangkan σ adalah parameter bentuk yang tidak berubah. 3.3.3 Raising to a Power Pada Bagian 3.2.3, telah membahas tentang fleksibilitas distribusi Weibull dalam menyesuaikan data keandalan. Distribusi Weibull adalah transformasi pangkat dari distribusi eksponensial. Ini adalah aplikasi dari jenis transformasi lain yang melibatkan peningkatan variabel acak menjadi pangkat. Pertimbangkan transformasi \\(Y = X^τ\\) dengan \\(τ&gt;0\\) , maka fungsi distribusi dari \\(Y\\) diberikan oleh \\[\\begin{equation} F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( X^{\\tau} \\leq y \\right) = \\Pr\\left( X \\leq y^{1/ \\tau} \\right) = F_{X}\\left( y^{1/ \\tau} \\right). \\end{equation}\\] Oleh karena itu, \\(pdf\\) dari bunga \\(f_Y(y)\\) dapat ditulis sebagai \\[\\begin{equation} f_{Y}(y) = \\frac{1}{\\tau} y^{(1/ \\tau) - 1} f_{X}\\left( y^{1/ \\tau} \\right). \\end{equation}\\] Di sisi lain, jika \\(τ &lt; 0\\) maka fungsi distribusi dari \\(Y\\) diberikan oleh \\[\\begin{equation} F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( X^{\\tau} \\leq y \\right) = \\Pr\\left( X \\geq y^{1/ \\tau} \\right) = 1 - F_{X}\\left( y^{1/ \\tau} \\right), \\end{equation}\\] dan \\[\\begin{equation} f_{Y}(y) = \\left| \\frac{1}{\\tau} \\right|{y^{(1/ \\tau) - 1}f}_{X}\\left( y^{1/ \\tau} \\right). \\end{equation}\\] Contoh 3.3.3. Asumsikan bahwa \\(X\\) mengikuti distribusi eksponensial dengan rata-rata \\(θ\\) dan pertimbangkan variabel yang ditransformasi \\(Y = X^τ\\) . Tunjukkan bahwa \\(Y\\) mengikuti distribusi Weibull ketika \\(τ\\) positif dan tentukan parameter-parameter dari distribusi Weibull. solusi: Karena \\(X\\) mengikuti distribusi eksponensial dengan rata-rata \\(\\theta\\), kita memiliki: \\[\\begin{equation} f_{X}(x) = \\frac{1}{\\theta}e^{- x/ \\theta} \\ \\ \\ \\, x &gt; 0. \\end{equation}\\] Dalam menyelesaikan persamaan untuk \\(x\\), kita dapatkan \\(x = y^{1/\\tau}\\). Mengambil turunan, kita memiliki: \\[\\begin{equation} \\left| \\frac{dx}{dy} \\right| = \\frac{1}{\\tau}{y^{\\frac{1}{\\tau}-1}}. \\end{equation}\\] maka \\[\\begin{equation} f_{Y}\\left( y \\right) = \\frac{1}{\\tau}{y^{\\frac{1}{\\tau} - 1}f}_{X}\\left( y^{\\frac{1}{\\tau}} \\right) \\\\ = \\frac{1}{\\tau \\theta }y^{\\frac{1}{\\tau} - 1}e^{- \\frac{y^{\\frac{1}{\\tau}}}{\\theta}} = \\frac{\\alpha}{\\beta}\\left( \\frac{y}{\\beta} \\right)^{\\alpha - 1}e^{- \\left( y/ \\beta \\right)^{\\alpha}}. \\end{equation}\\] di mana \\(α = 1/τ\\) dan \\(β = θ^τ\\). Kemudian, \\(Y\\) mengikuti distribusi Weibull dengan parameter bentuk \\(α\\) dan parameter skala \\(β\\) . 3.3.4 Exponentiation Distribusi normal adalah model yang sangat populer untuk sejumlah besar aplikasi ketika ukuran sampel besar, distribusi ini dapat berfungsi sebagai distribusi perkiraan untuk model lainnya. Jika variabel acak X memiliki distribusi normal dengan rata-rata \\(μ\\) dan varians \\(σ^2\\) maka \\(Y = e^X\\) memiliki distribusi lognormal dengan parameter \\(μ\\) dan \\(σ^2\\) . Variabel acak lognormal memiliki batas bawah nol, condong ke kanan, dan memiliki ekor kanan yang panjang. Distribusi lognormal biasanya digunakan untuk menggambarkan distribusi aset keuangan seperti harga saham. Distribusi ini juga digunakan untuk menyesuaikan jumlah klaim untuk asuransi mobil dan kesehatan. Ini adalah contoh jenis transformasi lain yang melibatkan eksponensial. Secara umum, pertimbangkan transformasi \\(Y = e^X\\) . Kemudian, fungsi distribusi dari \\(Y\\) diberikan oleh \\[\\begin{equation} F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( e^{X} \\leq y \\right) = \\Pr\\left( X \\leq \\log y \\right) = F_{X}\\left( \\log y \\right). \\end{equation}\\] Dengan mengambil turunan bahwa pdf bunga \\(f_Y(y)\\) dapat ditulis sebagai \\[\\begin{equation} f_{Y}(y) = \\frac{1}{y}f_{X}\\left( \\log y \\right). \\end{equation}\\] Sebagai kasus khusus yang penting, misalkan \\(X\\) berdistribusi normal dengan rata-rata \\(μ\\) dan varians \\(σ^2\\) . Maka, distribusi dari \\(Y = e^X\\) adalah \\[\\begin{equation} f_{Y}(y) = \\frac{1}{y}f_{X}\\left( \\log y \\right) = \\frac{1}{y \\sigma \\sqrt{2 \\pi}} \\exp \\left\\{-\\frac{1}{2}\\left(\\frac{ \\log y - \\mu}{\\sigma}\\right)^2\\right\\}. \\end{equation}\\] Ini dikenal sebagai distribusi lognormal. Contoh 3.3.4. Pertanyaan Ujian Aktuaria. Asumsikan bahwa \\(X\\) memiliki distribusi seragam pada interval \\((0, c)\\) dan mendefinisikan \\(Y = e^X\\) . Tentukan distribusi dari \\(Y\\). solusi: Kita mulai dengan fungsi distribusi kumulatif (\\(cdf\\)) dari \\(Y\\), \\[\\begin{equation} F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( e^{X} \\leq y \\right) = \\Pr\\left( X \\leq \\log y \\right) = F_{X}\\left( \\log y \\right). \\end{equation}\\] Mengambil turunan, kita memperoleh: \\[\\begin{equation} f_{Y}\\left( y \\right) = \\frac{1}{y}f_{X}\\left(\\log y \\right) = \\frac{1}{cy} . \\end{equation}\\] karena \\(0 &lt; x &lt; c\\), maka \\(1 &lt; y &lt; e^{c}\\). 3.3.5 Finite Mixtures Distribusi campuran merupakan cara yang berguna untuk memodelkan data yang diambil dari populasi yang heterogen. Populasi induk ini dapat dianggap dibagi menjadi beberapa subpopulasi dengan distribusi yang berbeda. 3.3.5.1 Two-point Mixture Jika fenomena yang mendasari beragam dan sebenarnya dapat digambarkan sebagai dua fenomena yang mewakili dua subpopulasi de_ngan modus yang berbeda, dapat membangun variabel acak campuran dua titik \\(X\\) . Diberikan variabel acak \\(X_1\\) dan \\(X_2\\) dengan pdf \\(fX_1(x)\\) dan \\(fX_2(x)\\) masing-masing, pdf dari \\(X\\) adalah rata-rata tertimbang dari komponen pdf \\(fX_1(x)\\) dan \\(fX_2(x)\\). Pdf dan fungsi distribusi dari \\(X\\) diberikan oleh \\[\\begin{equation} f_{X}\\left( x \\right) = af_{X_{1}}\\left( x \\right) + \\left( 1 - a \\right)f_{X_{2}}\\left( x \\right), \\end{equation}\\] dan \\[\\begin{equation} F_{X}\\left( x \\right) = aF_{X_{1}}\\left( x \\right) + \\left( 1 - a \\right)F_{X_{2}}\\left( x \\right), \\end{equation}\\] untuk \\(0&lt;a&lt;1\\) , dengan parameter pencampuran \\(a\\) dan \\((1-a)\\) masing-masing mewakili proporsi titik data yang termasuk dalam masing-masing dua subpopulasi. Rata-rata tertimbang ini dapat diterapkan pada sejumlah besaran terkait distribusi lainnya. Momen mentah ke-k dan fungsi pembangkit momen dari \\(X\\) diberikan oleh \\[\\begin{equation} \\mathrm{E}\\left( X^{k} \\right) = a\\mathrm{E}\\left( X_{1}^{K} \\right) + \\left( 1 - a \\right)\\mathrm{E}\\left( X_{2}^{k} \\right) \\end{equation}\\] dan \\[\\begin{equation} M_{X}(t) = aM_{X_{1}}(t) + \\left( 1 - a \\right)M_{X_{2}}(t), \\end{equation}\\] masing-masing. Contoh 3.3.5. Pertanyaan Ujian Aktuaria. Kumpulan polis asuransi terdiri dari dua jenis. 25% polis adalah Tipe 1 dan 75% polis adalah Tipe 2. Untuk polis Tipe 1, jumlah kerugian per tahun mengikuti distribusi eksponensial dengan rata-rata 200, dan untuk polis Tipe 2, jumlah kerugian per tahun mengikuti distribusi Pareto dengan parameter \\(α = 3\\) dan \\(θ = 200\\) . Untuk sebuah polis yang dipilih secara acak dari seluruh kumpulan kedua jenis polis tersebut, tentukan probabilitas bahwa kerugian tahunan akan kurang dari 100, dan tentukan rata-rata kerugiannya. solusi: Dua jenis kerugian tersebut adalah variabel acak \\(X_1\\) dan \\(X_2\\). \\(X_1\\) memiliki distribusi eksponensial dengan rata-rata 100, sehingga \\(F_{X_1}\\left(100\\right)=1-e^{-\\frac{100}{200}}=0.393\\). \\(X_2\\) memiliki distribusi Pareto dengan parameter \\(\\alpha=3\\) dan \\(\\theta=200\\), sehingga \\(F_{X_1}\\left(100\\right)=1-\\left(\\frac{200}{100+200}\\right)^3=0.704\\). Oleh karena itu, \\(F_X\\left(100\\right)=\\left(0.25\\times0.393\\right)+\\left(0.75\\times0.704\\right)=0.626\\). Kerugian rata-rata diberikan oleh \\[\\begin{equation} \\mathrm{E}\\left(X\\right)=0.25\\mathrm{E}\\left(X_1\\right)+0.75\\mathrm{E}\\left(X_2\\right)=\\left(0.25\\times200\\right)+\\left(0.75\\times100\\right)=125 \\end{equation}\\] 3.3.5.2 k-point Mixture Dalam kasus distribusi campuran berhingga, variabel acak yang diminati \\(X\\) memiliki probabilitas \\(p_i\\) untuk terambil dari subpopulasi homogen \\(i\\) dengan \\(i = 1,2,...,k\\) dan \\(k\\) adalah jumlah subpopulasi yang ditentukan pada awalnya dalam campuran. Parameter pencampuran \\(p_i\\) merepresentasikan proporsi observasi dari subpopulasi \\(i\\) . Pertimbangkan variabel acak \\(X\\) yang dihasilkan dari k subpopulasi yang berbeda, di mana subpopulasi \\(i\\) dimodelkan dengan distribusi kontinu \\(fX_i(x)\\) . Distribusi probabilitas dari \\(X\\) diberikan oleh \\[\\begin{equation} \\mathrm{E}\\left(X\\right)=0.25\\mathrm{E}\\left(X_1\\right)+0.75\\mathrm{E}\\left(X_2\\right)=\\left(0.25\\times200\\right)+\\left(0.75\\times100\\right)=125 \\end{equation}\\] dimana \\(0 &lt; p_{i} &lt; 1\\) dan \\(\\sum_{i = 1}^{k} p_{i} = 1\\) Model ini sering disebut sebagai campuran terbatas atau campuran k-point mixture. Fungsi distribusi, r momen mentah ke-k dan fungsi pembangkit momen dari k-point mixture ke-k diberikan sebagai \\[\\begin{equation} F_{X}\\left( x \\right) = \\sum_{i = 1}^{k}{p_{i}F_{X_{i}}\\left( x \\right)}, \\end{equation}\\] \\[\\begin{equation} F_{X}\\left( x \\right) = \\sum_{i = 1}^{k}{p_{i}F_{X_{i}}\\left( x \\right)}, \\end{equation}\\] \\[\\begin{equation} M_{X}(t) = \\sum_{i = 1}^{k}{p_{i}M_{X_{i}}(t)}, \\end{equation}\\] masing-masing Contoh 3.3.6. Pertanyaan Ujian Aktuaria. \\(Y_1\\) adalah campuran dari \\(X_1\\) dan \\(X_2\\) dengan bobot-bobot pencampuran \\(a\\) dan \\((1-a)\\). \\(Y_2\\) adalah campuran dari \\(X_3\\) dan \\(X_4\\) dengan bobot pencampuran \\(b\\) dan \\((1-b)\\). \\(Z\\) adalah campuran dari \\(Y_1\\) dan \\(Y_2\\) dengan bobot pencampuran \\(c\\) dan \\((1-c)\\). Tunjukkan bahwa \\(Z\\) adalah campuran dari \\(X_1, X_2, X_3 dan X_4\\) dan tentukan bobot pencampurannya. solusi: Dengan menerapkan rumus untuk distribusi campuran (mixed distribution), kita dapatkan: \\[\\begin{equation} f_{Y_{1}}\\left( x \\right) = af_{X_{1}}\\left( x \\right) + \\left( 1 - a \\right)f_{X_{2}}\\left( x \\right) \\end{equation}\\] \\[\\begin{equation} f_{Y_{2}}\\left( x \\right) = bf_{X_{3}}\\left( x \\right) + \\left( 1 - b \\right)f_{X_{4}}\\left( x \\right) \\end{equation}\\] \\[\\begin{equation} f_{Z}\\left( x \\right) = cf_{Y_{1}}\\left( x \\right) + \\left( 1 - c \\right)f_{Y_{2}}\\left( x \\right) \\end{equation}\\] Dengan menggantikan persamaan pertama dan kedua ke dalam persamaan ketiga, kita dapatkan: \\[\\begin{equation} f_{Z}\\left( x \\right) = c\\left\\lbrack af_{X_{1}}\\left( x \\right) + \\left( 1 - a \\right)f_{X_{2}}\\left( x \\right) \\right\\rbrack + \\left( 1 - c \\right)\\left\\lbrack bf_{X_{3}}\\left( x \\right) + \\left( 1 - b \\right)f_{X_{4}}\\left( x \\right) \\right\\rbrack \\end{equation}\\] \\[\\begin{equation} = caf_{X_{1}}\\left( x \\right) + c\\left( 1 - a \\right)f_{X_{2}}\\left( x \\right) + \\left( 1 - c \\right)bf_{X_{3}}\\left( x \\right) + (1 - c)\\left( 1 - b \\right)f_{X_{4}}\\left( x \\right) \\end{equation}\\] Kemudian, \\(Z\\) adalah campuran dari \\(X_1\\), \\(X_2\\), \\(X_3\\), dan \\(X_4\\), dengan bobot campuran \\(ca\\), \\(c(1−a)\\), \\((1−c)b\\), dan \\((1−c)(1−b)\\) secara berturut-turut. Mudah dilihat bahwa jumlah bobot campuran tersebut adalah satu. 3.3.6 Continuous Mixtures Campuran dengan jumlah subpopulasi yang sangat banyak (k menuju tak terhingga) sering disebut sebagai campuran kontinu. Dalam campuran kontinu, subpopulasi tidak dibedakan oleh parameter pencampuran diskrit tetapi oleh variabel kontinu \\(Θ\\) dimana \\(Θ\\) memainkan peran sebagai \\(p_i\\) dalam campuran berhingga. Pertimbangkan variabel acak \\(X\\) dengan distribusi yang bergantung pada parameter \\(Θ\\) , dimana \\(Θ\\) itu sendiri adalah variabel acak kontinu. Deskripsi ini menghasilkan model berikut untuk \\(X\\). \\[\\begin{equation} f_{X}\\left( x \\right) = \\int_{-\\infty}^{\\infty}{f_{X}\\left(x \\left| \\theta \\right. \\right)g_{\\Theta}( \\theta )} d \\theta , \\end{equation}\\] di mana \\(f_X(x|θ)\\) adalah distribusi bersyarat dari \\(X\\) pada nilai tertentu dari \\(Θ = θ\\) dan \\(g_Θ(θ)\\) adalah pernyataan probabilitas yang dibuat tentang parameter \\(θ\\) yang tidak diketahui . Dalam konteks Bayesian (dijelaskan pada Bagian 4.4), hal ini dikenal sebagai distribusi prior dari \\(Θ\\) (informasi sebelumnya atau pendapat ahli yang akan digunakan dalam analisis). Fungsi distribusi, k momen mentah ke-k dan fungsi pembangkit momen dari campuran kontinu diberikan sebagai \\[\\begin{equation} F_{X}\\left( x \\right) = \\int_{-\\infty}^{\\infty}{F_{X}\\left(x \\left| \\theta \\right. \\right) g_{\\Theta}(\\theta)} d \\theta, \\end{equation}\\] \\[\\begin{equation} \\mathrm{E}\\left( X^{k} \\right) = \\int_{-\\infty}^{\\infty}{\\mathrm{E}\\left( X^{k}\\left| \\theta \\right. \\right)g_{\\Theta}(\\theta)}d \\theta, \\end{equation}\\] \\[\\begin{equation} M_{X}(t) = \\mathrm{E}\\left( e^{t X} \\right) = \\int_{-\\infty}^{\\infty}{\\mathrm{E}\\left( e^{ tx}\\left| \\theta \\right. \\right)g_{\\Theta}(\\theta)}d \\theta, \\end{equation}\\] masing-masing Momen mentah ke-k ke-k dari distribusi campuran dapat ditulis ulang sebagai \\[\\begin{equation} \\mathrm{E}\\left( X^{k} \\right) = \\int_{-\\infty}^{\\infty}{\\mathrm{E}\\left( X^{k}\\left| \\theta \\right. \\right)g_{\\Theta}(\\theta)}d\\theta ~=~ \\mathrm{E}\\left\\lbrack \\mathrm{E}\\left( X^{k}\\left| \\Theta \\right. \\right) \\right\\rbrack . \\end{equation}\\] Dengan menggunakan hukum ekspektasi berulang (lihat Lampiran Bab 16), dapat mendefinisikan rata-rata dan varians dari \\(X\\) sebagai \\[\\begin{equation} \\mathrm{E}\\left( X \\right) = \\mathrm{E}\\left\\lbrack \\mathrm{E}\\left( X\\left| \\Theta \\right. \\right) \\right\\rbrack \\end{equation}\\] dan \\[\\begin{equation} \\mathrm{Var}\\left( X \\right) = \\mathrm{E}\\left\\lbrack \\mathrm{Var}\\left( X\\left| \\Theta \\right. \\right) \\right\\rbrack + \\mathrm{Var}\\left\\lbrack \\mathrm{E}\\left( X\\left| \\Theta \\right. \\right) \\right\\rbrack . \\end{equation}\\] Contoh 3.3.7. Pertanyaan Ujian Aktuaria. \\(X\\) memiliki distribusi normal dengan mean sebesar \\(Λ\\) sebesar 1 dan variansi sebesar 1. \\(Λ\\) memiliki distribusi normal dengan mean 1 dan varians 1. Tentukan mean dan varians dari \\(X\\) . solusi: X adalah campuran kontinu dengan rata-rata \\[\\begin{equation} \\mathrm{E}\\left(X\\right)=\\mathrm{E}\\left[\\mathrm{E}\\left(X\\middle|\\Lambda\\right)\\right]=\\mathrm{E}\\left(\\Lambda\\right)=1 \\text{ and } \\mathrm{V}\\left(X\\right)=\\mathrm{V}\\left[\\mathrm{E}\\left(X\\middle|\\Lambda\\right)\\right]+\\mathrm{E}\\left[\\mathrm{V}\\left(X\\middle|\\Lambda\\right)\\right]=\\mathrm{V}\\left(\\Lambda\\right)+\\mathrm{E}\\left(1\\right)=1+1=2. \\end{equation}\\] 3.4 modifikasi pertanggungan Coverage modifications atau modifikasi pertanggungan adalah perubahan yang dibuat pada syarat dan ketentuan polis asuransi. Perubahan ini dapat diprakarsai oleh pemegang polis atau perusahaan asuransi, dan dirancang untuk mengubah pertanggungan yang diberikan oleh polis. Modifikasi pertanggungan dapat dilakukan karena berbagai alasan. Sebagai contoh, pemegang polis mungkin ingin meningkatkan batas pertanggungan pada polis mereka untuk melindungi diri mereka sendiri dari potensi kerugian. Atau, mereka mungkin ingin menambah atau menghapus jenis pertanggungan tertentu, seperti menambahkan asuransi banjir pada polis pemilik rumah atau menghapus pertanggungan tabrakan dari polis mobil. pada bagian ini membahas mengenai 3.4.1 policy deductibles pada polis deductible biasa, pemegang polis setuju untuk menanggung sejumlah klaim asuransi sebelum perusahaan asuransi membayarkan klaim. Sehingga bagian kerugian yang ditanggung dan menjadi tanggung jawab pemegang polis untuk membayar deductible dengan uang mereka sendiri. sebagai contoh jika sebuah polis memiliki daductible sebesar Rp.500 dan pemegang polis mengalami kerugian dengan biaya sebesar Rp.2500, maka perusahaan asuransi hanya akan membayar Rp.2000 (yaitu total biaya perbaikan dikurangi deductible Rp.500) deductible sendiri di notasikan dengan \\(d\\), maka jika kerugian melebihi \\(d\\) atau nilai deductible, maka perusahaan asuransi bertanggung jawab untuk menanggung total kerugian dikurangin dengan deductible atau \\(d\\) tergantung dengan perjanjiannya, deductible dapat berlaku untuk setiap kerugian atau total dari seluruh kerugian. jumlah dari deductible biasanya dipilih pada saat pemegang polis membeli polis dan disesuaikan dengan kebutuhannya selama masa berlaku polis. deductible yang lebih tinggi akan menghasilkan pembayaran premi yang lebih rendah, dikarenakan pemegang polis menanggung lebih banyak saat terjadinya kerugian. lalu jika \\(X\\) di notasikan sebagai kerugian yang diterima oleh pemegang polis dan \\(Y\\) dinotasikan sebagai jumlah klaim yang dibayarkan oleh perusahaan asuransi, maka ada dua variabel berdasarkan pembayarannya kepada pemegang polis. a. pembayaran per kerugian b. pembayaran per pembayaran pada variabel perbayaran per kerugian, dinotasikan sebagai \\(Y^L\\) atau \\((X-d)_+\\) atau left censor, atau ketika jumlah atau total kerugian yang dialami kurang dari deductible, maka dinilai sama dengan 0 atau tidak dilakukan pembayaran. maka variabel ini didefinisikan sebagai \\[ Y^{L} = \\left( X - d \\right)_{+} = \\left\\{ \\begin{array}{cc} 0 &amp; X \\le d, \\\\ X - d &amp; X &gt; d \\end{array} \\right. . \\] disisi lain, variabel pembayaran per pembayaran dinotasikan sebagai \\(Y^P\\) didefinisikan ketika hanya terjadinya pembayaran, terutama \\(Y^P\\) sama dengan \\(X-d\\) dengan syarat \\([X&gt;d]\\), atau dinotasikan sebagai \\(Y^P=X-d||X&gt;d\\) atau dituliskan sebagai \\[ Y^{P} = \\left\\{ \\begin{matrix} \\text{Undefined} &amp; X \\le d \\\\ X - d &amp; X &gt; d . \\end{matrix} \\right. \\] disini \\(Y^P\\) disebut juga sebagai left truncated atau variabel kerugian berlebih, karena klaim yang lebih kecil dari \\(d\\) tidak dilaporkan dan nilai dari \\(d\\) berubah sebesar \\(d\\) ketika nilai distribusi dari nilai kerugian bersifat kontinu, namun distribusi dari \\(Y^L\\) adalah gabungan kombinasi dari komponen nilai diskrit dan kontinu. bagian diskrit terletak pada \\(Y=0\\) atau saat \\((X \\leq d)\\) dan komponen nilai kontinu terletak pada interval \\(Y&gt;0\\) atau saat \\(X&gt;d\\) 3.4.2 Policy Limit policy limit atau batas polis adalah bentuk jumlah maksimum yang dibayarkan oleh perusahaan asuransi untuk pertanggungan tertentu berdasarkan polis asuransinya. sehingga kerugian yang ditanggung dinotasikan sebagai \\(X\\), dan batas pertanggunannya atau batas polisnya dinotasikan sebagai \\(u\\), jika kerugian melebihi batas polis \\(X-u\\) harus dibayar oleh pemegang polis sendiri. batas polis yang lebih tinggi berarti premi yang dibayar oleh pemegang polis semakin besar. sebagai contoh sebuah polis mungkin memiliki batas pertanggungan sebesar Rp100.000 per kejadian, yang berarti bahwa perusahaan asuransi tidak akan membayar lebih dari Rp100.000 untuk setiap klaim atau tanggung jawab yang menjadi bagian dari polis. dimana biaya kerugian pemegang polis dinotasikan sebagai \\(X\\) dan klaim yang dibayarkan oleh perusahaan asuransi dinotasikan sebagai \\(Y\\), dan variabel policy limit dinotasikan sebagai \\(X \\land u\\). atau disebut sebagai right censored variable dikarenakan nilai dari \\(u\\) di set sama dengan \\(u\\). maka variabel \\(Y\\) didefinisikan sebagai \\[ Y = X \\land u = \\left\\{ \\begin{matrix} X &amp; X \\leq u \\\\ u &amp; X &gt; u. \\\\ \\end{matrix} \\right.\\ \\] pada batas polis, perbedaan antara \\(Y^L\\) dan \\(Y^P\\) tidak dibutuhkan dikarenakan perusahaan asuransi akan selalu melakukan pembayaran. dengan \\((X-u)\\) dan \\((X \\land u)\\) maka expektasi dari pembayaran terjadi tanpa modifikasi pertangguangan \\(X\\). jumlah ekspektasi pembayaran dari deductible \\(u\\) dan limit \\(u\\) maka, \\(X=(X-u)_++(X\\land u)\\) jika kerugian merupakan subjek dari deductible \\(d\\) dan limit \\(u\\), maka didefinisikan sebagai \\[ Y^{L} = \\left\\{ \\begin{matrix} 0 &amp; X \\leq d \\\\ X - d &amp; d &lt; X \\leq u \\\\ u - d &amp; X &gt; u. \\\\ \\end{matrix} \\right.\\ \\] maka, \\(Y^L\\) dapat dinyatakan sebagai \\(Y^L=(X\\land u)-(X\\land u)\\). 3.4.3 policy deductible and policy limit pada policy deductible atau pengurangan polis, jika kerugian yang dialami oleh pemegang polis kurang dari nilai deductible maka perusahaan asuransi tidak akan membayarkan kerugian tersebut, dan jika lebih besar dari nilai deductible maka klaim yang dibayarkan merupakan total dari kerugian dikurangi dengan nilai deductible, sehingga sisa biaya kerugian ditanggung pemegang polis. semakin besar nilai deductible maka besar premi yang perlu dibayarkan oleh pemegang polis semakin rendah pada policy limit atau pembatasaan polis, jika kerugian yang dialami oleh pemegang polis lebih besar dari nilai limit maka perusahaan asuransi tidak akan membayarkan kerugian tersebut, dan jika masih dibawah dari batas limit maka perusahaan asuransi akan selalu membayarkan total kerugian tersebut. akan tetapi jika lebih besar dari limit sisa biaya kerugian ditanggung pemegang polis. semakin besar nilai limit maka besar premi yang dibayarkan oleh pemegang polis semakin besar 3.4.4 Coinsurance and inflation coinsurance atau koasuransi adalah jenis pengaturan asuransi di mana dua atau lebih perusahaan asuransi berbagi risiko yang terkait dengan satu polis. Dalam pengaturan koasuransi, setiap perusahaan asuransi mengasumsikan sebagian risiko yang terkait dengan polis dan bertanggung jawab untuk membayar bagian proporsional dari setiap klaim yang muncul. Coinsurance sering digunakan pada asuransi properti dan asuransi kecelakaan, di mana besarnya risiko dapat melebihi kapasitas penanggung tunggal untuk menanggungnya. pada Policy Deductibles jumlah kerugian yang ditanggung oleh pemegang polis sampai dengan nilai dari deductible \\(d\\). kerugian yang dapat ditanggung juga dapat berupa presentase dari klaim. presentase \\(\\alpha\\) sering disebut sebagai faktor koasuransi. jika polis merupakan subjek dari deductible dan limit polis, maka koasuransi mengacu pada presentase klaim yang harus ditanggung oleh perusahaan asuransi. setelah dilakukan deductible dan limit pada polis maka, variabel pembayaran per kerugiaan atau \\(Y^L\\) didefinisikan sebagai: \\[ Y^{L} = \\left\\{ \\begin{matrix} 0 &amp; X \\leq d, \\\\ \\alpha\\left( X - d \\right) &amp; d &lt; X \\leq u, \\\\ \\alpha\\left( u - d \\right) &amp; X &gt; u. \\\\ \\end{matrix} \\right.\\ \\] jumlah maksimum yang dapat dibayarkan oleh perusahaan asuransi adalah \\(\\alpha (u-d)\\), dimana u adalah maksimum klaim yang dibayarkan dan pada Policy limit ketika kerugian merupakan subjek pada deductible \\(d\\) dan limit \\(u\\) untuk variabel per kerugian atau \\(Y^L\\), maka dapat dinyatakan sebagai \\(Y^L=(X\\land u)-(X\\land d)\\), maka pada koasuransi \\(Y^L\\) dapat dinyatakan sebagai\\(Y^L=\\alpha[(X\\land u)-(X\\land d)]\\). 3.4.5 Reinsurance Reinsurance atau Reasuransi adalah jenis asuransi yang digunakan perusahaan asuransi untuk mengalihkan sebagian risiko yang telah mereka tanggung dalam menjamin polis asuransi kepada perusahaan asuransi lain. Dalam pengaturan reasuransi, perusahaan asuransi menyerahkan sebagian risiko yang terkait dengan polis atau portofolio polis kepada perusahaan reasuransi, yang mengasumsikan risiko tersebut dengan imbalan sebagian premi yang dibayarkan oleh pemegang polis. Reasuransi biasanya digunakan oleh perusahaan asuransi untuk melindungi diri mereka sendiri dari kerugian akibat bencana atau untuk mengelola eksposur mereka terhadap risiko di lini bisnis tertentu. pada Policy deductible berdasarkan polis tersebut, pemegang polis harus membayar semua kerugian hingga batas nilai deductible, dan perusahaan asuransi hanya membayar jumlah (jika ada) di atas batas nilai deductible. terdapat peraturan dimana di mana perusahaan asuransi mengalihkan sebagian risiko polis dengan mendapatkan pertanggungan dari perusahaan asuransi lain dengan membayarkan juga premi asuransi. Reasuransi adalah pengaturan kontrak di mana perusahaan asuransi mengalihkan sebagian dari risiko yang diasuransikan dengan mendapatkan pertanggungan dari perusahaan asuransi lain dengan imbalan premi reasuransi. Dalam kontrak tersebut, penanggung utama atau perusahaan asuransi awal harus melakukan semua pembayaran yang diperlukan kepada pemegang polis hingga total pembayaran penanggung utama mencapai deductible reasuransi yang telah ditetapkan. lalu Perusahaan asuransi lainnya kemudian hanya bertanggung jawab untuk membayar kerugian di atas deductible reasuransi. Jumlah maksimum yang dipertahankan oleh penanggung utama dalam perjanjian reasuransi disebut retensi. 3.4.6 Coinsurance and Reinsurance Perbedaan utama antara reasuransi dan koasuransi adalah arah pengalihan risiko. Dalam pengaturan reasuransi, perusahaan asuransi mengalihkan risiko kepada perusahaan asuransi lain, sedangkan dalam pengaturan koasuransi, beberapa perusahaan asuransi berbagi risiko yang terkait dengan satu polis. Selain itu, reasuransi biasanya digunakan untuk melindungi penanggung dari kerugian akibat bencana atau untuk mengelola eksposur risiko mereka, sementara koasuransi sering digunakan untuk memungkinkan penanggung menanggung polis yang lebih besar daripada yang dapat mereka tangani sendiri. 3.5 Maximum Likelihood Estimation 3.5.1 Maximum Likelihood Estimators for Complete Data Hingga saat ini, bab ini berfokus pada distribusi parametrik yang biasa digunakan dalam aplikasi asuransi. Namun, agar berguna dalam pekerjaan terapan, distribusi ini harus menggunakan nilai “realistis” untuk parameter dan untuk ini kita beralih ke data. Pada tingkat dasar, kami berasumsi bahwa analis telah menyediakan sampel acak \\(X_1,... , X_n\\) dari distribusi dengan fungsi distribusi \\(F_x\\) (untuk singkatnya, kita terkadang menjatuhkan subskrip \\(X\\)). Vektor \\(θ\\) untuk menunjukkan kumpulan parameter untuk \\(F\\). Sebelum menggambar dari distribusi, kami mempertimbangkan hasil potensial yang dirangkum oleh variabel acak \\(X_i\\) (\\(i\\) = 1,2,..,n). Dari Bab sebelumnya yang membahas perkiraan distribusi frekuensi, dimana \\(Pr(X_1 = x_1,...,X_n = x_n)\\) untuk mengukur “kemungkinan” menggambar sampel \\({x_1,...,x_n}\\) Dengan data kontinu, kami menggunakan fungsi kerapatan probabilitas bersama alih-alih probabilitas bersama. Dengan asumsi independensi, pdf bersama dapat ditulis sebagai produk pdf. Dengan demikian, kami mendefinisikan kemungkinannya menjadi \\[\\begin{equation} L(\\boldsymbol \\theta) = \\prod_{i=1}^n f(x_i) . \\end{equation}\\] Dari notasi, perhatikan bahwa kami menganggap ini sebagai fungsi dari parameter di \\(θ\\), dengan data \\({x_1,...,x_n}\\) iadakan tetap. Penaksir kemungkinan maksimum adalah nilai parameter di \\(θ\\) yang memaksimalkan \\(L(θ)\\). Dari kalkulus, kita tahu bahwa memaksimalkan fungsi menghasilkan hasil yang sama dengan memaksimalkan logaritma suatu fungsi (ini karena logaritma adalah fungsi monoton). Karena kita mendapatkan hasil yang sama, untuk memudahkan pertimbangan komputasi, adalah umum untuk mempertimbangkan kemungkinan logaritmik, yang dilambangkan sebagai \\[\\begin{equation} l(\\boldsymbol \\theta) = \\log L(\\boldsymbol \\theta) = \\sum_{i=1}^n \\log f(x_i) . \\end{equation}\\] Contoh 3.5.1. Soal Ujian Aktuaria. Anda diberikan lima pengamatan berikut: 521, 658, 702, 819, 1217. Anda menggunakan Pareto parameter tunggal dengan fungsi distribusi: \\[F(x) = 1- \\left(\\frac{500}{x}\\right)^{\\alpha}, ~~~~ x&gt;500 .\\] Dengan \\(n = 5\\), fungsi log-likelihood adalah \\[l(\\alpha) = \\sum_{i=1}^5 \\log f(x_i;\\alpha ) = 5 \\alpha \\log 500 + 5 \\log \\alpha-(\\alpha+1) \\sum_{i=1}^5 \\log x_i.\\] Gambar dibawah menunjukkan kemungkinan logaritmik sebagai fungsi parameter \\(α\\) Kita dapat menentukan nilai maksimum kemungkinan logaritmik dengan mengambil turunan dan mengaturnya sama dengan nol. Ini menghasilkan \\[\\begin{array}{ll} \\frac{ \\partial}{\\partial \\alpha } l(\\alpha ) &amp;= 5 \\log 500 + 5 / \\alpha - \\sum_{i=1}^5 \\log x_i =_{set} 0 \\Rightarrow \\\\ \\hat{\\alpha}_{MLE} &amp;= \\frac{5}{\\sum_{i=1}^5 \\log x_i - 5 \\log 500 } = 2.453 . \\end{array}\\] Secara alami, ada banyak masalah di mana tidak praktis menggunakan perhitungan tangan untuk optimasi. Untungnya ada banyak rutinitas statistik yang tersedia seperti fungsi .R optim c1 &lt;- log(521)+log(658)+log(702)+log(819)+log(1217) nloglike &lt;- function(alpha){-(5*alpha*log(500)+5*log(alpha)-(alpha+1)*c1)} MLE &lt;- optim(par=1, fn=nloglike)$par ## Warning in optim(par = 1, fn = nloglike): one-dimensional optimization by Nelder-Mead is unreliable: ## use &quot;Brent&quot; or optimize() directly MLE ## [1] 2.453125 \\(2.453125\\) mengkonfirmasi hasil perhitungan tangan kita di mana penaksir kemungkinan maksimum Contoh 3.5.4. Dana Properti Wisconsin. Untuk melihat bagaimana penaksir kemungkinan maksimum bekerja dengan data nyata, kami kembali ke data klaim 2010 Cuplikan kode berikut menunjukkan cara menyesuaikan eksponensial, gamma, Pareto, lognormal, dan \\(GB2\\) Model. Untuk konsistensi, kode menggunakan package R dan VGAM. Akronim adalah singkatan dari Vector Generalized Linear and Additive Models; Seperti yang disarankan oleh namanya, paket ini dapat melakukan jauh lebih dari sesuai dengan model-model ini meskipun cukup untuk tujuan kita. Satu-satunya pengecualian adalah GB2 kepadatan yang tidak banyak digunakan di luar aplikasi asuransi; Namun, kita dapat mengkodekan kepadatan ini dan menghitung penaksir kemungkinan maksimum menggunakan optim General Purpose Optimizer. #library(VGAM) #claim_lev &lt;- read.csv(&quot;CLAIMLEVEL.csv&quot;, header = TRUE) #claim_data &lt;- subset(claim_lev, Year == 2010); # #Inference assuming a GB2 Distribution - this is more complicated # #The likelihood function of GB2 distribution (negative for optimization) #lik_gb2 &lt;- function (param) { # a_1 &lt;- param[1] # a_2 &lt;- param[2] # mu &lt;- param[3] # sigma &lt;- param[4] # yt &lt;- (log(claim_data$Claim) - mu) / sigma # logexpyt &lt;- ifelse(yt &gt; 23, yt, log(1 + exp(yt))) # logdens &lt;- a_1 * yt - log(sigma) - log(beta(a_1,a_2)) - # (a_1+a_2) * logexpyt - log(claim_data$Claim) # return(-sum(logdens)) #} # &quot;optim&quot; is a general purpose minimization function #gb2_bop &lt;- optim(c(1, 1, 0, 1), lik_gb2, method = c(&quot;L-BFGS-B&quot;), # lower = c(0.01, 0.01, -500, 0.01), # upper = c(500, 500, 500, 500), hessian = TRUE) # #Nonparametric Plot #plot(density(log(claim_data$Claim)), main = &quot;&quot;, xlab = &quot;Log Expenditures&quot;, # ylim = c(0 ,0.37)) #x &lt;- seq(0, 15, by = 0.01) # #Exponential #fit.exp &lt;- vglm(Claim ~ 1, exponential, data = claim_data) #theta = 1 / exp(coef(fit.exp)) #fexp_ex &lt;- dgamma(exp(x), scale = exp(-coef(fit.exp)), shape = 1) * exp(x) #lines(x, fexp_ex, col = &quot;red&quot;, lty =2) # #Inference assuming a gamma distribution #fit.gamma &lt;- vglm(Claim ~ 1, family = gamma2, data = claim_data) #theta &lt;- exp(coef(fit.gamma)[1]) / exp(coef(fit.gamma)[2]) # theta = mu / alpha #alpha &lt;- exp(coef(fit.gamma)[2]) #fgamma_ex &lt;- dgamma(exp(x), shape = alpha, scale = theta) * exp(x) #lines(x, fgamma_ex, col = &quot;blue&quot;, lty =3) # #Pareto #fit.pareto &lt;- vglm(Claim ~ 1, paretoII, loc = 0, data = claim_data) #fpareto_ex &lt;- dparetoII(exp(x), loc = 0, shape = exp(coef(fit.pareto)[2]), # scale = exp(coef(fit.pareto)[1])) * exp(x) #lines(x, fpareto_ex, col = &quot;purple&quot;) # #Lognormal #fit.LN &lt;- vglm(Claim ~ 1, family = lognormal, data = claim_data) #flnorm_ex &lt;- dlnorm(exp(x), mean = coef(fit.LN)[1], # sd = exp(coef(fit.LN)[2])) * exp(x) #lines(x, flnorm_ex, col = &quot;lightblue&quot;) # #Density for GB II #gb2_density &lt;- function (x) { # a_1 &lt;- gb2_bop$par[1] # a_2 &lt;- gb2_bop$par[2] # mu &lt;- gb2_bop$par[3] # sigma &lt;- gb2_bop$par[4] # xt &lt;- (log(x) - mu) / sigma # logexpxt &lt;- ifelse (xt &gt; 23, yt, log(1 + exp(xt))) # logdens &lt;- a_1 * xt - log(sigma) - log(beta(a_1, a_2)) - # (a_1+a_2) * logexpxt -log(x) # exp(logdens) # } #fGB2_ex = gb2_density(exp(x)) * exp(x) #lines(x, fGB2_ex, col=&quot;green&quot;) #legend(&quot;topleft&quot;, c(&quot;log(Expend)&quot;, &quot;Exponential&quot;, &quot;Gamma&quot;, &quot;Pareto&quot;, # &quot;Lognormal&quot;, &quot;GB2&quot;), cex=0.8, # lty = c(4,2,3,1,1,1), #4 is &quot;longdash&quot; # col = c(&quot;black&quot;,&quot;red&quot;,&quot;blue&quot;,&quot;purple&quot;,&quot;lightblue&quot;,&quot;green&quot;)) 3.5.2 Maximum Likehood Estimators using Modified Data Pada subbab ini, saya ingin menjelaskan estimasi kemungkinan maksimal pada grup data(group data), sensor data(cencored), dan data terpotong(truncated data). 3.5.2.1 MLE fro Group Data Pada bagian ini, saya ingin memperoleh perkiraan kemungkinan maksimum parameter data grup data(dalam kategori rentang) dan setiap pengamatan terhadap fungsi likehood adalah probabbilitas jatuh dalam kelompok tertentu(interval). Biarkan \\(N_j\\) mewakili jumlah pengamatan dalam interval \\((C_{j-1},C_j]\\). Fungsi likehood untuk grup data dinyatakan sebagai berikut: \\[\\begin{equation} L\\left( \\theta \\right) = \\prod_{j = 1}^{k}\\left\\lbrack F_X\\left( \\left. \\ c_{j} \\right|\\theta \\right) - F_X\\left( \\left. \\ c_{j - 1} \\right|\\theta \\right) \\right\\rbrack^{n_{j}}, \\end{equation}\\] Dimana, \\(C_0\\) adalah kemungkinan pengamatan kecil dan \\(C_k\\) adalah pengamatan terbesar yang mungkin. 3.5.2.1.1 Contoh For a group of policies, you are given that losses follow the distribution function \\(F_X(x) = 1 - \\frac{\\theta}{x},\\)for \\(\\theta &lt; x &lt; \\infty\\). Further, a sample of 20 losses resulted in the following: \\[{\\small \\begin{matrix}\\hline \\text{Interval} &amp; \\text{Number of Losses} \\\\ \\hline (\\theta, 10] &amp; 9 \\\\ (10, 25] &amp; 6 \\\\ (25, \\infty) &amp; 5 \\\\ \\hline \\end{matrix} }\\] Calculate the maksimum likehood estimate of \\(\\theta\\) Jawab: Kontribusi dari setiap dari 9 pengamatan dalam interval pertama terhadap fungsi likelihood adalah probabilitas dari \\(X \\leq 10\\); yaitu, \\(\\Pr\\left( X \\leq 10 \\right) = F_X\\left( 10 \\right)\\). Demikian pula, kontribusi dari masing-masing 6 dan 5 pengamatan dalam interval kedua dan ketiga adalah \\(\\Pr\\left( 10 &lt; X \\leq 25 \\right) = F_X\\left( 25 \\right) - F_X(10\\) dan \\(P(X&gt;25)=1-F_X(25)\\), secara berturut-turut. Fungsi likelihood diberikan oleh: \\[\\begin{array}{ll} L\\left( \\theta \\right) &amp; = \\left\\lbrack F_X\\left( 10 \\right) \\right\\rbrack^{9}\\left\\lbrack F_X\\left( 25 \\right) - F_X(10) \\right\\rbrack^{6}\\left\\lbrack 1 - F_X(25) \\right\\rbrack^{5} \\\\ &amp; = {\\left( 1 - \\frac{\\theta}{10} \\right)}^{9}\\left( \\frac{\\theta}{10} - \\frac{\\theta}{25} \\right)^{6}\\left( \\frac{\\theta}{25} \\right)^{5} \\\\ &amp; = {\\left( \\frac{10 - \\theta}{10} \\right)}^{9}\\left( \\frac{15\\theta}{250} \\right)^{6}\\left( \\frac{\\theta}{25} \\right)^{5}. \\end{array}\\] Setelah itu, mencari likehood algoritma \\[\\begin{array}{ll} \\log L \\left( \\theta \\right) &amp;= 9\\log \\left( 10 - \\theta \\right) + 6\\log \\theta + 5\\log \\theta - 9\\log 10 + 6\\log 15 - 6\\log 250 - 5\\log 25 \\\\ &amp;= 9\\log \\left( 10 - \\theta \\right) + 11\\log \\theta + constant . \\end{array}\\] Constant disini maksudnya adalah angka yang tidak bergantung pada \\(\\theta\\). Kita turunkan dan mendapatkan, \\[\\begin{equation} \\frac{d \\log L \\left( \\theta \\right)}{d \\theta} = \\frac{- 9}{\\left( 10 - \\theta \\right)} + \\frac{11}{\\theta} . \\end{equation}\\] Estimator maksimum likelihood, \\(\\hat{\\theta}\\), adalah solusi dari persamaan: \\[\\begin{equation} \\frac{- 9}{\\left( 10 - \\hat{\\theta} \\right)} + \\frac{11}{\\hat{\\theta}} = 0 \\end{equation}\\] yang menghasilkan \\(\\hat{\\theta} = 5.5\\) 3.5.2.2 MLE for Cencored Data Kontribusi pengamatan tersensor terhadap fungsi kemungkinan adalah probabilitas variabel acak melebihi batas spesifik ini. Perhatikan bahwa kontribusi data lengkap dan tersensor berbagi fungsi bertahan hidup, untuk titik lengkap fungsi bertahan hidup ini dikalikan dengan fungsi hazard, tetapi untuk pengamatan tersensor tidak demikian. Fungsi kemungkinan untuk data yang disensor kemudian diberikan oleh \\[\\begin{equation} L(\\theta) = \\left[ \\prod_{i=1}^r f_X(x_i) \\right] \\left[ S_X(u) \\right]^m , \\end{equation}\\] Dimana \\(R\\) adalah jumlah kerugian yang diketahui dibawah batas limit \\(u\\) \\(m\\) adalah jumla loss yang lebih besar dari batas \\(u\\) 3.5.2.3 MLE for Truncated Data berkaitan dengan estimasi kemungkinan maksimum dari distribusi kontinu dari variabel acak \\(X\\) ketika data tidak lengkap karena truncated atau pemotongan. Kontribusi terhadap fungsi kemungkinan pengamatan X terpotong di D akan menjadi probabilitas bersyarat dan \\(F_X(x)\\) akan digantikan oleh \\(\\frac{F_X(x)}{S_X(d)}\\). Fungsi likehood untuk data truncated diberikan \\[\\begin{equation} L(\\theta) = \\prod_{i=1}^k \\frac{f_X(x_i)}{S_X(d)} , \\end{equation}\\] Dimana \\(k\\) adalah jumlah loss yang lebih besar dari yang dapat dikurangkan \\(D\\) "],["model-selection-and-estimation.html", "Bab 4 Model Selection and Estimation 4.1 Nonparametric Inference 4.2 Model Selection 4.3 Estimasi Menggunakan Data Modifikasi 4.4 Bayesian Inference", " Bab 4 Model Selection and Estimation 4.1 Nonparametric Inference Di bagian ini, Anda mempelajari cara: Perkirakan momen, kuantil, dan distribusi tanpa mengacu pada distribusi parametrik Ringkas data secara grafis tanpa mengacu pada distribusi parametrik Tentukan ukuran yang meringkas penyimpangan parametrik dari kecocokan nonparametrik Gunakan estimator nonparametrik untuk memperkirakan parameter yang dapat digunakan untuk memulai prosedur estimasi parametrik 4.1.1 Estimasi Nonparametrik Pada bagian pembahasan sebelumnya telah mempelajari cara meringkas distribusi dengan cara menghitung, varians, kuantil/persentil, dan sebagainya. Untuk memperkirakan langkah-langkah ringkasan menggunakan kumpulan data, salah satu strateginya adalah: menganggap bentuk parametrik untuk distribusi, seperti binomial negatif untuk frekuensi atau distribusi gamma untuk tingkat keparahan, memperkirakan parameter distribusi itu, gunakan distribusi dengan estimasi parameter untuk menghitung ukuran ringkasan yang diinginkan. Ini adalah pendekatan parametrik . Strategi lain adalah memperkirakan ukuran ringkasan yang diinginkan langsung dari pengamatan tanpa mengacu pada model parametrik. Tidak mengherankan, ini dikenal sebagai pendekatan nonparametrik mempertimbangkan jenis skema pengambilan sampel yang paling dasar dan mengasumsikan bahwa observasi adalah realisasi dari serangkaian variabel acak \\(X_1, \\ldots, X_n\\) yang iid menarik dari distribusi populasi yang tidak diketahui \\(F( ⋅ )\\). Cara yang setara untuk mengatakan ini adalah itu \\(X_1, \\ldots, X_n\\), adalah sampel acak (dengan penggantian) dari F( ⋅) .Kemudian menjelaskan estimator nonparametrik dari banyak ukuran penting yang meringkas sebuah distribusi. 4.1.1.1 Estimator Momen Pada bagian 2.2.2. telah mendefinisikan momen untuk frekuensi dan pada bagian 3.1.1 untuk keparahan. Secara khusus, k -momen ke-, \\(\\mathrm{E~}[X^k] = \\mu^{\\prime}_k\\) , merangkum banyak aspek distribusi untuk berbagai pilihan k . Di Sini, μ′k kadang-kadang disebut k th momen populasi untuk membedakannya dari k momen sampel, \\[\\frac{1}{n} \\sum_{i=1}^n X_i^k ,\\] yang merupakan estimator nonparametrik yang sesuai. Dalam aplikasi tipikal, k adalah bilangan bulat positif, meskipun tidak perlu dalam teori. Kasus khusus yang penting adalah momen pertama di mana \\(k = 1\\) . Dalam hal ini, simbol prima ( \\(\\prime\\) ) dan 1 subskrip biasanya dijatuhkan dan satu digunakan \\(\\mu=\\mu^{\\prime}_1\\) untuk menunjukkan mean populasi, atau hanya mean . Estimator sampel yang sesuai untuk \\(μ\\) disebut rata-rata sampel , dilambangkan dengan bilah di atas variabel acak: \\[\\overline{X} =\\frac{1}{n} \\sum_{i=1}^n X_i .\\] Jenis ringkasan ukuran minat lainnya adalah k -momen pusat ke- , \\(\\mathrm{E~} [(X-\\mu)^k] = \\mu_k\\) . (Kadang-kadang, \\(\\mu^{\\prime}_k\\) disebut k -th momen mentah untuk membedakannya dari momen sentral μk .). Estimator nonparametrik, atau sampel, dari \\(\\mu_k\\) adalah \\[\\frac{1}{n} \\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^k .\\] Momen pusat kedua ( \\(k = 2\\) ) adalah kasus penting yang biasanya akan diberikan simbol baru, \\(\\sigma^2 = \\mathrm{E~} [(X-\\mu)^2]\\) , dikenal sebagai varians . Sifat penduga momen sampel dari varians seperti \\(n^{-1}\\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^2\\) telah dipelajari secara ekstensif tetapi bukan satu-satunya estimator yang mungkin. Versi yang paling banyak digunakan adalah versi di mana ukuran sampel efektif dikurangi satu, jadi kami mendefinisikannya \\[s^2 = \\frac{1}{n-1} \\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^2.\\] Membagi dengan \\(n − 1\\) alih-alih N masalah kecil ketika Anda memiliki ukuran sampel yang besar \\(N\\) seperti yang umum dalam aplikasi asuransi. Estimator varians sampel \\(s^2\\) tidak memihak dalam arti bahwa \\(\\mathrm{E~} [s^2] = \\sigma^2\\) , properti yang diinginkan terutama saat menginterpretasikan hasil analisis. 4.1.1.2 Fungsi Distribusi Empiris Kita telah melihat bagaimana menghitung estimator nonparametrik dari k saat ini \\(\\mathrm{E~} [X^k]\\) . Dengan cara yang sama, untuk fungsi apa pun yang diketahui g (⋅) , kita dapat memperkirakan \\(\\mathrm{E~} [\\mathrm{g}(X)]\\) menggunakan\\(n^{-1}\\sum_{i=1}^n \\mathrm{g}(X_i)\\) Sekarang perhatikan fungsinya \\(\\mathrm{g}(X) = I(X \\le x)\\) untuk tetap \\(X\\) . Di sini, notasi $I( ⋅ \\() adalah fungsi indikator ; itu mengembalikan 1 jika acara ( ⋅ ) benar dan 0 sebaliknya. Perhatikan bahwa sekarang variabel acak\\) g (X$) memiliki distribusi Bernoulli (distribusi binomial dengan \\(n = 1\\) ). Kita dapat menggunakan distribusi ini untuk dengan mudah menghitung jumlah seperti rata-rata dan varians. Misalnya, untuk pilihan ini \\(g (⋅)\\) , nilai harapannya adalah \\(\\mathrm{E~} [I(X \\le x)] = \\Pr(X \\le x) = F(x)\\) , fungsi distribusi dievaluasi pada \\(X\\) . Menggunakan prinsip analog , kami mendefinisikan estimator nonparametrik dari fungsi distribusi \\[ \\begin{aligned} F_n(x) &amp;= \\frac{1}{n} \\sum_{i=1}^n I\\left(X_i \\le x\\right) \\\\ &amp;= \\frac{\\text{number of observations less than or equal to }x}{n} . \\end{aligned} \\] Sebagai $F_N( ⋅ $) didasarkan hanya pada pengamatan dan tidak mengasumsikan keluarga parametrik untuk distribusi, itu nonparametrik dan juga dikenal sebagai fungsi distribusi empiris . Ia juga dikenal sebagai fungsi distribusi kumulatif empiris dan, dalam R, seseorang dapat menggunakan ecdf(.) fungsi tersebut untuk menghitungnya. Contoh 4.1.1. Kumpulan Data Mainan . Sebagai ilustrasi, pertimbangkan kumpulan data fiktif, atau “mainan”. \\(n = 10\\) observasi. Tentukan fungsi distribusi empiris. \\[ {\\small \\begin{array}{c|cccccccccc} \\hline i &amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;10 \\\\ X_i&amp; 10 &amp;15 &amp;15 &amp;15 &amp;20 &amp;23 &amp;23 &amp;23 &amp;23 &amp;30\\\\ \\hline \\end{array} }\\] Kemudian memeriksa bahwa rata-rata sampel adalah \\(\\overline{X} = 19.7\\) dan bahwa varians sampel adalah \\(S^2= 34,45556\\) . Fungsi distribusi empiris yang sesuai adalah \\[ \\begin{aligned} F_n(x) &amp;= \\left\\{ \\begin{array}{ll} 0 &amp; \\text{ for }\\ x&lt;10 \\\\ 0.1 &amp; \\text{ for }\\ 10 \\leq x&lt;15 \\\\ 0.4 &amp; \\text{ for }\\ 15 \\leq x&lt;20 \\\\ 0.5 &amp; \\text{ for }\\ 20 \\leq x&lt;23 \\\\ 0.9 &amp; \\text{ for }\\ 23 \\leq x&lt;30 \\\\ 1 &amp; \\text{ for }\\ x \\geq 30, \\end{array} \\right.\\end{aligned}\\] (xExample &lt;- c(10,rep(15,3),20,rep(23,4),30)) PercentilesxExample &lt;- ecdf(xExample) plot(PercentilesxExample, main=&quot;&quot;,xlab=&quot;x&quot;) 4.1.1.3 Quartiles, Percentiles and Quantiles Pada bagian 3.1.1 median , yaitu angka yang kira-kira setengah dari kumpulan data berada di bawah (atau di atasnya) . Kuartil pertama adalah angka yang kira-kira 25% datanya berada di bawahnya dan kuartil ketiga adalah angka yang kira-kira 75% datanya berada di bawahnya. 100 hal persentil adalah angka sehingga \\(100×p\\) persen dari data di bawahnya. Untuk menggeneralisasi konsep ini, pertimbangkan fungsi distribusi \\(F(⋅\\)) , yang mungkin kontinu atau tidak, dan biarkan Q menjadi pecahan sehingga \\(0 &lt; q&lt; 1\\) . Kami ingin mendefinisikan quantile , katakanlah \\(q_F\\) , menjadi bilangan sedemikian sehingga \\(F(q_F) \\approx q\\) . Perhatikan bahwa ketika \\(q=0.5\\) , \\(q_F\\) adalah median; Kapan \\(q=0.25\\) , \\(q_F\\) adalah kuartil pertama, dan seterusnya. Dengan cara yang sama, ketika \\(q = 0, 0.01, 0.02, \\ldots, 0.99, 1.00\\) , yang dihasilkan QF adalah persentil. Jadi, kuantil menggeneralisasikan konsep median, kuartil, dan persentil. Lebih tepatnya, untuk diberikan \\(0 &lt; q&lt; 1\\) , tentukan q kuantil \\(q_F\\) untuk menjadi nomor yang memenuhi: \\[ \\begin{equation} F(q_F-) \\le q \\le F(q_F) \\tag{4.1} \\end{equation}\\] Untuk mendapatkan pemahaman yang lebih baik tentang definisi ini, mari kita lihat beberapa kasus khusus. Pertama, pertimbangkan kasus di mana X adalah variabel acak kontinu sehingga fungsi distribusi \\(F(⋅)\\) tidak memiliki titik lompatan, seperti yang diilustrasikan pada Gambar 4.2 . Pada gambar ini, beberapa pecahan, Q1 , Q2 , Dan Q3 ditunjukkan dengan kuantil yang sesuai \\(q_{F,1} , q_{F,2} , dan q_{F,3}\\) . Dalam setiap kasus, dapat dilihat bahwa \\(F(q_F-)= F(q_F)\\) sehingga ada kuantil unik. Karena kita dapat menemukan invers unik dari fungsi distribusi di mana saja \\(0 &lt; q&lt; 1\\) , kita bisa menulis \\(q_F= F^{-1}(q)\\) Gambar 4.3 menunjukkan tiga kasus untuk fungsi distribusi. Panel kiri sesuai dengan kasus kontinu yang baru saja dibahas. Panel tengah menampilkan titik lompatan yang serupa dengan yang telah kita lihat dalam fungsi distribusi empiris Gambar 4.1 . Untuk nilai \\(q\\) ditampilkan di panel ini, kami masih memiliki nilai kuantil yang unik \\(q_F\\) . Meskipun ada banyak nilai Q seperti yang \\(F(q_F-) \\le q \\le F(q_F)\\) , untuk nilai tertentu dari \\(q\\) , hanya ada satu solusi untuk persamaan (4.1) . Panel kanan menggambarkan situasi di mana kuantil tidak dapat ditentukan secara unik untuk \\(q\\) ditampilkan karena ada berbagai \\(q_F\\) persamaan yang memuaskan (4.1) . Contoh 4.1.2. Kumpulan Data Mainan: Lanjutan. Tentukan kuantil yang sesuai dengan persentil ke-20, ke-50, dan ke-95. Solusi . Perhatikan Gambar 4.1 . Kasus \\(q=0.20\\) sesuai dengan panel tengah Gambar Gambar 4.3 , jadi persentil ke-20 adalah 15. Kasus \\(q=0.50\\) sesuai dengan panel kanan, jadi mediannya adalah angka antara 20 dan 23 inklusif. Banyak paket perangkat lunak menggunakan rata-rata 21,5 (misalnya R, seperti yang terlihat di bawah). Untuk persentil ke-95, solusinya adalah 30. Kita dapat melihat dari Gambar 4.1 bahwa 30 juga sesuai dengan persentil ke-99 dan ke-99,99. quantile(xExample, probs=c(0.2, 0.5, 0.95), type=6) Dengan mengambil rata-rata tertimbang antara pengamatan data, kuantil empiris yang dihaluskan dapat menangani kasus seperti panel kanan pada Gambar 4.3 . Itu Q kuantil empiris yang dihaluskan didefinisikan sebagai \\[\\hat{\\pi}_q = (1-h) X_{(j)} + h X_{(j+1)}\\] Di mana \\(j=\\lfloor(n+1)q\\rfloor\\) , Dan\\(X_{(1)}, \\ldots, X_{(n)}\\) adalah nilai yang diurutkan (dikenal sebagai statistik urutan ) yang sesuai dengan \\(X_1, \\ldots, X_n\\). (Ingat bahwa tanda kurung ⌊ ⋅ ⌋ adalah fungsi lantai yang menunjukkan nilai bilangan bulat terbesar.) Perhatikan bah wa \\(\\hat{\\pi}_q\\)$ hanyalah sebuah interpolasi linear antara \\(X_{( j )}\\) dan \\(X_{(j+1)}\\). Contoh 4.1.3. Kumpulan Data Mainan: Lanjutan. Tentukan persentil yang dihaluskan ke-50 dan ke-20. Solusi Ambil \\(n = 10\\) Dan \\(q= 0,5\\). Kemudian, \\(j=\\lfloor(11)(0.5) \\rfloor= \\lfloor 5.5 \\rfloor=5\\), . Maka kuantil empiris yang dihaluskan ke-0,5 adalah \\[\\hat{\\pi}_{0.5} = (1-0.5) X_{(5)} + (0.5) X_{(6)} = 0.5 (20) + (0.5)(23) = 21.5.\\] Sekarang ambil \\(n = 10\\) Dan \\(q= 0,2\\) . Pada kasus ini, \\(j=\\lfloor(11)(0.2)\\rfloor=\\lfloor 2.2 \\rfloor=2\\) . Maka kuantil empiris yang dihaluskan ke-0,2 adalah \\[\\hat{\\pi}_{0.2} = (1-0.2) X_{(2)} + (0.2) X_{(3)} = 0.8 (15) + (0.2)(15) = 15.\\] 4.1.1.4 Penduga Kepadatan Variabel Diskrit. Ketika variabel acak adalah diskrit, memperkirakan fungsi massa probabilitas \\(f(x) = \\Pr(X=x)\\) mudah. Kami hanya menggunakan rata-rata sampel, yang didefinisikan sebagai \\[f_n(x) = \\frac{1}{n} \\sum_{i=1}^n I(X_i = x),\\] yang merupakan proporsi sampel sama dengan X Variabel Berkelanjutan dalam Grup. Untuk variabel acak kontinu, pertimbangkan formulasi diskrit di mana domain dari F( ⋅ ) dipartisi oleh konstanta \\(\\{c_0 &lt; c_1 &lt; \\cdots &lt; c_k\\}\\) ke dalam interval bentuk \\([c_{j-1}, c_j)\\) , untuk \\(j=1, \\ldots, k\\) . Pengamatan data dengan demikian “dikelompokkan” berdasarkan interval di mana mereka jatuh. Kemudian, kita dapat menggunakan definisi dasar dari fungsi massa empiris, atau variasi seperti \\[f_n(x) = \\frac{n_j}{n \\times (c_j - c_{j-1})} \\ \\ \\ \\ \\ \\ c_{j-1} \\le x &lt; c_j,\\] Di mana \\(N_J\\) adalah jumlah pengamatan ( \\(X_i\\) ) yang termasuk dalam interval \\([c_{j-1}, c_j)\\). Variabel Berkelanjutan (tidak dikelompokkan). Memperluas gagasan ini ke contoh di mana kami mengamati data individual, perhatikan bahwa kami selalu dapat membuat pengelompokan arbitrer dan menggunakan rumus ini. Lebih formal, biarkan \\(b &gt; 0\\) menjadi konstanta positif kecil, yang dikenal sebagai bandwidth , dan menentukan penaksir kepadatan menjadi \\[\\begin{equation} f_n(x) = \\frac{1}{2nb} \\sum_{i=1}^n I(x-b &lt; X_i \\le x + b) \\tag{4.2} \\end{equation}\\] Secara lebih umum, tentukan penaksir kerapatan kernel dari pdf di X sebagai \\[\\begin{equation} f_n(x) = \\frac{1}{nb} \\sum_{i=1}^n w\\left(\\frac{x-X_i}{b}\\right) , \\tag{4.3} \\end{equation}\\] Di mana w adalah fungsi kerapatan probabilitas yang berpusat di sekitar 0. Perhatikan bahwa persamaan (4.2) adalah kasus khusus penduga kerapatan kernel di mana \\(w(x) = \\frac{1}{2}I(-1 &lt; x \\le 1)\\) , juga dikenal sebagai kernel seragam . Pilihan populer lainnya ditunjukkan pada Tabel 4.1 . \\[{\\small \\begin{matrix} \\begin{array}{l|cc} \\hline \\text{Kernel} &amp; w(x) \\\\ \\hline \\text{Uniform } &amp; \\frac{1}{2}I(-1 &lt; x \\le 1) \\\\ \\text{Triangle} &amp; (1-|x|)\\times I(|x| \\le 1) \\\\ \\text{Epanechnikov} &amp; \\frac{3}{4}(1-x^2) \\times I(|x| \\le 1) \\\\ \\text{Gaussian} &amp; \\phi(x) \\\\ \\hline \\end{array}\\end{matrix} }\\] Di Sini, \\(\\phi(\\cdot)\\) adalah fungsi kepadatan normal standar. Seperti yang akan kita lihat pada contoh berikut, pilihan bandwidth \\(B\\) hadir dengan tradeoff bias-varians antara mencocokkan fitur distribusi lokal dan mengurangi volatilitas. Contoh 4.1.4. Dana Properti. Gambar 4.4 menunjukkan histogram (dengan persegi panjang abu-abu yang diarsir) dari klaim properti logaritmik dari tahun 2010. Kurva tebal (biru) mewakili kerapatan kernel Gaussian di mana bandwidth dipilih secara otomatis menggunakan aturan ad hoc berdasarkan ukuran sampel dan volatilitas data ini . Untuk dataset ini, bandwidth ternyata b = 0,3255 . Sebagai perbandingan, kurva putus-putus (merah) menunjukkan penaksir densitas dengan lebar pita sama dengan 0,1 dan kurva halus berwarna hijau menggunakan lebar pita 1. Sebagaimana diantisipasi, lebar pita yang lebih kecil (0,1) menunjukkan mengambil rata-rata lokal dengan data yang lebih sedikit sehingga kita mendapatkan ide yang lebih baik dari rata-rata lokal, tetapi dengan harga volatilitas yang lebih tinggi. Sebaliknya, bandwidth yang lebih besar (1) memperhalus fluktuasi lokal, menghasilkan kurva yang lebih halus yang mungkin melewatkan gangguan pada rata-rata lokal. Untuk aplikasi aktuaria, kami terutama menggunakan estimator densitas kernel untuk mendapatkan kesan visual cepat dari data. Dari perspektif ini, Anda cukup menggunakan aturan ad hoc default untuk pemilihan bandwidth, mengetahui bahwa Anda memiliki kemampuan untuk mengubahnya tergantung pada situasi yang dihadapi. ClaimLev &lt;- read.csv(&quot;Data/CLAIMLEVEL.csv&quot;, header=TRUE); #nrow(ClaimLev); # 6258 ClaimData&lt;-subset(ClaimLev,Year==2010); #2010 subset #Density Comparison hist(log(ClaimData$Claim), main=&quot;&quot;, ylim=c(0,.35),xlab=&quot;Log Expenditures&quot;, freq=FALSE, col=&quot;lightgray&quot;) lines(density(log(ClaimData$Claim)), col=&quot;blue&quot;,lwd=2.5) lines(density(log(ClaimData$Claim), bw=1), col=&quot;green&quot;) lines(density(log(ClaimData$Claim), bw=.1), col=&quot;red&quot;, lty=3) legend(&quot;topright&quot;, c(&quot;b=0.3255 (default)&quot;, &quot;b=0.1&quot;, &quot;b=1.0&quot;), lty=c(1,3,1), lwd=c(2.5,1,1), col=c(&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;), cex=1) #density(log(ClaimData$Claim))$bw ##default bandwidth Estimator densitas nonparametrik, seperti estimator kernel, sering digunakan dalam praktik. Konsep ini juga dapat diperluas untuk memberikan versi halus dari fungsi distribusi empiris. Mengingat definisi penaksir densitas kernel, penaksir kernel dari fungsi distribusi dapat ditemukan sebagai \\[\\begin{aligned} \\tilde{F}_n(x) = \\frac{1}{n} \\sum_{i=1}^n W\\left(\\frac{x-X_i}{b}\\right).\\end{aligned}\\] Di mana \\(W\\) adalah fungsi distribusi yang terkait dengan densitas kernel \\(w\\) . Sebagai ilustrasi, untuk kernel yang seragam, kita punya \\(w(y) = \\frac{1}{2}I(-1 &lt; y \\le 1)\\) , Jadi \\[\\begin{aligned} W(y) = \\begin{cases} 0 &amp; y&lt;-1\\\\ \\frac{y+1}{2}&amp; -1 \\le y &lt; 1 \\\\ 1 &amp; y \\ge 1 \\\\ \\end{cases}\\end{aligned} .\\] Contoh 4.1.5. Soal Ujian Aktuaria. Anda mempelajari lima nyawa untuk memperkirakan waktu dari timbulnya penyakit hingga kematian. Waktu kematian adalah: \\[\\begin{array}{ccccc} 2 &amp; 3 &amp; 3 &amp; 3 &amp; 7 \\\\ \\end{array}\\] Menggunakan kernel segitiga dengan bandwidth 2 , hitung taksiran fungsi densitas pada 2,5. Solusi. Untuk perkiraan kepadatan kernel, kami punya \\[f_n(x) = \\frac{1}{nb} \\sum_{i=1}^n w\\left(\\frac{x-X_i}{b}\\right),\\] Di mana \\(n = 5\\) , \\(b = 2\\) , Dan \\(x = 2,5\\) . Untuk inti segitiga, \\(w(x) = (1-|x|)\\times I(|x| \\le 1)\\) . Dengan demikian, \\[\\begin{array}{c|c|c} \\hline X_i &amp; \\frac{x-X_i}{b} &amp; w\\left(\\frac{x-X_i}{b} \\right) \\\\ \\hline 2 &amp; \\frac{2.5-2}{2}=\\frac{1}{4} &amp; (1-\\frac{1}{4})(1) = \\frac{3}{4} \\\\ \\hline 3 &amp; &amp; \\\\ 3 &amp; \\frac{2.5-3}{2}=\\frac{-1}{4} &amp; \\left(1-\\left| \\frac{-1}{4} \\right| \\right)(1) = \\frac{3}{4} \\\\ 3 &amp; &amp; \\\\ \\hline 7 &amp; \\frac{2.5-7}{2}=-2.25 &amp; (1-|-2.25|)(0) = 0\\\\ \\hline \\end{array}\\] Kemudian perkiraan densitas kernel di \\(x = 2,5\\) adalah \\[f_n(2.5) = \\frac{1}{5(2)}\\left( \\frac{3}{4} + (3) \\frac{3}{4} + 0 \\right) = \\frac{3}{10}\\] 4.1.1.5 Prinsip Pengaya Salah satu cara untuk membuat penaksir nonparametrik dari beberapa kuantitas adalah dengan menggunakan prinsip analog atau plug-in di mana seseorang menggantikan cdf yang tidak diketahui \\(F\\) dengan estimasi yang diketahui seperti cdf empiris \\(F_N\\) . Jadi, jika kita mencoba memperkirakan \\(\\mathrm{E}~[\\mathrm{g}(X)]=\\mathrm{E}_F~[\\mathrm{g}(X)]\\) untuk fungsi generik g , maka kami mendefinisikan estimator nonparametrik menjadi \\(\\mathrm{E}_{F_n}~[\\mathrm{g}(X)]=n^{-1}\\sum_{i=1}^n \\mathrm{g}(X_i)\\). Untuk melihat cara kerjanya, sebagai kasus khusus dari g , kami menganggap kerugian per variabel acak pembayaran \\(Y = (X-d)_+\\) dan rasio eliminasi kerugian yang diperkenalkan di Bagian 3.4.1. Kita dapat mengungkapkan ini sebagai \\[LER(d) = \\frac{\\mathrm{E~}[X - (X-d)_+]}{\\mathrm{E~}[X]} =\\frac{\\mathrm{E~}[\\min(X,d)]}{\\mathrm{E~}[X]} ,\\] Contoh. 4.1.6. Klaim Cidera Tubuh dan Rasio Penghapusan Kerugian Kami menggunakan sampel 432 klaim mobil tertutup dari Boston dari Derrig, Ostaszewski, dan Rempala ( 2001 ) . Kerugian dicatat untuk pembayaran karena cedera tubuh dalam kecelakaan mobil. Kerugian tidak dapat dikurangkan tetapi dibatasi oleh berbagai jumlah pertanggungan maksimum yang juga tersedia dalam data. Ternyata hanya 17 dari 432 ( ≈ 4%) tunduk pada batasan kebijakan ini sehingga kami mengabaikan data ini untuk ilustrasi ini. Kerugian rata-rata yang dibayarkan adalah 6906 dalam dolar AS. Gambar 4.5 menunjukkan aspek lain dari distribusi. Secara khusus, panel sebelah kiri menunjukkan fungsi distribusi empiris, panel sebelah kanan memberikan plot kepadatan nonparametrik. Dampak kerugian cedera tubuh dapat dikurangi dengan pengenaan limit atau pembelian polis reasuransi (lihat Bagian 10.3). Untuk mengukur dampak dari alat mitigasi risiko ini, biasanya menghitung rasio eliminasi kerugian (LER) seperti yang diperkenalkan di Bagian 3.4.1. Fungsi distribusi tidak tersedia sehingga harus diestimasi dengan cara tertentu. Menggunakan prinsip plug-in, estimator nonparametrik dapat didefinisikan sebagai \\[LER_n(d) = \\frac{n^{-1} \\sum_{i=1}^n \\min(X_i,d)}{n^{-1} \\sum_{i=1}^n X_i} = \\frac{\\sum_{i=1}^n \\min(X_i,d)}{\\sum_{i=1}^n X_i} .\\] Gambar 4.6 menunjukkan estimator \\(LER_n(d)\\) untuk berbagai pilihan \\(d\\) . Misalnya, di \\(d= 1.000\\) dan punya \\(LER_n( 1000 ) ≈ 0,1442\\). Dengan demikian, memberlakukan batas 1.000 berarti ekspektasi klaim yang ditahan 14,42 persen lebih rendah bila dibandingkan dengan ekspektasi klaim dengan deductible nol. 4.1.2 Alat untuk Pemilihan Model dan Diagnostik Bagian sebelumnya memperkenalkan estimator nonparametrik di mana tidak ada bentuk parametrik yang diasumsikan tentang distribusi yang mendasarinya. Namun, dalam banyak aplikasi aktuaria, analis berusaha menggunakan kecocokan parametrik dari distribusi untuk kemudahan penjelasan dan kemampuan untuk memperluasnya ke situasi yang lebih kompleks seperti memasukkan variabel penjelas dalam pengaturan regresi. Saat memasang distribusi parametrik, seorang analis mungkin mencoba menggunakan distribusi gamma untuk mewakili sekumpulan data kerugian. Namun, analis lain mungkin lebih suka menggunakan distribusi Pareto. Bagaimana cara menentukan model mana yang akan dipilih? Alat nonparametrik dapat digunakan untuk menguatkan pemilihan model parametrik. Pada dasarnya, pendekatannya adalah untuk menghitung langkah-langkah ringkasan yang dipilih di bawah model parametrik yang dipasang dan membandingkannya dengan kuantitas yang sesuai di bawah model nonparametrik. Karena model nonparametrik tidak mengasumsikan distribusi tertentu dan hanya merupakan fungsi dari data, model ini digunakan sebagai tolok ukur untuk menilai seberapa baik distribusi/model parametrik mewakili data. Juga, ketika ukuran sampel meningkat, distribusi empiris hampir pasti menyatu dengan distribusi populasi yang mendasarinya (berdasarkan hukum jumlah besar yang kuat). Dengan demikian distribusi empiris adalah proksi yang baik untuk populasi. Perbandingan estimator parametrik dengan nonparametrik dapat mengingatkan analis akan kekurangan dalam model parametrik dan terkadang menunjukkan cara untuk meningkatkan spesifikasi parametrik. Prosedur diarahkan menilai validitas model yang dikenal sebagaidiagnostik model . 4.1.2.1 Perbandingan Grafik Distribusi Kita telah melihat teknik overlay grafik untuk tujuan perbandingan. Untuk memperkuat penerapan teknik ini, Gambar 4.7membandingkan distribusi empiris dengan dua distribusi pas parametrik. Panel kiri menunjukkan fungsi distribusi distribusi klaim. Titik-titik yang membentuk kurva “berbentuk S” mewakili fungsi distribusi empiris pada setiap pengamatan. Kurva biru tebal memberikan nilai yang sesuai untuk distribusi gamma yang pas dan ungu muda untuk distribusi Pareto yang pas. Karena Pareto lebih dekat dengan fungsi distribusi empiris daripada gamma, ini memberikan bukti bahwa Pareto adalah model yang lebih baik untuk kumpulan data ini. Panel kanan memberikan informasi serupa untuk fungsi kerapatan dan memberikan pesan yang konsisten. Berdasarkan (hanya) angka-angka ini, distribusi Pareto adalah pilihan yang jelas bagi analis. Untuk cara lain untuk membandingkan kesesuaian dua model yang cocok, pertimbangkan plot probabilitas-probabilitas (\\(pp\\)) . A \\[pp\\] plot membandingkan probabilitas kumulatif di bawah dua model. Untuk tujuan kami, kedua model ini adalah fungsi distribusi empiris nonparametrik dan model pas parametrik. Gambar 4.8 menunjukkan \\(pp\\) plot untuk data Dana Properti yang diperkenalkan di Bagian 1.3 . Gamma yang dipasang di sebelah kiri dan Pareto yang dipasang di sebelah kanan, dibandingkan dengan fungsi distribusi data empiris yang sama. Garis lurus mewakili kesetaraan antara dua distribusi yang dibandingkan, sehingga titik yang dekat dengan garis diinginkan. Seperti yang terlihat pada demonstrasi sebelumnya, Pareto jauh lebih dekat dengan distribusi empiris daripada gamma, memberikan bukti tambahan bahwa Pareto adalah model yang lebih baik. Itu QQ plot membandingkan dua model yang dipasang melalui kuantilnya. Seperti hal hal plot, kami membandingkan nonparametrik dengan model pas parametrik. Kuantil dapat dievaluasi pada setiap titik kumpulan data, atau pada kisi (misalnya, di 0 , 0,001 , 0,002 , … , 0,999 , 1,000 ), tergantung aplikasinya. Pada Gambar 4.9 , untuk setiap titik pada kisi tersebut, sumbu horizontal menampilkan kuantil empiris dan sumbu vertikal menampilkan kuantil parametrik yang sesuai (gamma untuk dua panel atas, Pareto untuk dua panel bawah). Kuantil diplot pada skala asli di panel kiri dan pada skala log di panel kanan untuk memungkinkan kita melihat di mana kekurangan distribusi yang pas. Garis lurus mewakili kesetaraan antara distribusi empiris dan distribusi pas. Dari plot ini, kita sekali lagi melihat bahwa Pareto secara keseluruhan lebih cocok daripada gamma. Selain itu, panel kanan bawah menunjukkan bahwa distribusi Pareto bekerja dengan baik dengan klaim besar, tetapi memberikan kecocokan yang lebih buruk untuk klaim kecil. Contoh 4.1.7. Soal Ujian Aktuaria. Grafik di bawah ini menunjukkan \\(pp\\) plot distribusi pas dibandingkan dengan sampel. Solusi. Ekor dari distribusi yang pas terlalu tebal di sebelah kiri, terlalu tipis di sebelah kanan, dan distribusi yang pas memiliki probabilitas yang lebih kecil di sekitar median daripada sampel. Untuk melihat ini, ingat bahwa hal hal plot grafik distribusi kumulatif dari dua distribusi pada sumbunya (empiris pada sumbu x dan dipasang pada sumbu y dalam kasus ini). Untuk nilai kecil dari X , model yang dipasang memberikan probabilitas yang lebih besar untuk berada di bawah nilai itu daripada yang terjadi dalam sampel (mis F( x ) &gt;FN( x ) ). Ini menunjukkan bahwa model memiliki ekor kiri yang lebih berat daripada datanya. Untuk nilai besar dari X , model kembali memberikan probabilitas yang lebih besar untuk berada di bawah nilai itu dan dengan demikian lebih kecil kemungkinannya untuk berada di atas nilai itu (mis S( x ) &lt;SN( x ) ). Hal ini menunjukkan bahwa model memiliki ekor kanan yang lebih ringan dari pada data. Selain itu, saat kita mulai dari 0,4 hingga 0,6 pada sumbu horizontal (dengan demikian melihat 20% tengah data), hal hal plot meningkat dari sekitar 0,3 menjadi 0,4. Ini menunjukkan bahwa model hanya menempatkan sekitar 10% dari probabilitas dalam kisaran ini. 4.1.2.2 Perbandingan Statistik Distribusi Saat memilih model, akan sangat membantu untuk menampilkan tampilan grafis. Namun, untuk melaporkan hasil, melengkapi tampilan grafis dengan statistik terpilih yang meringkas kebaikan kesesuaian model dapat efektif. Tabel 4.2 menyediakan tiga statistik kebaikan yang umum digunakan . Dalam tabel ini, \\(F_N\\) adalah distribusi empiris, \\(F\\) adalah distribusi pas atau hipotesis, dan \\(F_i^* = F(x_i)\\) . \\[{\\small \\begin{matrix} \\begin{array}{l|cc} \\hline \\text{Statistic} &amp; \\text{Definition} &amp; \\text{Computational Expression} \\\\ \\hline \\text{Kolmogorov-} &amp; \\max_x |F_n(x) - F(x)| &amp; \\max(D^+, D^-) \\text{ where } \\\\ ~~~\\text{Smirnov} &amp;&amp; D^+ = \\max_{i=1, \\ldots, n} \\left|\\frac{i}{n} - F_i^*\\right| \\\\ &amp;&amp; D^- = \\max_{i=1, \\ldots, n} \\left| F_i^* - \\frac{i-1}{n} \\right| \\\\ \\text{Cramer-von Mises} &amp; n \\int (F_n(x) - F(x))^2 f(x) dx &amp; \\frac{1}{12n} + \\sum_{i=1}^n \\left(F_i^* - (2i-1)/n\\right)^2 \\\\ \\text{Anderson-Darling} &amp; n \\int \\frac{(F_n(x) - F(x))^2}{F(x)(1-F(x))} f(x) dx &amp; -n-\\frac{1}{n} \\sum_{i=1}^n (2i-1) \\log\\left(F_i^*(1-F_{n+1-i})\\right)^2 \\\\ \\hline \\end{array} \\\\ \\end{matrix} }\\] Statistik Kolmogorov-Smirnov adalah perbedaan absolut maksimum antara fungsi distribusi yang dipasang dan fungsi distribusi empiris. Alih-alih membandingkan perbedaan antara titik tunggal, statistik Cramer-von Mises mengintegrasikan perbedaan antara fungsi distribusi empiris dan pas pada seluruh rentang nilai. Statistik Anderson-Darling juga mengintegrasikan perbedaan ini pada rentang nilai, meskipun diboboti oleh kebalikan dari varian. Oleh karena itu lebih menekankan pada ekor distribusi (yaitu kapan \\(F( x )\\) atau \\(1-F(x)=S(x)\\) kecil). Contoh 4.1.8. Soal Ujian Aktuaria (dimodifikasi). Contoh pembayaran klaim adalah: \\[\\begin{array}{ccccc} 29 &amp; 64 &amp; 90 &amp; 135 &amp; 182 \\\\ \\end{array}\\] Bandingkan distribusi klaim empiris dengan distribusi eksponensial dengan rata-rata 100 dengan menghitung nilai statistik uji Kolmogorov-Smirnov. Solusi. Untuk distribusi eksponensial dengan rata-rata 100 , fungsi distribusi kumulatif adalah \\(F(x)=1-e^{-x/100}\\) . Dengan demikian, \\[\\begin{array}{ccccc} \\hline x &amp; F(x) &amp; F_n(x) &amp; F_n(x-) &amp; \\max(|F(x)-F_n(x)|,|F(x)-F_n(x-)|) \\\\ \\hline 29 &amp; 0.2517 &amp; 0.2 &amp; 0 &amp; \\max(0.0517, 0.2517) = 0.2517 \\\\ 64 &amp; 0.4727 &amp; 0.4 &amp; 0.2 &amp; \\max(0.0727, 0.2727) = 0.2727 \\\\ 90 &amp; 0.5934 &amp; 0.6 &amp; 0.4 &amp; \\max(0.0066, 0.1934) = 0.1934 \\\\ 135 &amp; 0.7408 &amp; 0.8 &amp; 0.6 &amp; \\max(0.0592, 0.1408) = 0.1408 \\\\ 182 &amp; 0.8380 &amp; 1 &amp; 0.8 &amp; \\max(0.1620, 0.0380) = 0.1620 \\\\ \\hline \\end{array}\\] Oleh karena itu, statistik uji Kolmogorov-Smirnov adalah \\[KS = \\max(0.2517, 0.2727, 0.1934, 0.1408, 0.1620) = 0.2727 .\\] 4.1.3 Nilai Awal Metode pencocokan momen dan persentil merupakan metode estimasi nonparametrik yang memberikan alternatif kemungkinan maksimum. Umumnya, kemungkinan maksimum adalah teknik yang lebih disukai karena menggunakan data secara lebih efisien. (Lihat Lampiran Bab 17 untuk definisi efisiensi yang tepat.) Namun, metode pencocokan momen dan persentil berguna karena lebih mudah diinterpretasikan dan karena itu memungkinkan aktuaris atau analis untuk menjelaskan prosedur kepada orang lain. Selain itu, prosedur estimasi numerik (misalnya jika dilakukan di R) untuk kemungkinan maksimum adalah iteratif dan membutuhkan nilai awal untuk memulai proses rekursif. Meskipun banyak masalah yang kuat untuk pemilihan nilai awal, untuk beberapa situasi kompleks, penting untuk memiliki nilai awal yang mendekati nilai optimal (tidak diketahui). Metode momen dan pencocokan persentil adalah teknik yang dapat menghasilkan perkiraan yang diinginkan tanpa investasi komputasi yang serius dan dengan demikian dapat digunakan sebagai nilai awal untuk menghitung kemungkinan maksimum. 4.1.3.1 Metode Momen Metode ini merupakan estimasi parameter populasi dengan pendekatan momen parametrik menggunakan momen sampel empiris. pada momen ini, momen distribusi parametrik menggunakan momen empiris atau nonparametrik kemudian dapat dipecahkan secara aljabar untuk estimasi parameter. Contoh 4.1.9. Dana Properti. Untuk dana properti 2010, ada \\(n = 1 , 377\\) klaim individu (dalam ribuan dolar) dengan \\[m_1 = \\frac{1}{n} \\sum_{i=1}^n X_i = 26.62259 \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ m_2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 = 136154.6 .\\] Sesuaikan parameter distribusi gamma dan Pareto menggunakan metode momen. Solusi. Agar sesuai dengan distribusi gamma, kami memiliki \\(\\mu_1 = \\alpha \\theta\\) Dan \\(\\mu_2^{\\prime} = \\alpha(\\alpha+1) \\theta^2\\) . Menyamakan keduanya menghasilkan metode penaksir momen, aljabar mudah menunjukkannya \\[\\alpha = \\frac{\\mu_1^2}{\\mu_2^{\\prime}-\\mu_1^2} \\ \\ \\ \\text{and} \\ \\ \\ \\theta = \\frac{\\mu_2^{\\prime}-\\mu_1^2}{\\mu_1}.\\] Jadi, metode penduga momen adalah \\[\\begin{aligned} \\hat{\\alpha} &amp;= \\frac{26.62259^2}{136154.6-26.62259^2} = 0.005232809 \\\\ \\hat{\\theta} &amp;= \\frac{136154.6-26.62259^2}{26.62259} = 5,087.629. \\end{aligned}\\] Sebagai perbandingan, nilai kemungkinan maksimum berubah menjadi \\(\\hat{\\alpha}_{MLE} = 0.2905959\\) Dan \\(\\hat{\\theta}_{MLE} = 91.61378\\) , jadi ada perbedaan besar antara dua prosedur estimasi. Ini adalah salah satu indikasi, seperti yang telah kita lihat sebelumnya, bahwa model gamma kurang cocok. Sebaliknya, sekarang asumsikan distribusi Pareto sehingga \\(\\mu_1 = \\theta/(\\alpha -1)\\) Dan \\(\\mu_2^{\\prime} = 2\\theta^2/((\\alpha-1)(\\alpha-2) )\\) . Perhatikan bahwa ungkapan ini untuk μ′2 hanya berlaku untuk α &gt; 2 . Pertunjukan aljabar yang mudah \\[\\alpha = 1+ \\frac{\\mu_2^{\\prime}}{\\mu_2^{\\prime}-\\mu_1^2} \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ \\ \\theta = (\\alpha-1)\\mu_1.\\] Jadi, metode penduga momen adalah \\[ \\begin{aligned} \\hat{\\alpha} &amp;= 1+ \\frac{136154.6}{136154.6-26,62259^2} = 2.005233 \\\\ \\hat{\\theta} &amp;= (2.005233-1) \\cdot 26.62259 = 26.7619 \\end{aligned}\\] Nilai kemungkinan maksimum berubah menjadi \\(\\hat{\\alpha}_{MLE} = 0.9990936\\) Dan \\(\\hat{\\theta}_{MLE} = 2.2821147\\) . Sangat menarik bahwa \\(\\hat{\\alpha}_{MLE}&lt;1\\) ; untuk distribusi Pareto, ingat itu \\(α &lt; 1\\) berarti rata-ratanya tak terhingga. Ini adalah indikasi lain bahwa kumpulan data klaim properti adalah distribusi ekor panjang. Seperti contoh di atas, ada fleksibilitas dengan metode momen. Misalnya, kita dapat mencocokkan momen kedua dan ketiga alih-alih yang pertama dan kedua, menghasilkan estimator yang berbeda. Selain itu, tidak ada jaminan bahwa solusi akan ada untuk setiap masalah. Untuk data yang disensor atau terpotong, momen pencocokan dimungkinkan untuk beberapa masalah, tetapi secara umum, ini adalah skenario yang lebih sulit. Terakhir, untuk distribusi di mana momen tidak ada atau tidak terbatas, metode momen tidak tersedia. Sebagai alternatif, seseorang dapat menggunakan teknik pencocokan persentil. 4.1.3.2 Pencocokan Persentil Di bawah pencocokan persentil , kami memperkirakan kuantil atau persentil dari distribusi parametrik menggunakan kuantil atau persentil empiris (nonparametrik) yang dijelaskan di Bagian 4.1.1.3 . Contoh 4.1.10. Dana Properti. Untuk dana properti 2010, kami mengilustrasikan pencocokan pada kuantil. Secara khusus, distribusi Pareto secara intuitif menyenangkan karena solusi bentuk tertutup untuk kuantil. Ingatlah bahwa fungsi distribusi untuk distribusi Pareto adalah \\[F(x) = 1 - \\left(\\frac{\\theta}{x+\\theta}\\right)^{\\alpha}.\\] Aljabar mudah menunjukkan bahwa kita dapat menyatakan kuantil sebagai \\[F^{-1}(q) = \\theta \\left( (1-q)^{-1/\\alpha} -1 \\right).\\] untuk sebagian kecil q , \\(0 &lt; q&lt; 1\\). Tentukan estimasi parameter distribusi Pareto menggunakan kuantil empiris ke-25 dan ke-95. Solusi. Persentil ke-25 (kuartil pertama) ternyata adalah 0,78853 dan persentil ke-95 adalah 50.98293 (keduanya dalam ribuan dolar). Dengan dua persamaan \\[0.78853 = \\theta \\left( 1- (1-.25)^{-1/\\alpha} \\right) \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ 50.98293 = \\theta \\left( 1- (1-.75)^{-1/\\alpha} \\right)\\] dan dua yang tidak diketahui, solusinya adalah \\[\\hat{\\alpha} = 0.9412076 \\ \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ \\hat{\\theta} = 2.205617 .\\] Sehingga kesimpulannya adalah rutin numerik diperlukan untuk solusi ini karena tidak ada solusi analitik yang tersedia. Selanjutnya, ingatlah perkiraan kemungkinan maksimumadalah α^ML E= 0,9990936 Dan θ^ML E= 2,2821147 , sehingga pencocokan persentil memberikan perkiraan yang lebih baik untuk distribusi Pareto daripada metode momen. Contoh 4.1.11. Soal Ujian Aktuaria. Anda diberikan: Kerugian mengikuti distribusi loglogistik dengan fungsi distribusi kumulatif: \\[F(x) = \\frac{\\left(x/\\theta\\right)^{\\gamma}}{1+\\left(x/\\theta\\right)^{\\gamma}}\\] Contoh kerugiannya adalah: \\[\\begin{array}{ccccccccccc} 10 &amp;35 &amp;80 &amp;86 &amp;90 &amp;120 &amp;158 &amp;180 &amp;200 &amp;210 &amp;1500 \\\\ \\end{array}\\] Hitung estimasi dari \\(θ\\) dengan pencocokan persentil, menggunakan perkiraan persentil ke-40 dan ke-80 yang dihaluskan secara empiris. Solusi. Dengan 11 pengamatan, kami memiliki \\(j=\\lfloor(n+1)q\\rfloor = \\lfloor 12(0.4) \\rfloor = \\lfloor 4.8\\rfloor=4\\). Dengan interpolasi, perkiraan persentil ke-40 yang dihaluskan secara empiris adalah \\(\\hat{\\pi}_{0.4} = (1-h) X_{(j)} + h X_{(j+1)} = 0.2(86)+0.8(90)=89.2\\). Demikian pula, untuk perkiraan persentil yang dihaluskan secara empiris ke-80, kami memiliki \\(12 ( 0,8 ) = 9,6\\) jadi perkiraannya \\(\\hat{\\pi}_{0.8} = 0.4(200)+0.6(210)=206\\). Dengan menggunakan distribusi kumulatif loglogistik, kita perlu menyelesaikan dua persamaan berikut untuk parameter \\({\\hat{\\theta}}\\) Dan \\({\\hat{\\gamma}}\\) : \\[0.4=\\frac{(89.2/{\\hat{\\theta}})^{\\hat{\\gamma}}}{1+(89.2/{\\hat{\\theta}})^{\\hat{\\gamma}}} \\ \\ \\ \\text{and} \\ \\ \\ \\ 0.8=\\frac{(206/{\\hat{\\theta}})^{\\hat{\\gamma}}}{1+(206/{\\hat{\\theta}})^{\\hat{\\gamma}}} .\\] Pemecahan untuk setiap ekspresi kurung memberi \\(\\frac{2}{3}=(89.2/\\theta)^{\\hat{\\gamma}}\\) Dan \\(4=(206/{\\hat{\\theta}})^{\\hat{\\gamma}}\\) . Mengambil rasio persamaan kedua dengan yang pertama memberi \\(6=(206/89.2)^{\\hat{\\gamma}}\\Rightarrow {\\hat{\\gamma}}=\\frac{\\log(6)}{\\log(206/89.2)} = 2.1407\\). Kemudian \\(4^{1/2.1407}=206/{\\hat{\\theta}} \\Rightarrow {\\hat{\\theta}}=107.8\\). Seperti metode momen, pencocokan persentil hampir terlalu fleksibel dalam arti bahwa estimator dapat bervariasi tergantung pada persentil berbeda yang dipilih. Misalnya, seorang aktuaris dapat menggunakan estimasi pada persentil ke-25 dan ke-95 sedangkan yang lain menggunakan persentil ke-20 dan ke-80. Secara umum estimasi parameter akan berbeda dan tidak ada alasan kuat untuk memilih salah satu dari yang lain. Seperti halnya metode momen, pencocokan persentil menarik karena memberikan teknik yang dapat diterapkan dengan mudah dalam situasi tertentu dan memiliki dasar intuitif. Meskipun sebagian besar aplikasi aktuaria menggunakan estimator kemungkinan maksimum, akan lebih mudah untuk memiliki pendekatan alternatif seperti metode momen dan pencocokan persentil yang tersedia. 4.2 Model Selection Menjelaskan proses pemilihan model berdasarkan: dataset dalam sampel atau pelatihan, dataset out -of-sampel atau uji, dan metode yang menggabungkan pendekatan ini dikenal sebagai cross-validation . 4.2.1 Pemilihan Model Iteratif Dalam memeriksa data secara grafis, membuat hipotesis struktur model, dan membandingkan data dengan model kandidat untuk merumuskan model yang lebih baik. Box ( 1980 ) menggambarkan ini sebagai proses berulang yang ditunjukkan pada Gambar dibawah ini src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.2.1-1png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Proses berulang ini memberikan resep yang berguna untuk menyusun tugas menentukan model untuk mewakili satu set data. Langkah pertama, tahap perumusan model, dilakukan dengan memeriksa data secara grafis dan menggunakan pengetahuan hubungan sebelumnya, seperti dari teori ekonomi atau praktik industri. Langkah kedua dalam iterasi adalah fitting berdasarkan asumsi model yang ditentukan. Asumsi ini harus konsisten dengan data untuk menggunakan model secara valid. Langkah ketiga adalah pemeriksaan diagnostik ; data dan model harus konsisten satu sama lain sebelum kesimpulan tambahan dapat dibuat. Pengecekan diagnostik adalah bagian penting dari formulasi model; itu dapat mengungkapkan kesalahan yang dilakukan pada langkah sebelumnya dan memberikan cara untuk memperbaiki kesalahan ini. 4.2.2 Model Selection Based on a Training Dataset Biasanya merujuk ke kumpulan data yang digunakan untuk analisis sebagai kumpulan data dalam sampel atau pelatihan . Teknik yang tersedia untuk memilih model tergantung pada apakah hasilnya X diskrit, kontinu, atau campuran dari keduanya, meskipun prinsipnya sama. Grafik dan Tindakan Ringkasan Dasar lainnya. Mulailah dengan meringkas data secara grafis dan dengan statistik yang tidak bergantung pada bentuk parametrik tertentu. Tes Rasio Kemungkinan. Untuk membandingkan kecocokan model, jika satu model merupakan bagian dari model lainnya, maka uji rasio kemungkinan dapat digunakan; pendekatan umum untuk pengujian rasio kemungkinan Kebaikan Statistik Fit. Secara umum, model bukan himpunan bagian yang tepat satu sama lain sehingga statistik kecocokan secara keseluruhan sangat membantu untuk membandingkan model. Kriteria informasi adalah salah satu jenis kebaikan statistik. Untuk memilih distribusi yang sesuai, statistik yang membandingkan kecocokan parametrik dengan alternatif nonparametrik. 4.2.3 Model Selection Based on a Test Dataset Validasi model adalah proses konfirmasi bahwa model yang diusulkan sesuai, terutama mengingat tujuan penyelidikan. Keterbatasan penting dari proses pemilihan model hanya berdasarkan data dalam sampel adalah bahwa hal itu dapat rentan terhadap data-snooping , yaitu menyesuaikan sejumlah besar model ke satu set data. Memilih model hanya berdasarkan data dalam sampel juga tidak mendukung tujuan inferensi prediktif . 4.2.4 Model Selection Based on Cross-Validation Meskipun validasi out-of-sample adalah standar emas dalam pemodelan prediktif, tidak selalu praktis untuk melakukannya. Alasan utamanya adalah kita memiliki ukuran sampel yang terbatas dan kriteria pemilihan model di luar sampel dalam persamaan (4.4) bergantung pada pemisahan data secara acak . Ini berarti bahwa analis yang berbeda, bahkan ketika mengerjakan kumpulan data yang sama dan pendekatan pemodelan yang sama, dapat memilih model yang berbeda. Prosedur Validasi Silang. Sebagai alternatif, seseorang dapat menggunakan cross-validation , sebagai berikut. Prosedur dimulai dengan menggunakan mekanisme acak untuk membagi data menjadi K himpunan bagian dengan ukuran yang kira-kira sama yang dikenal sebagai lipatan , di mana analis biasanya menggunakan 5 hingga 10. Selanjutnya, yang satu menggunakan yang pertama K-1 subsampel untuk memperkirakan parameter model. Kemudian, “prediksi” hasil untuk K th subsampel dan gunakan ukuran seperti pada persamaan (4.4) untuk meringkas kecocokan. Sekarang, ulangi ini dengan menahan masing-masing K subsampel, meringkas dengan statistik out-of-sample. Jadi, rangkumlah ini K statistik, biasanya dengan rata-rata, untuk memberikan satu statistik keseluruhan untuk tujuan perbandingan. Ulangi langkah-langkah ini untuk beberapa model kandidat dan pilih model dengan statistik validasi silang terendah secara keseluruhan. 4.3 Estimasi Menggunakan Data Modifikasi Penjelasan pada subbab ini: Mendeskripsikan data yang dikelompokkan, disensor, dan terpotong Perkirakan distribusi parametrik berdasarkan data yang dikelompokkan, disensor, dan terpotong Perkirakan distribusi secara nonparametrik berdasarkan data yang dikelompokkan, disensor, dan terpotong 4.3.1 Estimasi Parametrik menggunakan Data Modifikasi Seperti yang kita ketahui bahwa Estimasi parametrik bersifat kuantitatif dan menggunakan statistik untuk menghitung perkiraan jumlah sumber daya yang dibutuhkan untuk menyelesaikan proyek Anda, baik itu biaya atau waktu, atau bahkan sumber daya manusia. Bagian 3.5 memperkenalkan konsep observasi yang “ dimodifikasi ” karena dua jenis batasan umum: penyensoran dan pemotongan. Misalnya, adalah umum untuk berpikir tentang asuransi yang dapat dikurangkan sebagai menghasilkan data yang terpotong (dari kiri) atau batasan polis sebagai menghasilkan data yang disensor (dari kanan). Sudut pandang ini dari perusahaan asuransi utama (penjual asuransi). Secara khusus, bagian ini akan membahas metode estimasi parametrik untuk tiga alternatif data individual, lengkap, dan tidak dimodifikasi: data dengan sensor interval hanya tersedia dalam kelompok, data yang terbatas ataudisensor , dan data yang tidak dapat diamati karena pemotongan . 4.3.1.1 Estimasi Parametrik menggunakan Data yang Dikelompokkan Pertimbangkan sampel ukuran N diamati dari distribusinya \\(F( ⋅ )\\), tetapi dalam kelompok sehingga kita hanya mengetahui kelompok tempat setiap pengamatan jatuh, bukan nilai pastinya. Ini disebut sebagai data yang dikelompokkan atau disensor interval . Memformalkan ide ini, misalkan ada k kelompok atau interval yang dibatasi oleh batas \\(C_0&lt;C_1&lt; ⋯ &lt;C_k.\\) Untuk setiap pengamatan, kami hanya mengamati interval jatuhnya \\(((C_{j − 1},C_J))\\), bukan nilai yang tepat. Dengan demikian, kita hanya mengetahui jumlah observasi pada setiap interval. Konstanta \\({C_0&lt;C_1&lt; ⋯ &lt;C_k}\\) membentuk beberapa partisi dari domain \\(F( ⋅ )\\). Kemudian probabilitas pengamatan \\(X_i\\) jatuh di \\(J\\)th interval ke- adalah \\[ Pr(X_i \\in (c_{j-1},c_j])=F(c_j)-F(c_{j-1}) \\] Fungsi massa probabilitas yang sesuai untuk pengamatan adalah src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.3.1-1.png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Sekarang, tentukan \\(N_J\\) menjadi jumlah pengamatan yang termasuk dalam \\(J\\)th interval, \\((C_{j − 1},C_J]\\). Jadi, fungsi kemungkinan (sehubungan dengan parameter) \\(θ\\)) adalah src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.3.1-2.png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Dan fungsi log-kemungkinan adalah src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.3.1-3png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Diberikan data : 1. Kerugian mengikuti distribusi eksponensial dengan rata-rata \\(θ\\). 2. Sebuah sampel acak dari 20 kerugian didistribusikan sebagai berikut: src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/example4.3-1.png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Hitung estimasi kemungkinan maksimum dari \\(θ\\) \\[ \\begin{aligned} L(\\theta) &amp;= F(1000)^7[F(2000)-F(1000)]^6[1-F(2000)]^7 \\\\ &amp;= (1-e^{-1000/\\theta})^7(e^{-1000/\\theta} - e^{-2000/\\theta})^6(e^{-2000/\\theta})^7 \\\\ &amp;= (1-p)^7(p-p^2)^6(p^2)^7 \\\\ &amp;= p^{20}(1-p)^{13} \\end{aligned} \\] di mana \\(p = e^{-1000/θ}\\). Memaksimalkan ekspresi ini sehubungan dengan \\(p\\) setara dengan memaksimalkan kemungkinan terhadap \\(θ\\). Maksimum terjadi pada \\(p=\\frac{20}{33}\\). sehingga \\(\\hat{\\theta}=\\frac{-1000}{\\log(20/33)}= 1996.90\\) 4.3.1.2 Cencored Data Penyensoran terjadi ketika kita hanya mencatat nilai yang terbatas dari sebuah observasi. Bentuk yang paling umum adalah penyensoran kanan, di mana kita mencatat nilai yang lebih kecil dari variabel dependen “benar” dan nilai penyensoran. Dengan menggunakan notasi, dengan X mewakili hasil yang diminati, seperti kerugian akibat kejadian yang diasuransikan atau waktu hingga kejadian. Dengan \\(C_U\\) menyatakan jumlah penyensoran. Dengan pengamatan tersensor kanan, mencatat \\(X_U^* = min(X, C_U) = X∧C_U\\). Lalu juga mencatat apakah penyensoran telah terjadi atau tidak. \\(δ_U = I(X≤C_U)\\) adalah variabel biner yang bernilai 0 jika penyensoran terjadi dan 1 jika tidak, yaitu, \\(δ_U\\) menunjukkan apakah X tidak disensor atau tidak. Sebagai contoh \\(C_U\\) dapat merepresentasikan batas atas pertanggungan sebuah polis asuransi. Kerugian dapat melebihi jumlah \\(C_U\\) tetapi perusahaan asuransi hanya memiliki \\(C_U\\) dalam catatannya sebagai jumlah yang dibayarkan dan tidak memiliki jumlah kerugian aktual \\(X\\) dalam catatannya. Sama halnya dengan penyensoran kiri, dapat mencatat yang lebih besar dari variabel yang diminati dan variabel yang disensor. Jika \\(C_L\\) digunakan untuk merepresentasikan jumlah penyensoran, maka mencatat \\(X_L^*=max(X,C_L)\\) bersama dengan indikator penyensoran \\(δ_L=I(X&gt;C_L)\\). Sebagai contoh, reasuradur akan menanggung kerugian penanggung yang lebih besar dari \\(C_L\\) ini berarti reasuradur bertanggung jawab atas kelebihan \\(X_L^*\\) pada \\(C_L\\). Dengan menggunakan notasi, kerugian reasuradur adalah \\(Y = X_L^*L-C_L\\) Untuk melihat hal ini, pertama-tama pertimbangkan kasus di mana pemegang polis mengalami kerugian \\(X &lt; C_L\\). Kemudian, penanggung akan membayar seluruh klaim dan \\(Y=C_L-C_L=0\\) tidak ada kerugian bagi reasuradur. Sebaliknya, jika kerugian \\(X≥C_L\\) maka \\(Y = X-C_L\\) merupakan klaim yang ditahan oleh reasuradur. Dengan kata lain, jika terjadi kerugian, reasuradur mencatat jumlah sebenarnya jika melebihi batas \\(C_L\\) dan jika tidak, hanya mencatat akan mengalami kerugian sebesar 0. 4.3.1.3 Truncated data Pengamatan yang disensor dicatat untuk studi, meskipun dalam bentuk yang terbatas. Sebaliknya, hasil yang terpotong adalah jenis data yang hilang. Sebuah hasil berpotensi terpotong ketika ketersediaan pengamatan bergantung pada hasil. Dalam asuransi, biasanya pengamatan terpotong kiri pada \\(C_L\\) ketika jumlahnya adalah \\[ \\begin{aligned} Y &amp;= \\left\\{ \\begin{array}{cl} \\text{we do not observe }X &amp; X \\le C_L \\\\ X &amp; X &gt; C_L \\end{array} \\right.\\end{aligned} \\] Dengan kata lain, jika X kurang dari ambang batas \\(C_L\\) maka ia tidak teramati. \\(C_L\\) dapat merepresentasikan deductible dari sebuah polis asuransi. Jika kerugian yang diasuransikan kurang dari deductible, maka perusahaan asuransi mungkin tidak mengamati atau mencatat kerugian sama sekali. Jika kerugian melebihi deductible, maka kelebihan \\(X-C_L\\) adalah klaim yang ditanggung oleh penanggung. Dimana dapat didefinisikan kerugian per pembayaran sebagai \\[ \\begin{aligned} Y^{P} = \\left\\{ \\begin{matrix} \\text{Undefined} &amp; X \\le d \\\\ X - d &amp; X &gt; d \\end{matrix} \\right. \\end{aligned} \\] sehingga jika kerugian melebihi deductible, kami mencatat jumlah kelebihan \\(X-d\\). Hal ini sangat penting ketika mempertimbangkan jumlah yang akan dibayarkan oleh perusahaan asuransi. Namun, untuk tujuan estimasi pada bagian ini, tidak terlalu penting jika kita mengurangkan konstanta yang diketahui seperti \\(C_L = d\\). Sehingga, untuk variabel terpotong \\(Y\\) kita menggunakan konvensi yang lebih sederhana dan tidak mengurangkan \\(d\\). Demikian pula untuk data terpotong kanan, jika X melebihi ambang batas \\(C_U\\) maka data tersebut tidak diobservasi. Dalam hal ini, jumlahnya adalah \\[ \\begin{aligned} Y &amp;= \\left\\{ \\begin{array}{cl} X &amp; X \\le C_U \\\\ \\text{we do not observe }X &amp; X &gt; C_U. \\end{array} \\right.\\end{aligned} \\] Contoh klasik dari pemotongan dari kanan termasuk X sebagai ukuran jarak ke bintang. Ketika jaraknya melebihi tingkat tertentu \\(C_U\\) maka bintang tersebut tidak lagi dapat diamati. Gambar dibawah ini membandingkan pengamatan yang terpotong dan tersensor. Nilai-nilai X yang lebih besar dari batas penyensoran “atas” \\(C_U\\) tidak teramati sama sekali (tersensor kanan), sedangkan nilai X yang lebih kecil dari batas pemotongan “bawah” \\(C_L\\) tetap diamati, tetapi diamati sebagai \\(C_L\\) daripada nilai X yang sebenarnya (tersensor kiri). 4.3.1.4 Parametric Estimation using Cencored and Truncated data Untuk mempermudah, dapat diasumsikan jumlah penyensoran tidak acak dan hasil yang kontinu X . Sebagai permulaan, pertimbangkan kasus data tersensor kanan di mana merekam \\(X_U^* = min(X, C_U) = X∧C_U\\)) dan indikator penyensoran \\(δ = I(X≤C_U)\\) . Jika penyensoran terjadi sehingga \\(δ=0\\) maka \\(X&gt;C_U\\) dan peluangnya adalah \\(Pr(X&gt;C_U)=1-F(C_U)\\). Jika penyensoran tidak terjadi sehingga \\(δ = 1\\) maka \\(X≤C_U\\) dan likelihoodnya adalah \\(f(x)\\) . Ringkasnya, didapatkan likelihood dari sebuah pengamatan tunggal sebagai \\[ \\begin{aligned} \\left\\{ \\begin{array}{ll} 1-F(C_U) &amp; \\text{if }\\delta=0 \\\\ f(x) &amp; \\text{if } \\delta = 1 \\end{array} \\right. = \\left\\{ f(x)\\right\\}^{\\delta} \\left\\{1-F(C_U)\\right\\}^{1-\\delta} . \\end{aligned} \\] Ekspresi ruas kanan memungkinkan dalam menyajikan peluang dengan lebih ringkas. Sekarang, untuk sampel ke-i dengan ukuran n , peluangnya adalah \\[ \\begin{aligned} L(\\theta) = \\prod_{i=1}^n \\left\\{ f(x_i)\\right\\}^{\\delta_i} \\left\\{1-F(C_{Ui})\\right\\}^{1-\\delta_i} = \\prod_{\\delta_i=1} f(x_i) \\prod_{\\delta_i=0} \\{1-F(C_{Ui})\\} \\end{aligned} \\] dengan waktu penyensoran potensial \\({(C_{U1},...,C_{Un})}\\) . Di sini, notasi “\\(∏{δi} = 1\\)” berarti mengambil hasil kali dari pengamatan yang tidak disensor, dan demikian pula untuk “\\(∏{δi} = 0\\)” Di sisi lain, data terpotong ditangani dalam inferensi kemungkinan melalui probabilitas bersyarat. Secara khusus, kontribusi likelihood dapat disesuaikan dengan membaginya dengan probabilitas bahwa variabel tersebut diamati. Sebagai rangkuman, kami memiliki kontribusi berikut pada fungsi likelihood untuk enam jenis hasil: src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/table4-1.png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Untuk hasil yang diketahui dan data yang disensor, kemungkinannya adalah \\[ \\begin{aligned} L(\\theta) = \\prod_{E} f(x_i) \\prod_{R} \\{1-F(C_{Ui})\\} \\prod_{L} F(C_{Li}) \\prod_{I} (F(C_{Ui})-F(C_{Li})), \\end{aligned} \\] di mana \\(&quot;∏_E&quot;\\) adalah hasil kali pengamatan dengan nilai Exact, dan demikian pula untuk Right-,Left- and Interval-censoring. Untuk data yang disensor kanan dan terpotong kiri, kemungkinannya adalah \\[ \\begin{aligned} L(\\theta) = \\prod_{E} \\frac{f(x_i)}{1-F(C_{Li})} \\prod_{R} \\frac{1-F(C_{Ui})}{1-F(C_{Li})}, \\end{aligned} \\] dan juga untuk kombinasi lainnya. Example 4.3.2. Actuarial Exam Question Diberikan data : Sebuah contoh kerugian adalah: 600 700 900 Tidak ada informasi yang tersedia mengenai kerugian sebesar 500 atau kurang. Kerugian diasumsikan mengikuti distribusi eksponensial dengan rata-rata \\(θ\\). Hitung estimasi kemungkinan maksimum dari \\(θ\\) Pengamatan ini terpotong pada angka 500. Kontribusi dari setiap pengamatan terhadap fungsi likelihood adalah \\(\\frac{f(x)}{1-F(500)} = \\frac{\\theta^{-1}e^{-x/\\theta}}{e^{-500/\\theta}}\\) Lalu Fungsi Likelihoodnya adalah \\(L(\\theta)= \\frac{\\theta^{-1} e^{-600/\\theta} \\theta^{-1} e^{-700/\\theta} \\theta^{-1} e^{-900/\\theta}}{(e^{-500/\\theta})^3} = \\theta^{-3}e^{-700/\\theta}\\) Log-Likehoodnya adalah \\(l(\\theta) = \\log L(\\theta) = -3 \\log \\theta - 700 \\theta^{-1}\\) Memaksimalkan ekspresi ini dengan menetapkan turunan terhadap θ sama dengan 0, Maka memiliki \\(L&#39;(\\theta) = -3 \\theta^{-1} + 700 \\theta^{-2} = 0 \\ \\Rightarrow \\ \\hat{\\theta} = \\frac{700}{3} = 233.33 .\\) 4.3.2 Nonparametric Estimation using Modified Data Estimator nonparametrik memberikan tolok ukur yang berguna, sehingga akan sangat membantu untuk memahami prosedur estimasi untuk data yang dikelompokkan, disensor, dan dipotong 4.3.2.1 Grouped Data Pengamatan dapat dikelompokkan (juga disebut sebagai interval tersensor) dalam arti bahwa pengamatan sebagai bagian dari salah satu dari k interval dalam bentuk \\((c_{j-1},c_j)\\) , untuk \\(j = 1,...,k\\) . Pada batas-batasnya, fungsi distribusi empiris didefinisikan dengan cara yang biasa: \\[ \\begin{aligned} F_n(c_j) = \\frac{\\text{number of observations } \\le c_j}{n} \\end{aligned} \\] Ogive Estimator Untuk nilai lain dari \\(x∈(c_{j-1},c_j)\\) dapat mengestimasi fungsi distribusi dengan ogive estimator yang menginterpolasi secara linear antara \\(F_n(c_{j-1})\\) dan \\(Fn_(c_j)\\) yaitu nilai dari batas-batas \\(F_n(c_{j-1})\\) dan \\(Fn_(c_j)\\) dihubungkan dengan sebuah garis lurus. Hal ini secara formal dapat dinyatakan sebagai \\[ \\begin{aligned} F_n(x) = \\frac{c_j-x}{c_j-c_{j-1}} F_n(c_{j-1}) + \\frac{x-c_{j-1}}{c_j-c_{j-1}} F_n(c_j) \\ \\ \\ \\text{for } c_{j-1} \\le x &lt; c_j \\end{aligned} \\] Sehinga Densitas yang sesuai adalah \\[ \\begin{aligned} f_n(x) = F^{\\prime}n(x) = \\frac{F_n(c_j)-F_n(c{j-1})}{c_j - c_{j-1}} \\ \\ \\ \\text{for } c_{j-1} &lt; x &lt; c_j . \\end{aligned} \\] Example 4.3.4. Actuarial Exam Question Diberikan informasi berikut ini mengenai jumlah klaim untuk 100 klaim: src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/example4.3.4-1.png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Dengan menggunakan ogive, hitunglah estimasi probabilitas bahwa klaim yang dipilih secara acak adalah antara 2000 dan 6000. Pada batas-batasnya, fungsi distribusi empiris didefinisikan dengan cara yang biasa, sehingga memiliki \\(F_{100}(1000) = 0.16, \\ F_{100}(3000)=0.38, \\ F_{100}(5000)=0.63, \\ F_{100}(10000)=0.81\\) Untuk ukuran klaim lainnya, penaksir ogive melakukan interpolasi linier di antara nilai-nilai ini: \\[ \\begin{array}{ll} F_{100}(2000) &amp;= 0.5F_{100}(1000) + 0.5F_{100}(3000) = 0.5(0.16)+0.5(0.38)=0.27 \\\\ F_{100}(6000) &amp;=0.8F_{100}(5000)+0.2F_{100}(10000) = 0.8(0.63)+0.2(0.81)=0.666 \\end{array} \\] Dengan demikian, probabilitas klaim antara 2000 dan 6000 adalah \\(F_{100}(6000) - F_{100}(2000) = 0.666-0.27 = 0.396\\) 4.3.2.2 Right-Censored Empirical Distribution Function Akan sangat berguna untuk mengkalibrasi penaksir parametrik dengan metode nonparametrik yang tidak bergantung pada bentuk parametrik distribusi. Penaksir batas produk menurut (Kaplan dan Meier 1958) merupakan penaksir yang terkenal untuk fungsi distribusi dengan adanya penyensoran. Motivasi untuk Penaksir Batas Produk Kaplan-Meier Untuk menjelaskan mengapa product-limit bekerja dengan sangat baik dengan observasi tersensor, pertama-tama dapat melihat ke kasus tanpa penyensoran. Di sini, fungsi distribusi empiris \\(F_n(x)\\) adalah penaksir tak bias dari fungsi distribusi \\(F(x)\\) . Hal ini karena \\(F_n(x)\\) adalah rata-rata dari variabel indikator yang masing-masing tidak bias, yaitu, \\(E [I(X_i≤x)]=Pr(X_i≤x)=F(x)\\) Sekarang misalkan hasil acak disensor di sebelah kanan dengan jumlah yang membatasi, katakanlah, CU sehingga dapat mencatat yang lebih kecil dari keduanya, \\(X^* = min(X, C_U)\\) . Untuk nilai-nilai \\(x\\) yang lebih kecil dari \\(C_U\\), variabel indikator masih memberikan penaksir yang tidak bias terhadap fungsi distribusi sebelum kita mencapai batas penyensoran. Artinya, \\(E [I(X^∗≤x)]=F(x)\\) karena \\(I(X^∗≤x)=I(X≤x)\\) untuk \\(x&lt;C_U\\) . Dengan cara yang sama, \\(E[I(X^∗&gt;x)]=1-F(x)=S(x)\\) . Tetapi, untuk \\(x&gt;C_U\\) , \\(I(X^∗≤x)\\) secara umum bukan merupakan penaksir tak bias dari F(x). Sebagai alternatif, pertimbangkan dua peubah acak yang memiliki batas penyensoran yang berbeda. Sebagai ilustrasi, misalkan kita mengamati \\(X^∗1=min(X_1,5)\\) dan \\(X^∗2 = min(X_2,10)\\) di mana \\(X_1\\) dan \\(X_2\\) adalah undian independen dari distribusi yang sama. Untuk \\(x≤5\\) fungsi distribusi empiris \\(F_2(x)\\) adalah penaksir tak bias dari \\(F(x)\\). Akan tetapi, untuk \\(5&lt;x≤10\\) pengamatan pertama tidak dapat digunakan untuk fungsi distribusi karena adanya batasan penyensoran. Sebagai gantinya, strategi yang dikembangkan oleh (Kaplan dan Meier 1958) adalah dengan menggunakan \\(S_2(5)\\) sebagai penaksir dari \\(S(5)\\) dan kemudian menggunakan observasi kedua untuk mengestimasi fungsi survival bersyarat pada kelangsungan hidup hingga waktu ke-5, \\(Pr(X&gt;x|X&gt;5)=\\frac{S(x)}{S(5)}\\) . Secara khusus, untuk \\(5&lt;x≤10\\) penaksir dari fungsi survival adalah \\[ \\begin{aligned} \\hat{S}(x) = S_2(5) \\times I(X_2^* &gt; x ) \\end{aligned} \\] Kaplan-Meier Product Limit Estimator Dengen memperluas ide dalam setiap observasi i,dengan ui menjadi batas atas penyensoran \\((=∞) jikatidakadapenyensoran\\). Dengan demikian, nilai yang tercatat adalah xi dalam kasus tidak ada penyensoran dan ui jika ada penyensoran. Dengan \\(t_1&lt;⋯&lt;t_k\\)menjadi k titik berbeda di mana kerugian yang tidak disensor terjadi, dan biarkan \\(s_j\\) adalah jumlah kerugian yang tidak tersensor \\(x_i\\) yang tidak tersensor pada \\(t_j\\). Himpunan risiko yang sesuai adalah jumlah observasi yang aktif (tidak tersensor) pada nilai yang kurang dari \\(t_j\\) yang dinotasikan sebagai \\(R_j = \\sum_{i=1}^n I(x_i \\geq t_{j}) + \\sum_{i=1}^n I(u_i \\geq t_{j})\\) Dengan notasi ini, penaksir product-limit dari fungsi distribusi \\[ \\begin{equation} \\hat{F}(x) = \\left\\{ \\begin{array}{ll} 0 &amp; x&lt;t_{1} \\\\ 1-\\prod_{j:t_{j} \\leq x}\\left( 1-\\frac{s_j}{R_{j}}\\right) &amp; x \\geq t_{1} \\end{array} \\right. . \\tag{4.6} \\end{equation} \\] Sebagai contohnya, jika x lebih kecil dari kerugian terkecil yang tidak tersensor, maka \\(x&lt;t1\\) dan \\(F^(x)=0\\) . Sebagai contoh lain, jika \\(x\\) berada di antara kerugian tersensor terkecil kedua dan ketiga, maka \\(x∈(t_2,t_3]\\) dan \\(\\hat{F}(x) = 1 - \\left(1- \\frac{s_1}{R_{1}}\\right)\\left(1- \\frac{s_2}{R_{2}}\\right)\\) .Taksiran yang sesuai dari fungsi survival adalah \\(\\hat{S}(x) = 1 - \\hat{F}(x)\\) 4.3.3 Example 4.3.5. Actuarial Exam Question. Berikut ini adalah contoh dari 10 pembayaran: \\[ 4 \\space \\space 4 \\space \\space 5+\\space \\space 5+ \\space\\space 5+ \\space\\space 8 \\space\\space 10+ \\space\\space 10+ \\space\\space 12 \\space\\space 15 \\] dimana + menunjukkan bahwa kerugian telah melebihi batas polis. Dengan menggunakan estimator batas produk Kaplan-Meier, hitunglah probabilitas bahwa kerugian pada suatu polis melebihi 11, \\(\\hat{S}(11)\\) Terdapat empat waktu kejadian (pengamatan yang tidak disensor). Untuk setiap waktu tj kita dapat menghitung jumlah kejadian sj dan himpunan risiko \\(R_j\\) sebagai berikut: src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/example4.3.6-1.png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Dengan demikian, estimasi Kaplan-Meier dari S(11) adalah \\[ \\begin{aligned} \\hat{S}(11) &amp;= \\prod_{j:t_j\\leq 11} \\left( 1- \\frac{s_j}{R_j} \\right) = \\prod_{j=1}^{2} \\left( 1- \\frac{s_j}{R_j} \\right)\\\\ &amp;= \\left(1-\\frac{2}{10} \\right) \\left(1-\\frac{1}{5} \\right) = (0.8)(0.8)= 0.64. \\\\ \\end{aligned} \\] Right-Censored, Left-Truncated Empirical Distribution Function Selain penyensoran kanan, selanjutnya adalah memperluas kerangka kerja untuk memungkinkan data terpotong ke kiri. Seperti sebelumnya, untuk setiap observasi i , dengan \\(u_i\\) menjadi batas penyensoran atas ( \\(=∞\\) jika tidak ada penyensoran). Selanjutnya, \\(d_i\\) merupakan batas pemotongan bawah (0 jika tidak ada pemotongan). Dengan demikian, nilai yang tercatat (jika lebih besar dari \\(d_i\\) ) adalah \\(x_i\\) dalam kasus tidak ada penyensoran dan \\(u_i\\) jika ada penyensoran. Lalu untuk $t_1&lt;⋯&lt;t_k $menjadi \\(k\\) titik-titik yang berbeda di mana sebuah kejadian yang menarik terjadi, dan biarkan \\(s_j\\) adalah jumlah kejadian yang terekam \\(x_i\\) pada titik waktu \\(t_j\\). Himpunan risiko yang sesuai adalah \\(R_j = \\sum_{i=1}^n I(x_i \\geq t_{j}) + \\sum_{i=1}^n I(u_i \\geq t_{j}) - \\sum_{i=1}^n I(d_i \\geq t_{j}).\\) Dengan definisi baru dari himpunan risiko ini, penaksir batas hasil kali dari fungsi distribusi adalah seperti pada persamaan product limit estimator. Rumus Greenwood (Greenwood 1926) menurunkan rumus untuk estimasi varians dari penaksir batas-produk menjadi \\(\\widehat{Var}(\\hat{F}(x)) = (1-\\hat{F}(x))^{2} \\sum {j:t{j} \\leq x} \\dfrac{s_j}{R_{j}(R_{j}-s_j)}.\\) Seperti biasa, dapat mengacu pada akar kuadrat dari estimasi varians sebagai kesalahan standar, sebuah kuantitas yang secara rutin digunakan dalam interval kepercayaan dan untuk pengujian hipotesis. Untuk menghitungnya, metode survfit R mengambil sebuah objek data survival dan membuat sebuah objek baru yang berisi estimasi Kaplan-Meier dari fungsi survival bersama dengan interval kepercayaan. Metode Kaplan-Meier (type='kaplan-meier') digunakan secara default untuk membuat estimasi kurva survival. Fungsi survival diskrit yang dihasilkan memiliki massa titik pada waktu kejadian yang diamati (tanggal pelepasan) \\(t_j\\) dimana probabilitas suatu kejadian yang diberi ketahanan hidup pada durasi tersebut diestimasi sebagai jumlah kejadian yang diamati pada durasi sj dibagi dengan jumlah subjek yang terpapar atau ‘berisiko’ sesaat sebelum durasi kejadian \\(R_j\\). Penaksir Alternatif Dua jenis estimasi alternatif juga tersedia untuk metode survfit. Alternatif pertama (type='fh2') menangani hubungan, pada dasarnya, dengan mengasumsikan bahwa beberapa kejadian pada durasi yang sama terjadi dalam urutan yang berubah-ubah. Alternatif lain (type='fleming-harrington') menggunakan estimasi Nelson-Aalen (Aalen 1978) dari fungsi hazard kumulatif untuk mendapatkan estimasi fungsi survival. Estimasi bahaya kumulatif \\(H^(x)\\) dimulai dari nol dan bertambah pada setiap durasi kejadian yang diamati \\(t_j\\) dengan jumlah kejadian \\(s_j\\) dibagi dengan jumlah yang berisiko \\(R_j\\). Dengan notasi yang sama seperti di atas, penaksir Nelson-Äalen dari fungsi distribusi adalah \\[ \\begin{aligned} \\hat{F}_{NA}(x) &amp;= \\left\\{ \\begin{array}{ll} 0 &amp; x&lt;t_{1} \\\\ 1- \\exp \\left(-\\sum_{j:t_{j} \\leq x}\\frac{s_j}{R_j} \\right) &amp; x \\geq t_{1} \\end{array} \\right. .\\end{aligned} \\] Itu merupakan hasil dari estimator Nelson-Äalen dari fungsi hazard kumulatif \\(\\hat{H}(x)=\\sum_{j:t_j\\leq x} \\frac{s_j}{R_j}\\) dan hubungan antara fungsi survival dan fungsi hazard kumulatif, \\(\\hat{S}_{NA}(x)=e^{-\\hat{H}(x)}\\) 4.4 Bayesian Inference Penjelasan pada subbab ini: Jelaskan model Bayesian sebagai alternatif dari pendekatan frequentist dan rangkum lima komponen dari pendekatan pemodelan ini. Ringkas distribusi parameter posterior dan gunakan distribusi posterior ini untuk memprediksi hasil baru. Gunakan distribusi konjugat untuk menentukan distribusi parameter posterior. 4.4.1 Introduction to Bayesian Inference Sampai saat ini, metode inferensial kami berfokus pada pengaturan frequentist , di mana sampel diambil berulang kali dari suatu populasi. Vektor parameter θ adalah tetap belum diketahui, sedangkan hasil X adalah realisasi variabel acak. Sebaliknya, di bawah kerangka Bayesian , kami melihat parameter model dan data sebagai variabel acak. Kami tidak yakin tentang parameternya θ dan gunakan alat probabilitas untuk mencerminkan ketidakpastian ini. Dibawah ini merupakan rumus aturan bayes: src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.4.1-1.png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Di mana, Pr(parameters): adalah distribusi parameter, yang dikenal sebagai distribusi sebelumnya . Pr(data|parameters): adalah distribusi sampling. Dalam konteks frequentist, ini digunakan untuk membuat kesimpulan tentang parameter dan dikenal sebagai kemungkinan . Pr(parameters|data):adalah distribusi parameter setelah mengamati data, yang dikenal sebagai distribusi posterior . Pr(data): adalah distribusi marjinal dari data. Ini umumnya diperoleh dengan mengintegrasikan (atau menjumlahkan) distribusi gabungan data dan parameter di atas nilai parameter. 4.4.2 Bayesian Model Distribusi Sebelumnya. Secara khusus, pikirkan tentang parameter θ sebagai vektor acak dan biarkan π( θ ) menunjukkan fungsi massa atau kepadatan yang sesuai. Ini adalah pengetahuan yang kita miliki sebelum hasil diamati dan disebut distribusi sebelumnya . Biasanya, distribusi sebelumnya adalah distribusi reguler dan terintegrasi atau dijumlahkan menjadi satu, tergantung pada apakah θ kontinu atau diskrit. Namun, kami mungkin sangat tidak yakin (atau tidak tahu) tentang distribusinya θ ; mesin Bayesian memungkinkan situasi berikut \\[ \\int \\pi(\\theta) ~d\\theta = \\infty, \\] dalam hal ini, \\(\\pi(\\cdot)\\) disebut priot yang tidak tepat Distribusi Bersama. Distribusi hasil dan parameter model adalah distribusi gabungan dari dua besaran acak. Fungsi kerapatan persendiannya dilambangkan sebagai \\[ f(x , \\boldsymbol \\theta) = f(x|\\boldsymbol \\theta )\\pi(\\boldsymbol \\theta) \\] Distribusi Hasil Marginal. Distribusi hasil dapat dinyatakan sebagai \\[ f(x) = \\int f(x | \\boldsymbol \\theta)\\pi(\\boldsymbol \\theta) ~d \\boldsymbol \\theta. \\] Ini analog dengan distribusi campuran frequentist. Dalam distribusi campuran, kami menggabungkan (atau “mencampur”) subpopulasi yang berbeda. Dalam konteks Bayesian, distribusi marjinal adalah kombinasi dari realisasi parameter yang berbeda (dalam beberapa literatur, Anda dapat menganggap ini sebagai kombinasi “keadaan alam” yang berbeda). Distribusi Parameter Posterior. Setelah hasil diamati (karenanya terminologi “posterior”), seseorang dapat menggunakan teorema Bayes untuk menulis fungsi kerapatan sebagai \\[ \\pi(\\boldsymbol \\theta | x) =\\frac{f(x , \\boldsymbol \\theta)}{f(x)} =\\frac{f(x|\\boldsymbol \\theta )\\pi(\\boldsymbol \\theta)}{f(x)} . \\] 4.4.3 Bayesian Inference 4.4.3.1 Summarizing the Posterior Distributiob of Paremeters Salah satu cara untuk meringkas distribusi adalah dengan menggunakan pernyataan tipe interval kepercayaan . Untuk meringkas distribusi parameter posterior , interval \\([a,b]\\) dikatakan sebagai \\(100(1-\\alpha)\\%\\) interval kredibilitas untuk \\(\\theta\\) jika \\[ \\Pr (a \\le \\theta \\le b | \\mathbf{x}) \\ge 1- \\alpha. \\] Estimasi Bayes adalah nilai yang meminimalkan kerugian yang diharapkan \\[ \\mathrm{E~}[ l(\\hat{\\theta}, \\theta)] \\] 4.4.3.2 Bayesian Predictive Distribution Untuk jenis inferensi statistik lainnya, seringkali menarik untuk “memprediksi” nilai hasil acak yang belum diamati. Khususnya untuk data baru y, distribusi prediktifnya adalah \\[ f(y|x) = \\int f(y|\\theta) \\pi(\\theta|x) d\\theta . \\] Ini juga kadang-kadang disebut distribusi “prediktif posterior” karena distribusi data baru tergantung pada kumpulan data dasar. Menggunakan kerugian kesalahan kuadrat untuk fungsi kerugian, prediksi Bayesian dari Y adalah \\[ \\begin{aligned} \\mathrm{E}(Y|X) &amp;= \\int ~y f(y|X) dy = \\int y \\left(\\int f(y|\\theta) \\pi(\\theta|X) d\\theta \\right) dy \\\\ &amp;= \\int \\left(\\int y f(y|\\theta) ~dy \\right) \\pi(\\theta|X) ~d\\theta \\\\ &amp;= \\int \\mathrm{E}(Y|\\theta) \\pi(\\theta|X) ~d\\theta . \\end{aligned} \\] Seperti disebutkan sebelumnya, untuk beberapa situasi distribusi parameter adalah diskrit, tidak kontinu. Memiliki serangkaian kemungkinan parameter yang terpisah memungkinkan kita untuk menganggapnya sebagai “keadaan alam” alternatif, sebuah interpretasi yang membantu. 4.4.4 Conjugate Distributions Untuk menghubungkan distribusi parameter sebelum dan sesudah, kami memiliki hubungan \\[ \\begin{array}{ccc} \\pi(\\boldsymbol \\theta | x) &amp; = &amp; \\frac{f(x|\\boldsymbol \\theta )\\pi(\\boldsymbol \\theta)}{f(x)} \\\\ &amp; \\propto &amp; f(x|\\boldsymbol \\theta ) \\pi(\\boldsymbol \\theta) \\\\ \\text{Posterior} &amp; \\text{is proportional to} &amp; \\text{likelihood} \\times \\text{prior} . \\end{array} \\] Untuk distribusi konjugasi, posterior dan sebelumnya milik keluarga distribusi yang sama. Ilustrasi berikut melihat kasus khusus gamma-Poisson, yang paling terkenal dalam aplikasi aktuaria. "],["aggregate-loss-models.html", "Bab 5 Aggregate Loss Models 5.1 Introduction 5.2 Moments and Distribution 5.3 5.4 Menghitung Distribusi Klaim Agregat", " Bab 5 Aggregate Loss Models 5.1 Introduction Sub bab ini membahas mengenai pembangunan model probabilitas untuk menggambarkan klaim agregat oleh sistem asuransi yang terjadi dalam periode waktu tertentu. Sistem asuransi dapat berupa polis tunggal, kontrak asuransi kelompok, lini bisnis , atau seluruh buku bisnis perusahaan asuransi. Dalam bab ini, klaim agregat mengacu pada jumlah klaim dari portofolio kontrak asuransi. Pertimbangkan portofolio asuransi dari \\(N\\) kontrak individu, dan \\(S\\) menunjukkan kerugian agregat portofolio dalam jangka waktu tertentu. Ada dua pendekatan untuk memodelkan kerugian agregat \\(S\\) , model risiko individu dan model risiko kolektif. Model risiko individu menekankan kerugian dari masing-masing kontrak individu dan mewakili kerugian agregat sebagai: \\[S_n=X_1 +X_2 +\\cdots+X_n,\\] Di mana \\(X_i~(i=1,\\ldots,n)\\) diinterpretasikan sebagai jumlah kerugian dari \\(X_i\\) kontrak. \\(N\\) menunjukkan jumlah kontrak dalam portofolio dan dengan demikian merupakan angka tetap daripada variabel acak. Untuk model risiko individu, biasanya diasumsikan \\(X_i\\) ini independen. Karena fitur kontrak yang berbeda seperti cakupan dan paparan , \\(X_i\\) belum tentu terdistribusi secara identik. Fitur penting dari distribusi masing-masing \\(X_i\\) adalah massa probabilitas pada nol yang sesuai dengan peristiwa tidak adanya klai Model risiko kolektif mewakili kerugian agregat dalam hal distribusi frekuensi dan distribusi keparahan: \\[S_N=X_1 +X_2 + \\cdots + X_N .\\] Sejumlah klaim acak \\(N\\) yang dapat mewakili baik jumlah kerugian atau jumlah pembayaran. Sebaliknya, dalam model risiko individual biasanya menggunakan sejumlah kontrak tetap \\(N\\).\\(X_1, X_2, \\ldots, X_N\\) sebagai representasi dari jumlah masing-masing kerugian. Setiap kerugian mungkin atau mungkin tidak sesuai dengan kontrak unik. Misalnya, mungkin ada banyak klaim yang timbul dari satu kontrak. Itu wajar untuk dipikirkan \\(X_i&gt;0\\) karena jika \\(X_i=0\\) maka tidak ada klaim yang terjadi. Biasanya kita menganggap bahwa kondisional pada \\(X_{1},X_{2},\\ldots ,X_{n}\\) adalah iid variabel acak. Distribusi dari N dikenal sebagai distribusi frekuensi , dan distribusi umum dari \\(X\\) dikenal sebagai distribusi keparahan . Dengan berasumsi \\(N\\) Dan \\(X\\) sendiri. Dengan model risiko kolektif, sehingga dapat menguraikan kerugian agregat menjadi frekuensi \\(( N )\\) proses dan tingkat keparahan \\(( X )\\) model. Fleksibilitas ini memungkinkan analis untuk mengomentari dua komponen terpisah ini. Misalnya, pertumbuhan penjualan karena standar penjaminan emisi yang lebih rendah dapat menyebabkan frekuensi kerugian yang lebih tinggi tetapi mungkin tidak memengaruhi keparahan. Demikian pula, inflasi atau kekuatan ekonomi lainnya dapat berdampak pada keparahan tetapi tidak pada frekuensi. 5.2 Moments and Distribution Jadi model risiko keleksif \\(SN=X_1+...+X_N\\) dan tidak bergantung pada N Misalkan \\(μ = E(X_i)\\) dan \\(σ^2=Var(X_i)\\) untuk semua \\(i\\) Dengan demikian, bersyarat pada N kita memiliki ekspektasi jumlah adalah jumlah ekspektasi dan varians. \\[ \\begin{aligned} {\\rm E}(S|N) &amp;= {\\rm E}(X_1 + \\cdots + X_N|N) = \\mu N \\\\ {\\rm Var}(S|N) &amp;= {\\rm Var}(X_1 + \\cdots + X_N|N) = \\sigma^2 N. \\end{aligned} \\] Dengan menggunakan hukum ekspektasi berulang,rata-rata kerugian agregat adalah \\({\\rm E}(S_N)={\\rm E}_N[{\\rm E}_S(S|N)] = {\\rm E}_N(N\\mu) = \\mu ~{\\rm E}(N).\\) Dengan menggunakan hukum varians total, varians dari kerugian agregat adalah \\[ \\begin{aligned} {\\rm Var}(S_N) &amp;= {\\rm E}_N[{\\rm Var}(S_N|N)] + {\\rm Var}_N[{\\rm E}(S_N|N)] \\\\ &amp;= \\mathrm{E}_N \\left[ \\sigma^2 N \\right] + \\mathrm{Var}_N\\left[ \\mu N \\right] \\\\ &amp;=\\sigma^2~{\\rm E}[N] + \\mu^2~ {\\rm Var}[N] . \\end{aligned} \\] Kasus Khusus: Frekuensi Berdistribusi Poisson. Jika \\(N∼Poi(λ)\\) maka \\[ \\begin{aligned} \\mathrm{E}(N) &amp;= \\mathrm{Var}(N) = \\lambda\\\\ \\mathrm{E}(S_N) &amp;= \\lambda ~\\mathrm{E}(X)\\\\ \\mathrm{Var}(S_N) &amp;= \\lambda (\\sigma^2 + \\mu^2) = \\lambda ~\\mathrm{E} (X^2). \\end{aligned} \\] 5.2.0.1 Actuarial Exam Question Jumlah kecelakaan mengikuti distribusi Poisson dengan rata-rata 12. Setiap kecelakaan menghasilkan 1, 2, atau 3 penuntut dengan probabilitas masing-masing 1/2, 1/3, dan 1/6. Hitunglah varians dalam jumlah total penuntut. JAWABAN \\[ \\begin{aligned} &amp; \\mathrm{E}(X^2) = 1^2 \\left( \\frac{1}{2}\\right) + 2^2\\left(\\frac{1}{3} \\right) + 3^2\\left(\\frac{1}{6}\\right) = \\frac{10}{3} \\\\ \\Rightarrow &amp;\\mathrm{Var}(S_N) = \\lambda \\ \\mathrm{E}(X^2) = 12\\left(\\frac{10}{3}\\right) = 40 . \\end{aligned} \\] Sebagai alternatif, Dapat menggunakan pendekatan umum, \\(\\mathrm{Var}(S_N) = \\sigma^2 \\mathrm{E}(N) + \\mu^2 \\mathrm{Var}(N)\\), Dimana \\[ \\begin{aligned} \\mathrm{E}(N) &amp;= \\mathrm{Var}(N) = 12 \\\\ \\mu &amp;= \\mathrm{E}(X) = 1\\left(\\frac{1}{2}\\right) + 2\\left(\\frac{1}{3}\\right) + 3\\left(\\frac{1}{6}\\right) = \\frac{5}{3} \\\\ \\sigma^2 &amp;= \\mathrm{E}(X^2) - [\\mathrm{E}(X)]^2 = \\frac{10}{3} - \\frac{25}{9} = \\frac{5}{9} \\\\ \\Rightarrow \\ \\mathrm{Var}(S_N) &amp;= \\left(\\frac{5}{9}\\right)\\left(12\\right) + \\left(\\frac{5}{3}\\right)^2\\left(12\\right) = 40 . \\end{aligned} \\] Secara umum, momen-momen SN dapat diturunkan dari fungsi pembangkit momen (mgf). Karena \\(X_i\\) adalah iid, dapat dinyatakan mgf dari X sebagai \\(M_{X(t)}= E(e^{tX})\\) . Dengan menggunakan hukum ekspektasi yang diiterasi, mgf dari \\(S_N\\) adalah \\[ \\begin{aligned} M_{S_N}(t) &amp;= \\mathrm{E}(e^{t S_N})=\\mathrm{E}_N[\\mathrm{E}(e^{tS_N}|N)]\\\\ &amp;= \\mathrm{E}_N \\left[ ~\\mathrm{E}\\left( e^{t(X_1+\\cdots+X_N)}\\right) ~\\right] = \\mathrm{E}_N \\left[ \\mathrm{E}(e^{tX_1})\\cdots\\mathrm{E}(e^{tX_N}) \\right] ~~ \\text{since } X_i \\text{&#39;s are independent} \\\\ &amp;= \\mathrm{E}N[~(M{X}(t))^N~] . \\end{aligned} \\] Lalu dapat melihat fungsi pembangkit probabilitas(pgf) dari N adalah \\(P_N(z)= E(Z^N)\\). Dengan menyatakan \\(M_X(t)=z\\), lalu mengganti ke dalam ekspresi untuk mgf dari SN di atas, maka diperoleh \\[ \\begin{aligned} M_{S_N}(t) = \\mathrm{E~}(z^N) = P_{N}(z) = P_{N}[M_{X}(t)]. \\end{aligned} \\] Demikian pula, jika \\(S_N\\) merupakan diskrit, dapat menunjukkan juga pgf dari \\(S_N\\) adalah \\[ \\begin{aligned} P_{S_N}(z) = P_{N}[P_{X}(z)] . \\end{aligned} \\] Untuk mendapatkan \\(E(S_N) = M′S_N(0)\\) dapat menggunakan aturan rantai: \\(M_{S_N}&#39;(t) = \\frac{\\partial}{\\partial t} P_{N}(M_{X}(t)) = P_{N}&#39;(M_{X}(t)) M_{X}&#39;(t)\\\\\\) Lalu Memanggil \\(M_{X}(0) = 1, M_{X}&#39;(0) = \\mathrm{E}(X) = \\mu, P_{N}&#39;(1) = \\mathrm{E}(N)\\) Jadi, \\(\\mathrm{E}(S_N) = M_{S_N}&#39;(0) = P_{N}&#39;(M_{X}(0)) M_{X}&#39;(0) = \\mu {\\rm E}(N) .\\) Demikian pula, dapat menggunakan relasi \\(E(S^2_N) = M′′_{S_N}(0)\\) untuk mendapatkan \\(\\mathrm{Var}(S_N) = \\sigma^2 \\mathrm{E}(N) + \\mu^2 \\mathrm{Var}(N).\\) Special Case. Poisson Frequency. Misalkan \\(N∼Poi(λ)\\) dengan demikian, pgf dari \\(N\\) adalah \\(P_N(z) = e^{λ(z-1)}\\) dan mgf dari \\(S_N\\) adalah \\[ \\begin{aligned} M_{S_N}(t) &amp;= P_N[M_X(t)] = e^{\\lambda(M_{X}(t) - 1)}. \\end{aligned} \\] Mengambil hasil turunan \\[ \\begin{aligned} M_{S_N}&#39;(t) &amp;= e^{\\lambda(M_{X}(t) - 1)}~ \\lambda~ M_{X}&#39;(t) = M_{S_N}(t) ~\\lambda ~M_{X}&#39;(t)\\\\ M_{S_N}&#39;&#39;(t) &amp;= M_{S_N}(t) ~\\lambda~ M_{X}&#39;&#39;(t) + [~M_{S_N}(t)~\\lambda~ M_{X}&#39;(t)~] ~\\lambda~ M_{X}&#39;(t) . \\end{aligned} \\] Mengevaluasi hal ini pada t = 0 menghasilkan \\[ \\begin{aligned} \\mathrm{E}(S_N) &amp;= M_{S_N}&#39;(0) = \\lambda \\mathrm{E}(X) = \\lambda \\mu \\end{aligned} \\] Lalu \\[ \\begin{aligned} M_{S_N}&#39;&#39;(0) &amp;= \\lambda \\mathrm{E}(X^2) + \\lambda^2 \\mu^2\\\\ \\Rightarrow \\mathrm{Var}(S_N) &amp;= \\lambda \\mathrm{E}(X^2) + \\lambda^2 \\mu^2 - (\\lambda \\mu)^2 = \\lambda~ \\mathrm{E}(X^2). \\end{aligned} \\] 5.2.0.2 Example 5.3.2. Actuarial Exam Question Dimisalkan produser acara kuis televisi yang memberikan hadiah uang tunai. Jumlah hadiah(N) dan jumlah hadiah(X) memiliki distribusi sebagai berikut: \\[{\\small \\begin{matrix} \\begin{array}{ccccc}\\hline n &amp; \\Pr(N=n) &amp; &amp; x &amp; \\Pr(X=x)\\\\ \\hline 1 &amp; 0.8 &amp; &amp; 0 &amp; 0.2 \\\\ 2 &amp; 0.2 &amp; &amp; 100 &amp; 0.7 \\\\ &amp; &amp; &amp; 1000 &amp; 0.1\\\\\\hline \\end{array} \\end{matrix} }\\] Sehingga Anggaran untuk hadiah sama dengan jumlah hadiah uang tunai yang diharapkan ditambah dengan deviasi standar dari jumlah hadiah uang tunai. Hitung anggaran! JAWABAN Diperlukan untuk menghitung rata-rata dan standar deviasi dari agregat (jumlah) hadiah uang tunai. Momen-momen dari distribusi frekuensi N adalah \\[ \\begin{aligned} \\mathrm{E}(N) &amp;= 1 (0.8) + 2 (0.2) =1.2\\\\ \\mathrm{E}(N^2) &amp;= 1^2 (0.8) + 2^2 (0.2) =1.6\\\\ \\mathrm{Var}(N) &amp;= \\mathrm{E}(N^2) - \\left[ \\mathrm{E}(N) \\right]^2= 0.16 . \\end{aligned} \\] Momen-momen dari distribusi tingkat keparahan X adalah \\[ \\begin{aligned} \\mathrm{E}(X) &amp;= 0 (0.2) + 100 (0.7) + 1000 (0.1) = 170 = \\mu\\\\ \\mathrm{E}(X^2) &amp;= 0^2 (0.2) + 100^2 (0.7) + 1000^2 (0.1) = 107,000\\\\ \\mathrm{Var}(X) &amp;= \\mathrm{E}(X^2) - \\left[ \\mathrm{E}(X) \\right]^2=78,100 = \\sigma^2 . \\end{aligned} \\] Dengan demikian, rata-rata dan varians dari keseluruhan hadiah uang tunai adalah \\[ \\begin{aligned} \\mathrm{E}(S_N) &amp;= \\mu \\mathrm{E}(N) = 170 (1.2) = 204 \\\\ \\mathrm{Var}(S_N) &amp;= \\sigma^2 \\mathrm{E}(N) + \\mu^2 \\mathrm{Var}(N)\\\\ &amp;= 78,100 (1.2) + 170^2 (0.16) = 98,344 . \\end{aligned} \\] Sehingga anggaran yang dibutuhkan sebagai berikut \\[ \\begin{aligned} Budget &amp;= \\mathrm{E}(S_N) + \\sqrt{\\mathrm{Var}(S_N)} \\\\ &amp;= 204 + \\sqrt{98,344} = 517.60 . \\end{aligned} \\] Distribusi \\(S_N\\) disebut distribusi majemuk, dan dapat diturunkan berdasarkan konvolusi \\(F_X\\) sebagai berikut: \\[ \\begin{aligned} F_{S_N}(s) &amp;= \\Pr \\left(X_1 + \\cdots + X_N \\le s \\right) \\\\ &amp;= \\mathrm{E} \\left[ \\Pr \\left(X_1 + \\cdots + X_N \\le s|N=n \\right) \\right]\\\\ &amp;= \\mathrm{E} \\left[ F_{X}^{\\ast N}(s) \\right] \\\\ &amp;= p_0 + \\sum_{n=1}^{\\infty }p_n F_{X}^{\\ast n}(s) . \\end{aligned} \\] Ketika \\(E(N)\\) dan \\(Var(N)\\) diketahui, kita juga dapat menggunakan suatu jenis teorema limit pusat untuk mengestimasi distribusi \\(S_N\\) seperti pada model risiko individu. Yaitu, \\(\\frac{S_N - \\mathrm{E}(S_N)}{\\sqrt{\\mathrm{Var}(S_N)}}\\) kira-kira mengikuti distribusi normal standar \\(N(0,1)\\) . Dari jenis teorema limit pusat ini, aproksimasi bekerja dengan baik jika \\(E[N]\\) cukup besar. 5.2.1 Stop-loss Insurance Modifikasi pertanggungan pada tingkat polis perorangan Pertanggungan atas kerugian agregat \\(S_N\\) , yang dikenakan sebuah deductible \\(d\\) disebut dengan . Nilai yang diharapkan dari jumlah kerugian agregat yang melebihi deductible, \\[ \\begin{eqnarray*} \\mathrm{E}[(S-d)_+] \\end{eqnarray*} \\] dikenal sebagai premi stop-loss bersih. Untuk menghitung premi stop-loss neto, kita memiliki \\[ \\begin{eqnarray*} \\mathrm{E}(S_N-d)_+ &amp;=&amp; \\left\\{\\begin{array}{ll} \\int_{d}^{\\infty}(s-d) f_{S_N}(s) ds&amp; \\text{for continuous } S_N\\\\ \\sum_{s&gt;d}(s-d) f_{S_N}(s) &amp; \\text{for discrete } S_N\\\\ \\end{array}\\right.\\\\ &amp;=&amp; \\mathrm{E}(S_N) - \\mathrm{E}(S_N\\wedge d)\\\\ \\end{eqnarray*} \\] ### Actuarial Exam Question Dalam satu minggu tertentu, jumlah proyek yang mengharuskan Anda bekerja lembur memiliki distribusi geometris dengan \\(β = 2\\) . Untuk setiap proyek, distribusi jumlah jam lembur dalam seminggu, X adalah sebagai berikut \\[ {\\small \\begin{matrix} \\begin{array}{ccc} \\hline x &amp; &amp; f(x)\\\\ \\hline 5 &amp; &amp; 0.2 \\\\ 10 &amp; &amp; 0.3 \\\\ 20 &amp; &amp; 0.5\\\\ \\hline \\end{array} \\end{matrix} } \\] Jumlah proyek dan jumlah jam lembur tidak bergantung. Anda akan dibayar untuk jam lembur yang melebihi 15 jam dalam seminggu. Hitunglah jumlah jam lembur yang akan diterima dalam seminggu. JAWABAN Jumlah proyek dalam seminggu yang membutuhkan kerja lembur memiliki distribusi \\(N∼Geo(β=2)\\) sedangkan jumlah jam kerja lembur per proyek memiliki distribusi \\(X\\) seperti yang telah dijelaskan di atas. Jumlah keseluruhan jam lembur dalam seminggu adalah SN dan oleh karena itu kita mencari \\(\\mathrm{E}(S_N-15)_+ = \\mathrm{E}(S_N) - \\mathrm{E}(S_N \\wedge 15).\\) Untuk mencari \\(\\mathrm{E}(S_N) = \\mathrm{E}(X) ~\\mathrm{E}(N)\\), maka akan didapat \\[ \\begin{aligned} &amp;\\mathrm{E}(X) = 5(0.2) + 10(0.3)+ 20(0.5)= 14 \\\\ &amp;\\mathrm{E}(N) = 2 \\\\ \\Rightarrow \\ &amp;\\mathrm{E}(S) = \\mathrm{E}(X) ~ \\mathrm{E}(N) = 14(2) = 28 . \\end{aligned} \\] Untuk mencari \\(\\mathrm{E} (S_N \\wedge 15) = 0 \\Pr (S_N=0) + 5 \\Pr(S_N=5) + 10 \\Pr(S_N=10) + 15 \\Pr(S_N \\geq 15))\\), maka akan didapat \\[ \\begin{aligned} \\Pr(S_N=0) &amp;= \\Pr(N=0) = \\frac{1}{1+\\beta} = \\frac{1}{3} \\\\ \\Pr(S_N=5) &amp;= \\Pr(X=5, \\ N=1) = 0.2 \\left(\\frac{2}{9} \\right)= \\frac{0.4}{9}\\\\ \\Pr(S_N=10) &amp;= \\Pr(X=10, \\ N=1) + \\Pr(X_1=X_2=5, N=2) \\\\ &amp;= 0.3 \\left(\\frac{2}{9} \\right) + (0.2)(0.2) \\left( \\frac{4}{27} \\right)= 0.0726 \\\\ \\Pr(S_N \\geq 15) &amp;= 1 - \\left(\\frac{1}{3} + \\frac{0.4}{9} + 0.0726 \\right) = 0.5496\\\\ \\Rightarrow \\mathrm{E}(S_N \\wedge 15) &amp;= 0 \\Pr (S_N=0) + 5 \\Pr(S_N=5) + 10 \\Pr(S_N=10) + 15 \\Pr(S_N \\geq 15) \\\\ &amp;= 0 \\left( \\frac{1}{3} \\right) + 5 \\left( \\frac{0.4}{9} \\right) + 10 (0.0726) + 15 (0.5496) = 9.193 .\\\\ \\end{aligned} \\] Oleh Karena itu: \\[ \\begin{aligned} \\mathrm{E}(S_N-15)_+ &amp;= \\mathrm{E}(S_N) - \\mathrm{E}(S_N \\wedge 15) \\\\ &amp;= 28 - 9.193 = 18.807 . \\end{aligned} \\] Recursive Net Stop-Loss Premium Calculation Untuk kasus diskrit, ini dapat dihitung secara rekursif sebagai \\[ \\begin{aligned} \\mathrm{E}\\left[ \\left( S_N-(j+1)h \\right) _{+} \\right]=\\mathrm{E}\\left[ ( S_N-jh )_{+} \\right] -h \\left( 1-F_{S_N}(jh) \\right) . \\end{aligned} \\] Ini mengasumsikan bahwa dukungan \\(S_N\\) tersebar secara merata di atas unit-unit h. Untuk menetapkan ini, kita mengasumsikan bahwa \\(h = 1\\) Kita memiliki \\[ \\begin{aligned} \\mathrm{E}\\left[ \\left( S_N-(j+1) \\right) _{+} \\right] &amp;=\\mathrm{E}(S_N) - \\mathrm{E}[S_N\\wedge (j+1)] \\ ,\\ \\text{ and } \\\\ \\mathrm{E}\\left[ \\left( S_N - j \\right)_+ \\right] &amp;=\\mathrm{E}(S_N) - \\mathrm{E}[S_N\\wedge j] \\end{aligned} \\] Jadi, \\[ \\begin{aligned} \\mathrm{E}\\left[ \\left(S_N-(j+1) \\right) _{+}\\right] - \\mathrm{E}\\left[ ( S_N-j )_{+} \\right] &amp;= \\left\\{\\mathrm{E}(S_N) - \\mathrm{E}(S_N\\wedge (j+1)) \\right\\} - \\left\\{\\mathrm{E}(S_N) - \\mathrm{E}(S_N\\wedge j) \\right\\} \\\\ &amp;= \\mathrm{E}\\left(S_N \\wedge j \\right) - \\mathrm{E}\\left[ S \\wedge (j+1) \\right] \\end{aligned} \\] Maka kita dapat menulis \\[ \\begin{aligned} \\mathrm{E}\\left[S_N\\wedge (j+1)\\right] &amp;= \\sum_{x=0}^{j}xf_{S_N}(x) + (j+1)~\\Pr(S_N \\ge j+1) \\\\ &amp;= \\sum_{x=0}^{j-1}x f_{S_N}(x) + j~\\Pr(S_N=j) + (j+1)~\\Pr(S_N \\ge j+1) \\end{aligned} \\] Demikian pula, \\[ \\begin{aligned} \\mathrm{E}(S_N\\wedge j) = \\sum_{x=0}^{j-1}xf_{S_N}(x) + j~\\Pr(S_N\\ge j) \\end{aligned} \\] Dengan ekspresi ini, kami memiliki \\[ \\begin{aligned} &amp; \\mathrm{E}\\left[ \\left( S_N-(j+1) \\right) _{+} \\right] - \\mathrm{E~}\\left[ ( S_N-j )_{+} \\right] \\\\ &amp;= \\mathrm{E}\\left(S_N \\wedge j \\right) - \\mathrm{E}\\left[ S \\wedge (j+1) \\right] \\\\ &amp;= \\left\\{ \\sum_{x=0}^{j-1}xf_{S_N}(x) + j~\\Pr(S_N\\ge j) \\right\\} - \\left\\{ \\sum_{x=0}^{j-1}x f_{S_N}(x) + j~\\Pr(S_N=j) + (j+1)~\\Pr(S_N \\ge j+1) \\right\\} \\\\ &amp;= j~\\left[\\Pr(S_N \\geq j) - \\Pr(S_N=j) \\right]- (j+1)~\\Pr(S_N \\ge j+1) \\\\ &amp;= j~\\Pr( S_N &gt; j) - (j+1)~\\Pr(S_N \\ge j+1) ~~~~ \\text{ (note } \\Pr(S_N &gt; j) = \\Pr(S_N \\geq j+1) \\text{)} \\\\ &amp;= -\\Pr(S_N\\ge j+1) = -\\left[1 - F_{S_N}(j)\\right], \\end{aligned} \\] sesuai kebutuhan. 5.2.2 Actuarial Exam Question - Continued Ingatlah bahwa tujuan dari pertanyaan ini adalah untuk menghitung \\(E(S_N-15)_+\\) . Perhatikan bahwa dukungan dari \\(S_N\\) berjarak sama dengan satuan 5, sehingga pertanyaan ini juga dapat dikerjakan secara rekursif, menggunakan ekspresi di atas dengan langkah-langkah \\(h=5\\) : Step 1: \\[\\begin{aligned} \\mathrm{E~}(S_N-5)_+ &amp;= \\mathrm{E}(S_N) - 5 \\left[1-\\Pr(S_N \\leq 5) \\right] \\ &amp;= 28 - 5 \\left(1 - \\frac{1}{3}\\right) \\ &amp;= 28 - 5 \\left(\\frac{2}{3}\\right) \\ &amp;= 28 - \\frac{10}{3} \\ &amp;= \\frac{74}{3} \\approx 24.6667. \\end{aligned}\\] Step 2: \\[ \\begin{aligned} \\mathrm{E~}(S_N-10)+ &amp;= \\mathrm{E~}(S_N-5)+ - 5 \\left[1-\\Pr(S_N \\leq 10) \\right] \\ &amp;= \\frac{74}{3} - 5\\left( 1 - \\Pr(S_N &gt; 10) \\right) \\ &amp;= \\frac{74}{3} - 5\\left( 1 - \\Pr(S_N \\geq 11) \\right) \\ &amp;= \\frac{74}{3} - 5\\left( 1 - \\left(1 - \\Pr(S_N \\leq 10)\\right) \\right) \\ &amp;= \\frac{74}{3} - 5\\left( 1 - \\left(1 - \\left(1 - \\Pr(S_N &gt; 10)\\right)\\right) \\right) \\ &amp;= \\frac{74}{3} - 5\\left( 1 - \\left(1 - \\left(1 - \\frac{1}{3}\\right)\\right) \\right) \\ &amp;= \\frac{74}{3} - 5\\left( 1 - \\left(1 - \\frac{1}{3}\\right) \\right) \\ &amp;= \\frac{74}{3} - 5\\left( \\frac{2}{3} \\right) \\ &amp;= \\frac{74}{3} - \\frac{10}{3} \\ &amp;= \\frac{64}{3} \\approx 21.3333. \\end{aligned} \\] Step 3: \\[ \\begin{aligned} \\mathrm{E~}(S_N-15)+ &amp;= \\mathrm{E~}(S_N-10)+ - 5 [1-\\Pr(S_N \\leq 15)] \\ &amp;= \\mathrm{E~}(S_N-10)_+ - 5\\Pr (S_N\\ge 15) \\ &amp;= \\frac{49}{3} - 5 (1 - \\Pr(S_N \\leq 15)) \\ &amp;= \\frac{49}{3} - 5 (1 - 0.5596) \\ &amp;= \\frac{49}{3} - 5 (0.4404) \\ &amp;= \\frac{49}{3} - 2.202 \\ &amp;= \\frac{49}{3} - \\frac{22.02}{10} \\ &amp;= \\frac{490 - 220.2}{30} \\ &amp;= \\frac{269.8}{30} \\ &amp;\\approx 8.9933. \\end{aligned} \\] 5.2.3 Analytic Results Terdapat beberapa kombinasi distribusi frekuensi klaim dan tingkat keparahan yang menghasilkan distribusi yang mudah dihitung untuk kerugian agregat. Bagian ini memberikan beberapa contoh sederhana. Meskipun contoh-contoh ini mudah dihitung, namun pada umumnya terlalu sederhana untuk digunakan dalam praktik. 5.2.3.1 Example Salah satunya adalah ekspresi bentuk tertutup untuk distribusi kerugian agregat dengan mengasumsikan distribusi frekuensi geometris dan distribusi tingkat keparahan eksponensial. Asumsikan bahwa jumlah klaim \\(N\\) adalah geometrik dengan rata-rata \\(E(N)=β\\) dan jumlah klaim \\(X\\) adalah eksponensial dengan \\(E(X)=θ\\) .Dapat diingat bahwa pgf dari N dan pgf dari X adalah: \\[ \\begin{aligned} P_N (z) &amp;=\\frac{1}{1- \\beta (z-1)}\\\\ M_{X}(t) &amp;=\\frac{1}{1-\\theta t} . \\end{aligned} \\] Dengan demikian, mgf dari kerugian agregat \\(S_N\\) dapat dinyatakan dengan dua cara \\[ \\begin{eqnarray} M_{S_N}(t) &amp;=&amp; P_N [M_{X}(t)] = \\frac{1}{1 - \\beta \\left( \\frac{1}{1-\\theta t} - 1\\right)} \\nonumber\\\\ &amp;=&amp; 1+ \\frac{\\beta}{1+\\beta} \\left([1-\\theta(1+\\beta)t]^{-1}-1 \\right)\\\\ &amp;=&amp; \\frac{1}{1+\\beta}(1) +\\frac{\\beta}{1+\\beta} \\left( \\frac{1}{1-\\theta (1+\\beta)t}\\right) . \\end{eqnarray} \\] Sehingga, \\(S_N\\) ekuivalen dengan distribusi majemuk \\(S_N=X^∗_1+⋯+X^∗_N∗\\) dengan \\(N^∗\\) adalah Bernoulli dengan rata-rata \\(β/(1+β)\\) dan \\(X^∗\\) adalah eksponensial dengan mean \\(θ(1+β)\\). Untuk melihat hal ini, kita periksa mgf dari S : \\[ \\begin{aligned} M_{S_N}(t) = P_N [M_{X}(t)] = P_{N^{*}} [M_{X^{*}}(t)], \\end{aligned} \\] Dimana, \\[ \\begin{aligned} P_{N^*} (z) &amp;=1+ \\frac{\\beta}{1+ \\beta} (z-1),\\\\ M_{X^*} (t) &amp;=\\frac{1}{1- {{\\theta(1+\\beta)}} t}. \\end{aligned} \\] \\(S_N\\) juga ekuivalen dengan campuran dua titik dari 0 dan \\(X^∗\\). Secara khusus, \\[ \\begin{array}{cl} S_N &amp;= \\left\\{ \\begin{array}{cl} 0 &amp; {\\rm with~ probability ~Pr}(N^*=0) = 1/(1+\\beta) \\\\ Y^{*} &amp; {\\rm with~ probability ~Pr}(N^*=1) = \\beta/(1+\\beta) . \\end{array} \\right. \\end{array} \\] Fungsi distribusi \\(S_N\\) dalah \\[ \\begin{eqnarray*} \\Pr(S_N=0) &amp;=&amp; \\frac{1}{1+\\beta}\\\\ \\Pr(S_N&gt;s) &amp;=&amp; \\Pr(X^*&gt;s) =\\frac{\\beta}{1+\\beta} \\exp\\left( -\\frac{s}{ \\theta (1+\\beta)}\\right) \\end{eqnarray*} \\] dengan pdf untuk \\(s&gt;0\\) \\[ \\begin{eqnarray*} f_{S_N}(s) = \\frac{\\beta}{\\theta (1+\\beta)^2}\\exp\\left( -\\frac{s}{ \\theta (1+\\beta)}\\right) . \\end{eqnarray*} \\] 5.2.4 Tweedie Distribution Pada bagian ini, kita akan membahas distribusi gabungan tertentu di mana jumlah klaim memiliki distribusi Poisson dan jumlah klaim memiliki distribusi gamma. Spesifikasi ini menghasilkan apa yang dikenal sebagai distribusi Tweedie. Distribusi Tweedie memiliki probabilitas massa pada nol dan komponen kontinu untuk nilai positif. Karena fitur ini, distribusi ini banyak digunakan dalam pemodelan klaim asuransi, di mana massa nol ditafsirkan sebagai tidak ada klaim dan komponen positif sebagai jumlah klaim. Secara khusus, pertimbangkan model risiko kolektif \\(S_N = X_1 + ⋯ + X_N\\). Dengan menganggap bahwa N memiliki distribusi Poisson dengan mean \\(λ\\) dan masing-masing \\(X_i\\) memiliki distribusi gamma dengan parameter bentuk \\(α\\) dan parameter skala \\(γ\\) . Distribusi Tweedie diturunkan sebagai jumlah Poisson dari variabel gamma. Untuk memahami distribusi \\(S_N\\) pertama-tama kita akan melihat probabilitas massa pada nilai nol. Kerugian agregat adalah nol ketika tidak ada klaim yang terjadi, yaitu \\({\\rm Pr}(S_N=0)= {\\rm Pr}(N=0)=e^{-\\lambda}.\\) Selain itu, perhatikan bahwa \\(S_N\\) bersyarat pada N = n yang dinotasikan dengan \\(S_n = X_1 + ⋯ + X_n\\) mengikuti distribusi gamma dengan bentuk \\(nα\\) dan skala \\(γ\\) . Dengan demikian, untuk \\(s&gt;0\\) densitas dari distribusi Tweedie dapat dihitung sebagai \\[ \\begin{aligned} f_{S_N}(s)&amp;=\\sum_{n=1}^{\\infty} p_n f_{S_n}(s)\\\\ &amp;=\\sum_{n=1}^{\\infty}e^{-\\lambda}\\frac{(\\lambda)^n}{n!}\\frac{\\gamma^{na}}{\\Gamma(n\\alpha)}s^{n\\alpha-1}e^{-s\\gamma} . \\end{aligned} \\] Dengan demikian, distribusi Tweedie dapat dianggap sebagai campuran antara distribusi nol dan distribusi bernilai positif, yang membuatnya menjadi alat yang mudah digunakan untuk memodelkan klaim asuransi dan untuk menghitung premi murni. Rata-rata dan varians dari model Poisson gabungan Tweedie adalah: \\({\\rm E} (S_N)=\\lambda\\frac{\\alpha}{\\gamma}~~~~{\\rm and}~~~~{\\rm Var} (S_N)=\\lambda\\frac{\\alpha(1+\\alpha)}{\\gamma^2}.\\) Sebagai fitur penting lainnya, distribusi Tweedie adalah kasus khusus dari model dispersi eksponensial, sebuah kelas model yang digunakan untuk menggambarkan komponen acak dalam model linier umum. Untuk melihat hal ini, kami mempertimbangkan reparameterisasi berikut: \\[ \\begin{equation*} \\lambda=\\frac{\\mu^{2-p}}{\\phi(2-p)},~~~~\\alpha=\\frac{2-p}{p-1},~~~~\\frac{1}{\\gamma}=\\phi(p-1)\\mu^{p-1} . \\end{equation*} \\] Dengan hubungan di atas, kita dapat menunjukkan bahwa distribusi \\(S_N\\) adalah \\(f_{S_N}(s)=\\exp\\left[\\frac{1}{\\phi}\\left(\\frac{-s}{(p-1)\\mu^{p-1}}-\\frac{\\mu^{2-p}}{2-p}\\right)+C(s;\\phi)\\right]\\) Dimana \\[ C(s;\\phi)=\\left\\{\\begin{array}{ll} \\displaystyle 0 &amp; {\\rm if}~ s=0 \\\\ \\displaystyle \\log \\sum\\limits_{n \\ge 1} \\left\\{\\frac{(1/\\phi)^{1/(p-1)}s^{(2-p)/(p-1)}}{(2-p)(p-1)^{(2-p)/(p-1)}}\\right\\}^{n}\\frac{1}{n!~\\Gamma[n(2-p)/(p-1)]s} &amp; {\\rm if}~ s&gt;0 . \\end{array}\\right. \\] Oleh karena itu, distribusi \\(S_N\\) termasuk ke dalam keluarga eksponensial dengan parameter \\(μ\\) , \\(ϕ\\) , dan \\(1&lt;p&lt;2\\) , dan kita memiliki \\({\\rm E} (S_N)=\\mu~~~~{\\rm and}~~~~{\\rm Var} (S_N)=\\phi\\mu^{p}.\\) Hal ini memungkinkan kita untuk menggunakan distribusi Tweedie dengan model linear umum untuk memodelkan klaim. Perlu juga disebutkan dua kasus pembatas dari model Tweedie: \\(p→1\\) menghasilkan distribusi Poisson dan \\(p → 2\\) menghasilkan distribusi gamma. Dengan demikian, model Tweedie mengakomodasi situasi di antara distribusi gamma dan Poisson, yang secara intuitif masuk akal karena merupakan jumlah Poisson dari variabel acak gamma. ======= 5.3 5.4 Menghitung Distribusi Klaim Agregat Bagian ini membahas dua pendekatan praktis untuk menghitung distribusi kerugian agregat, yaitu metode rekursif dan simulasi. 5.3.1 metode rekursif penggunaan metode rekursif untuk membangun model majemuk dengan komponen frekuensi \\(N\\) yang termasuk dalam kelas \\((a,b,0)\\) atau \\((a,b,1)\\), dan komponen tingkat keparahan \\(X\\) yang memiliki distribusi diskrit. Namun, jika distribusi tingkat keparahan \\(X\\) kontinu, praktik yang umum dilakukan adalah mendiskritkan distribusinya terlebih dahulu agar metode rekursif dapat diterapkan. Dalam hal ini, diasumsikan bahwa N termasuk dalam kelas \\((a,b,1)\\), sehingga nilai probabilitas \\(N\\) pada waktu \\(k\\) dinyatakan sebagai \\(pk = (a + bk) pk-1\\). Selanjutnya, diasumsikan bahwa support (nilai yang mungkin) dari \\(X\\) terbatas pada \\({0,1,...,m}\\), dan distribusinya diskrit. Oleh karena itu, fungsi probabilitas dari \\(S_N\\) dapat dinyatakan dalam \\[\\begin{aligned} f_{S_N}(s)&amp;=\\Pr (S_N=s) \\\\ &amp;=\\frac{1}{1-af_{X}(0)}\\left\\{ \\left[ p_1 -(a+b)p_{0}\\right] f_X (s)+\\sum_{x=1}^{s\\wedge m}\\left( a+\\frac{bx}{s} \\right) f_X (x)f_{S_N}(s-x)\\right\\}. \\end{aligned}\\] Jika \\(N\\) berada dalam kelas \\((a,b,0)\\) maka \\(p1 = (a + b)p0\\) dan seterusnya \\[\\begin{align*} f_{S_N}(s)=\\frac{1}{1-af_X (0)}\\left\\{ \\sum_{x=1}^{s\\wedge m}\\left( a+\\frac{bx }{s}\\right) f_X (x)f_{S_N}(s-x)\\right\\}. \\end{align*}\\] karena model ARIMA yang digunakan berbeda. Persamaan tersebut hanya memperhitungkan faktor skala \\(a\\) dan \\(b\\) dan mengakumulasi probabilitas dari setiap nilai \\(x\\) dari \\(X\\) hingga mencapai nilai \\(s\\) yang diinginkan 5.3.1.1 contoh Jumlah klaim dalam suatu periode \\(N\\) memiliki distribusi geometrik dengan mean 4. Besarnya setiap klaim \\(X\\) mengikuti \\(Pr(X=x)=0.25\\), untuk \\(x=1,2,3,4\\). Jumlah klaim dan jumlah klaim bersifat independen. \\(S_N\\) adalah jumlah klaim keseluruhan pada periode tersebut. Hitunglah \\(F_{S_N}(3)\\). Solusi Distribusi tingkat keparahan \\(X\\) adalah sebagai berikut \\(f_X(x)=\\frac14\\), \\(x=1,2,3,4\\). Distribusi frekuensi \\(N\\) adalah geometris dengan rata-rata 4, yang merupakan anggota dari kelas \\((a,b,0)\\) dengan \\(b=0\\), \\(a=\\frac\\beta{1+\\beta}=\\frac45\\), dan \\(p0=\\frac1{1+\\beta}=\\frac15\\). nilai dari komponen tingkat keparahan \\(X\\) adalah \\({1,…,m=4}\\), yang bersifat diskrit dan terbatas. Dengan demikian, kita dapat menggunakan metode rekursif \\[\\begin{aligned} f_{S_N} (x) &amp;= 1 \\sum_{y=1}^{x\\wedge m} (a+0) f_X (y) f_{S_N} (x-y) \\\\ &amp;= \\frac{4}{5} \\sum_{y=1}^{x\\wedge m} f_X (y) f_{S_N} (x-y) . \\end{aligned}\\] Solusi ditemukan dengan menggunakan metode rekursif, di mana fungsi probabilitas \\(f_{S_N}(x)\\) untuk setiap nilai \\(x\\) dihitung menggunakan rumus \\(f_{S_N}(x) = \\sum_{y=1}^{x\\wedge m} (a+0) f_X(y) f_{S_N}(x-y)\\), di mana \\(m=4\\) adalah nilai maksimum dari distribusi nilai klaim \\(X\\), dan \\(a=\\frac{\\beta}{1+\\beta}=\\frac{4}{5}\\) dan \\(p_0=\\frac{1}{1+\\beta}=\\frac{1}{5}\\) adalah parameter dari distribusi frekuensi geometri yang diberikan. khususnya kita memiliki \\[\\begin{aligned} f_{S_N} (0) &amp;= \\Pr(N=0) = p_0=\\frac{1}{5}\\\\ f_{S_N} (1) &amp;= \\frac{4}{5}\\sum_{y=1}^{1} f_X (y) f_{S_N} (1-y) = \\frac{4}{5} f_X(1) f_{S_N}(0)\\\\ &amp;= \\frac{4}{5}\\left( \\frac{1}{4}\\right)\\left(\\frac{1}{5} \\right) = \\frac{1}{25}\\\\ f_{S_N} (2) &amp;= \\frac{4}{5}\\sum_{y=1}^{2} f_X (y) f_{S_N} (2-y) = \\frac{4}{5} \\left[ f_X(1)f_{S_N}(1) + f_X(2) f_{S_N}(0) \\right] \\\\ &amp;= \\frac{4}{5}\\left[ \\frac{1}{4} \\left( \\frac{1}{25} + \\frac{1}{5}\\right) \\right] = \\frac{4}{5}\\left( \\frac{6}{100}\\right) = \\frac{6}{125}\\\\ f_{S_N} (3) &amp;= \\frac{4}{5} \\left[ f_X(1) f_{S_N}(2) + f_X(2)f_{S_N}(1) + f_X(3) f_{S_N}(0) \\right]\\\\ &amp;= \\frac{4}{5}\\left[ \\frac{1}{4} \\left( \\frac{1}{25} + \\frac{1}{5} + \\frac{6}{125}\\right) \\right] = \\frac{1}{5}\\left( \\frac{5+25+6}{125}\\right) = 0.0576\\\\ \\Rightarrow \\ F_{S_N} (3) &amp;= f_{S_N} (0)+f_{S_N} (1)+f_{S_N} (2) +f_{S_N} (3) = 0.3456 . \\end{aligned}\\] Setelah menghitung nilai \\(f_{S_N}(0)\\), \\(f_{S_N}(1)\\), \\(f_{S_N}(2)\\), dan \\(f_{S_N}(3)\\), fungsi distribusi kumulatif \\(F_{S_N}(3)\\) diperoleh dengan menjumlahkan nilai-nilai tersebut. Hasil akhirnya adalah \\(F_{S_N}(3) = 0.3456\\). 5.3.2 simulasi Distribusi kerugian agregat dapat dievaluasi dengan menggunakan simulasi Monte Carlo. Untuk kerugian agregat, Simulasi Monte Carlo digunakan untuk menghasilkan sampel acak dari kerugian keseluruhan berdasarkan distribusi probabilitas yang dianggap sesuai untuk distribusi frekuensi dan tingkat keparahan klaim. gunanya adalah seseorang dapat menghitung distribusi empiris dari \\(S_N\\) dengan menggunakan sampel acak. Nilai ekspektasi dan varians dari kerugian agregat juga dapat diperkirakan dengan menggunakan rata-rata sampel dan varians sampel dari nilai simulasi. Sekarang kita rangkum prosedur simulasi untuk model kerugian agregat. Misalkan \\(m\\) adalah ukuran sampel acak yang dihasilkan dari kerugian agregat. Individual Risk Model: $S_n = X_1 + ⋯ + X_n $ misalkan \\(j=1,...,m\\) menjadi penghitung, dimulai dari \\(j = 1\\) Hitung setiap realisasi kerugian individu \\(x_{ij}\\) untuk \\(i=1,...,n\\) . Sebagai contoh, hal ini dapat dilakukan dengan menggunakan metode transformasi invers Hitung kerugian keseluruhan \\(s_j = x_{1j} + ⋯ + x_{nj}\\). terkahir Ulangi dua langkah di atas untuk \\(j=2,...,m\\) untuk mendapatkan sampel berukuran \\(m\\) dari \\(S_n\\), dengan kata lain \\({s_1,...,s_m}\\). Collective Risk Model : \\(S_n = X_1 + ... + X_n\\) misalkan \\(j=1,...,m\\) menjadi penghitung, dimulai dari \\(j = 1\\) Hitung jumlah klaim \\(n_j\\) dari distribusi frekuensi \\(N\\). Diberikan \\(n_j\\), hasilkan jumlah setiap klaim secara independen dari distribusi tingkat keparahan \\(X\\), dilambangkan dengan \\(x_{1j},...,x_{{n_j}j}\\). Hitung kerugian keseluruhan \\(s_j = x_{1j} + ⋯ + x_{{n_j}j}\\). Ulangi tiga langkah di atas untuk \\(j=2,...,m\\) untuk mendapatkan sampel berukuran \\(m\\) dari \\(S_N\\), dengan kata lain \\({s_1,...,s_m}\\) Dengan sampel acak \\(S\\), distribusi empiris dapat dihitung sebagai \\[\\begin{aligned} \\hat{F}_S(s)=\\frac{1}{m}\\sum_{i=1}^{m}I(s_i\\leq s), \\end{aligned}\\] Untuk individual risk model, kerugian keseluruhan dihitung sebagai jumlah kerugian individu yang acak, sedangkan untuk collective Risk Model, kerugian keseluruhan dihitung sebagai jumlah kerugian dari sejumlah klaim. Dalam kedua kasus, sampel acak dihasilkan dari distribusi probabilitas yang dianggap sesuai, dan kemudian distribusi empiris dari sampel tersebut dihitung untuk memperkirakan distribusi probabilitas dari kerugian agregat. dimana \\(I(\\cdot)\\) adalah fungsi indikator. distribusi empiris \\(\\hat{F}_S(s)\\) akan dikonvergen ke \\({F}_S(s)\\), dikarenakan ukuran sampel \\(m\\rightarrow \\infty\\) Dalam perhitungannya, asumsi-asumsi awal dibuat tentang distribusi probabilitas dan parameter-parameternya, kemudian model-model ini diestimasi menggunakan data yang tersedia dan kualitas kecocokan model dievaluasi menggunakan alat validasi model. Proses ini memberikan cara yang berguna untuk memperkirakan risiko yang terkait dengan kerugian agregat, dan dapat membantu perusahaan atau organisasi dalam merencanakan dan mengelola risiko mereka. Prosedur di atas mengasumsikan bahwa distribusi probabilitas, termasuk nilai parameter, dari distribusi frekuensi dan tingkat keparahan telah diketahui. Dalam praktiknya, kita perlu mengasumsikan distribusi-distribusi ini terlebih dahulu, mengestimasi parameter-parameternya dari data, dan kemudian menilai kualitas kecocokan model dengan menggunakan berbagai alat validasi model. Sebagai contoh, asumsi-asumsi dalam model risiko kolektif menyarankan estimasi dua tahap di mana satu model dikembangkan untuk jumlah klaim \\(N\\) dari data jumlah klaim, dan model lainnya dikembangkan untuk tingkat keparahan klaim \\(X\\) dari data jumlah klaim. "],["simulation-and-resampling.html", "Bab 6 Simulation and Resampling 6.1 Dasar-Dasar Simulasi 6.2 Bootstrap dan Resampling 6.3 Cross Validation 6.4 Importance Sampling 6.5 6.5.1 Metropolis Hastings 6.6 6.5.2 Gibbs Sampler", " Bab 6 Simulation and Resampling Bagian 6.1 memperkenalkan simulasi, alat komputasi luar biasa yang sangat berguna dalam pengaturan multivariat yang kompleks. Bagian 6.2 memperkenalkan resampling dalam konteks bootstrap untuk menentukan ketepatan estimator. Resampling merupakan proses simulasi untuk menggambar dari distribusi empiris. 6.1 Dasar-Dasar Simulasi Menghasilkan sekitar realisasi independen yang terdistribusi secara merata Ubah realisasi yang terdistribusi secara seragam menjadi pengamatan dari distribusi probabilitas yang menarik Hitung jumlah bunga dan tentukan ketepatan jumlah yang dihitung 6.1.1 Menghasilkan Pengamatan Seragam Independen Generator Kongruensi Linier. Linear Congruential Generators (LCG) adalah sebuah metode yang membangkitkan bilangan acak yang banyak dipergunakan dalam program komputer. Pada metode ini, dilakukan perulangan pada periode waktu tertentu atau setelah sekian kali pembangkitan.Untuk menghasilkan urutan angka acak, mulailah dengan \\(B_0\\) , nilai awal yang dikenal sebagai ‘seed’ . Nilai ini diperbarui menggunakan hubungan rekursif \\[B_{n+1} = (a B_n + c) \\text{ modulo }m, ~~ n=0, 1, 2, \\ldots .\\] Algoritma ini disebut \\(a\\). Kasus \\(c = 0\\) disebut generator kongruensial perkalian Untuk nilai ilustrasi dari \\(a\\) Dan $m4 , menggunakan Microsoft Visual Basic \\(m=2^{24}\\) , \\(a = 1 , 140 , 671 , 485\\) , Dan \\(c = 12 , 820 , 163\\). Ini adalah mesin yang mendasari pembuatan angka acak dalam program Microsoft Excel. Urutan yang digunakan oleh analis didefinisikan sebagai \\(U_n=B_n/m.\\). Analis dapat menginterpretasikan urutan \\(U_{i}\\) menjadi (kira-kira) identik dan independen terdistribusi secara seragam pada interval (0,1). Untuk mengilustrasikan algoritme, maka pertimbangkan hal berikut. Contoh 6.1.2. Menghasilkan Nomor Acak Seragam di R. Kode berikut menunjukkan cara menghasilkan tiga angka seragam (0,1) dalam R menggunakan perintah runif. Fungsi set.seed() di R digunakan untuk membuat hasil yang dapat direproduksi saat menulis kode yang melibatkan pembuatan variabel yang mengambil nilai acak. set.seed(2017) U &lt;- runif(3) knitr::kable(U, digits=5, align = &quot;c&quot;, col.names = &quot;Uniform&quot;) Uniform 0.92424 0.53718 0.46920 6.1.2 Metode Transformasi Invers Metode transformasi invers digunakan untuk membangkitkan data acak dari distribusi peluang kontinu yang diketahui bentuk fungsinya. Dengan urutan bilangan acak seragam, kemudian diubah menjadi distribution of interest (\\(F\\)). \\[X_i=F^{-1}\\left( U_i \\right) .\\] \\[F^{-1}(y) = \\inf_x ~ \\{ F(x) \\ge y \\}\\] inf singkatan dari infimum atau batas bawah terbesar. Ini pada dasarnya adalah nilai \\(x\\) terkecil yang memenuhi pertidaksamaan \\(\\{F(x) \\ge y\\}\\). Hasilnya adalah urutan \\(X_{i}\\) kira-kira iid dengan fungsi distribusi \\(F\\) jika \\(U_{i}\\) adalah iid dengan fungsi distribusi seragam ( 0 , 1 ). Contoh 6.1.3. Menghasilkan Bilangan Acak Eksponensial. Misalkan ingin menghasilkan pengamatan dari distribusi eksponensial dengan parameter skala \\(θ\\) sehingga \\(F(x) = 1 - e^{-x/\\theta}\\). Untuk menghitung transformasi invers, maka dapat menggunakan langkah-langkah berikut: \\[\\begin{aligned} y = F(x) &amp;\\Leftrightarrow y = 1-e^{-x/\\theta} \\\\ &amp;\\Leftrightarrow -\\theta \\ln(1-y) = x = F^{-1}(y) . \\end{aligned}\\] Jadi, jika \\(U\\) memiliki distribusi seragam (0,1), maka \\(X = -\\theta \\ln(1-U)\\) memiliki distribusi eksponensial dengan parameter \\(θ\\). Seperti pada Contoh 6.1.2 kemudian mengubahnya menjadi variabel acak terdistribusi eksponensial independen dengan rata-rata \\(10\\). Sebagai alternatif, menggunakan fungsi rexp pada R digunakan untuk mensimulasikan sekumpulan bilangan acak yang diambil dari distribusi eksponensial. set.seed(2017) U &lt;- runif(3) X1 &lt;- -10*log(1-U) set.seed(2017) X2 &lt;- rexp(3, rate = 1/10) Contoh 6.1.4. Menghasilkan Angka Acak Pareto. Misalkan ingin menghasilkan pengamatan dari distribusi Pareto dengan parameter \\(α\\) dan \\(θ\\) sehingga \\(F(x) = 1 - \\left(\\frac{\\theta}{x+\\theta} \\right)^{\\alpha}\\). Untuk menghitung transformasi invers, maka dapat menggunakan langkah-langkah berikut: \\[\\begin{aligned} y = F(x) &amp;\\Leftrightarrow 1-y = \\left(\\frac{\\theta}{x+\\theta} \\right)^{\\alpha} \\\\ &amp;\\Leftrightarrow \\left(1-y\\right)^{-1/\\alpha} = \\frac{x+\\theta}{\\theta} = \\frac{x}{\\theta} +1 \\\\ &amp;\\Leftrightarrow \\theta \\left((1-y)^{-1/\\alpha} - 1\\right) = x = F^{-1}(y) .\\end{aligned}\\] Dengan demikian, \\(X = \\theta \\left((1-U)^{-1/\\alpha} - 1\\right)\\) memiliki distribusi Pareto dengan parameter \\(α\\) dan \\(θ\\) . Contoh 6.1.5. Menghasilkan Bilangan Acak Bernoulli. Misalkan ingin mensimulasikan variabel acak dari distribusi Bernoulli dengan parameter \\(Q= 0,85\\). Grafik fungsi distribusi kumulatif pada Gambar diatas menunjukkan bahwa fungsi kuantil dapat ditulis sebagai berikut. \\[\\begin{aligned} F^{-1}(y) = \\left\\{ \\begin{array}{cc} 0 &amp; 0&lt;y \\leq 0.85 \\\\ 1 &amp; 0.85 &lt; y \\leq 1.0 . \\end{array} \\right. \\end{aligned}\\] Jadi, dengan transformasi invers kita dapat mendefinisikan \\[\\begin{aligned} X = \\left\\{ \\begin{array}{cc} 0 &amp; 0&lt;U \\leq 0.85 \\\\ 1 &amp; 0.85 &lt; U \\leq 1.0 \\end{array} \\right. \\end{aligned}\\] Misalnya, ingin menghasilkan tiga angka acak untuk diperoleh set.seed(2017) U &lt;- runif(3) X &lt;- 1*(U &gt; 0.85) Contoh 6.1.6. Menghasilkan Angka Acak dari Distribusi Diskrit. Pertimbangkan waktu kegagalan mesin dalam lima tahun pertama. Distribusi waktu kegagalan diberikan sebagai: Dengan menggunakan grafik fungsi distribusi pada gambar diatas , dengan transformasi invers dapat definisikan \\[\\small{ \\begin{aligned} X = \\left\\{ \\begin{array}{cc} 1 &amp; 0&lt;U \\leq 0.1 \\\\ 2 &amp; 0.1 &lt; U \\leq 0.3\\\\ 3 &amp; 0.3 &lt; U \\leq 0.4\\\\ 4 &amp; 0.4 &lt; U \\leq 0.8 \\\\ 5 &amp; 0.8 &lt; U \\leq 1.0 . \\end{array} \\right. \\end{aligned} }\\] Untuk variabel acak diskrit umum mungkin tidak ada urutan hasil. Misalnya, seseorang dapat memiliki salah satu dari lima jenis produk asuransi jiwa dan dapat menggunakan algoritme berikut untuk menghasilkan hasil acak: \\[{\\small \\begin{aligned} X = \\left\\{ \\begin{array}{cc} \\textrm{whole life} &amp; 0&lt;U \\leq 0.1 \\\\ \\textrm{endowment} &amp; 0.1 &lt; U \\leq 0.3\\\\ \\textrm{term life} &amp; 0.3 &lt; U \\leq 0.4\\\\ \\textrm{universal life} &amp; 0.4 &lt; U \\leq 0.8 \\\\ \\textrm{variable life} &amp; 0.8 &lt; U \\leq 1.0 . \\end{array} \\right. \\end{aligned} }\\] Analis lain dapat menggunakan prosedur alternatif seperti: \\[{\\small \\begin{aligned} X = \\left\\{ \\begin{array}{cc} \\textrm{whole life} &amp; 0.9&lt;U&lt;1.0 \\\\ \\textrm{endowment} &amp; 0.7 \\leq U &lt; 0.9\\\\ \\textrm{term life} &amp; 0.6 \\leq U &lt; 0.7\\\\ \\textrm{universal life} &amp; 0.2 \\leq U &lt; 0.6 \\\\ \\textrm{variable life} &amp; 0 \\leq U &lt; 0.2 . \\end{array} \\right. \\end{aligned} }\\] Kedua algoritma menghasilkan (dalam jangka panjang) probabilitas yang sama, misalnya, \\(\\Pr(\\textrm{whole life})=0.1\\) , Dan seterusnya. Jadi, tidak ada yang salah ini menunjukkan bahwa ada lebih dari satu cara untuk mencapai suatu tujuan. Demikian pula, dapat menggunakan algoritme alternatif untuk hasil yang diurutkan (seperti waktu kegagalan 1, 2, 3, 4, atau 5, di atas). Contoh 6.1.7. Menghasilkan Angka Acak dari Distribusi Hybrid. Pertimbangkan variabel acak yaitu 0 dengan probabilitas 70% dan terdistribusi secara eksponensial dengan parameter \\(\\theta= 10,000\\) dengan probabilitas 30%. Dalam aplikasi asuransi, ini mungkin sesuai dengan peluang 70% tidak memiliki klaim asuransi dan peluang klaim 30% - jika klaim terjadi, maka itu didistribusikan secara eksponensial. Fungsi distribusi, digambarkan pada gambar dibawah ini , diberikan sebagai \\[\\begin{aligned} F(y) = \\left\\{ \\begin{array}{cc} 0 &amp; x&lt;0 \\\\ 1 - 0.3 \\exp(-x/10000) &amp; x \\ge 0 . \\end{array} \\right. \\end{aligned}\\] Dari Gambar diatas dapat dilihat bahwa transformasi invers untuk membangkitkan variabel acak dengan fungsi distribusi ini adalah \\[\\begin{aligned} X = F^{-1}(U) = \\left\\{ \\begin{array}{cc} 0 &amp; 0&lt; U \\leq 0.7 \\\\ -1000 \\ln (\\frac{1-U}{0.3}) &amp; 0.7 &lt; U &lt; 1 . \\end{array} \\right. \\end{aligned}\\] 6.1.3 (6.1.3) Presisi Simulasi Setelah mengetahui cara menghasilkan realisasi simulasi independen dari distribusi bunga, maka dapat menyusun distribusi empiris (distribusi empiris mengelompokkan data ke dalam suatu interval, di mana frekuensi data dalam setiap interval dapat digunakan untuk menentukan frekuensi relatifnya) dan memperkirakan distribusi yang diperlukan. Banyak dari aplikasi ini dapat direduksi menjadi masalah perkiraan \\(\\mathrm{E~}[h(X)]\\) , Di mana \\(h(\\cdot)\\) adalah beberapa fungsi yang diketahui. Berdasarkan simulasi R (replikasi), sehingga didapatkan \\(X_1,\\ldots,X_R\\). Dari sampel yang disimulasikan ini, dapat menghitung rata-rata sebagai berikut. \\[\\overline{h}_R=\\frac{1}{R}\\sum_{i=1}^{R} h(X_i)\\] sebagai perkiraan simulasi dari \\(\\mathrm{E~}[h(X)]\\). Untuk memperkirakan ketepatan perkiraan tersebut, maka menggunakan varians simulasi \\[s_{h,R}^2 = \\frac{1}{R-1} \\sum_{i=1}^{R}\\left( h(X_i) -\\overline{h}_R \\right) ^2.\\] Dari independensi, kesalahan standar estimasi adalah \\(s_{h,R}/\\sqrt{R}\\). Kesalahan standar estimasi dapat dibuat sekecil dengan meningkatkan jumlah replikasi \\(R\\). Contoh 6.1.8. Manajemen portofolio. Pada Bagian 3.4 telah mempelajari cara menghitung nilai ekspektasi polis dengan deductible. Sebagai contoh dari sesuatu yang tidak dapat dilakukan dengan ekspresi bentuk tertutup, kemudian akan mempertimbangkan dua risiko. (Ini adalah variasi dari contoh yang lebih kompleks yang akan dibahas sebagai Contoh 10.3.6). Dengan mempertimbangkan dua risiko properti dari perusahaan telekomunikasi: \\(X_1\\) - bangunan, dimodelkan menggunakan distribusi gamma dengan rata-rata 200 dan parameter skala 100. \\(X_2\\) - kendaraan bermotor, dimodelkan menggunakan distribusi gamma dengan mean 400 dan parameter skala 200. Nyatakan risiko total sebagai \\(X = X_1 + X_2\\). Untuk penyederhanaan, dapat diasumsikan bahwa risiko ini tidak bergantung. Untuk mengelola risiko maka diperlukan perlindungan atau penjamin asuransi dan bersedia mempertahankan jumlah bangunan dan kendaraan bermotor kecil secara internal, hingga \\(M\\). Jumlah acak lebih dari \\(M\\) akan memiliki pengaruh yang tidak terduga pada anggaran dan karenanya untuk jumlah ini dapat mencari perlindungan asuransi. Dinyatakan secara matematis, risiko yang dipertahankan adalah \\(Y_{retained}=\\min(X_1 + X_2,M)\\) dan bagian penanggung adalah \\(Y_{insurer} = X- Y_{retained}\\). Misalnya \\(M= 400\\) serta \\(R = 1000000\\). A. Dengan pengaturan tersebut, ingin menentukan perkiraan jumlah klaim dan standar deviasi terkait dari (i) yang ditahan, (ii) yang diterima oleh perusahaan asuransi, dan (iii) total jumlah keseluruhan. # Simulate the risks nSim &lt;- 1e6 #number of simulations set.seed(2017) #set seed to reproduce work X1 &lt;- rgamma(nSim ,alpha1,scale = theta1) X2 &lt;- rgamma(nSim ,alpha2,scale = theta2) # Portfolio Risks X &lt;- X1 + X2 Yretained &lt;- pmin(X, M) Yinsurer &lt;- X - Yretained Kemudian jumlah klaim yang diharapkan adalah # Expected Claim Amounts ExpVec &lt;- t(as.matrix(c(mean(Yretained),mean(Yinsurer),mean(X)))) sdVec &lt;- t(as.matrix(c(sd(Yretained),sd(Yinsurer),sd(X)))) outMat &lt;- rbind(ExpVec, sdVec) colnames(outMat) &lt;- c(&quot;Retained&quot;, &quot;Insurer&quot;,&quot;Total&quot;) row.names(outMat) &lt;- c(&quot;Mean&quot;,&quot;Standard Deviation&quot;) round(outMat,digits=2) B. Untuk klaim yang diasuransikan, kesalahan standar perkiraan simulasi adalah \\(s_{h,R}/\\sqrt{1000000} =/\\sqrt{1000000} =0.281\\). Untuk contoh ini, simulasi cepat dan nilai yang besar seperti 1000000 adalah pilihan yang mudah. Namun, untuk masalah yang lebih kompleks, ukuran simulasi mungkin menjadi masalah. Yinsurefct &lt;- function(numSim){ X1 &lt;- rgamma(numSim,alpha1,scale = theta1) X2 &lt;- rgamma(numSim,alpha2,scale = theta2) # Portfolio Risks X &lt;- X1 + X2 Yinsurer &lt;- X - pmin(X, M) return(Yinsurer) } R &lt;- 1e3 nPath &lt;- 20 set.seed(2017) simU &lt;- matrix(Yinsurefct(R*nPath),R,nPath) sumP2 &lt;- apply(simU, 2, cumsum)/(1:R) matplot(1:R,sumP2[,1:20],type=&quot;l&quot;,col=rgb(1,0,0,.2), ylim=c(100, 400), xlab=expression(paste(&quot;Number of Simulations (&quot;, italic(&#39;R&#39;), &quot;)&quot;)), ylab=&quot;Expected Insurer Claims&quot;) abline(h=mean(Yinsurer),lty=2) bonds &lt;- cbind(1.96*sd(Yinsurer)*sqrt(1/(1:R)),-1.96*sd(Yinsurer)*sqrt(1/(1:R))) matlines(1:R,bonds+mean(Yinsurer),col=&quot;red&quot;,lty=1) Dari grafik diatas dapat dilihat, semakin banyak jumlah simulasi R maka semakin sedikit jumlah klaim yang diharapkan. Penentuan Jumlah Simulasi Misalkan ingin berada dalam 1% dari rata-rata dengan kepastian 95%. Artinya, \\(\\Pr \\left( |\\overline{h}_R - \\mathrm{E~}[h(X)]| \\le 0.01 \\mathrm{E~}[h(X)] \\right) \\ge 0.95\\). Menurut teorema limit pusat, perkiraan harus terdistribusi secara normal dan mengharapkan R cukup besar untuk \\(0.01 \\mathrm{E~}[h(X)]/\\sqrt{\\mathrm{Var~}[h(X)]/R}) \\ge 1.96\\) . (Ingat bahwa 1,96 adalah persentil ke-97,5 dari distribusi normal standar.) Mengganti \\(\\mathrm{E~}[h(X)]\\) Dan \\(\\mathrm{Var~}[h(X)]\\) dengan estimasi,sehingga \\[\\frac{.01\\overline{h}_R}{s_{h,R}/\\sqrt{R}}\\geq 1.96\\] \\[\\begin{equation} R \\geq 38,416\\frac{s_{h,R}^2}{\\overline{h}_R^2}. \\tag{6.1} \\end{equation}\\] Contoh 6.1.9. Pilihan Perkiraan. Sebuah aplikasi penting dari simulasi adalah pendekatan dari \\(\\mathrm{E~}[h(X)]\\). Dalam contoh ini, kami menunjukkan bahwa pilihan dari \\(h(\\cdot)\\) fungsi dan distribusi \\(X\\) dapat berperan. Pertimbangkan pertanyaan berikut: apa itu \\(\\Pr[X&gt;2]\\). Kapan \\(X\\) mempunyai sebuah distribusi Cauchy (distribusi probabilitas kontinu), dengan fungsi kepadatan \\(f(x) =\\left(\\pi(1+x^2)\\right)^{-1}\\), pada garis sebenarnya? Nilai sebenarnya adalah \\[\\Pr\\left[X&gt;2\\right] = \\int_2^\\infty \\frac{dx}{\\pi(1+x^2)} .\\] true_value &lt;- integrate(function(x) 1/(pi*(1+x^2)),lower=2,upper=Inf)$value true_value ## [1] 0.1475836 Perkiraan 1. Sebagai alternatif, seseorang dapat menggunakan teknik simulasi untuk memperkirakan besaran tersebut. Dari kalkulus, dapat memeriksa bahwa fungsi kuantil dari distribusi Cauchy adalah \\(F^{-1}(y) = \\tan \\left( \\pi(y-0.5) \\right)\\) . Kemudian, dengan variasi seragam (0,1) yang disimulasikan, \\(U_1, \\ldots, U_R\\), sehingga dapat membangun estimator Q &lt;- function(u) tan(pi*(u-.5)) R &lt;- 1e6 set.seed(1) X &lt;- Q(runif(R)) p1 &lt;- mean(X&gt;2) se.p1 &lt;- sd(X&gt;2)/sqrt(R) p1 ## [1] 0.147439 se.p1 ## [1] 0.0003545432 Dengan satu juta simulasi, diperoleh estimasi sebesar 0,14744 dengan standard error 0,355 (dibagi 1000). Dapat dibuktikan bahwa varian dari \\(P_1\\) teratur \\(0.127/R\\). Perkiraan 2. Dengan pilihan lain dari \\(h(\\cdot)\\) Dan \\(f(\\cdot)\\) adalah mungkin untuk mengurangi ketidakpastian bahkan dengan menggunakan jumlah simulasi yang sama \\(R\\) . Untuk memulai, seseorang dapat menggunakan simetri distribusi Cauchy untuk menulis \\(\\Pr[X&gt;2]=0.5\\cdot\\Pr[|X|&gt;2]\\) . Dengan ini, dapat membuat estimator baru \\[p_2 = \\frac{1}{2R}\\sum_{i=1}^R \\mathrm{I}(|F^{-1}(U_i)|&gt;2) .\\] Dengan satu juta simulasi, diperoleh estimasi sebesar 0,14748 dengan standard error 0,228 (dibagi 1000). Dapat dibuktikan bahwa varian dari \\(P_2\\) teratur \\(0.052/R\\). Perkiraan 3. Integral tak wajar dapat ditulis dengan sifat simetri sederhana (karena fungsinya simetris dan integral pada garis real sama dengan 1 ). \\[\\int_2^\\infty \\frac{dx}{\\pi(1+x^2)}=\\frac{1}{2}-\\int_0^2\\frac{dx}{\\pi(1+x^2)} .\\] \\[p_3 = \\frac{1}{2}-\\frac{1}{R}\\sum_{i=1}^R h_3(2U_i), ~~~~~~\\text{where}~h_3(x)=\\frac{2}{\\pi(1+x^2)} .\\] Dengan satu juta simulasi, diperoleh estimasi sebesar 0,14756 dengan standard error 0,169 (dibagi 1000). Dapat dibuktikan bahwa varian dari \\(P_3\\) teratur \\(0,0285 / R\\). Perkiraan 4. Akhirnya, seseorang juga dapat mempertimbangkan beberapa perubahan variabel dalam integral. \\[\\int_2^\\infty \\frac{dx}{\\pi(1+x^2)}=\\int_0^{1/2}\\frac{y^{-2}dy}{\\pi(1-y^{-2})} .\\] \\[p_4 = \\frac{1}{R}\\sum_{i=1}^R h_4(U_i/2),~~~~~\\text{where}~h_4(x)=\\frac{1}{2\\pi(1+x^2)} .\\] Dengan satu juta simulasi, diperoleh estimasi sebesar 0,14759 dengan standard error 0,01 (dibagi 1000). Dapat dibuktikan bahwa varian dari \\(P_4\\) teratur \\(0,00009 / R\\) , yang jauh lebih kecil dari yang lainnya. Tabel berikut merupakan rangkuman dari empat pilihan \\(h(\\cdot)\\) Dan \\(f(\\cdot)\\)) untuk memperkirakan \\(\\Pr[X&gt;2] = 0,14758\\). Kesalahan standar bervariasi. Jadi, jika memiliki tingkat akurasi yang diinginkan, maka jumlah simulasi sangat bergantung pada bagaimana menulis integral yang akan diaproksimasi. 6.1.4 Simulasi dan Inferensi Statistik Simulasi tidak hanya membantu dalam memperkirakan nilai yang diharapkan tetapi juga berguna dalam menghitung aspek lain dari fungsi distribusi. Secara khusus, ini sangat berguna ketika distribusi statistik uji terlalu rumit untuk diturunkan. Dalam hal ini, seseorang dapat menggunakan simulasi untuk memperkirakan distribusi referensi. Contoh 6.1.10. Uji Distribusi Kolmogorov-Smirnov. Misalkan terdapata \\(n = 100\\) observasi \\(\\{x_1,\\cdots,x_n\\}\\) yang, tidak diketahui oleh analis, dihasilkan dari distribusi gamma dengan parameter \\(\\alpha = 6\\) Dan \\(\\theta=2\\) . Analis percaya bahwa data berasal dari distribusi lognormal dengan parameter 1 dan 0,4 dan ingin menguji asumsi ini. set.seed(1) n &lt;- 100 x &lt;- rgamma(n, 6, 2) u=seq(0,7,by=.01) vx = c(0,sort(x)) vy = (0:n)/n par(mfrow=c(1,2)) hist(x,probability = TRUE,main=&quot;Histogram&quot;, col=&quot;light blue&quot;, border=&quot;white&quot;,xlim=c(0,7),ylim=c(0,.4)) lines(u,dlnorm(u,1,.4),col=&quot;red&quot;,lty=2) plot(vx,vy,type=&quot;l&quot;,xlab=&quot;x&quot;,ylab=&quot;Cumulative Distribution&quot;,main=&quot;Empirical cdf&quot;) lines(u,plnorm(u,1,.4),col=&quot;red&quot;,lty=2) Dari grafik diatas dapat dilihat bahwa garis putus-putus merah tersebut sesuai dengan distribusi lognormal yang dihipotesiskan. Perlu digaris bawahi bahwa statistik Kolmogorov-Smirnov sama dengan perbedaan terbesar antara distribusi empiris dan hipotesis. Ini \\(\\max_x |F_n(x)-F_0(x)|\\), Di mana \\(F_0\\) adalah distribusi lognormal yang dihipotesiskan, sehingga # test statistic D &lt;- function(data, F0){ F &lt;- Vectorize(function(x) mean((data&lt;=x))) n &lt;- length(data) x &lt;- sort(data) d1=abs(F(x+1e-6)-F0(x+1e-6)) d2=abs(F(x-1e-6)-F0(x-1e-6)) return(max(c(d1,d2))) } D(x,function(x) plnorm(x,1,.4)) ## [1] 0.09703627 ks.test(x, plnorm, mean=1, sd=0.4) ## ## Asymptotic one-sample Kolmogorov-Smirnov test ## ## data: x ## D = 0.097037, p-value = 0.3031 ## alternative hypothesis: two-sided Secara khusus, untuk menghitung P-value, maka hasilkan ribuan sampel acak dari \\(LN(1,0.4)\\) distribusi (dengan ukuran yang sama), dan menghitung secara empiris distribusi statistik, ns &lt;- 1e4 d_KS &lt;- rep(NA,ns) # compute the test statistics for a large (ns) number of simulated samples for(s in 1:ns) d_KS[s] &lt;- D(rlnorm(n,1,.4),function(x) plnorm(x,1,.4)) mean(d_KS&gt;D(x,function(x) plnorm(x,1,.4))) ## [1] 0.2843 hist(d_KS,probability = TRUE,col=&quot;light blue&quot;,border=&quot;white&quot;,xlab=&quot;Test Statistic&quot;,main=&quot;&quot;) lines(density(d_KS),col=&quot;red&quot;) abline(v=D(x,function(x) plnorm(x,1,.4)),lty=2,col=&quot;red&quot;) Distribusi yang disimulasikan berdasarkan 10.000 sampel acak dirangkum grafik diatas. Di sini, statistik melebihi nilai empiris (0,09704) dalam 28,43%, sedangkan P-value adalah 0,3031. Baik untuk simulasi maupun teoretis P-value, kesimpulannya adalah data tidak memberikan bukti yang cukup untuk menolak hipotesis distribusi lognormal. Meskipun hanya perkiraan, pendekatan simulasi bekerja dalam berbagai distribusi dan uji statistik tanpa perlu mengembangkan nuansa teori yang mendasari untuk setiap situasi. Berikut ringkasan prosedur untuk mengembangkan distribusi simulasi dan p-value sebagai berikut: Gambarlah sampel berukuran n , katakanlah, \\(X_1, \\ldots, X_n\\), dari fungsi distribusi yang diketahui \\(F\\). Hitung statistik minat, dilambangkan sebagai \\(\\hat{\\theta}(X_1, \\ldots, X_n)\\). Panggil ini \\(\\hat{\\theta}^r\\) untuk replikasi ke -r . Ulangi ini \\(r=1, \\ldots, R\\) kali untuk mendapatkan sampel statistik, \\(\\hat{\\theta}^1, \\ldots,\\hat{\\theta}^R\\). Dari sampel statistik pada Langkah 2, \\(\\{\\hat{\\theta}^1, \\ldots,\\hat{\\theta}^R\\}\\), hitung ukuran ringkasan minat, seperti p-value. 6.2 Bootstrap dan Resampling Subbab ini akan mempelajari : Hasilkan distribusi bootstrap nonparametrik untuk statistik minat Gunakan distribusi bootstrap untuk menghasilkan estimasi presisi untuk statistik yang diminati, termasuk bias, standar deviasi, dan interval kepercayaan Lakukan analisis bootstrap untuk distribusi parametrik 6.2.1 Dasar-dasar Bootstrap Metode bootstrap adalah metode berbasis resampling data sampel dengan syarat pengembalian pada datanya dalam menyelesaikan statistik ukuran suatu sampel dengan harapan sampel tersebut mewakili data populai sebenarnya, biasanya ukuran resampling diambil secara ribuan kali agar dapat mewakili data populasinya. Algoritma resamplign umum dengan \\(\\{X_1, \\ldots, X_n\\}\\) untuk menunjukkan sampel asli dan \\(\\{X_1^*, \\ldots, X_n^*\\}\\) menunjukkan undian yang disimulasikan. Untuk setiap sampel, \\(n\\) merupakan undian simulasi, jumlah yang sama dengan ukuran sampel asli. Untuk membedakan prosedur ini dari simulasi, biasanya digunakan \\(B\\) (untuk bootstrap) sebagai jumlah sampel yang disimulasikan. Sehingga dapat dituliskan \\(\\{X_1^{(b)}, \\ldots, X_n^{(b)}\\}\\). Ada dua metode resampling dasar, model-free dan model-based , masing-masing sebagai nonparametrik dan parametrik . Pengundian yang disimulasikan berasal dari fungsi distribusi empiris \\(F_n(\\cdot)\\) , jadi setiap undian berasal \\(\\{X_1, \\ldots, X_n\\}\\) dengan probabilitas \\(1/n\\). Bootstrap Nonparametrik Gagasan bootstrap nonparametrik adalah menggunakan metode transformasi terbalik \\(F_N\\) , fungsi distribusi kumulatif empiris, digambarkan pada grafik dibawah ini. Karena \\(F_N\\) adalah step-function, \\(F_n^{-1}\\) subtitusi nilai-nilai \\(\\{x_1,\\cdots,x_n\\}\\) sehingga jika \\(y\\in(0,1/n)\\) (dengan probabilitas \\(1 / n\\) ) dengan menggambar nilai terkecil ( \\(\\min\\{x_i\\}\\) ) jika \\(y\\in(1/n,2/n)\\) (dengan probabilitas \\(1 / n\\) ) dengan menggambar nilai terkecil kedua, … jika \\(y\\in((n-1)/n,1)\\) (dengan probabilitas \\(1 / n\\) ) kami menggambar nilai terbesar ( \\(\\max\\{x_i\\}\\) ) Menggunakan metode transformasi terbalik dengan \\(F_N\\) berarti pengambilan sampel dari \\(\\{x_1,\\cdots,x_n\\}\\), dengan probabilitas \\(1 / n\\) . Menghasilkan sampel ukuran bootstrap \\(B\\) berarti pengambilan sampel dari \\(\\{x_1,\\cdots,x_n\\}\\) , dengan probabilitas \\(1 / n\\) , dengan penggantian. set.seed(1) n &lt;- 10 x &lt;- rexp(n, 1/6) m &lt;- 8 bootvalues &lt;- sample(x, size=m, replace=TRUE) 6.2.2 Presisi Bootstrap: Bias, Standar Deviasi, dan Mean Square Error Berikut adalah rangkuman prosedur bootstrap nonparametrik sebagai berikut: Dari sampel \\(\\{X_1, \\ldots, X_n\\}\\), gambar sampel berukuran n (dengan penggantian), katakanlah, \\(X_1^*, \\ldots, X_n^*\\) . Dari undian yang disimulasikan, hitung statistik minat, dilambangkan sebagai \\(\\hat{\\theta}(X_1^*, \\ldots, X_n^*)\\) . Panggil ini \\(\\hat{\\theta}_b^*\\) untuk ulangan ke-b . Ulangi ini \\(b=1, \\ldots, B\\) kali untuk mendapatkan sampel statistik \\(\\hat{\\theta}_1^*, \\ldots,\\hat{\\theta}_B^*\\). Dari sampel statistik pada Langkah 2,\\(\\{\\hat{\\theta}_1^*, \\ldots, \\hat{\\theta}_B^*\\}\\) hitung ukuran ringkasan minat. Pada bagian ini, ada tiga langkah ringkasan yaitu bias, standar deviasi, dan mean square error ( MSE ). Tabel dibawah ini merangkum ketiga ukuran. Di Sini, \\(\\overline{\\hat{\\theta^*}}\\) adalah rata-rata dari \\(\\{\\hat{\\theta}_1^*, \\ldots,\\hat{\\theta}_B^*\\}\\). # Example from Derrig et al BIData &lt;- read.csv(&quot;Data/DerrigResampling.csv&quot;, header =T) BIData$Censored &lt;- 1*(BIData$AmountPaid &gt;= BIData$PolicyLimit) BIDataUncensored &lt;- subset(BIData, Censored == 0) LER.boot &lt;- function(ded, data, indices){ resample.data &lt;- data[indices,] sumClaims &lt;- sum(resample.data$AmountPaid) sumClaims_d &lt;- sum(pmin(resample.data$AmountPaid,ded)) LER &lt;- sumClaims_d/sumClaims return(LER) } ##Derrig et al set.seed(2019) dVec2 &lt;- c(4000, 5000, 10500, 11500, 14000, 18500) OutBoot &lt;- matrix(0,length(dVec2),6) for (i in 1:length(dVec2)) { OutBoot[i,1] &lt;- dVec2[i] results &lt;- boot(data=BIDataUncensored, statistic=LER.boot, R=1000, ded=dVec2[i]) OutBoot[i,2] &lt;- results$t0 biasboot &lt;- mean(results$t)-results$t0 -&gt; OutBoot[i,3] sdboot &lt;- sd(results$t) -&gt; OutBoot[i,4] temp &lt;- boot.ci(results) OutBoot[i,5] &lt;- temp$normal[2] OutBoot[i,6] &lt;- temp$normal[3] } Berdasarkan tabel diatas hasil estimasi bootstrap. Misalnya, di D= 14000 , estimasi nonparametrik LER adalah 0,97678. Ini memiliki perkiraan bias 0,00018 dengan standar deviasi 0,00701. Untuk beberapa aplikasi, mungkin ingin menerapkan estimasi bias ke estimasi asli untuk memberikan estimator yang dikoreksi bias. Untuk ilustrasi ini, biasnya kecil sehingga koreksi semacam itu tidak relevan. Standar deviasi bootstrap memberikan ukuran presisi. Untuk satu penerapan standar deviasi dapat menggunakan pendekatan normal untuk membuat selang kepercayaan. Misalnya, pada R fungsi boot.ci menghasilkan interval kepercayaan normal sebesar 95%. Ini dihasilkan dengan membuat interval dua kali panjang standar deviasi bootstrap 1,95994, berpusat di sekitar estimator yang dikoreksi bias (1,95994 adalah kuantil ke-97,5 dari distribusi normal). Misalnya, CI 95% normal yang lebih rendah di \\(D= 14000\\) adalah \\((0.97678-0.00018)- 1.95994*0.00701\\). Contoh 6.2.2. Memperkirakan \\(\\exp(\\mu)\\) . Bootstrap dapat digunakan untuk mengukur bias estimator, misalnya. Pertimbangkan di sini sampel \\(\\mathbf{x}=\\{x_1,\\cdots,x_n\\}\\) adalah rata-rata μ . sample_x &lt;- c(2.46,2.80,3.28,3.86,2.85,3.67,3.37,3.40,5.22,2.55, 2.79,4.50,3.37,2.88,1.44,2.56,2.00,2.07,2.19,1.77) Misalkan kuantitas bunga adalah \\(\\theta=\\exp(\\mu)\\). Penaksir alami akan menjadi \\(\\widehat{\\theta}_1=\\exp(\\overline{x})\\). Estimator ini bias (karena ketidaksetaraan Jensen) tetapi tidak bias secara asimtotik. Untuk sampel, perkiraannya adalah sebagai berikuT (theta_1 &lt;- exp(mean(sample_x))) ## [1] 19.13463 Seseorang dapat menggunakan teorema limit pusat untuk mendapatkan koreksi menggunakan \\[\\overline{X}\\approx\\mathcal{N}\\left(\\mu,\\frac{\\sigma^2}{n}\\right)\\text{ where }\\sigma^2=\\text{Var}[X_i] ,\\] sehingga dengan fungsi pembangkit momen normal didapatkan \\[\\mathrm{E}~\\left[\\exp(\\overline{X})\\right] \\approx \\exp\\left(\\mu+\\frac{\\sigma^2}{2n}\\right) .\\] Oleh karena itu, seseorang dapat mempertimbangkan secara alami \\[\\widehat{\\theta}_2=\\exp\\left(\\overline{x}-\\frac{\\widehat{\\sigma}^2}{2n}\\right) .\\] n &lt;- length(sample_x) (theta_2 &lt;- exp(mean(sample_x)-var(sample_x)/(2*n))) ## [1] 18.73334 Sebagai strategi lain, seseorang juga dapat menggunakan pendekatan Taylor untuk mendapatkan penaksir yang lebih akurat (seperti dalam metode delta) \\[g(\\overline{x})=g(\\mu)+(\\overline{x}-\\mu)g&#39;(\\mu)+(\\overline{x}-\\mu)^2\\frac{g&#39;&#39;(\\mu)}{2}+\\cdots\\] Alternatif selanjutnya adalah menggunakan strategi bootstrap dengan sampel bootstrap \\(\\mathbf{x}^{\\ast}_{b}\\) sehingga \\(\\overline{x}^{\\ast}_{b}\\). \\[\\widehat{\\theta}_3=\\frac{1}{B}\\sum_{b=1}^B\\exp(\\overline{x}^{\\ast}_{b}) .\\] library(boot) results &lt;- boot(data=sample_x, statistic=function(y,indices) exp(mean(y[indices])), R=1000) theta_3 &lt;- mean(results$t) Ini menghasilkan tiga estimator, estimator mentah \\(\\widehat{\\theta}_1=19.135\\), koreksi urutan kedua \\(\\widehat{\\theta}_2= 18.733\\), dan estimator bootstrap \\(\\widehat{\\theta}_3= 19.388\\). Bagaimana cara kerjanya dengan ukuran sampel yang berbeda? Diasumsikan bahwa \\(X_i\\) dihasilkan dari distribusi lognormal \\(LN(0,1)\\) , sehingga \\(\\mu = \\exp(0 + 1/2) = 1.648721\\) Dan \\(\\theta = \\exp(1.648721)= 5,200326\\). Dengan menggunakan simulasi untuk menggambar ukuran sampel. param &lt;- function(x){ n &lt;- length(x) theta_1 &lt;- exp(mean(x)) theta_2 &lt;- exp(mean(x)-var(x)/(2*n)) results &lt;- boot(data=x, statistic=function(y,indices) exp(mean(y[indices])), R=999) theta_3 &lt;- mean(results$t) return(c(theta_1,theta_2,theta_3)) } set.seed(2074) ns&lt;- 200 est &lt;- function(n){ call_param &lt;- function(i) param(rlnorm(n,0,1)) V &lt;- Vectorize(call_param)(1:ns) apply(V,1,median) } VN=seq(15,100,by=5) Est &lt;- Vectorize(est)(VN) matplot(VN,t(Est),type=&quot;l&quot;, col=2:4, lty=2:4, ylim=exp(exp(1/2))+c(-1,1), xlab=&quot;sample size (n)&quot;, ylab=&quot;estimator&quot;) abline(h=exp(exp(1/2)),lty=1, col=1) legend(&quot;topleft&quot;, c(&quot;raw estimator&quot;, &quot;second order correction&quot;, &quot;bootstrap&quot;), col=2:4,lty=2:4, bty=&quot;n&quot;) Hasil perbandingan dirangkum dalam gambar diatas menunjukkan bahwa estimator bootstrap mendekati nilai parameter sebenarnya untuk hampir semua ukuran sampel. Bias dari ketiga estimator berkurang dengan meningkatnya ukuran sampel. 6.2.3 Interval Keyakinan Prosedur bootstrap menghasilkan \\(B\\) bentuk ulang dari \\(\\hat{\\theta}_1^*, \\ldots,\\hat{\\theta}_B^*\\) dari penaksir \\(\\hat{\\theta}\\) . Dalam Contoh 6.2.1, dapat dilihat bagaimana menggunakan pendekatan normal standar untuk membuat interval kepercayaan untuk parameter yang diinginkan. Namun, mengingat poin utamanya adalah menggunakan bootstrapping untuk menghindari ketergantungan pada asumsi perkiraan normalitas, tidak mengherankan jika tersedia interval kepercayaan alternatif. Untuk estimator \\(\\hat{\\theta}\\) , interval kepercayaan bootstrap dasar adalah \\[\\begin{equation} \\left(2 \\hat{\\theta} - q_U, 2 \\hat{\\theta} - q_L \\right) , \\tag{6.2} \\end{equation}\\] Di mana \\(q_L\\) Dan \\(q_U\\) adalah kuantil 2,5% bawah dan atas dari sampel bootstrap \\(\\hat{\\theta}_1^*, \\ldots,\\hat{\\theta}_B^*\\) Untuk melihat dari mana asalnya, mula-mula \\((q_L, q_U)\\) menyediakan interval 95% untuk \\(\\hat{\\theta}_1^*, \\ldots,\\hat{\\theta}_B^*\\) . Jadi, untuk acak \\(\\hat{\\theta}_b^*\\), ada kemungkinan 95% itu \\(q_L \\le \\hat{\\theta}_b^* \\le q_U\\). Membalikkan pertidaksamaan dan menjumlahkan \\(\\hat{\\theta}\\) ke setiap sisi memberikan interval 95% \\[\\hat{\\theta} -q_U \\le \\hat{\\theta} - \\hat{\\theta}_b^* \\le \\hat{\\theta} -q_L .\\] Jadi, \\(\\left( \\hat{\\theta}-q_U, \\hat{\\theta} -q_L\\right)\\) adalah interval 95% untuk \\(\\hat{\\theta} - \\hat{\\theta}_b^*\\). Ide perkiraan bootstrap mengatakan bahwa ini juga merupakan interval 95% untuk \\(\\theta - \\hat{\\theta}\\). Dengan menambahkan \\(\\hat{\\theta}\\) ke setiap sisi memberikan interval 95% dalam persamaan diatas. Banyak alternatif interval bootstrap yang tersedia. Yang paling mudah dijelaskan adalah interval bootstrap persentil yang didefinisikan sebagai \\((q_L,q_U)\\). Contoh 6.2.3. Klaim Cidera Tubuh dan Tindakan Risiko. Untuk melihat bagaimana interval kepercayaan bootstrap bekerja, dengan kembali ke klaim otomatis cedera tubuh yang dipertimbangkan dalam Contoh 6.2.1 . Alih-alih rasio eliminasi kerugian, misalkan ingin memperkirakan persentil ke-95 \\(F^{-1}(0.95)\\) dan ukuran didefinisikan sebagai \\[TVaR_{0.95}[X] = \\mathrm{E}[X | X &gt; F^{-1}(0.95)] .\\] Pengukuran ini disebut dengan ekor nilai berisiko; itu adalah nilai yang diharapkan dari X bersyarat X melebihi persentil ke-95. Bagian 10.2 menjelaskan bagaimana quantiles dan tail value-at-risk adalah dua contoh paling penting dari apa yang disebut sebagai ukuran risiko . Untuk saat ini, hanya akan menganggap ini sebagai ukuran yang ingin diperkirakan. Untuk persentil, dengan menggunakan estimator nonparametrik \\(F^{-1}_n(0.95)\\) didefinisikan dalam Bagian 4.1.1.3 . Untuk tail value-at-risk, menggunakan prinsip plug-in untuk menentukan estimator nonparametrik \\[TVaR_{n,0.95}[X] = \\frac{\\sum_{i=1}^n X_i I(X_i &gt; F^{-1}_n(0.95))}{\\sum_{i=1}^n I(X_i &gt; F^{-1}_n(0.95))} ~.\\] Dalam ungkapan ini, penyebut menghitung jumlah pengamatan yang melebihi persentil ke-95 \\(F^{-1}_n(0.95)\\) . Pembilang menjumlahkan kerugian untuk pengamatan yang melebihi \\(F^{-1}_n(0.95)\\) . Tabel dibawah ini merangkum penaksir untuk pecahan terpilih. # Example from Derrig et al #BIData &lt;- read.csv(&quot;Data/DerrigResampling.csv&quot;, header =T) BIData$Censored &lt;- 1*(BIData$AmountPaid &gt;= BIData$PolicyLimit) BIDataUncensored &lt;- subset(BIData, Censored == 0) set.seed(2017) PercentVec &lt;- c(0.50, 0.80, 0.90, 0.95, 0.98) OutBoot1 &lt;- matrix(0,5,10) for (i in 1:length(PercentVec)) { OutBoot1[i,1] &lt;- PercentVec[i] results &lt;- boot(data=BIDataUncensored$AmountPaid, statistic=function(X,indices) quantile(X[indices],PercentVec[i]), R=1000) if (i==1){bootreal &lt;- results$t} OutBoot1[i,2] &lt;- results$t0 OutBoot1[i,3] &lt;- mean(results$t)-results$t0 OutBoot1[i,4] &lt;- sd(results$t) temp &lt;- boot.ci(results, type = c(&quot;norm&quot;, &quot;basic&quot;, &quot;perc&quot;)) OutBoot1[i,5] &lt;- temp$normal[2] OutBoot1[i,6] &lt;- temp$normal[3] OutBoot1[i,7] &lt;- temp$basic[4] OutBoot1[i,8] &lt;- temp$basic[5] OutBoot1[i,9] &lt;- temp$percent[4] OutBoot1[i,10] &lt;- temp$percent[5] } Misalnya, ketika pecahannya adalah 0,50, dapat melihat bahwa kuantil 2,5 bawah dan atas dari simulasi bootstrap adalah \\(q_L= 6000\\) dan \\(q_U= 6700\\). Ini membentuk interval kepercayaan bootstrap persentil. Dengan estimator nonparametrik \\(6500\\), ini menghasilkan batas bawah dan atas interval kepercayaan dasar masing-masing \\(6300\\) dan \\(7000\\). CTE.boot &lt;- function(data, indices, RiskLevel){ resample.data &lt;- data[indices,] X &lt;- resample.data$AmountPaid cutoff &lt;- quantile(X, RiskLevel) CTE &lt;- sum(X*(X &gt; cutoff))/sum(X &gt; cutoff) return(CTE) } set.seed(2017) PercentVec &lt;- c(0.50, 0.80, 0.90, 0.95, 0.98) OutBoot1 &lt;- matrix(0,5,10) for (i in 1:length(PercentVec)) { OutBoot1[i,1] &lt;- PercentVec[i] results &lt;- boot(data=BIDataUncensored, statistic=CTE.boot, R=1000, RiskLevel=PercentVec[i]) OutBoot1[i,2] &lt;- results$t0 OutBoot1[i,3] &lt;- mean(results$t)-results$t0 OutBoot1[i,4] &lt;- sd(results$t) temp &lt;- boot.ci(results, type = c(&quot;norm&quot;, &quot;basic&quot;, &quot;perc&quot;)) OutBoot1[i,5] &lt;- temp$normal[2] OutBoot1[i,6] &lt;- temp$normal[3] OutBoot1[i,7] &lt;- temp$basic[4] OutBoot1[i,8] &lt;- temp$basic[5] OutBoot1[i,9] &lt;- temp$percent[4] OutBoot1[i,10] &lt;- temp$percent[5] } Tabel di atas menunjukkan kalkulasi serupa untuk tail value-at-risk. Dalam setiap kasus, dapat melihat bahwa deviasi standar bootstrap meningkat seiring dengan peningkatan fraksi. Hal ini karena ada lebih sedikit pengamatan untuk memperkirakan kuantil seiring meningkatnya fraksi, yang menyebabkan ketidaktepatan yang lebih besar. Interval kepercayaan juga menjadi lebih lebar. Menariknya, tampaknya tidak ada pola yang sama dalam estimasi bias tersebut. 6.2.4 Bootstrap Parametrik Gagasan dari bootstrap nonparametrik adalah untuk mengambil sampel ulang dengan menggambar variabel independen dari fungsi distribusi kumulatif empiris \\(F_n\\). Sebaliknya, dengan bootstrap parametrik, kami menarik variabel independen dari \\(F_{\\widehat{\\theta}}\\) di mana distribusi yang mendasarinya diasumsikan dalam keluarga parametrik \\(\\mathcal{F}=\\{F_{\\theta},\\theta\\in\\Theta\\}\\) . Biasanya, parameter dari distribusi ini diperkirakan berdasarkan sampel dan dinotasikan sebagai \\(\\hat{\\theta}\\). contoh 6.2.4. distribusi lognormal. Pertimbangkan lagi kumpulan datanya sample_x &lt;- c(2.46,2.80,3.28,3.86,2.85,3.67,3.37,3.40, 5.22,2.55,2.79,4.50,3.37,2.88,1.44,2.56,2.00,2.07,2.19,1.77) Bootstrap klasik (nonparametrik) didasarkan pada contoh berikut. x &lt;- sample(sample_x,replace=TRUE) Sebagai gantinya, untuk bootstrap parametrik harus mengasumsikan bahwa distribusi dari \\(x_i\\) adalah dari kelompok tertentu. Sebagai contoh, kode berikut menggunakan distribusi lognormal. library(MASS) fit &lt;- fitdistr(sample_x, dlnorm, list(meanlog = 1, sdlog = 1)) fit x &lt;- rlnorm(length(sample_x), meanlog=fit$estimate[1], sdlog=fit$estimate[2]) set.seed(2074) CV &lt;- matrix(NA,1e5,2) for(s in 1:nrow(CV)){ x1 &lt;- sample(sample_x,replace=TRUE) x2 &lt;- rlnorm(length(sample_x), meanlog=fit$estimate[1], sdlog=fit$estimate[2]) CV[s,] &lt;- c(sd(x1)/mean(x1),sd(x2)/mean(x2)) } plot(density(CV[,1]),col=&quot;red&quot;,main=&quot;&quot;,xlab=&quot;Coefficient of Variation&quot;, lty=1) lines(density(CV[,2]),col=&quot;blue&quot;,lty=2) abline(v=sd(sample_x)/mean(sample_x),lty=3) legend(&quot;topright&quot;,c(&quot;nonparametric&quot;,&quot;parametric(LN)&quot;), col=c(&quot;red&quot;,&quot;blue&quot;),lty=1:2,bty=&quot;n&quot; Grafik di atas membandingkan distribusi bootstrap untuk koefisien variasi, yang satu berdasarkan pendekatan nonparametrik dan yang lainnya berdasarkan pendekatan parametrik, dengan asumsi distribusi lognormal. Contoh 6.2.5. Pengamatan yang Disensor Bootstrap. Bootstrap parametrik menarik realisasi simulasi dari perkiraan parametrik dari fungsi distribusi. Dengan cara yang sama, sehingga dapat menggambar realisasi simulasi dari estimasi fungsi distribusi. Sebagai salah satu contoh, dengan mengambil dari estimasi yang dihaluskan dari fungsi distribusi yang diperkenalkan di Bagian 4.1.1.4 . Kasus khusus lainnya, yang dipertimbangkan di sini adalah menggambar estimasi dari estimator Kaplan-Meier yang dibahas di Bagian 4.3.2.2. Dengan cara ini, dapat ditangani pengamatan yang disensor. Secara khusus, kembali ke data cedera tubuh pada Contoh 6.2.1 dan 6.2.3 tetapi sekarang menyertakan 17 klaim yang disensor oleh batasan kebijakan. Dalam Contoh 4.3.6 menggunakan kumpulan data lengkap ini untuk mengestimasi estimator Kaplan-Meier dari fungsi survival yang diperkenalkan di Bagian 4.3.2.2 . Tabel 6.6 menyajikan estimasi bootstrap kuantil dari estimator fungsi survival Kaplan-Meier. Ini termasuk perkiraan presisi bootstrap, bias dan standar deviasi, serta interval kepercayaan dasar 95%. # Example from Derrig et al library(survival) # for Surv(), survfit() BIData$UnCensored &lt;- 1*(BIData$AmountPaid &lt; BIData$PolicyLimit) ## KM estimate KM0 &lt;- survfit(Surv(AmountPaid, UnCensored) ~ 1, type=&quot;kaplan-meier&quot;, data=BIData) set.seed(2019) PercentVec &lt;- c(0.50, 0.80, 0.90, 0.95, 0.98) OutBoot1 &lt;- matrix(NA,5,6) KM.survobj &lt;- Surv(BIData$AmountPaid, BIData$UnCensored) for (i in 1:length(PercentVec)) { OutBoot1[i,1] &lt;- PercentVec[i] results &lt;- bootkm(KM.survobj, q=1-PercentVec[i], B=1000, pr = FALSE) if (i==1){bootreal &lt;- results} OutBoot1[i,2] &lt;- quantile(KM0, PercentVec[i])$quantile OutBoot1[i,3] &lt;- mean(results)-OutBoot1[i,2] OutBoot1[i,4] &lt;- sd(results) # temp &lt;- boot.ci(results, type = c(&quot;norm&quot;, &quot;basic&quot;,&quot;perc&quot;)) OutBoot1[i,5] &lt;- 2*OutBoot1[i,2]-quantile(results,.975, type=6) OutBoot1[i,6] &lt;- 2*OutBoot1[i,2]-quantile(results,.025, type=6) } Hasil pada tabel di atas konsisten dengan hasil untuk subsampel tanpa sensor pada Tabel 6.4 . Pada tabel di atas tercatat kesulitan dalam memperkirakan kuantil pada pecahan besar karena penyensoran. Namun, untuk fraksi berukuran sedang (0,50, 0,80, dan 0,90), estimasi nonparametrik Kaplan-Meier (KM NP) dari kuantil konsisten dengan Tabel 6.4 . Standar Deviasi bootstrap lebih kecil pada 0,50 (sesuai dengan median) tetapi lebih besar pada level 0,80 dan 0,90. Analisis data tersensor yang dirangkum dalam tabel di atas menggunakan lebih banyak data daripada analisis subsampel tanpa sensor pada Tabel 6.4 , tetapi juga mengalami kesulitan dalam mengekstraksi informasi untuk kuantil besar. 6.3 Cross Validation Dalam bagian ini, kita akan mempelajari caranya: Membandingkan dan membedakan validasi silang dengan teknik simulasi dan metode bootstrap. Menggunakan teknik validasi silang untuk pemilihan model Menjelaskan metode jackknife sebagai kasus khusus validasi silang dan menghitung estimasi bias dan kesalahan standar jackknife Validasi silang, yang diperkenalkan secara singkat pada Bagian 4.2.4, adalah teknik yang didasarkan pada hasil simulasi. Sekarang kita akan membandingkan dan membedakan validasi silang dengan teknik simulasi lain yang telah diperkenalkan dalam bab ini.” Simulasi, atau Monte-Carlo, yang diperkenalkan pada Bagian 6.1, memungkinkan kita untuk menghitung nilai ekspektasi dan rangkuman distribusi statistik lainnya, seperti nilai-p, dengan mudah. Bootstrap, dan metode resampling lainnya yang diperkenalkan pada Bagian 6.2, menyediakan estimator presisi, atau variabilitas, statistik. Validasi silang penting ketika menilai seberapa akurat model prediktif akan bekerja dalam praktiknya. Tumpang tindih memang ada, namun tetap saja akan sangat membantu untuk memikirkan tujuan luas yang terkait dengan setiap metode statistik. Untuk membahas validasi silang, mari kita ingat kembali dari Bagian 4.2 beberapa ide kunci dari validasi model. Ketika menilai, atau memvalidasi, sebuah model, kita melihat kinerja yang diukur pada data baru, atau setidaknya bukan data yang digunakan untuk mencocokkan model. Pendekatan klasik, yang dijelaskan di Bagian 4.2.3, adalah membagi sampel menjadi dua: satu bagian (dataset pelatihan) digunakan untuk menyesuaikan model dan bagian lainnya (dataset pengujian) digunakan untuk memvalidasi. Namun, keterbatasan dari pendekatan ini adalah bahwa hasilnya bergantung pada pembagian; meskipun keseluruhan sampel tetap, pembagian antara sub-sampel pelatihan dan pengujian bervariasi secara acak. Sampel pelatihan yang berbeda berarti parameter estimasi model akan berbeda. Parameter model yang berbeda dan sampel uji yang berbeda berarti statistik validasi akan berbeda. Dua orang analis dapat menggunakan data yang sama dan model yang sama, namun mencapai kesimpulan yang berbeda tentang kelayakan suatu model (berdasarkan pembagian acak yang berbeda), sebuah situasi yang membuat frustasi. 6.3.1 k-Fold Cross-Validation Untuk mengurangi kesulitan ini, biasanya digunakan pendekatan validasi silang seperti yang diperkenalkan di Bagian 4.2.4. Ide utamanya adalah meniru pendekatan pengujian/pelatihan dasar untuk validasi model dengan mengulanginya berkali-kali melalui rata-rata dari beberapa bagian data yang berbeda. Keuntungan utamanya adalah bahwa statistik validasi tidak terikat pada model parametrik (atau nonparametrik) tertentu - seseorang dapat menggunakan statistik nonparametrik atau statistik yang memiliki interpretasi ekonomi - sehingga dapat digunakan untuk membandingkan model yang tidak bersarang (tidak seperti prosedur rasio kemungkinan). Contoh 6.3.1. Dana Properti Wisconsin. Untuk data dana properti 2010 yang diperkenalkan pada Bagian 1.3, kami mencocokkan distribusi gamma dan Pareto dengan 1.377 data klaim. Untuk rincian kecocokan terkait, lihat Lampiran Bagian 15.4.4. Sekarang kita mempertimbangkan statistik Kolmogorov-Smirnov yang diperkenalkan di Bagian 4.1.2.2. Ketika seluruh dataset telah sesuai, statistik kecocokan Kolmogorov-Smirnov untuk distribusi gamma adalah 0,2639 dan untuk distribusi Pareto adalah 0,0478. Nilai yang lebih rendah untuk distribusi Pareto menunjukkan bahwa distribusi ini lebih cocok daripada gamma. Untuk melihat bagaimana validasi silang k-lipatan bekerja, kami membagi data secara acak menjadi \\(k=8\\) kelompok, atau lipatan, yang masing-masing memiliki sekitar \\(1377/8≈172\\) pengamatan. Kemudian, kami mencocokkan model gamma dan Pareto pada set data dengan tujuh lipatan pertama (sekitar $172⋅7 = 120$4 pengamatan), menentukan estimasi parameter, dan kemudian menggunakan model-model yang cocok dengan data yang ditahan untuk menentukan statistik Kolmogorov-Smirnov. library(VGAM) ## Loading required package: stats4 ## Loading required package: splines ## ## Attaching package: &#39;VGAM&#39; ## The following objects are masked from &#39;package:boot&#39;: ## ## logit, simplex library(MASS) claim_lev &lt;- read.csv(&quot;data/CLAIMLEVEL.csv&quot;, header = TRUE) claim_data &lt;- subset(claim_lev, Year == 2010); # Randomly re-order the data - &quot;shuffle it&quot; n &lt;- nrow(claim_data) set.seed(12347) cvdata &lt;- claim_data[sample(n), ] # Number of folds k &lt;- 8 cvalvec &lt;- matrix(0,2,k) for (i in 1:k) { indices &lt;- (((i-1) * round((1/k)*nrow(cvdata))) + 1):((i*round((1/k) * nrow(cvdata)))) # Pareto fit.pareto &lt;- vglm(Claim ~ 1, paretoII, loc = 0, data = cvdata[-indices,]) ksResultPareto &lt;- ks.test(cvdata[indices,]$Claim, &quot;pparetoII&quot;, loc = 0, shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1])) cvalvec[1,i] &lt;- ksResultPareto$statistic # Gamma fit.gamma &lt;- glm(Claim ~ 1, data = cvdata[-indices,], family = Gamma(link = log)) gamma_theta &lt;- exp(coef(fit.gamma)) * gamma.dispersion(fit.gamma) alpha &lt;- 1 / gamma.dispersion(fit.gamma) ksResultGamma &lt;- ks.test(cvdata[indices,]$Claim, &quot;pgamma&quot;, shape = alpha, scale = gamma_theta) cvalvec[2,i] &lt;- ksResultGamma$statistic } KScv &lt;- rowSums(cvalvec)/k Hasilnya tampak pada Gambar 6.12 di mana sumbu horizontal adalah Fold=1. Proses ini diulangi untuk tujuh lipatan lainnya. Hasil yang dirangkum dalam Gambar 6.12 menunjukkan bahwa Pareto secara konsisten memberikan distribusi prediktif yang lebih dapat diandalkan daripada gamma. # Plot the statistics matplot(1:k,t(cvalvec),type=&quot;b&quot;, col=c(1,3), lty=1:2, ylim=c(0,0.4), pch = 0, xlab=&quot;Fold&quot;, ylab=&quot;KS Statistic&quot;) legend(&quot;left&quot;, c(&quot;Pareto&quot;, &quot;Gamma&quot;), col=c(1,3),lty=1:2, bty=&quot;n&quot;) “Figure 6.2:” Statistik Kolmogorov-Smirnov (KS) yang telah divalidasi silang untuk Data Klaim Dana Asuransi. Garis hitam solid untuk distribusi Pareto, garis putus-putus hijau untuk distribusi gamma. Statistik KS mengukur deviasi terbesar antara distribusi yang sesuai dengan distribusi empiris untuk masing-masing dari 8 kelompok, atau lipatan, data yang dipilih secara acak. 6.3.2 6.3.2 Leave-One-Out Cross-Validation Kasus khusus di mana \\(k=n\\) dikenal sebagai validasi silang tinggalkan-satu-keluar. Kasus ini secara historis sangat menonjol dan terkait erat dengan jackknifestatistik yang merupakan pendahulu dari teknik bootstrap. Meskipun kita menyajikannya sebagai kasus khusus validasi silang, akan sangat membantu jika kami memberikan definisi eksplisit. Pertimbangkan sebuah statistik umum \\(θˆ = t(x)\\) yang merupakan penaksir untuk sebuah parameter yang diminati \\(θ\\). Ide dari jackknife adalah menghitung n nilai \\(θˆ_{-i} = t(x-i)\\), di mana \\(x-i\\) adalah subsampel dari \\(x\\) dengan nilai \\(ke-i\\) dihilangkan. Rata-rata dari nilai-nilai ini dilambangkan sebagai \\[\\overline{\\widehat{\\theta}}_{(\\cdot)}=\\frac{1}{n}\\sum_{i=1}^n \\widehat{\\theta}_{-i} .\\] Nilai-nilai ini dapat digunakan untuk membuat estimasi bias dari statistik \\(\\hatθ\\) \\[\\begin{equation} Bias_{jack} = (n-1) \\left(\\overline{\\widehat{\\theta}}_{(\\cdot)} - \\widehat{\\theta}\\right) \\tag{6.3} \\end{equation}\\] serta estimasi standar deviasi \\[\\begin{equation} s_{jack} =\\sqrt{\\frac{n-1}{n}\\sum_{i=1}^n \\left(\\widehat{\\theta}_{-i} -\\overline{\\widehat{\\theta}}_{(\\cdot)}\\right)^2} ~. \\tag{6.4} \\end{equation}\\] Contoh 6.3.2. Koefisien Variasi. Sebagai ilustrasi, pertimbangkan sebuah sampel fiktif kecil \\(x = {x_1,...,x_n}\\) dengan realisasi sample_x &lt;- c(2.46,2.80,3.28,3.86,2.85,3.67,3.37,3.40, 5.22,2.55,2.79,4.50,3.37,2.88,1.44,2.56,2.00,2.07,2.19,1.77) Misalkan kita tertarik dengan \\(\\theta = CV = \\sqrt{\\mathrm{Var~}[X]}/\\mathrm{E~}[X]\\) Dengan dataset ini, estimator koefisien variasi menjadi 0,31196. Namun, seberapa handalkah estimasi tersebut? Untuk menjawab pertanyaan ini, kita dapat menghitung estimator pisau lipat dari bias dan deviasi standarnya. Kode berikut ini menunjukkan bahwa penaksir jackknife untuk bias adalah \\(Bias_{jack} = -0,00627\\) dan standar deviasi jackknife adalah \\(s_{jack} = 0,01293\\). CVar &lt;- function(x) sqrt(var(x))/mean(x) JackCVar &lt;- function(i) sqrt(var(sample_x[-i]))/mean(sample_x[-i]) JackTheta &lt;- Vectorize(JackCVar)(1:length(sample_x)) BiasJack &lt;- (length(sample_x)-1)*(mean(JackTheta) - CVar(sample_x)) sd(JackTheta) ## [1] 0.01293001 Contoh 6.3.3. Klaim Cidera Badan dan Rasio Eliminasi Kerugian. Pada Contoh 6.2.1, kita telah menunjukkan bagaimana menghitung estimasi bootstrap dari bias dan deviasi standar untuk rasio eliminasi kerugian dengan menggunakan data klaim cedera badan pada Contoh 4.1.11. Sekarang kita menindaklanjuti dengan memberikan jumlah yang sebanding dengan menggunakan statistik jackknife. Tabel 6.7 merangkum hasil estimasi jackknife. Tabel ini menunjukkan bahwa estimasi jackknife terhadap bias dan deviasi standar dari rasio eliminasi kerugian \\(E [min (X, d)]/E [X]\\) sebagian besar konsisten dengan metodologi bootstrap. Selain itu, kita dapat menggunakan standar deviasi untuk membangun interval kepercayaan berbasis normal, yang berpusat di sekitar penaksir yang dikoreksi bias. Sebagai contoh, pada \\(d = 14000\\), kita melihat pada Contoh 4.1.11 bahwa estimasi nonparametrik dari \\(LER\\) adalah 0.97678. Estimasi ini memiliki bias sebesar 0,00010, sehingga menghasilkan estimator terkoreksi-bias sebesar 0,97688. Interval kepercayaan 95% dihasilkan dengan membuat interval dua kali panjang 1,96 deviasi standar jackknife, yang berpusat pada estimator terkoreksi bias (1,96 adalah perkiraan kuantil ke-97,5 dari distribusi normal standar). library(boot) # Example from Derrig et al BIData &lt;- read.csv(&quot;data/DerrigResampling.csv&quot;, header =T) BIData$Censored &lt;- 1*(BIData$AmountPaid &gt;= BIData$PolicyLimit) BIDataUncensored &lt;- subset(BIData, Censored == 0) LER.boot &lt;- function(ded, data, indices){ resample.data &lt;- data[indices,] sumClaims &lt;- sum(resample.data$AmountPaid) sumClaims_d &lt;- sum(pmin(resample.data$AmountPaid,ded)) LER &lt;- sumClaims_d/sumClaims return(LER) } x &lt;- BIDataUncensored$AmountPaid LER.jack&lt;- function(ded,i){ LER &lt;- sum(pmin(x[-i],ded))/sum(x[-i]) return(LER) } LER &lt;- function(ded) sum(pmin(x,ded))/sum(x) ##Derrig et al set.seed(2019) dVec2 &lt;- c(4000, 5000, 10500, 11500, 14000, 18500) OutJack &lt;- matrix(0,length(dVec2),8) for (j in 1:length(dVec2)) { OutJack[j,1] &lt;- dVec2[j] results &lt;- boot(data=BIDataUncensored, statistic=LER.boot, R=1000, ded=dVec2[j]) OutJack[j,2] &lt;- results$t0 biasboot &lt;- mean(results$t)-results$t0 -&gt; OutJack[j,3] sdboot &lt;- sd(results$t) -&gt; OutJack[j,4] temp &lt;- boot.ci(results) LER.jack.ded&lt;- function(i) LER.jack(ded=dVec2[j],i) JackTheta.ded &lt;- Vectorize(LER.jack.ded)(1:length(x)) OutJack[j,5] &lt;- BiasJack.ded &lt;- (length(x)-1)*(mean(JackTheta.ded) - LER(ded=dVec2[j])) OutJack[j,6] &lt;- sd(JackTheta.ded) OutJack[j,7:8] &lt;- mean(JackTheta.ded)+qt(c(0.025,0.975),length(x)-1)*OutJack[j,6] } Table 6.7. Estimasi Jackknife dari LER pada Deductible yang Dipilih d NP Estimate Bootstrap Bias Bootstrap SD Jackknife Bias Jackknife SD Lower Jackknife 95% CI Upper Jackknife 95% CI 4000 0.54113 0.00011 0.01237 0.00031 0.00061 0.53993 0.54233 5000 0.64960 0.00027 0.01412 0.00033 0.00068 0.64825 0.65094 10500 0.93563 0.00004 0.01017 0.00019 0.00053 0.93460 0.93667 11500 0.95281 -0.00003 0.00941 0.00016 0.00047 0.95189 0.95373 14000 0.97678 0.00016 0.00687 0.00010 0.00034 0.97612 0.97745 18500 0.99382 0.00014 0.00331 0.00003 0.00017 0.99350 0.99415 Diskusi. Salah satu dari banyak hal menarik tentang kasus khusus leave-one-out adalah kemampuan untuk mereplikasi estimasi dengan tepat. Artinya, ketika ukuran lipatan hanya satu, maka tidak ada ketidakpastian tambahan yang disebabkan oleh validasi silang. Ini berarti bahwa para analis dapat mereplikasi pekerjaan satu sama lain dengan tepat, sebuah pertimbangan yang penting. Statistik Jackknife dikembangkan untuk memahami ketepatan estimator, menghasilkan estimator bias dan deviasi standar pada persamaan (6.3) dan (6.4). Hal ini sesuai dengan tujuan yang telah kita kaitkan dengan teknik bootstrap, bukan metode validasi silang. Hal ini menunjukkan bagaimana teknik statistik dapat digunakan untuk mencapai tujuan yang berbeda. 6.3.3 Cross-Validation and Bootstrap Bootstrap berguna untuk memberikan estimator presisi, atau variabilitas, dari statistik. Hal ini juga berguna untuk validasi model. Pendekatan bootstrap untuk validasi model mirip dengan prosedur validasi leave-one-out dan k-fold: Buat sampel bootstrap dengan mengambil sampel ulang (dengan penggantian) \\(n\\) indeks dalam \\({1, ⋯, n}\\). Ini akan menjadi sampel pelatihan kita. Perkirakan model yang sedang dipertimbangkan berdasarkan sampel ini. Uji, atau sampel validasi, terdiri dari pengamatan yang tidak dipilih untuk pelatihan. Mengevaluasi model yang cocok (berdasarkan data pelatihan) dengan menggunakan data uji. Ulangi proses ini beberapa kali (katakanlah \\(B\\)). Ambil rata-rata dari hasil-hasilnya dan pilih model berdasarkan statistik evaluasi rata-rata. Contoh 6.3.4. Dana Properti Wisconsin. Kembali ke Contoh 6.3.1 di mana kita menyelidiki kecocokan distribusi gamma dan Pareto pada data dana properti. Kita kembali membandingkan kinerja prediksi menggunakan statistik Kolmogorov-Smirnov (KS), namun kali ini menggunakan prosedur bootstrap untuk membagi data antara sampel pelatihan dan pengujian. Berikut ini adalah kode ilustrasinya. library(goftest) n &lt;- nrow(claim_data) set.seed(12347) indices &lt;- 1:n # Number of Bootstrap Samples B &lt;- 100 cvalvec &lt;- matrix(0,2,B) for (i in 1:B) { bootindex &lt;- unique(sample(indices, size=n, replace= TRUE)) traindata &lt;- claim_data[bootindex,] testdata &lt;- claim_data[-bootindex,] # Pareto fit.pareto &lt;- vglm(Claim ~ 1, paretoII, loc = 0, data = traindata) ksResultPareto &lt;- ks.test(testdata$Claim, &quot;pparetoII&quot;, loc = 0, shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1])) cvalvec[1,i] &lt;- ksResultPareto$statistic # Gamma fit.gamma &lt;- glm(Claim ~ 1, data = traindata, family = Gamma(link = log)) gamma_theta &lt;- exp(coef(fit.gamma)) * gamma.dispersion(fit.gamma) alpha &lt;- 1 / gamma.dispersion(fit.gamma) ksResultGamma &lt;- ks.test(testdata$Claim, &quot;pgamma&quot;, shape = alpha, scale = gamma_theta) cvalvec[2,i] &lt;- ksResultGamma$statistic } KSBoot &lt;- rowSums(cvalvec)/B Kami melakukan pengambilan sampel dengan menggunakan B= 100 ulangan. Statistik KS rata-rata untuk distribusi Pareto adalah 0,058 dibandingkan dengan rata-rata untuk distribusi gamma, 0,262. Hal ini konsisten dengan hasil sebelumnya dan memberikan bukti lain bahwa Pareto adalah model yang lebih baik untuk data ini dibandingkan dengan gamma. 6.4 Importance Sampling Bagian 6.1 memperkenalkan teknik Monte Carlo dengan menggunakan teknik inversi: untuk membangkitkan sebuah variabel acak \\(X\\) dengan distribusi \\(F\\), terapkan \\(F^{-1}\\) pada pemanggilan sebuah generator acak (seragam pada interval satuan). Bagaimana jika kita ingin menggambar sesuai dengan \\(X\\), dengan syarat \\(X∈[a,b]\\)? Seseorang dapat menggunakan mekanisme terima-tolak: menarik \\(x\\) dari distribusi \\(F\\) jika \\(x\\in[a,b]\\): simpan (“terima”) jika \\(x\\notin[a,b]\\): gambar yang lain (“tolak”) Amati bahwa dari n nilai yang awalnya dihasilkan, kita simpan di sini hanya \\([F(b)-F(a)] ⋅ n\\) hasil imbang, rata-rata. Contoh 6.4.1. Penarikan dari Distribusi Normal. Misalkan kita menggambar dari distribusi normal dengan rata-rata 2,5 dan varians 1, \\(N(2,5,1)\\), tetapi hanya tertarik pada gambar yang lebih besar dari \\(a≥2\\) dan kurang dari \\(b≤4\\). Artinya, kita hanya dapat menggunakan \\(F(4)-F(2)=Φ(4-2.5)-Φ(2-2.5) = 0.9332 - 0.3085 = 0.6247\\) proporsi undian. Gambar 6.13 menunjukkan bahwa beberapa hasil undian berada di dalam interval \\((2,4)\\) dan beberapa di luarnya. mu = 2.5 sigma = 1 a = 2 b = 4 Fa = pnorm(a,mu,sigma) Fb = pnorm(b,mu,sigma) pic_ani = function(){ u=seq(0,5,by=.01) plot(u,pnorm(u,mu,sigma),col=&quot;white&quot;,ylab=&quot;&quot;,xlab=&quot;&quot;) rect(-1,-1,6,2,col=rgb(1,0,0,.2),border=NA) rect(a,Fa,b,Fb,col=&quot;white&quot;,border=NA) lines(u,pnorm(u,mu,sigma),lwd=2) abline(v=c(a,b),lty=2,col=&quot;red&quot;) ru &lt;- runif(1) clr &lt;- &quot;red&quot; if((qnorm(ru,mu,sigma)&gt;=a)&amp;(qnorm(ru,mu,sigma)&lt;=b)) clr &lt;- &quot;blue&quot; segments(-1,ru,qnorm(ru,mu,sigma),ru,col=clr,lwd=2) arrows(qnorm(ru,mu,sigma),ru,qnorm(ru,mu,sigma),0,col=clr,lwd=2,length = .1) } for (i in 1:numAnimation) {pic_ani()} Sebagai gantinya, seseorang dapat menggambar menurut distribusi bersyarat \\(F^⋆\\) yang didefinisikan sebagai \\[F^{\\star}(x) = \\Pr(X \\le x | a &lt; X \\le b) =\\frac{F(x)-F(a)}{F(b)-F(a)}, \\ \\ \\ \\text{for } a &lt; x \\le b .\\] Dengan menggunakan metode inverse transform pada Bagian 6.1.2, kita mendapatkan hasil imbang \\[X^\\star=F^{\\star-1}\\left( U \\right) = F^{-1}\\left(F(a)+U\\cdot[F(b)-F(a)]\\right)\\] memiliki distribusi \\(F⋆^\\). Dinyatakan dengan cara lain, definisikan \\[\\tilde{U} = (1-U)\\cdot F(a)+U\\cdot F(b)\\] dan kemudian gunakan \\(F^{-1}(\\tilde{U})\\). Dengan pendekatan ini, setiap undian dihitung. Hal ini dapat dikaitkan dengan mekanisme pengambilan sampel kepentingan: kita menarik lebih sering di wilayah yang kita harapkan memiliki kuantitas yang memiliki kepentingan. Transformasi ini dapat dianggap sebagai “perubahan ukuran.” pic_ani = function(){ u=seq(0,5,by=.01) plot(u,pnorm(u,mu,sigma),col=&quot;white&quot;,ylab=&quot;&quot;,xlab=&quot;&quot;) rect(-1,-1,6,2,col=rgb(1,0,0,.2),border=NA) rect(a,Fa,b,Fb,col=&quot;white&quot;,border=NA) lines(u,pnorm(u,mu,sigma),lwd=2) abline(h=pnorm(c(a,b),mu,sigma),lty=2,col=&quot;red&quot;) ru &lt;- runif(1) rutilde &lt;- (1-ru)*Fa+ru*Fb segments(-1,rutilde,qnorm(rutilde,mu,sigma),rutilde,col=&quot;blue&quot;,lwd=2) arrows(qnorm(rutilde,mu,sigma),rutilde,qnorm(rutilde,mu,sigma),0,col=&quot;blue&quot;,lwd=2,length = .1) } for (i in 1:numAnimation) {pic_ani()} Pada Contoh 6.4.1., kebalikan dari distribusi normal sudah tersedia (dalam R, fungsinya adalah qnorm). Namun, untuk aplikasi lain, hal ini tidak terjadi. Kemudian, kita cukup menggunakan metode numerik untuk menentukan \\(X^⋆\\) sebagai solusi dari persamaan \\(F(X^\\star) =\\tilde{U}\\) di mana \\(\\tilde{U}=(1-U)\\cdot F(a)+U\\cdot F(b)\\)). Lihat kode ilustrasi berikut ini. pic_ani = function(){ u=seq(0,5,by=.01) plot(u,pnorm(u,mu,sigma),col=&quot;white&quot;,ylab=&quot;&quot;,xlab=&quot;&quot;) rect(-1,-1,6,2,col=rgb(1,0,0,.2),border=NA) rect(2,-1,4,2,col=&quot;white&quot;,border=NA) lines(u,pnorm(u,mu,sigma),lty=2) pnormstar &lt;- Vectorize(function(x){ y=(pnorm(x,mu,sigma)-Fa)/(Fb-Fa) if(x&lt;=a) y &lt;- 0 if(x&gt;=b) y &lt;- 1 return(y) }) qnormstar &lt;- function(u) as.numeric(uniroot((function (x) pnormstar(x) - u), lower = 2, upper = 4)[1]) lines(u,pnormstar(u),lwd=2) abline(v=c(2,4),lty=2,col=&quot;red&quot;) ru &lt;- runif(1) segments(-1,ru,qnormstar(ru),ru,col=&quot;blue&quot;,lwd=2) arrows(qnormstar(ru),ru,qnormstar(ru),0,col=&quot;blue&quot;,lwd=2,length = .1) } for (i in 1:numAnimation) {pic_ani()} Sebenarnya materi yang dari web untuk 6.5 itu masih sedang dalam penulisan dan belum selesai dalam pengeditan. Jadi apa yang ditulis disini hanya memberikan gambaran besarnya saja. Ide dari teknik Monte Carlo bergantung pada hukum bilangan besar (yang menjamin konvergensi rata-rata terhadap integral) dan teorema limit pusat (yang digunakan untuk mengukur ketidakpastian dalam perhitungan). Perlu diingat kembali jika (\\(X_i\\)) adalah urutan ke-i dari variabel acak dengan distribusi F, maka \\[ \\frac{1}{\\sqrt{n}}\\left(\\sum_{i=1}^n h(X_i)-\\int h(x)dF(x)\\right)\\overset{\\mathcal{L}}{\\rightarrow }\\mathcal{N}(0,\\sigma^2),\\text{ as }n\\rightarrow\\infty , \\] atau beberapa varian \\(σ^2&gt;0\\) . Namun sebenarnya, teorema ergodik dapat digunakan untuk melemahkan hasil sebelumnya, karena independensi variabel tidak diperlukan. Lebih tepatnya, jika (\\(X_i\\)) adalah Proses Markov dengan ukuran invarian \\(μ\\) , di bawah beberapa asumsi teknis tambahan, maka dapat diperoleh \\[ \\frac{1}{\\sqrt{n}}\\left(\\sum_{i=1}^n h(X_i)-\\int h(x)d\\mu(x)\\right)\\overset{\\mathcal{L}}{\\rightarrow }\\mathcal{N}(0,\\sigma_\\star^2),\\text{ as }n\\rightarrow\\infty. \\] untuk beberapa varian \\(σ^2_⋆&gt;0\\) . Oleh karena itu, dari sifat ini, dapat melihat bahwa tidak selalu mungkin untuk menghasilkan nilai-nilai independen dari F , tetapi untuk menghasilkan proses Markov dengan ukuran invarian F , dan untuk mempertimbangkan rata-rata dari proses (tidak harus independen). Dengan mempertimbangkan kasus vektor Gaussian terkendala: kami ingin menghasilkan pasangan acak dari vektor acak \\(X\\) , tetapi kami hanya tertarik pada kasus di mana jumlah komposisinya cukup besar, yang dapat ditulis \\(X^T1&gt;m\\) untuk nilai nyata \\(m\\) . Tentu saja, dimungkinkan untuk menggunakan algoritme terima-tolak, tetapi kami telah melihat bahwa ini mungkin sangat tidak efisien. Satu dapat menggunakan Metropolis Hastingsand Gibbs sampler untuk menghasilkan proses Markov dengan ukuran invarian tersebut. 6.5 6.5.1 Metropolis Hastings Algoritma agak sederhana untuk dihasilkan dari \\(f\\) : dapat dimulai dengan nilai layak \\(x_1\\) . Kemudian, pada langkah \\(t\\) , kita perlu menentukan kernel transisi : diberikan \\(x_t\\) , kita memerlukan distribusi bersyarat untuk \\(X_{t+1}\\) diberikan \\(x_t\\) . Algoritme akan bekerja dengan baik jika distribusi bersyarat itu dapat dengan mudah disimulasikan. dengan \\(π(⋅|xt)\\) menunjukkan probabilitas itu. Gambarkan nilai potensial \\(x^⋆_{t+1}\\) , dan \\(u\\) , dari distribusi seragam. Selanjutnya Menghitung \\(R= \\frac{f(x_{t+1}^\\star)}{f(x_t)}\\) jika \\(u&lt;r\\) , lalu atur \\(x_{t+1}=x^⋆_t\\) jika \\(u≤r\\) , maka atur \\(x_{t+1}=x_t\\) Di sini r disebut rasio penerimaan selanjutnya menerima nilai baru dengan probabilitas r (atau sebenarnya yang terkecil antara 1 dan r karena r dapat melebihi 1 ). Misalnya, asumsikan bahwa \\(f(⋅|xt)\\) seragam pada \\([x_t−ε,x_t+ε]\\) untuk beberapa \\(ε&gt;0\\) , dan di mana$ $f (distribusi target kita) adalah \\(N(0,1)\\) . Kami tidak akan pernah menarik dari \\(f\\) , tetapi kami akan menggunakannya untuk menghitung rasio penerimaan kami di setiap langkah. metrop1 &lt;- function(n=1000,eps=0.5){ vec &lt;- matrix(NA, n, 3) x=0 vec[1] &lt;- x for (i in 2:n) { innov &lt;- runif(1,-eps,eps) mov &lt;- x+innov R &lt;- min(1,dnorm(mov)/dnorm(x)) u &lt;- runif(1) if (u &lt; R) x &lt;- mov vec[i,] &lt;- c(x,mov,R) } return(vec)} #install.packages(&#39;gifski&#39;) #if (packageVersion(&#39;knitr&#39;) &lt; &#39;1.20.14&#39;) { # remotes::install_github(&#39;yihui/knitr&#39;) #} vec &lt;- metrop1(25) u=seq(-3,3,by=.01) pic_ani = function(k){ plot(1:k,vec[1:k,1],pch=19,xlim=c(0,25),ylim=c(-2,2),xlab=&quot;&quot;,ylab=&quot;&quot;) if(vec[k+1,1]==vec[k+1,2]) points(k+1,vec[k+1,1],col=&quot;blue&quot;,pch=19) if(vec[k+1,1]!=vec[k+1,2]) points(k+1,vec[k+1,1],col=&quot;red&quot;,pch=19) points(k+1,vec[k+1,2],cex=1.5) arrows(k+1,vec[k,1]-.5,k+1,vec[k,1]+.5,col=&quot;green&quot;,angle=90,code = 3,length=.1) polygon(c(k+dnorm(u)*10,rep(k,length(u))),c(u,rev(u)),col=rgb(0,1,0,.3), border=NA) segments(k,vec[k,1],k+dnorm(vec[k,1])*10,vec[k,1]) segments(k,vec[k+1,2],k+dnorm(vec[k+1,2])*10,vec[k+1,2]) text(k,2,round(vec[k+1,3],digits=3)) } for (k in 2:23) {pic_ani(k)} Selanjutnya dapat menggunakan simulasi, maka didapat vec &lt;- metrop1(10000) simx &lt;- vec[1000:10000,1] par(mfrow=c(1,4)) plot(simx,type=&quot;l&quot;) hist(simx,probability = TRUE,col=&quot;light blue&quot;,border=&quot;white&quot;) lines(u,dnorm(u),col=&quot;red&quot;) qqnorm(simx) acf(simx,lag=100,lwd=2,col=&quot;light blue&quot;) 6.6 6.5.2 Gibbs Sampler Dapat mempertimbangkan beberapa vektor \\(X=(X_1,⋯,X_d)\\) dengan komponen independen, \\(X_i∼E(λ_i)\\) . Selanjutnya mengambil sampel untuk sampel dari \\(X\\) yang diberikan \\(X^T1&gt;s\\) untuk beberapa ambang batas \\(s&gt;0\\) . beberapa titik awal x0 , Mengambil secara acak \\(i∈{1,⋯,d}\\) \\(X_i\\) mengingat \\(X_i&gt;s−x^T_{(−i)}1\\) berdistribusi Eksponensial \\(E(λ_i)\\) Menggambar \\(Y∼E(λ_i)\\) dan atur \\(x_i=y+(s−x^T_{(−i)}1)_+\\) hingga \\(x^T_{(−i)}1+x_i&gt;s\\) sim &lt;- NULL lambda &lt;- c(1,2) X &lt;- c(3,3) s &lt;- 5 for(k in 1:1000){ i &lt;- sample(1:2,1) X[i] &lt;- rexp(1,lambda[i])+max(0,s-sum(X[-i])) while(sum(X)&lt;s){ X[i] &lt;- rexp(1,lambda[i])+max(0,s-sum(X[-i])) } sim &lt;- rbind(sim,X) } plot(sim,xlim=c(1,11),ylim=c(0,4.3)) polygon(c(-1,-1,6),c(-1,6,-1),col=&quot;red&quot;,density=15,border=NA) abline(5,-1,col=&quot;red&quot;) Konstruksi urutan (algoritma MCMC bersifat iteratif) dapat divisualisasikan di bawah ini lambda &lt;- c(1,2) X &lt;- c(3,3) sim &lt;- X s &lt;- 5 for(k in 1:100){ set.seed(k) i &lt;- sample(1:2,1) X[i] &lt;- rexp(1,lambda[i])+max(0,s-sum(X[-i])) while(sum(X)&lt;s){ X[i] &lt;- rexp(1,lambda[i])+max(0,s-sum(X[-i])) } sim &lt;- rbind(sim,X) } pic_ani = function(n){ plot(sim[1:n,],xlim=c(1,11),ylim=c(0,5),xlab=&quot;&quot;,ylab=&quot;&quot;) i=which(apply(sim[(n-1):n,],2,diff)==0) if(i==1) abline(v=sim[n,1],col=&quot;grey&quot;) if(i==2) abline(h=sim[n,2],col=&quot;grey&quot;) if(n&gt;=1) points(sim[n,1],sim[n,2],pch=19,col=&quot;blue&quot;,cex=1.4) if(n&gt;=2) points(sim[n-1,1],sim[n-1,2],pch=19,col=&quot;red&quot;,cex=1.4) polygon(c(-1,-1,6),c(-1,6,-1),col=&quot;red&quot;,density=15,border=NA) abline(5,-1,col=&quot;red&quot;) } for (i in 2:100) {pic_ani(i)} "],["premium-foundations.html", "Bab 7 Premium Foundations 7.1 Pengenalan Ratemaking 7.2 Metode Penentuan Tarif Gabungan 7.3 Prinsip-prinsip Penetapan Harga 7.4 Risiko Heterogen 7.5 Development and Trending", " Bab 7 Premium Foundations 7.1 Pengenalan Ratemaking Pada bagian ini, Anda akan belajar cara: Menggambarkan ekspektasi sebagai metode dasar untuk menentukan premi asuransi Menganalisis persamaan akuntansi untuk menghubungkan premi dengan kerugian, biaya, dan keuntungan Merangkum strategi untuk memperluas penetapan harga untuk mencakup risiko heterogen dan tren dari waktu ke waktu. Bab ini menjelaskan bagaimana menentukan harga yang tepat untuk produk asuransi, yang dikenal sebagai premi. Premi adalah jumlah uang yang dibebankan untuk perlindungan asuransi terhadap kejadian yang tidak pasti. Dalam asuransi, harga/premi ini dikenal sebagai tarif karena dinyatakan dalam unit standar, misalnya harga per seribu dolar pertanggungan atas rumah atau manfaat jika terjadi kematian. Namun, keunikan asuransi adalah bahwa biaya perlindungan asuransi tidak diketahui pada saat penjualan kontrak. Biaya mungkin tidak terungkap selama berbulan-bulan atau bertahun-tahun, tergantung pada kejadian yang diasuransikan. Oleh karena itu, penetapan harga asuransi berbeda dengan pendekatan ekonomi pada umumnya. Dalam pendekatan penetapan harga aktuaria tradisional, harga ditentukan sebagai fungsi dari biaya asuransi. Premi dianggap sebagai sumber pendapatan yang menyediakan pembayaran klaim, biaya kontrak, dan margin operasi, yang dapat dirumuskan dalam persamaan akuntansi: \\[\\begin{equation}\\small{\\text{Premium = Loss + Expense + UW Profit} .}\\tag{7.1}\\end{equation}\\] Namun, ada pasar asuransi di mana harga aktuaria hanya memberikan masukan untuk harga pasar umum. Untuk memperkuat perbedaan ini, premi berbasis biaya aktuaria kadang-kadang dikenal sebagai harga teknis. Oleh karena itu, keputusan perusahaan seperti penetapan harga harus dievaluasi dengan mengacu pada dampaknya terhadap nilai pasar perusahaan. Tujuan ini lebih komprehensif daripada gagasan statis tentang maksimalisasi laba. Istilah Biaya dapat dibagi menjadi biaya yang bervariasi berdasarkan premi, seperti komisi penjualan, dan yang tidak, seperti biaya bangunan dan gaji karyawan. Istilah Keuntungan UW adalah singkatan dari keuntungan underwriting dan dapat mencakup biaya modal. Persamaan ini berlaku untuk jumlah banyak kontrak, atau portofolio, dan digunakan untuk membantu menetapkan premi, seperti dengan menetapkan tujuan laba. Istilah kerugian dalam persamaan tersebut didasarkan pada biaya yang diharapkan, karena sulit untuk memprediksi kerugian yang tepat untuk masing-masing kontrak. Namun, teks tersebut mengakui bahwa pendekatan ini mengasumsikan adanya ketidakpastian dan memperkenalkan prinsip-prinsip premi alternatif yang memasukkan ketidakpastian ke dalam penetapan harga. Bab ini juga memperluas pertimbangan penetapan harga ke kumpulan risiko yang heterogen dan membahas perkembangan dan tren pengalaman kerugian untuk mengembangkan tingkat suku bunga ke depan. Terakhir, bab ini memperkenalkan metode untuk memilih premi dengan membandingkan metode pemeringkatan premi dengan kerugian dari portofolio yang ditahan dan memilih metode yang menghasilkan kecocokan terbaik dengan data yang ditahan. Bab ini juga mencakup suplemen teknis mengenai peraturan pemerintah tentang tarif asuransi. 7.2 Metode Penentuan Tarif Gabungan Pada bagian ini, Anda akan belajar tentang: Definisi pure premium sebagai biaya kerugian serta dalam hal frekuensi dan keparahan. Menghitung tarif yang diindikasikan menggunakan pure premiums, biaya, dan beban keuntungan. Definisi rasio kerugian. Menghitung perubahan tarif yang diindikasikan menggunakan rasio kerugian. Membandingkan metode pure premium dan rasio kerugian untuk menentukan premi. Dalam kasus ini, diasumsikan terdapat \\(n\\) kontrak asuransi dengan kerugian (losses) \\(X1,...,Xn\\). Kontrak-kontrak tersebut memiliki distribusi kerugian yang sama dan dianggap sebagai portofolio homogen yang terdiri dari kontrak-kontrak yang sama. Hal ini dapat diterapkan pada asuransi pribadi seperti asuransi mobil atau asuransi rumah di mana perusahaan asuransi menulis banyak kontrak pada risiko yang sangat mirip. Selain itu, asumsi tentang distribusi yang identik tidak terlalu membatasi karena dalam bagian selanjutnya akan diperkenalkan variabel paparan yang memungkinkan pengalaman dapat diskalakan agar dapat dibandingkan. Dalam kasus ini, diasumsikan bahwa \\(X1,...,Xn\\) adalah iid (independen dan identik terdistribusi). 7.2.1 Metode Penghitungan Premi Murni Dalam metode ini, diperoleh estimasi kerugian yang diharapkan dengan menghitung rata-rata dari kerugian yang terjadi pada seluruh polis dalam suatu kumpulan (n polis). \\[\\begin{equation}\\small{\\mathrm{E}(X) \\approx \\frac{\\sum_{i=1}^n X_i}{n} =\\frac{\\text{Kerugian}}{\\text{Eksposur}} = \\text{Premi Murni}.}\\end{equation}\\] Dalam kasus risiko homogen, di mana semua polis dianggap sama, jumlah polis n dapat digunakan sebagai ukuran eksposur. Namun, pada Bagian 7.4.1, konsep eksposur diperluas ketika polis tidak memiliki karakteristik yang sama. Untuk mendapatkan premi murni, kita juga dapat menggunakan pendekatan frekuensi-keparahan. Dalam hal ini, premi murni dihitung sebagai hasil kali antara frekuensi klaim dan besar kerugian. \\[\\begin{equation}\\small{\\text{Premi Murni} = \\frac{\\text{jumlah klaim}}{\\text{Eksposur}} \\times\\frac{\\text{Kerugian}}{\\text{jumlah klaim}} = \\text{frekuensi} \\times \\text{keparahan}.}\\end{equation}\\] Ketika menggunakan metode premi murni, dapat digunakan baik rata-rata kerugian (biaya kerugian) maupun pendekatan frekuensi-keparahan untuk menentukan premi. Untuk lebih mendekatkan diri pada aplikasi dalam praktik, sekarang kita kembali ke persamaan (7.1) yang menyertakan biaya. Persamaan (7.1) juga mengacu pada Laba UW untuk laba underwriting. Ketika diskalakan dengan premi, ini dikenal sebagai pembebanan laba. Karena klaim tidak pasti, perusahaan asuransi harus memiliki modal untuk memastikan bahwa semua klaim dibayar. Memegang modal ekstra ini adalah biaya menjalankan bisnis, investor di perusahaan perlu dikompensasi untuk ini, dengan demikian pemuatan ekstra. Sekarang kita menguraikan Beban menjadi beban yang bervariasi berdasarkan premi, Variabel, beban yang tidak bervariasi,dan Premi Tetap, sehingga Beban = Variabel + Premi Tetap. Dengan menganggap biaya variabel dan laba sebagai bagian dari premi, kita mendefinisikan \\[\\begin{equation}\\small{V = \\frac{\\text{Variable}}{\\text{Premium}} ~~~ \\text{and}~~~Q = \\frac{\\text{UWProfit}}{\\text{Premium}} ~.}\\end{equation}\\] Dengan definisi dan persamaan (7.1) ini, kita dapat menulis \\[\\begin{equation}\\small{\\begin{matrix}\\begin{array}{ll}\\text{Premium} &amp;= \\text{Losses + Fixed} + \\text{Premium} \\times \\frac{\\text{Variable + UW Profit}}{\\text{Premium}} \\\\&amp; = \\text{Losses + Fixed} + \\text{Premium} \\times (V+Q) .\\end{array}\\end{matrix}}\\end{equation}\\] Penyelesaian untuk hasil premi \\[\\begin{equation}\\small{\\text{Premium} = \\frac{\\text{Losses + Fixed}}{1-V-Q} .}\\end{equation}\\] Dibagi dengan eksposur, tarif dapat dihitung sebagai \\[\\begin{equation}\\begin{matrix}\\begin{array}{ll}\\text{Rate} &amp;= \\frac{\\text{Premium}}{\\text{Exposure}} = \\frac{\\text{Losses/Exposure + Fixed/Exposure}}{1-V-Q} \\\\&amp;= \\frac{\\text{Pure Premium + Fixed/Exposure}}{1-V-Q} ~.\\end{array}\\end{matrix}\\end{equation}\\] Dengan kata lain, ini adalah \\[\\begin{equation}\\small{\\text{Rate} =\\frac{\\text{pure premium + fixed expense per exposure}}{\\text{1 - variable expense factor - profit and contingencies factor}} .}\\end{equation}\\] 7.2.2 Metode Rasio Kerugian Rasio kerugian adalah rasio jumlah kerugian terhadap premi \\[\\begin{equation}\\small{\\text{Loss Ratio} = \\frac{\\text{Loss}}{\\text{Premium}} .}\\end{equation}\\] Ketika menentukan premi, agak berlawanan dengan intuisi untuk menekankan rasio ini karena komponen premi dimasukkan ke dalam penyebut. Seperti yang akan kita lihat, metode rasio kerugian mengembangkan perubahan tingkat daripada tingkat; kita dapat menggunakan perubahan tingkat untuk memperbarui pengalaman masa lalu untuk mendapatkan tingkat saat ini. Untuk melakukan hal ini, perubahan tingkat terdiri dari rasio rasio kerugian pengalaman terhadap rasio kerugian target. Faktor penyesuaian ini kemudian diterapkan pada rate saat ini untuk mendapatkan rate yang baru. Untuk melihat cara kerjanya dalam konteks yang sederhana, mari kita kembali ke persamaan (7.1) tetapi sekarang abaikan biaya untuk mendapatkan $ Premi = Kerugian + Keuntungan UW $. Membagi dengan premi menghasilkan \\[\\begin{equation}\\small{\\frac{\\text{UW Profit}}{\\text{Premium}} = 1 - LR = 1 -\\frac{\\text{Loss}}{\\text{Premium}} .}\\end{equation}\\] Misalkan kita memiliki pemuatan laba “target” baru, katakanlah \\(Q_{target}\\) . Dengan asumsi bahwa kerugian, eksposur, dan hal-hal lain mengenai kontrak tetap sama, maka untuk mencapai target pemuatan laba yang baru, kita akan menyesuaikan premi. Gunakan ICF untuk faktor perubahan yang ditunjukkan yang didefinisikan melalui ekspresi \\[\\begin{equation}\\small{\\frac{\\text{New UW Profit}}{\\text{Premium}} = Q_{target} = 1 -\\frac{\\text{Loss}}{ICF\\times \\text{Premium}}.}\\end{equation}\\] Menyelesaikan untuk \\(ICF\\), kita mendapatkan \\[\\small{ ICF = \\frac{\\text{Loss}}{\\text{Premium} \\times (1-Q_{target})} = \\frac{LR}{1-Q_{target}}. }\\] Jadi, sebagai contoh, jika kita memiliki rasio kerugian saat ini = 85% dan target keuntungan \\(Q_{target} = 0,20\\), maka \\(ICF = 0,85/0,80 = 1,0625\\), yang berarti kita meningkatkan premi sebesar 6,25%. Sekarang mari kita lihat bagaimana hal ini bekerja dengan biaya dalam persamaan (7.1). Kita dapat menggunakan pengembangan yang sama seperti pada Bagian 7.2.1 dan mulai dengan persamaan (7.2), selesaikan pembebanan laba untuk mendapatkan \\[\\begin{equation}\\small{Q = 1 - \\frac{\\text{Loss+Fixed}}{\\text{Premium}} - V .}\\end{equation}\\] Kita menginterpretasikan kuantitas \\(Rugi + Premi Tetap + V\\) sebagai “rasio biaya operasional”. Sekarang, tetapkan persentase keuntungan Q pada target dan sesuaikan premi melalui “faktor perubahan yang ditunjukkan” $ ICF \\[\\begin{equation}\\small{Q_{target} = 1-\\frac{\\text{Loss + Fixed}}{\\text{Premium}\\times ICF} - V .}\\end{equation}\\] Menyelesaikan untuk hasil $ ICF$ \\[\\begin{equation}{\\small\\begin{array}{ll}ICF &amp;= \\frac{\\text{Loss + Fixed}}{\\text{Premium} \\times (1 - V -Q_{target})} \\\\&amp;= \\frac{\\text{Loss Ratio + Fixed Expense Ratio}}{1 - V - Q_{target}} .\\end{array}}\\end{equation}\\] Contoh. Faktor Perubahan Indikasi Rasio Kerugian. Anggap informasi berikut: Proyeksi rasio kerugian dan LAE akhir = 65% Proyeksi rasio biaya tetap = 6,5% Biaya variabel = 25% Target keuntungan UW = 10% Dengan asumsi ini, dengan persamaan (7.3), faktor perubahan yang diindikasikan dapat dihitung sebagai berikut: \\[\\small{ICF = \\frac{\\text{(Losses + Fixed)}/\\text{Premium}}{ 1 - V - Q_{target}} = \\frac{0.65 + 0.065}{1-0.25- 0.10} = 1.10 .}\\] Ini berarti tingkat tarif rata-rata secara keseluruhan harus dinaikkan sebesar 10%. 7.3 Prinsip-prinsip Penetapan Harga Di bagian ini, Anda akan belajar cara: Menjelaskan prinsip-prinsip harga aktuarial yang umum Menjelaskan sifat-sifat dari prinsip-prinsip harga Memilih prinsip harga berdasarkan sifat yang diinginkan. Pendekatan dalam penetapan harga berbeda-beda tergantung pada jenis kontrak. Sebagai contoh, produk mobil pribadi adalah produk yang tersedia secara luas di seluruh dunia dan dikenal sebagai bagian dari pasar asuransi umum ritel di Inggris. Di sini, kita dapat mengharapkan penetapan harga berdasarkan sejumlah besar kontrak independen, situasi di mana harapan kerugian memberikan titik awal yang sangat baik. Sebaliknya, seorang aktuaris mungkin ingin menetapkan harga untuk kontrak asuransi yang dikeluarkan kepada majikan besar yang mencakup manfaat kesehatan yang kompleks bagi ribuan karyawan. Dalam contoh ini, pengetahuan tentang seluruh distribusi kerugian potensial, bukan hanya nilai yang diharapkan, sangat penting untuk memulai negosiasi penetapan harga. Untuk mencakup berbagai aplikasi potensial, bagian ini menjelaskan prinsip-prinsip premi umum dan sifat-sifatnya yang dapat digunakan untuk memutuskan apakah suatu prinsip tertentu dapat diterapkan dalam situasi yang diberikan atau tidak. 7.3.1 Prinsip-Prinsip Premi Bab ini memperkenalkan prinsip-prinsip penetapan harga aktuarial tradisional yang menyediakan harga berdasarkan distribusi kerugian asuransi saja; harga tidak tergantung pada permintaan asuransi atau aspek biaya lainnya seperti biaya-biaya. Diasumsikan bahwa kerugian \\(X\\) memiliki fungsi distribusi \\(F(⋅)\\) dan bahwa ada beberapa aturan (yang dalam matematika dikenal sebagai fungsi), katakanlah \\(H\\), yang mengambil \\(F(⋅)\\) ke dalam garis bilangan real positif, dilambangkan sebagai \\(P=H(F)\\). Untuk tujuan notasi, seringkali lebih nyaman untuk menggantikan variabel acak \\(X\\) dengan fungsinya dan menuliskan \\(P=H(X)\\). Tabel 7.1 menyediakan beberapa contoh. Tabel 7.1. Prinsip-Prinsip Premi yang Umum \\[\\small{ \\begin{array}{ll} \\text{Description } &amp; \\text{Definition } (H(X)) \\\\\\hline \\text{Net (pure) premium} &amp; {\\rm E}[X] \\\\ \\text{Expected value} &amp; (1+\\alpha){\\rm E}[X]\\\\ \\text{Standard deviation} &amp; {\\rm E}[X]+\\alpha ~SD(X)\\\\ \\text{Variance} &amp; {\\rm E}[X]+\\alpha ~{\\rm Var}(X)\\\\ \\text{Zero utility} &amp; \\text{solution of }u(w) = {\\rm E} [u(w + P - X)]\\\\ \\text{Exponential} &amp; \\frac{1}{\\alpha} \\log {\\rm E} [e^{\\alpha X}]\\\\ \\hline \\end{array} }\\] Sebuah prinsip premi mirip dengan ukuran risiko yang diperkenalkan pada Bagian 10.3. Secara matematis, keduanya adalah aturan yang memetakan variabel acak kerugian yang diminati menjadi nilai numerik. Dari sudut pandang praktis, prinsip premi memberikan panduan seberapa banyak asuransi akan membebankan biaya untuk menerima risiko \\(X\\). Sebaliknya, ukuran risiko mengkuantifikasi tingkat ketidakpastian, atau tingkat risiko, yang dapat digunakan oleh asuransi untuk memutuskan tingkat modal yang harus dipertahankan untuk tetap solvent. Premi net, atau murni, pada dasarnya mengasumsikan tidak adanya ketidakpastian. Prinsip nilai harapan, deviasi standar, dan varian masing-masing menambahkan beban eksplisit untuk ketidakpastian melalui parameter risiko \\(α≥0\\). Untuk prinsip nol utilitas, kita menganggap perusahaan asuransi dengan fungsi utilitas \\(u(⋅)\\) dan kekayaan w sama-sama tidak peduli untuk menerima dan tidak menerima risiko \\(X\\). Dalam hal ini, \\(P\\) dikenal sebagai harga kesetaraan atau, dalam ekonomi, harga reservasi. Dengan utilitas eksponensial, prinsip nol utilitas berkurang menjadi prinsip premi eksponensial, yaitu, mengasumsikan \\(u(x)=(1−e−αx)/α\\). Untuk nilai parameter risiko yang kecil, prinsip varian hampir sama dengan prinsip premi eksponensial, seperti yang diilustrasikan dalam kasus khusus berikut. 7.3.2 Sifat Prinsip Premium Sifat-sifat dari prinsip-prinsip premi membantu memandu pemilihan prinsip premi dalam aplikasi. Tabel 7.2 menyediakan contoh-contoh sifat-sifat dari prinsip-prinsip premi. Tabel 7.2. Properti Umum dari Prinsip-prinsip Premium. \\[\\small{ \\begin{array}{ll} \\text{Description } &amp; \\text{Definition }\\\\\\hline \\text{Nonnegative loading} &amp; H(X) \\ge {\\rm E}[X] \\\\ \\text{Additivity} &amp; H(X_1+X_2) = H(X_1) + H(X_2), \\text{ for independent }X_1, X_2 \\\\ \\text{Scale invariance} &amp; H(cX) = c H(X), \\text{ for }c \\ge 0 \\\\ \\text{Consistency} &amp; H(c+X) = c + H(X)\\\\ \\text{No rip-off } &amp; H(X) \\le \\max \\{X\\}\\\\ \\hline \\end{array} }\\] Ini hanya merupakan subset dari banyak properti yang dikutip dalam literatur aktuaria. Sebagai contoh, makalah tinjauan dari Young (2014) mencantumkan 15 properti. Lihat juga properti yang dijelaskan sebagai aksioma kohesif yang kami perkenalkan untuk pengukur risiko dalam Bagian 10.3. Beberapa properti yang tercantum di Tabel 7.2 ringan dalam arti bahwa properti tersebut hampir selalu terpenuhi. Misalnya, properti tidak ada penipuan menunjukkan bahwa biaya premi akan lebih kecil dari nilai kerugian maksimal \\(X\\) (di sini, kami menggunakan notasi \\(max{X}\\) untuk nilai maksimal ini yang didefinisikan sebagai “essential supremum” dalam matematika). Properti lain mungkin tidak begitu ringan. Sebagai contoh, untuk portofolio risiko independen, aktuaris mungkin ingin agar properti aditivitas terpenuhi. Mudah dilihat bahwa properti ini terpenuhi untuk prinsip premi nilai harapan, varians, dan eksponensial tetapi tidak untuk prinsip simpangan baku. Contoh lain adalah properti konsistensi yang tidak terpenuhi untuk prinsip nilai harapan ketika parameter pemuatan risiko \\(α\\) positif. Prinsip invariansi skala dikenal sebagai homogenitas derajat satu dalam ekonomi. Misalnya, ini memungkinkan kita untuk bekerja dengan mata uang yang berbeda (misalnya, dari dolar ke euro) serta sejumlah aplikasi lainnya dan akan dibahas lebih lanjut dalam Bagian 7.4 berikutnya. Meskipun prinsip yang secara umum diterima, kami mencatat bahwa prinsip ini tidak berlaku untuk nilai besar \\(X\\) yang mungkin berbatasan dengan kendala surplus dari perusahaan asuransi; jika perusahaan asuransi memiliki probabilitas besar menjadi tidak likuid, maka perusahaan asuransi tersebut mungkin tidak ingin menggunakan penetapan harga linear. Mudah diperiksa bahwa prinsip ini terpenuhi untuk prinsip nilai harapan dan simpangan baku, meskipun tidak untuk prinsip varians dan eksponensial. 7.4 Risiko Heterogen Di bagian ini, Anda akan belajar bagaimana: Mendeskripsikan paparan asuransi dalam hal distribusi skala Menjelaskan paparan dalam hal jenis-jenis asuransi umum seperti asuransi mobil dan asuransi pemilik rumah Mendeskripsikan bagaimana faktor rating dapat digunakan untuk memperhitungkan heterogenitas di antara risiko dalam suatu kumpulan Mengukur dampak faktor rating melalui relativitas Seperti yang dicatat di Bagian 7.1, terdapat banyak variasi dalam risiko yang diasuransikan, fitur kontrak, dan orang yang diasuransikan. Sebagai contoh, Anda mungkin memiliki saudara kembar yang bekerja di kota yang sama dan menghasilkan jumlah uang yang relatif sama. Namun, ketika memilih pilihan dalam asuransi sewa untuk mengasuransikan isi apartemen Anda, Anda dapat membayangkan perbedaan dalam jumlah isi yang akan diasuransikan, pilihan deductible untuk jumlah risiko yang ditanggung, dan mungkin juga perbedaan tingkat ketidakpastian mengingat tingkat keamanan lingkungan tempat tinggal Anda. Orang-orang dan risiko yang mereka asuransikan berbeda. Ketika memikirkan tentang kumpulan risiko yang berbeda (heterogen), satu opsi adalah untuk menetapkan harga sama untuk semua risiko. Hal ini umum terjadi dalam program asuransi banjir atau kesehatan yang disponsori oleh pemerintah. Namun, hal ini juga umum terjadi untuk menetapkan harga yang berbeda di mana perbedaan tersebut sebanding dengan risiko yang diasuransikan. 7.4.1 Paparan Risiko Salah satu cara untuk membuat risiko heterogen dapat dibandingkan adalah melalui konsep exposure. Untuk menjelaskan exposure, mari gunakan distribusi skala yang telah dipelajari pada Bab 3. Untuk mengingat kembali distribusi skala, misalkan \\(X\\) memiliki distribusi parametrik dan kita mendefinisikan versi yang diskalakan sebagai \\(R=X/E, E&gt;0\\) . Jika \\(R\\) ada dalam keluarga parametrik yang sama dengan \\(X\\) , maka distribusinya dikatakan sebagai distribusi skala. Seperti yang telah kita lihat, distribusi gamma, eksponensial, dan Pareto adalah contoh dari distribusi skala. Secara intuitif, ide di balik exposure adalah untuk membuat risiko lebih dapat dibandingkan satu sama lain. Misalnya, mungkin terdapat risiko \\(X_1,...,X_n\\) berasal dari distribusi yang berbeda dan namun, dengan memilih exposure yang tepat, tarif \\(R_1,...,R_n\\) berasal dari distribusi yang sama. Di sini, kita menafsirkan tarif \\(R_i=X_i/E_i\\) sebagai kerugian dibagi dengan exposure. Tabel 7.3 menyediakan beberapa contoh. Kami mencatat bahwa tabel ini mengacu pada tahun mobil dan rumah yang “diperoleh”, konsep yang akan dijelaskan di Bagian 7.5. Tabel 7.3. Paparan yang Biasa Digunakan dalam Jenis Asuransi yang Berbeda. \\[\\small{ \\begin{matrix} \\begin{array}{ll} \\text{Type of Insurance} &amp; \\text{Exposure Basis} \\\\\\hline \\text{Personal Automobile} &amp; \\text{Earned Car Year, Amount of Insurance Coverage} \\\\ \\text{Homeowners} &amp; \\text{Earned House Year, Amount of Insurance Coverage}\\\\ \\text{Workers Compensation} &amp; \\text{Payroll}\\\\ \\text{Commercial General Liability} &amp; \\text{Sales Revenue, Payroll, Square Footage, Number of Units}\\\\ \\text{Commercial Business Property} &amp; \\text{Amount of Insurance Coverage}\\\\ \\text{Physician&#39;s Professional Liability} &amp; \\text{Number of Physician Years}\\\\ \\text{Professional Liability} &amp; \\text{Number of Professionals (e.g., Lawyers or Accountants)}\\\\ \\text{Personal Articles Floater} &amp; \\text{Value of Item} \\\\ \\hline \\end{array} \\end{matrix} }\\] Sebuah paparan adalah jenis faktor penilaian, konsep yang kami definisikan secara eksplisit di Bagian selanjutnya, yaitu 7.4.2. Biasanya, ini adalah faktor penilaian yang paling penting, sangat penting sehingga premi dan kerugian dikutip secara “per paparan”. Untuk pemodelan frekuensi dan keparahan, biasanya dipikirkan bahwa aspek frekuensi proporsional terhadap paparan dan aspek keparahan dalam hal kerugian per klaim (tidak bergantung pada paparan). Namun, hal ini tidak mencakup seluruh cerita. Untuk banyak jenis bisnis, paparan yang proporsional terhadap inflasi sangat nyaman. Inflasi biasanya dilihat sebagai tidak terkait dengan frekuensi tetapi proporsional terhadap keparahan. Kriteria untuk Memilih Paparan Sebuah dasar paparan harus memenuhi kriteria berikut. Ini harus: menjadi ukuran yang akurat dari paparan kuantitatif terhadap kerugian, mudah bagi perusahaan asuransi untuk menentukan (pada saat kebijakan dimulai) dan tidak dapat dimanipulasi oleh tertanggung, mudah dipahami oleh tertanggung dan dapat dihitung oleh perusahaan asuransi, memperhatikan setiap dasar paparan yang ada sebelumnya yang telah ditetapkan dalam industri, dan untuk beberapa jenis bisnis, proporsional terhadap inflasi. Dengan cara ini, tarif tidak sensitif terhadap perubahan nilai uang dari waktu ke waktu karena perubahan ini tercakup dalam dasar paparan. Untuk menjelaskan, pertimbangkan cakupan mobil pribadi. Sebagai gantinya dari dasar paparan “tahun mobil yang diperoleh”, sebuah ukuran yang lebih akurat dari paparan kuantitatif terhadap kerugian mungkin adalah jumlah mil yang dikemudikan. Secara historis, pengukuran ini sulit ditentukan pada saat kebijakan diterbitkan dan rentan terhadap manipulasi oleh tertanggung sehingga masih tidak biasa digunakan. Perangkat modern telematika yang memungkinkan pencatatan mil yang akurat sedang mengubah penggunaan variabel ini di beberapa pasar. Sebagai contoh lain, ukuran paparan dalam properti bisnis komersial, misalnya asuransi kebakaran, umumnya adalah jumlah cakupan asuransi. Ketika nilai properti tumbuh dengan inflasi, jumlah cakupan asuransi juga akan bertambah. Dengan demikian, tarif yang dikutip per jumlah cakupan asuransi kurang sensitif terhadap inflasi daripada sebaliknya. 7.4.2 Faktor Penilaian Faktor penilaian, atau variabel penilaian, adalah karakteristik dari pemegang polis atau risiko yang diasuransikan yang mempengaruhi tarif. Sebagai contoh, ketika Anda membeli asuransi mobil, kemungkinan perusahaan asuransi memiliki tarif yang berbeda berdasarkan usia, jenis kelamin, jenis mobil, tempat parkir mobil, riwayat kecelakaan, dan sebagainya. Variabel-variabel ini dikenal sebagai faktor penilaian. Meskipun beberapa variabel dapat bersifat kontinu, seperti usia, sebagian besar bersifat kategorikal - faktor adalah label yang digunakan untuk variabel kategorikal. Bahkan, bahkan dengan variabel kontinu seperti usia, umumnya dilakukan kategorisasi dengan membuat kelompok seperti “muda”, “menengah”, dan “tua” untuk tujuan penilaian. Tabel 7.4 hanya memberikan sedikit contoh. Di banyak yurisdiksi, pasar asuransi pribadi (seperti asuransi mobil dan rumah) sangat kompetitif - menggunakan 10 atau 20 variabel untuk tujuan penilaian tidak jarang terjadi. Tabel 7.4. Faktor Penilaian yang Umum Digunakan dalam Jenis Asuransi yang Berbeda. \\[\\small{ \\begin{matrix} \\begin{array}{l|l}\\hline \\text{Type of Insurance} &amp; \\text{Rating Factors}\\\\\\hline\\hline \\text{Personal Automobile} &amp; \\text{Driver Age and Gender, Model Year, Accident History}\\\\ \\text{Homeowners} &amp; \\text{Amount of Insurance, Age of Home, Construction Type}\\\\ \\text{Workers Compensation} &amp; \\text{Occupation Class Code}\\\\ \\text{Commercial General Liability} &amp; \\text{Classification, Territory, Limit of Liability}\\\\ \\text{Medical Malpractice} &amp; \\text{Specialty, Territory, Limit of Liability}\\\\ \\text{Commercial Automobile} &amp; \\text{Driver Class, Territory, Limit of Liability}\\\\ \\hline \\end{array} \\end{matrix} }\\] Contoh. Kerugian dan Premi Berdasarkan Jumlah Asuransi dan Wilayah. Untuk mengilustrasikan, Tabel 7.5 menyajikan satu set data fiktif kecil dari Werner dan Modlin (2016). Data terdiri dari kerugian dan biaya penyesuaian kerugian (LossLAE), yang didekomposisi berdasarkan tiga tingkat jumlah asuransi (AOI), dan tiga wilayah (Terr). Untuk setiap kombinasi AOI dan Terr, kami juga memiliki jumlah kebijakan yang dikeluarkan, yang diberikan sebagai Exposure. Tabel 7.5. Kerugian dan Premi Berdasarkan Jumlah Asuransi dan Wilayah. \\[\\small{ \\begin{matrix} \\begin{array}{cc|rrr} \\hline AOI &amp; Terr &amp; Exposure &amp; LossLAE &amp; Premium \\\\\\hline \\text{Low} &amp; 1 &amp; 7 &amp; 210.93 &amp; 335.99 \\\\ \\text{Medium} &amp; 1 &amp; 108 &amp; 4,458.05 &amp; 6,479.87 \\\\ \\text{High} &amp; 1 &amp; 179 &amp; 10,565.98 &amp; 14,498.71 \\\\\\hline \\text{Low} &amp; 2 &amp; 130 &amp; 6,206.12 &amp; 10,399.79 \\\\ \\text{Medium} &amp; 2 &amp; 126 &amp; 8,239.95 &amp; 12,599.75 \\\\ \\text{High} &amp; 2 &amp; 129 &amp; 12,063.68 &amp; 17,414.65 \\\\\\hline \\text{Low} &amp; 3 &amp; 143 &amp; 8,441.25 &amp; 14,871.70 \\\\ \\text{Medium} &amp; 3 &amp; 126 &amp; 10,188.70 &amp; 16,379.68 \\\\ \\text{High} &amp; 3 &amp; 40 &amp; 4,625.34 &amp; 7,019.86 \\\\ \\hline \\text{Total} &amp; &amp; 988 &amp; 65,000.00 &amp; 99,664.01 \\\\\\hline \\hline \\end{array} \\end{matrix} }\\] Dalam kasus ini, faktor penilaian AOI dan Terr menghasilkan sembilan sel. Perhatikan bahwa kita dapat menggabungkan sel “wilayah satu dengan jumlah asuransi rendah” dengan sel lain karena hanya ada 7 kebijakan di sel tersebut. Melakukan hal ini adalah wajar - pertimbangan semacam ini merupakan salah satu tugas utama analis. Garis besar dalam pemilihan variabel dijelaskan pada Bab 8, termasuk Pelengkap Teknis TS 8.B. Sebagai alternatif, kita juga dapat memperkuat informasi tentang sel (Terr 1, AOI Rendah) dengan “meminjam” informasi dari sel tetangga (misalnya, wilayah lain dengan AOI yang sama, atau jumlah AOI yang berbeda di dalam Terr 1). Ini adalah subjek kredibilitas yang diperkenalkan dalam Bab 9. Untuk memahami dampak faktor penilaian, umumnya digunakan relatif. Relatif membandingkan risiko yang diharapkan pada tingkat faktor penilaian tertentu dengan nilai dasar yang diterima. Dalam buku ini, kami bekerja dengan relatif yang didefinisikan melalui rasio; juga mungkin untuk mendefinisikan relatif melalui selisih aritmatika. Oleh karena itu, relatif kami didefinisikan sebagai: \\[\\text{Relativity}_j = \\frac{\\text{(Loss/Exposure)}_j}{\\text{(Loss/Exposure)}_{Base}} .\\] Contoh. Kerugian dan Premi Berdasarkan Jumlah Asuransi dan Wilayah - Lanjutan. Metode klasifikasi tradisional hanya mempertimbangkan satu variabel klasifikasi pada satu waktu - mereka univariat. Oleh karena itu, jika kami ingin mendapatkan relatif untuk kerugian dan biaya penyesuaian kerugian (LossLAE) berdasarkan jumlah asuransi, kami mungkin akan menjumlahkan wilayah untuk mendapatkan informasi yang ditampilkan di Tabel 7.6. Tabel 7.6. Kerugian dan Relatifitas berdasarkan Jumlah Asuransi. \\[\\small{ \\begin{matrix} \\begin{array}{c|rrrr} \\hline AOI &amp; Exposure &amp; LossLAE &amp; Loss/Exp &amp;Relativity \\\\\\hline \\text{Low} &amp; 280 &amp; 14858.3 &amp; 53.065 &amp;0.835 \\\\ \\text{Medium} &amp; 360 &amp; 22886.7 &amp; 63.574 &amp;1.000 \\\\ \\text{High} &amp; 348 &amp; 27255.0 &amp; 78.319 &amp; 1.232 \\\\\\hline \\text{Total} &amp; 988 &amp; 65,000.0 &amp; \\\\\\hline \\hline \\end{array} \\end{matrix} }\\] Oleh karena itu, kerugian dan biaya penyesuaian kerugian per unit paparan adalah 23,2% lebih tinggi untuk risiko dengan jumlah asuransi tinggi dibandingkan dengan yang memiliki jumlah asuransi sedang. Relatifitas ini tidak mengontrol wilayah. Pengenalan faktor rating memungkinkan analis untuk membuat sel yang mendefinisikan koleksi risiko kecil - tujuannya adalah memilih kombinasi faktor rating yang tepat sehingga semua risiko dalam sel dapat diperlakukan secara sama. Dalam terminologi statistik, kita ingin semua risiko dalam sel memiliki distribusi yang sama (tergantung pada penskalaan oleh variabel paparan). Ini adalah dasar dari penetapan harga asuransi. Semua risiko dalam sel memiliki harga yang sama per paparan, namun risiko dari sel yang berbeda dapat memiliki harga yang berbeda. Dengan kata lain, perusahaan asuransi diizinkan untuk menetapkan tarif yang berbeda untuk risiko yang berbeda; diskriminasi risiko legal dan dilakukan secara rutin. Namun demikian, dasar diskriminasi, pemilihan faktor risiko, adalah subjek dari debat yang luas. Komunitas aktuaria, manajemen asuransi, regulator, dan advokat konsumen semua merupakan peserta aktif dalam debat ini. Lampiran Teknis TS 7.A menjelaskan masalah ini dari sudut pandang regulasi. Selain kriteria statistik untuk menilai signifikansi faktor rating, analis harus memperhatikan masalah bisnis perusahaan (misalnya, apakah mahal untuk menerapkan faktor rating?), kriteria sosial (apakah variabel berada di bawah kendali pemegang polis?), kriteria hukum (apakah ada regulasi yang melarang penggunaan faktor rating seperti gender?), dan masalah sosial lainnya. Pertanyaan-pertanyaan ini sebagian besar di luar cakupan teks ini. Namun demikian, karena mereka sangat mendasar untuk penetapan harga asuransi, gambaran singkat diberikan di Bab 8, termasuk Lampiran Teknis TS 8.B. 7.5 Development and Trending Seperti yang telah kita lihat di Bagian 7.2, perusahaan asuransi mempertimbangkan informasi agregat untuk penetapan tarif seperti paparan risiko, premi, biaya, klaim, dan pembayaran. Informasi agregat ini juga berguna untuk mengelola aktivitas perusahaan asuransi; laporan keuangan umumnya dibuat setidaknya setahun sekali dan seringkali secara triwulanan. Pada setiap tanggal pelaporan keuangan yang diberikan, informasi tentang kebijakan dan klaim terbaru akan terus berlanjut dan tidak lengkap; bagian ini memperkenalkan konsep-konsep untuk memproyeksikan informasi risiko agar berguna untuk tujuan penetapan tarif. Informasi tentang risiko, seperti paparan, premi, jumlah klaim, kerugian, dan faktor rating, biasanya diorganisir ke dalam tiga basis data: - Basis data kebijakan - berisi informasi tentang risiko yang diasuransikan, pemegang polis, dan ketentuan kontrak - Basis data klaim - berisi informasi tentang setiap klaim; ini terhubung dengan basis data kebijakan. - Basis data pembayaran - berisi informasi tentang setiap transaksi klaim, biasanya pembayaran tetapi juga perubahan pada cadangan kasus. Ini terhubung dengan basis data klaim. Dengan basis data terperinci ini, dalam prinsipnya mudah untuk menjumlahkan detail tingkat kebijakan menjadi informasi agregat yang diperlukan untuk laporan keuangan. Bagian ini menggambarkan berbagai ukuran ringkasan yang umum digunakan. 7.5.1 Paparan dan Premi Periode pelaporan keuangan adalah jangka waktu yang tetap dalam kalender; kami menggunakan 1 Januari hingga 31 Desember untuk contoh dalam buku ini meskipun periode pelaporan lain juga umum. Periode pelaporan adalah tetap tetapi kebijakan dapat dimulai kapan saja selama setahun. Meskipun semua kebijakan memiliki durasi kontrak yang sama selama satu tahun (misalnya), karena waktu mulai yang berbeda, kebijakan dapat berakhir kapan saja selama pelaporan keuangan. Gambar 7.1 menunjukkan empat kebijakan yang menggambarkan. Perlu adanya standar mengenai jenis-jenis ukuran yang paling berguna untuk merangkum pengalaman dalam periode pelaporan tertentu karena perbedaan waktu mulai dan berakhir ini. (gambar 7.5-1) Beberapa ukuran paparan yang umum digunakan adalah: - Paparan tertulis, jumlah paparan pada kebijakan yang diterbitkan (ditulis) selama periode yang dimaksud, - Paparan diperoleh, unit paparan yang benar-benar terpapar risiko selama periode tersebut, yaitu, di mana perlindungan telah diberikan - Paparan belum diperoleh, mewakili bagian dari paparan tertulis di mana perlindungan belum diberikan sampai saat itu, dan - Paparan dalam kekuatan, unit paparan yang terpapar risiko pada titik waktu tertentu. Tabel di bawah memberikan perhitungan terperinci untuk empat kebijakan ilustratif: "],["risk-classification.html", "Bab 8 Risk Classification 8.1 Introduction 8.2 Poisson Regression Model 8.3 Categorical Variables and Multiplicative Tariff 8.4 Further Resources and Contributors", " Bab 8 Risk Classification 8.1 Introduction Dalam materi ini akan mempelajari beberapa materi seperti : - Alasan premi harus bervariasi di antara pemegang polis dengan karakteristik risiko yang berbeda. - Arti dari spiral seleksi yang merugikan. - Pentingnya klasifikasi risiko. Melalui kontrak asuransi, pemegang polis secara efektif mengalihkan risiko mereka kepada perusahaan asuransi dengan imbalan premi. Agar perusahaan asuransi dapat bertahan dalam bisnisnya, pendapatan premi yang dikumpulkan dari kumpulan pemegang polis setidaknya harus sama dengan pengeluaran manfaat. Pada produk asuransi umum di mana premi dibebankan untuk satu periode, katakanlah setiap tahun, premi asuransi bruto berdasarkan prinsip ekuivalensi dinyatakan sebagai \\[ \\begin{eqnarray*} \\textrm{Premi Bruto = Kerugian yang Diharapkan + Biaya yang Diharapkan + Keuntungan}\\\\ \\end{eqnarray*} \\] Dengan mengabaikan biaya gesekan yang terkait dengan biaya administrasi dan keuntungan, premi bersih atau murni yang dibebankan oleh perusahaan asuransi harus sama dengan kerugian yang diharapkan terjadi dari risiko yang dialihkan dari pemegang polis. Jika semua pemegang polis dalam kumpulan asuransi memiliki profil risiko yang sama, maka penanggung cukup membebankan premi yang sama untuk semua pemegang polis karena mereka memiliki ekspektasi kerugian yang sama. Namun pada kenyataannya, pemegang polis hampir tidak homogen. Sebagai contoh, risiko kematian dalam asuransi jiwa tergantung pada karakteristik pemegang polis, seperti usia, jenis kelamin, dan gaya hidup. Dalam asuransi mobil, karakteristik tersebut dapat mencakup usia, pekerjaan, jenis atau penggunaan mobil, dan daerah tempat tinggal pengemudi. Pengetahuan tentang karakteristik atau variabel ini dapat meningkatkan kemampuan menghitung premi yang adil untuk pemegang polis individu, karena dapat digunakan untuk memperkirakan atau memprediksi kerugian yang diharapkan dengan lebih akurat. Seleksi yang Tidak Menguntungkan (Adverse Selection). Memang, jika perusahaan asuransi tidak membedakan karakteristik risiko pemegang polis individu dan hanya membebankan premi yang sama kepada semua tertanggung berdasarkan kerugian rata-rata dalam portofolio, perusahaan asuransi akan menghadapi seleksi yang merugikan, situasi di mana individu dengan peluang kerugian yang lebih tinggi akan tertarik masuk ke dalam portofolio dan individu yang berisiko rendah akan ditolak. Sebagai contoh, pertimbangkan industri asuransi kesehatan di mana status merokok merupakan faktor risiko yang penting untuk mortalitas dan morbiditas. Sebagian besar perusahaan asuransi kesehatan di pasar membutuhkan premi yang berbeda tergantung pada status merokok, sehingga perokok membayar premi yang lebih tinggi daripada yang bukan perokok, dengan karakteristik lain yang identik. Sekarang anggaplah ada sebuah perusahaan asuransi, kita sebut saja EquitabAll, yang menawarkan premi yang sama untuk semua tertanggung tanpa memandang status merokok, tidak seperti kompetitor lainnya. Premi bersih EquitabAll secara alami merupakan perhitungan kerugian mortalitas rata-rata untuk perokok dan bukan perokok. Artinya, premi bersih adalah rata-rata tertimbang dari kerugian dengan bobot masing-masing proporsi perokok dan bukan perokok. Dengan demikian, mudah untuk melihat bahwa seorang perokok akan memiliki insentif yang baik untuk membeli asuransi dari EquitabAll daripada dari perusahaan asuransi lain karena premi yang ditawarkan oleh EquitabAll relatif lebih rendah. Pada saat yang sama, orang yang bukan perokok akan lebih memilih untuk membeli asuransi dari tempat lain yang menawarkan premi yang lebih rendah, yang dihitung hanya dari kelompok bukan perokok. Akibatnya, akan ada lebih banyak perokok dan lebih sedikit non-perokok dalam portofolio EquitabAll, yang menyebabkan kerugian yang lebih besar dari yang diperkirakan dan karenanya premi yang lebih tinggi untuk tertanggung di periode berikutnya untuk menutupi biaya yang lebih tinggi. Dengan meningkatnya premi baru di periode berikutnya, para non-perokok di EquitabAll akan memiliki insentif yang lebih besar untuk berpindah asuransi. Ketika siklus ini terus berlanjut dari waktu ke waktu, EquitabAll secara bertahap akan mempertahankan lebih banyak perokok dan lebih sedikit non-perokok dalam portofolionya dengan premi yang terus dinaikkan, yang pada akhirnya menyebabkan runtuhnya bisnis. Dalam literatur, fenomena ini dikenal sebagai adverse selection spiral atau spiral kematian. Oleh karena itu, memasukkan dan membedakan karakteristik risiko individu yang penting dalam proses penetapan harga asuransi adalah komponen yang relevan untuk penentuan premi yang adil bagi pemegang polis individu dan keberlanjutan jangka panjang perusahaan asuransi. Faktor Pemeringkatan. Untuk memasukkan karakteristik risiko yang relevan dari pemegang polis dalam proses penentuan harga, perusahaan asuransi mempertahankan beberapa sistem klasifikasi yang menempatkan setiap pemegang polis pada salah satu kelas risiko berdasarkan sejumlah kecil karakteristik risiko yang dianggap paling relevan. Karakteristik yang digunakan dalam sistem klasifikasi ini disebut faktor peringkat, yang merupakan variabel apriori dalam arti diketahui sebelum kontrak dimulai (misalnya, jenis kelamin, status kesehatan, jenis kendaraan, dan lain-lain, yang diketahui selama underwriting). Semua pemegang polis yang memiliki faktor risiko yang sama akan dimasukkan ke dalam kelas risiko yang sama, dan dianggap homogen dari sudut pandang penetapan harga; akibatnya, perusahaan asuransi membebankan premi atau tarif yang sama kepada mereka. Mengenai faktor risiko dan premi, Standar Praktik Aktuaria (ASOP) No. 12 dari Dewan Standar Aktuaria (2018) menyatakan bahwa aktuaris harus memilih karakteristik risiko yang terkait dengan hasil yang diharapkan, dan bahwa tarif dalam sistem klasifikasi risiko akan dianggap adil jika perbedaan tarif mencerminkan perbedaan material dalam biaya yang diharapkan untuk karakteristik risiko. Dalam proses pemilihan faktor risiko, ASOP juga mensyaratkan aktuaris untuk mempertimbangkan hal-hal berikut: hubungan karakteristik risiko dan hasil yang diharapkan, kausalitas, objektivitas, kepraktisan, hukum yang berlaku, praktik industri, dan praktik bisnis. Suplemen Teknis TS 8.B memberikan diskusi tambahan mengenai pemilihan faktor penilaian. Di sisi kuantitatif, tugas penting bagi aktuaris dalam membangun kerangka kerja klasifikasi risiko adalah membangun model statistik yang dapat menentukan kerugian yang diharapkan dengan berbagai faktor peringkat pemegang polis. Pendekatan standarnya adalah dengan mengadopsi model regresi yang menghasilkan kerugian yang diharapkan sebagai output ketika faktor-faktor risiko yang relevan diberikan sebagai input. Pada bab ini kita akan mempelajari tentang regresi Poisson, yang dapat digunakan ketika kerugian merupakan variabel count, sebagai contoh utama dari alat penetapan harga asuransi. 8.2 Poisson Regression Model Model regresi Poisson telah berhasil digunakan dalam berbagai aplikasi dan memiliki keunggulan dalam memungkinkan ekspresi bentuk tertutup untuk kuantitas yang penting. Pada bagian ini kami memperkenalkan regresi Poisson sebagai perluasan alami dari distribusi Poisson. Pada Materi ini akan mempelajari : Memahami regresi Poisson sebagai alat yang nyaman untuk menggabungkan distribusi Poisson individu. Memperdalam pemahaman tentang konsep paparan dan pentingnya. Secara formal mempelajari cara merumuskan model regresi Poisson menggunakan variabel indikator ketika variabel penjelasnya bersifat kategorikal. 8.2.1 Need for Poisson Regression Untuk memperkenalkan regresi Poisson, Dapat Mempertimbangkan portofolio asuransi kesehatan hipotetis di mana semua pemegang polis memiliki usia yang sama dan hanya satu faktor risiko, status merokok, yang relevan. Status merokok dengan demikian merupakan variabel kategorikal dengan dua tingkatan : perokok dan bukan perokok. Karena ada dua level untuk status merokok, kita dapat menunjukkan perokok dan bukan perokok dengan masing-masing level 1 dan 2. Di sini penomorannya sewenang-wenang; status merokok adalah variabel kategori nominal. Misalkan sekarang kita tertarik untuk menetapkan harga asuransi kesehatan di mana premi untuk setiap pemegang polis ditentukan oleh jumlah kunjungan pasien rawat jalan ke kantor dokter selama setahun. Biaya medis untuk setiap kunjungan diasumsikan sama terlepas dari status merokok untuk kesederhanaan. Oleh karena itu, jika kami percaya bahwa status merokok merupakan faktor risiko yang valid dalam asuransi kesehatan ini, wajar untuk mempertimbangkan pengamatan dari perokok secara terpisah dari non-perokok. Pada Tabel 8.1 disajikan data untuk portofolio ini. \\[{\\small \\begin{matrix} \\begin{array}{cc|cc|cc} \\hline \\text{Smoker} &amp; \\text{(level 1)} &amp; \\text{Non-smoker}&amp;\\text{(level 2)} &amp; &amp; \\text{Both}\\\\ \\text{Count} &amp; \\text{Observed} &amp; \\text{Count} &amp; \\text{Observed} &amp; \\text{Count} &amp; \\text{Observed} \\\\ \\hline 0 &amp; 2213 &amp; 0 &amp; 6671 &amp; 0 &amp; 8884 \\\\ 1 &amp; 178 &amp; 1 &amp; 430 &amp; 1 &amp; 608 \\\\ 2 &amp; 11 &amp; 2 &amp; 25 &amp; 2 &amp; 36 \\\\ 3 &amp; 6 &amp; 3 &amp; 9 &amp; 3 &amp; 15 \\\\ 4 &amp; 0 &amp; 4 &amp; 4 &amp; 4 &amp; 4 \\\\ 5 &amp; 1 &amp; 5 &amp; 2 &amp; 5 &amp; 3 \\\\ \\hline \\text{Total} &amp; 2409 &amp; \\text{Total} &amp; 7141 &amp; \\text{Total} &amp; 9550 \\\\ \\text{Mean} &amp; 0.0926 &amp; \\text{Mean} &amp; 0.0746 &amp; \\text{Mean} &amp; 0.0792 \\\\ \\hline \\end{array} \\end{matrix} }\\] Karena dataset ini berisi hitungan acak, kami mencoba untuk menyesuaikan distribusi Poisson untuk setiap tingkat. Fungsi massa peluang dari distribusi Poisson dengan rata-rata \\(μ\\) diberikan oleh: \\[ \\begin{equation} \\Pr(Y=y)=\\frac{\\mu^y e^{-\\mu}}{y!},\\qquad y=0,1,2, \\ldots \\tag{8.1} \\end{equation} \\] dan \\[E (Y) = Var (Y) = μ\\] Dalam konteks regresi, umumnya menggunakan \\(μ\\) untuk parameter rata-rata daripada parameter Poisson \\(λ\\), meskipun keduanya sesuai. mle (estimator maksimum likelihood) dari distribusi Poisson diberikan oleh rata-rata sampel. Jadi jika kita menyebut parameter rata-rata Poisson untuk setiap tingkat sebagai \\(μ_{(1)}\\) (perokok) dan \\(μ_{(2)}\\) (bukan perokok), kita dapat melihat dari Tabel 8.1 bahwa \\(\\hat{\\mu}_{(1)}=0.0926\\) dan \\(\\hat{\\mu}_{(2)}=0.0746\\). Contoh sederhana ini menunjukkan gagasan dasar klasifikasi risiko. Tergantung pada status merokok, pemegang polis akan memiliki karakteristik risiko yang berbeda yang dapat dimasukkan melalui variasi parameter rata-rata Poisson untuk menghitung premi yang adil. Dalam contoh ini, rasio frekuensi kerugian yang diharapkan adalah \\(\\hat{\\mu}_{(1)}/\\hat{\\mu}_{(2)}=1.2402\\), yang berarti bahwa perokok cenderung mengunjungi kantor dokter 24.02% lebih sering dibandingkan dengan bukan perokok. Juga penting untuk dicatat bahwa jika penanggung menetapkan premi yang sama untuk semua pemegang polis tanpa memperhatikan status merokok, berdasarkan karakteristik rata-rata dari portofolio, seperti yang terjadi pada EquitabAll yang dijelaskan dalam Pengantar, frekuensi yang diharapkan (atau premi) \\(\\hat{\\mu}\\) adalah 0.0792, diperoleh dari kolom terakhir Tabel 8.1. Dapat diverifikasi bahwa \\[ \\begin{equation} \\hat{\\mu} = \\left(\\frac{n_1}{n_1+n_2}\\right)\\hat{\\mu}_{(1)}+\\left(\\frac{n_2}{n_1+n_2}\\right)\\hat{\\mu}_{(2)}=0.0792, \\tag{8.2} \\end{equation} \\] di mana \\(n_i\\) adalah jumlah pengamatan dalam setiap tingkat. Jelas, premi ini adalah rata-rata tertimbang dari premi untuk setiap tingkat dengan bobot yang sama dengan proporsi tertanggung di tingkat tersebut. Regresi Poisson sederhana Dalam contoh di atas, kita telah menyesuaikan distribusi Poisson untuk setiap tingkat secara terpisah, tetapi sebenarnya kita dapat menggabungkannya secara bersama-sama dalam satu model Poisson yang mencakup status merokok dan bukan merokok. Hal ini dapat dilakukan dengan menghubungkan parameter rata-rata Poisson dengan faktor risiko. Dengan kata lain, kita membuat rata-rata Poisson, yang merupakan frekuensi kerugian yang diharapkan, merespons perubahan dalam status merokok. Pendekatan konvensional untuk menangani variabel kategorikal adalah dengan mengadopsi variabel indikator atau dummy yang mengambil nilai 1 atau 0, sehingga kita mengaktifkan satu tingkat dan mematikan yang lainnya. Oleh karena itu, kita dapat mengusulkan penggunaan \\[ \\begin{equation} \\mu=\\beta_0+\\beta_1 x_1 \\tag{8.3} \\end{equation} \\] atau, lebih umum, dalam bentuk log linear \\[ \\begin{equation} \\log \\mu=\\beta_0+\\beta_1 x_1, \\tag{8.4} \\end{equation} \\] di mana \\(x_1\\) adalah variabel indikator dengan \\[ \\begin{equation} x_1= \\begin{cases} 1 &amp; \\text{if smoker}, \\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\tag{8.5} \\end{equation} \\] Umumnya beberapa orang lebih suka hubungan log linear daripada yang linear untuk mencegah menghasilkan nilai \\(μ\\) yang negatif, yang dapat terjadi ketika terdapat banyak faktor risiko dan tingkatan yang berbeda. Penyusunan dalam (8.4) dan (8.5) menghasilkan parameter frekuensi Poisson yang berbeda tergantung pada tingkat dalam faktor risiko: \\[ \\begin{equation} \\log \\mu= \\begin{cases} \\beta_0+\\beta_1 \\\\ \\beta_0 \\end{cases} \\quad \\text{or equivalently,}\\qquad \\mu= \\begin{cases} e^{\\beta_0+\\beta_1} &amp; \\text{if smoker (level 1)}, \\\\ e^{\\beta_0} &amp; \\text{if non-smoker (level 2)} . \\end{cases} \\tag{8.6} \\end{equation} \\] Ini adalah bentuk paling sederhana dari regresi Poisson. Perlu dicatat bahwa kita membutuhkan satu variabel indikator tunggal untuk memodelkan dua tingkat dalam kasus ini. Sebagai alternatif, juga mungkin menggunakan dua variabel indikator melalui skema pengkodean yang berbeda. Skema ini melibatkan penghilangan istilah intersepsi sehingga (8.4) dimodifikasi menjadi \\[ \\begin{equation} \\log \\mu=\\beta_1 x_1+\\beta_2 x_2, \\tag{8.7} \\end{equation} \\] di mana \\(x_2\\) adalah variabel indikator kedua dengan \\[ x_2= \\begin{cases} 1 &amp; \\text{if non-smoker}, \\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\] Lalu akan didapat \\[ \\begin{equation} \\log \\mu= \\begin{cases} \\beta_1 \\\\ \\beta_2 \\end{cases} \\quad \\text{or}\\qquad \\mu= \\begin{cases} e^{\\beta_1} &amp; \\text{if smoker (level 1)}, \\\\ e^{\\beta_2} &amp; \\text{if non-smoker (level 2)}. \\end{cases} \\tag{8.8} \\end{equation} \\] Hasil numerik dari (8.6) sama dengan (8.8) karena semua koefisien diberikan sebagai angka dalam estimasi aktual, dengan pengaturan yang pertama lebih umum dalam sebagian besar teks; kita juga menggunakan pengaturan yang pertama. Dengan model regresi Poisson ini, kita dapat dengan mudah memahami bagaimana koefisien β0 dan β1 terkait dengan frekuensi kerugian yang diharapkan pada setiap tingkat. Menurut (8.6), rata-rata Poisson untuk perokok, μ(1), diberikan oleh \\[ \\mu_{(1)}=e^{\\beta_0+\\beta_1}=\\mu_{(2)} \\,e^{\\beta_1} \\quad \\text{or}\\quad \\mu_{(1)}/\\mu_{(2)} =e^{\\beta_1} \\] di mana \\(μ_{(2)}\\) adalah rata-rata Poisson untuk bukan perokok. Hubungan ini antara perokok dan bukan perokok menunjukkan cara yang berguna untuk membandingkan risiko yang terkait dengan tingkatan yang berbeda dari faktor risiko tertentu. Artinya, peningkatan proporsional dalam frekuensi kerugian yang diharapkan para perokok dibandingkan dengan bukan perokok diberikan oleh faktor perkalian \\(e^{β_1}\\). Dengan kata lain, jika kita mengatur frekuensi kerugian yang diharapkan para bukan perokok sebagai nilai dasar, frekuensi kerugian yang diharapkan para perokok diperoleh dengan mengalikan nilai dasar dengan \\(e^{β_1}\\). Menangani kasus dengan banyak tingkatan Kita dapat dengan mudah memperluas kasus dengan dua tingkatan menjadi kasus dengan banyak tingkatan di mana l tingkatan berbeda terlibat untuk satu faktor rating tunggal. Untuk ini, umumnya kita membutuhkan l-1 variabel indikator untuk merumuskan \\[ \\begin{equation} \\log \\mu=\\beta_0+\\beta_1 x_1+\\cdots+\\beta_{l-1} x_{l-1}, \\tag{8.9} \\end{equation} \\] di mana \\(xk_\\) adalah variabel indikator yang bernilai 1 jika kebijakan berada pada level k dan 0 jika tidak, untuk \\(k=1,2,…,l−1\\). Dengan menghilangkan variabel indikator yang terkait dengan level terakhir dalam (8.9), kita secara efektif memilih level l sebagai kasus dasar atau level referensi, tetapi pilihan ini bersifat arbitrari dan tidak berpengaruh secara numerik. Parameter Poisson yang dihasilkan untuk kebijakan pada level k kemudian menjadi, dari (8.9), \\[ \\mu= \\begin{cases} e^{\\beta_0+\\beta_k} &amp; \\text{if the policy belongs to level } k, (k=1,2, ..., l-1), \\\\ e^{\\beta_0} &amp; \\text{if the policy belongs to level } l. \\end{cases} \\] Oleh karena itu, jika kita menunjukkan parameter Poisson untuk kebijakan pada level \\(k\\) sebagai \\(μ_{(k)}\\) , kita dapat menghubungkan parameter Poisson untuk level-level yang berbeda melalui \\(\\mu_{(k)}=\\mu_{(l)}\\, e^{\\beta_k}\\) , \\(k=1,2,…,l−1\\). Hal ini menunjukkan bahwa, seperti pada kasus dua level, frekuensi kerugian yang diharapkan pada level k diperoleh dari nilai dasar dikalikan dengan faktor relatif \\(e^{\\beta_k}\\). Interpretasi relatif ini menjadi lebih kuat ketika terdapat banyak faktor risiko dengan multi-level, dan mengarah pada pemahaman yang lebih baik tentang risiko yang mendasari dan prediksi kerugian masa depan yang lebih akurat. Akhirnya, perlu dicatat bahwa nilai rata-rata Poisson yang bervariasi sepenuhnya ditentukan oleh parameter koefisien \\(β_k\\), yang akan diestimasi dari dataset; prosedur estimasi parameter akan dibahas lebih lanjut dalam bab ini. 8.2.2 Poisson Regression Sekarang kita akan menjelaskan regresi Poisson dalam pengaturan formal dan lebih umum. Mari kita anggap ada n pemegang polis independen dengan satu set faktor penilaian yang ditandai oleh vektor k-variabel \\(\\mathbf{ x}_i=(1, x_{i1}, \\ldots, x_{ik})^{\\prime}\\)′ untuk pemegang polis ke-i, dan pemegang polis tersebut mencatat jumlah kerugian \\(y_i \\in \\{0,1,2, \\ldots \\}\\) dari periode pengamatan kerugian terakhir, untuk i=1,…,n. Dalam literatur regresi, nilai-nilai \\(x_{i1}, \\ldots, x_{ik}\\) umumnya dikenal sebagai variabel penjelas, karena ini adalah pengukuran yang memberikan informasi tentang variabel yang diminati \\(y_i\\). Pada dasarnya, analisis regresi adalah metode untuk mengkuantifikasi hubungan antara variabel yang diminati dan variabel penjelas. Kita juga berasumsi, untuk saat ini, bahwa semua pemegang polis memiliki periode pengamatan kerugian yang sama selama satu unit, atau paparan yang sama sebesar 1, untuk menjaga kesederhanaan; kita akan membahas lebih banyak detail mengenai paparan dalam subbagian berikutnya. Kami menjelaskan regresi Poisson melalui fungsi rata-ratanya. Untuk ini, kami pertama-tama menunjukkan \\(μ_i\\) sebagai jumlah kerugian yang diharapkan dari pemegang polis ke-i dalam spesifikasi Poisson (8.1): \\[ \\begin{equation} \\mu_i=\\mathrm{E~}{(y_i|\\mathbf{ x}_i)}, \\qquad y_i \\sim Pois(\\mu_i), \\, i=1, \\ldots, n. \\tag{8.10} \\end{equation} \\] Kondisi di dalam harapan pada persamaan (8.10) menunjukkan bahwa frekuensi kerugian μi adalah respons yang diharapkan oleh model terhadap himpunan faktor risiko atau variabel penjelas yang diberikan. Pada dasarnya, rata-rata bersyarat \\(\\mathrm{E~}{(y_i|\\mathbf{ x}_i)}\\) pada (8.10) dapat memiliki bentuk yang berbeda tergantung pada bagaimana kita menentukan hubungan antara x dan y. Pilihan standar untuk regresi Poisson adalah mengadopsi fungsi eksponensial, seperti yang telah kami sebutkan sebelumnya, sehingga \\[ \\begin{equation} \\mu_i=\\mathrm{E~}{(y_i|\\mathbf{ x}_i)}=e^{\\mathbf{ x}^{\\prime}_i\\beta}, \\qquad y_i \\sim Pois(\\mu_i), \\, i=1, \\ldots, n. \\tag{8.11} \\end{equation} \\] Di sini, \\(\\beta=(\\beta_0, \\ldots, \\beta_k)^{\\prime}\\)′ adalah vektor koefisien sehingga \\(\\mathbf{ x}^{\\prime}_i \\boldsymbol \\beta=\\beta_0+\\beta_1x_{i1} +\\ldots+\\beta_k x_{ik}\\). Fungsi eksponensial pada (8.11) memastikan bahwa μi&gt;0 untuk setiap himpunan faktor penilaian \\(x_i\\). Seringkali (8.11) ditulis ulang dalam bentuk log-linear \\[ \\begin{equation} \\log \\mu_i=\\log \\mathrm{E~}{(y_i|\\mathbf{ x}_i)}=\\mathbf{ x}^{\\prime}_i \\boldsymbol \\beta, \\qquad y_i \\sim Pois(\\mu_i), \\, i=1, \\ldots, n \\tag{8.12} \\end{equation} \\] untuk mengungkapkan hubungan ketika sisi kanan ditetapkan sebagai bentuk linear, \\(\\mathbf{ x}^{\\prime}_i\\beta\\). Sekali lagi, kita melihat bahwa pemetaan ini berfungsi dengan baik karena kedua sisi (8.12), logμi dan \\(\\mathbf{ x}_i\\beta\\), sekarang dapat mencakup semua nilai real. Ini adalah rumusan dari regresi Poisson, dengan asumsi bahwa semua pemegang polis memiliki periode paparan yang sama. Namun, ketika paparan berbeda di antara pemegang polis, seperti yang sering terjadi dalam kebanyakan kasus praktis, kita perlu merevisi rumusan ini dengan menambahkan komponen paparan sebagai istilah tambahan dalam (8.12). 8.2.3 Incorporating Exposure Konsep Paparan Kami pertama kali melihat konsep paparan dalam Bagian 7.4. Untuk menentukan ukuran kerugian potensial dalam jenis asuransi apa pun, kita selalu harus mengetahui paparannya yang sesuai. Konsep paparan merupakan komponen yang sangat penting dalam penetapan harga asuransi, meskipun kita biasanya menganggapnya sebagai hal yang biasa. Misalnya, ketika kita mengatakan frekuensi klaim yang diharapkan dari kebijakan asuransi kesehatan adalah 0,2, itu tidak berarti banyak tanpa spesifikasi paparan seperti, dalam kasus ini, per bulan atau per tahun. Faktanya, semua premi dan kerugian perlu memiliki paparan yang ditentukan dengan tepat dan harus dikutip sesuai; jika tidak, semua analisis statistik dan prediksi selanjutnya akan terdistorsi. Dalam bagian sebelumnya, kita mengasumsikan satuan paparan yang sama untuk semua pemegang polis, tetapi ini jarang terjadi dalam praktik. Dalam asuransi kesehatan, misalnya, dua pemegang polis yang berbeda dengan jangka waktu perlindungan asuransi yang berbeda (misalnya, 3 bulan dan 12 bulan) dapat mencatat jumlah klaim yang sama. Karena jumlah klaim yang diharapkan akan berbanding lurus dengan lamanya perlindungan, kita tidak boleh memperlakukan pengalaman kerugian kedua pemegang polis ini secara identik dalam proses pemodelan. Hal ini mendorong kebutuhan konsep paparan dalam regresi Poisson. Distribusi Poisson dalam (8.1) diparameterisasi melalui rata-ratanya. Untuk memahami paparan, kita secara alternatif memarameterisasi pmf Poisson dalam hal parameter laju λ, berdasarkan definisi proses Poisson: \\[ \\begin{equation} \\Pr(Y=y)=\\frac{(\\lambda t)^y e^{-\\lambda t}}{y!},\\qquad y=0,1,2, \\ldots \\tag{8.13} \\end{equation} \\] Dengan \\[\\mathrm{E~}{(Y)}=\\mathrm{Var~}{(Y)}=\\lambda t\\] Di sini, λ dikenal sebagai laju atau intensitas per periode satuan dari proses Poisson dan t mewakili lamanya waktu atau paparan, sebuah nilai konstan yang diketahui. Untuk λ yang diberikan, distribusi Poisson (8.13) menghasilkan jumlah kerugian yang diharapkan yang lebih besar ketika paparan t semakin besar. Jelas, (8.13) berkurang menjadi (8.1) ketika t=1, yang berarti bahwa rata-rata dan laju menjadi sama untuk paparan 1, kasus yang kita pertimbangkan dalam subbagian sebelumnya. Pada dasarnya, paparan tidak perlu diukur dalam satuan waktu dan dapat mewakili hal-hal yang berbeda tergantung pada masalah yang ada. Misalnya: Dalam asuransi kesehatan, laju dapat menjadi kejadian penyakit tertentu per 1.000 orang dan paparan adalah jumlah orang yang dipertimbangkan dalam satuan 1.000. Dalam asuransi otomotif, laju dapat menjadi jumlah kecelakaan per tahun pengemudi dan paparan adalah lamanya periode pengamatan untuk pengemudi dalam satuan tahun. Untuk kompensasi pekerja yang mencakup gaji yang hilang akibat cedera atau penyakit yang terkait dengan pekerjaan, laju dapat menjadi probabilitas cedera selama bekerja per dolar dan paparan adalah jumlah gaji dalam dolar. Dalam pemasaran, laju dapat menjadi jumlah pelanggan yang masuk ke toko per jam dan paparan adalah jumlah jam yang diamati. Dalam rekayasa sipil, laju dapat menjadi jumlah retakan besar pada jalan raya setiap 10 km dan paparan adalah panjang jalan yang dipertimbangkan dalam satuan 10 km. Dalam pemodelan risiko kredit, laju dapat menjadi jumlah kejadian gagal bayar per 1000 perusahaan dan paparan adalah jumlah perusahaan yang dipertimbangkan dalam satuan 1.000. Aktuaris mungkin dapat menggunakan basis paparan yang berbeda untuk kerugian yang dapat diasuransikan. Misalnya, dalam asuransi otomotif, baik jumlah kilometer yang ditempuh maupun jumlah bulan yang ditanggung oleh asuransi dapat digunakan sebagai basis paparan. Yang pertama lebih akurat dan berguna dalam pemodelan kerugian akibat kecelakaan mobil, tetapi lebih sulit untuk diukur dan dikelola oleh perusahaan asuransi. Oleh karena itu, basis paparan yang baik mungkin bukan yang terbaik secara teoritis karena berbagai kendala praktis. Sebagai aturan, basis paparan harus mudah ditentukan, dapat diukur dengan akurat, diterima secara hukum dan sosial, dan bebas dari manipulasi potensial oleh pemegang polis. Menggabungkan paparan dalam regresi Poisson Karena paparan mempengaruhi rata-rata Poisson, konstruksi regresi Poisson membutuhkan pemisahan yang hati-hati antara laju dan paparan dalam proses pemodelan. Dalam konteks asuransi, mari kita sebutkan laju kejadian kerugian dari pemegang polis ke-i sebagai \\(λ_i\\) , paparan yang diketahui (panjang perlindungan) sebagai mi , dan perkiraan jumlah kerugian di bawah paparan yang diberikan sebagai \\(μ_i\\) . Maka rumusan regresi Poisson dalam (8.11) dan (8.12) harus direvisi dengan mempertimbangkan (8.13) sebagai berikut: \\[ \\begin{equation} \\mu_i=\\mathrm{E~}{(y_i|\\mathbf{ x}_i)}=m_i \\,\\lambda_i=m_i \\, e^{\\mathbf{ x}^{\\prime}_i \\boldsymbol \\beta}, \\qquad y_i \\sim Pois(\\mu_i), \\, i=1, \\ldots, n, \\tag{8.14} \\end{equation} \\] yang menghasilkan \\[ \\begin{equation} \\log \\mu_i=\\log m_i+\\mathbf{ x}^{\\prime}_i \\boldsymbol \\beta, \\qquad y_i \\sim Pois(\\mu_i), \\, i=1, \\ldots, \\tag{8.15} \\end{equation} \\] Menambahkan \\(log\\space m_i\\) dalam (8.15) tidak menyebabkan masalah dalam fitting karena kita selalu dapat menentukan ini sebagai variabel penjelas tambahan, karena merupakan konstanta yang diketahui, dan mengatur koefisien menjadi 1. Dalam literatur, log paparan, \\(log\\space m_i\\), biasanya disebut sebagai offset. 8.3 Categorical Variables and Multiplicative Tariff Dalam bagian ini,  akan mempelajari: Model tarif multiplikatif ketika faktor penilaian bersifat kategorikal. Bagaimana membangun model regresi Poisson berdasarkan struktur tarif multiplikatif. 8.3.1 Rating Factors and Tariff Dalam praktiknya, sebagian besar faktor penilaian dalam asuransi adalah variabel kategorikal, yang berarti mereka mengambil salah satu dari sejumlah nilai yang telah ditentukan sebelumnya. Contoh variabel kategorikal meliputi jenis kelamin, jenis kendaraan, wilayah tempat tinggal pengemudi, dan pekerjaan. Variabel kontinu, seperti usia atau jarak tempuh mobil, juga dapat dikelompokkan berdasarkan rentang dan diperlakukan sebagai variabel kategorikal. Oleh karena itu, kita dapat membayangkan bahwa dengan sedikit faktor penilaian, akan ada banyak pemegang polis yang masuk ke dalam kelas risiko yang sama, dikenai premi yang sama. Untuk sisa bab ini, kita berasumsi bahwa semua faktor penilaian adalah variabel kategorikal. Untuk mengilustrasikan bagaimana variabel kategorikal digunakan dalam proses penetapan harga, kita akan mempertimbangkan sebuah asuransi mobil hipotetis dengan hanya dua faktor penilaian: Jenis kendaraan: Jenis A (milik pribadi) dan B (dimiliki oleh perusahaan). Kita menggunakan indeks j=1 dan 2 untuk masing-masing level faktor penilaian ini. Rentang usia pengemudi: Muda (usia &lt; 25), menengah (25 ≤ usia &lt; 60), dan usia tua (usia ≥ 60). Kita menggunakan indeks k=1, 2, dan 3 untuk masing-masing faktor penilaian ini. Dari aturan klasifikasi ini, kita dapat membuat tabel atau daftar terorganisir, seperti yang ditunjukkan dalam Tabel 8.2, yang dikumpulkan dari semua pemegang polis. Jelas terdapat 2×3=6 kelas risiko yang berbeda secara total. Setiap baris tabel menunjukkan kombinasi karakteristik risiko yang berbeda dari masing-masing pemegang polis. Tujuan kita adalah untuk menghitung enam premi yang berbeda untuk masing-masing kombinasi ini. Setelah premi untuk setiap baris ditentukan menggunakan paparan dan jumlah klaim yang diberikan, perusahaan asuransi dapat menggantikan dua kolom terakhir dalam Tabel 8.2 dengan satu kolom yang berisi premi yang dihitung. Tabel baru ini kemudian dapat digunakan sebagai panduan untuk menentukan premi bagi pemegang polis baru dengan faktor penilaian selama proses penjaminan. Dalam asuransi non-hayat, tabel (atau set tabel) atau daftar yang berisi setiap set faktor penilaian dan premi yang terkait disebut sebagai tarif. Setiap kombinasi unik dari faktor penilaian dalam sebuah tarif disebut sebagai sel tarif; dengan demikian, dalam Tabel 8.2 jumlah sel tarif adalah enam, sama dengan jumlah kelas risiko. Table 8.2. Loss Record of the Illustrative Auto Insurer \\[ {\\small \\begin{matrix} \\begin{array}{ccrrc} \\hline \\text{Rating} &amp;\\text{factors} &amp; \\text{Exposure} &amp; \\text{Claim count} \\\\ \\text{Type }(j) &amp; \\text{Age }(k) &amp; \\text{in year} &amp; \\text{observed}\\\\ \\hline \\hline 1 &amp; 1 &amp; 89.1 &amp; 9\\\\ 1 &amp; 2 &amp; 208.5&amp; 8\\\\ 1 &amp; 3 &amp; 155.2 &amp; 6 \\\\ 2 &amp; 1 &amp; 19.3 &amp; 1 \\\\ 2 &amp; 2 &amp; 360.4 &amp; 13 \\\\ 2 &amp; 3 &amp; 276.7 &amp; 6 \\\\ \\hline \\end{array} \\end{matrix} } \\] Mari kita perhatikan informasi kerugian di Tabel 8.2 dengan lebih cermat. Paparan dalam setiap baris mewakili jumlah dari panjang jangka waktu perlindungan asuransi, atau waktu berlaku, dalam tahun, dari semua pemegang polis dalam sel tarif tersebut. Demikian pula jumlah klaim dalam setiap baris adalah jumlah klaim dalam setiap sel. Secara alami, paparan dan jumlah klaim bervariasi karena adanya jumlah pengemudi yang berbeda di antara sel-sel tersebut, serta periode waktu berlaku yang berbeda di antara pengemudi dalam setiap sel. Dalam kerangka regresi Poisson, kita menyebut paparan dan jumlah klaim sel \\((j,k)\\) sebagai \\(m_{jk}\\) dan \\(y_{jk}\\), secara berurutan, dan mendefinisikan jumlah klaim per satuan paparan sebagai \\(z_{jk}= \\frac{y_{jk}}{ m_{jk}}, \\qquad j=1,2;\\, k=1, 2,3.\\) Sebagai contoh, \\(z_{12}=8/208.5=0.03837\\), yang berarti bahwa seorang pemegang polis dalam sel tarif (1,2) akan mengalami 0.03837 kecelakaan jika diasuransikan selama satu tahun secara rata-rata. Kumpulan nilai zij kemudian sesuai dengan parameter tingkat dalam distribusi Poisson (8.13) karena mereka adalah tingkat kejadian per satuan paparan. Dengan kata lain, kita memiliki \\(z_{jk}=\\hat{\\lambda}_{jk}\\) di mana \\({\\lambda}_{jk}\\) adalah parameter tingkat Poisson. Namun, menghasilkan nilai \\(z_{ij}\\) tidak banyak memberikan informasi selain membandingkan frekuensi kerugian rata-rata antar kelas risiko. Untuk sepenuhnya memanfaatkan dataset ini, kita akan membangun model penetapan harga dari Tabel 8.2 menggunakan regresi Poisson, untuk bagian tersisa dari bab ini. Kami mencatat bahwa catatan kerugian yang sebenarnya yang digunakan oleh perusahaan asuransi biasanya mencakup banyak faktor risiko lainnya, dalam hal ini jumlah sel akan tumbuh secara eksponensial. Tarif tersebut kemudian akan terdiri dari satu set tabel, bukan satu tabel, yang dipisahkan oleh beberapa faktor penilaian dasar, seperti jenis kelamin atau wilayah. 8.3.2 Multiplicative Tariff Model Dalam subbagian ini, kami memperkenalkan model tarif multiplicative, struktur penetapan harga yang populer yang dapat digunakan secara alami dalam kerangka regresi Poisson. Pengembangan di sini didasarkan pada Tabel 8.2. Ingatlah bahwa jumlah kerugian dari seorang pemegang polis dijelaskan oleh model regresi Poisson dengan tingkat \\(λ\\) dan paparan \\(m\\), sehingga jumlah kerugian yang diharapkan menjadi \\(mλ\\). Karena m adalah konstanta yang diketahui, kami pada dasarnya tertarik pada pemodelan \\(λ\\), sehingga merespons perubahan faktor penilaian. Di antara bentuk fungsional lain yang mungkin, kami umumnya memilih relasi multiplicative untuk memodelkan tingkat Poisson \\(\\lambda_{jk}\\) untuk sel (j,k): \\[ \\begin{equation} \\lambda_{jk}= f_0 \\times f_{1j} \\times f_{2k}, \\qquad j=1,2;\\, k=1, 2,3. \\tag{8.16} \\end{equation} \\] Di sini \\(\\{ f_{1j}, j=1,2\\}\\) adalah parameter yang terkait dengan dua level dalam faktor penilaian pertama, jenis mobil, dan \\(\\{ f_{2k}, k=1,2,3\\}\\) terkait dengan tiga level dalam kelompok usia, faktor penilaian kedua. Sebagai contoh, tingkat Poisson untuk seorang pemegang polis berusia menengah dengan kendaraan Tipe B diberikan oleh \\(\\lambda_{22}=f_0 \\times f_{12} \\times f_{22}\\). Istilah pertama f0 adalah nilai dasar yang akan dibahas segera. Oleh karena itu, enam parameter ini dipahami sebagai representasi numerik dari level dalam setiap faktor penilaian, dan harus diestimasi dari dataset. Bentuk multiplicative (8.16) mudah dipahami dan digunakan, karena dengan jelas menunjukkan bagaimana jumlah kerugian yang diharapkan (per satuan paparan) berubah saat setiap faktor penilaian bervariasi. Misalnya, jika f11=1 dan f12=1,2, maka jumlah kerugian yang diharapkan dari seorang pemegang polis dengan kendaraan tipe B akan 20% lebih besar daripada tipe A, ketika faktor lainnya sama. Dalam asuransi non-hidup, parameter f1j dan f2k dikenal sebagai relativitas karena mereka menentukan seberapa banyak jumlah kerugian yang diharapkan harus berubah relatif terhadap nilai dasar f0. Ide relativitas ini sangat nyaman dalam praktik, karena kita dapat menentukan premi untuk seorang pemegang polis dengan hanya mengalikan serangkaian relativitas yang sesuai dengan nilai dasar. Menghilangkan faktor penilaian yang ada atau menambahkan yang baru juga transparan dengan struktur multiplicative ini. Selain itu, perusahaan asuransi dapat menyesuaikan premi keseluruhan untuk semua pemegang polis dengan mengontrol nilai dasar f0 tanpa mengubah relativitas individu. Namun, dengan mengadopsi bentuk multiplicative, kita secara implisit mengasumsikan bahwa tidak ada interaksi serius di antara faktor risiko. Ketika menggunakan bentuk multiplicative, kita perlu mengatasi masalah identifikasi. Artinya, untuk setiap c&gt;0, kita dapat menulis \\[ \\lambda_{jk}= f_0 \\times \\frac{f_{1j}}{c} \\times c\\,f_{2k}. \\] Dengan membandingkan dengan (8.16), kita melihat bahwa parameter tingkat yang identik λjk dapat diperoleh untuk relativitas individu yang sangat berbeda. Over-parametrization ini, yang berarti banyak set parameter yang berbeda menghasilkan model yang identik, jelas membutuhkan beberapa pembatasan pada f1j dan f2k. Praktik standar adalah membuat satu relativitas dalam setiap faktor penilaian sama dengan satu. Ini dapat dilakukan secara sembarang dalam teori, tetapi praktik standar adalah membuat relativitas dari kelas yang paling umum (kelas dasar) sama dengan satu. Kami akan mengasumsikan bahwa kendaraan Tipe A dan pengemudi muda adalah kelas yang paling umum, yaitu f11=1 dan f21=1. Dengan cara ini, semua relativitas lainnya ditentukan dengan unik. Sel tariff (j,k)=(1,1) kemudian disebut sel tarif dasar, di mana tingkatnya sederhana menjadi λ11=f0, sesuai dengan nilai dasar sesuai (8.16). Dengan demikian, nilai dasar f0 umumnya diinterpretasikan sebagai tingkat Poisson dari sel tarif dasar. Sekali lagi, (8.16) diubah logaritmik dan ditulis ulang sebagai \\[ \\begin{equation} \\log \\lambda_{jk}= \\log f_0 + \\log f_{1j} + \\log f_{2k}, \\tag{8.17} \\end{equation} \\] karena lebih mudah dalam proses estimasi, mirip dengan (8.12). Bentuk log linier ini membuat log relativitas level dasar dalam setiap faktor penilaian menjadi nol, yaitu \\(\\log f_{11}=\\log f_{21}=0\\), dan mengarah pada ekspresi alternatif berikut yang lebih eksplisit untuk (8.17): \\[ \\begin{equation} \\small{ \\log \\lambda_{jk}=\\begin{cases} \\log f_0 + \\quad 0 \\quad \\,\\,+ \\quad 0 \\quad \\,\\,&amp; \\text{for a policy in cell }(1,1), \\\\ \\log f_0+ \\quad 0 \\quad \\,\\,+\\log f_{22}&amp; \\text{for a policy in cell }(1,2), \\\\ \\log f_0+ \\quad 0 \\quad \\,\\,+\\log f_{23}&amp; \\text{for a policy in cell }(1,3), \\\\ \\log f_0+\\log f_{12}+ \\quad 0 \\quad \\,\\,&amp; \\text{for a policy in cell }(2,1), \\\\ \\log f_0+\\log f_{12}+\\log f_{22}&amp; \\text{for a policy in cell }(2,2), \\\\ \\log f_0+\\log f_{12}+\\log f_{23}&amp; \\text{for a policy in cell }(2,3). \\\\ \\end{cases} } \\tag{8.18} \\end{equation} \\] Ini dengan jelas menunjukkan bahwa parameter tingkat Poisson λ bervariasi di antara sel tariff yang berbeda, dengan bentuk log linier yang sama digunakan dalam kerangka regresi Poisson. Bahkan pembaca dapat melihat bahwa (8.18) adalah versi yang diperluas dari ekspresi sebelumnya (8.6) dengan beberapa faktor risiko dan bahwa log relativitas sekarang berperan sebagai parameter βi. Oleh karena itu, semua relativitas dapat dengan mudah diestimasi melalui penyesuaian regresi Poisson dengan set variabel indikator yang dipilih dengan tepat. 8.3.3 Poisson Regression for Multiplicative Tariff Variabel Indikator untuk Sel Tarif Kami sekarang menjelaskan bagaimana relativitas dapat dimasukkan ke dalam regresi Poisson. Seperti yang telah terlihat sebelumnya dalam bab ini, kami menggunakan variabel indikator untuk mengatasi variabel kategori. Untuk perusahaan asuransi mobil contoh kami, oleh karena itu, kami mendefinisikan variabel indikator untuk faktor risiko pertama sebagai \\[ x_1= \\begin{cases} 1 &amp; \\text{ for vehicle type B}, \\\\ 0 &amp; \\text{ otherwise}. \\end{cases} \\] Untuk faktor risiko kedua, kami menggunakan dua variabel indikator untuk rentang usia, yaitu, \\[ x_2= \\begin{cases} 1 &amp; \\text{for age band 2}, \\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\] dan \\[ x_3= \\begin{cases} 1 &amp; \\text{for age band 3}, \\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\] Kombinasi (x1,x2,x3) kemudian secara efektif dan secara unik dapat menentukan setiap kelas risiko. Dengan melihat bahwa variabel indikator yang terkait dengan Tipe A dan Rentang Usia 1 diabaikan, kami melihat bahwa sel tarif (j,k) = (1,1) memainkan peran sebagai sel dasar. Kami menekankan bahwa pemilihan tiga variabel indikator di atas telah dilakukan dengan hati-hati sehingga konsisten dengan pemilihan tingkat dasar dalam model tarif multiplicative pada subbab sebelumnya (yaitu, f11 = 1 dan f21 = 1). Dengan variabel indikator yang diusulkan, kita dapat menulis ulang logaritma tingkat (8.17) sebagai \\[ \\begin{equation} \\log \\lambda_{}= \\log f_0+ \\log f_{12} \\times x_1 + \\log f_{22} \\times x_2 +\\log f_{23} \\times x_3, \\tag{8.19} \\end{equation} \\] yang identik dengan (8.18) ketika setiap nilai triple diterapkan. Sebagai contoh, kita dapat memverifikasi bahwa sel tarif dasar (j,k) = (1,1) sesuai dengan \\((x_1, x_2,x_3)=(0, 0, 0)\\), dan pada gilirannya menghasilkan \\(\\log \\lambda=\\log f_0\\) atau \\(λ = f_0\\) dalam (8.19) seperti yang diinginkan. Regresi Poisson untuk model tarif Dalam spesifikasi ini, mari kita anggap ada n pemegang polis dalam portofolio dengan karakteristik risiko pemegang polis ke-i diberikan oleh vektor variabel penjelas \\(\\mathbf{ x}_i=(1, x_{i1}, x_{i2},x_{i3})^{\\prime}\\)′, untuk i=1,…,n. Kemudian, kita dapat menganggap (8.19) sebagai \\(\\log \\lambda_{i}= \\beta_0+ \\beta_1 \\, x_{i1} + \\beta_{2} \\, x_{i2} +\\beta_3 \\, x_{i3}=\\mathbf{ x}^{\\prime}_i \\boldsymbol \\beta, \\qquad i=1, \\ldots, n,\\) di mana \\(β_0,...,β_3\\) dapat dipetakan ke relativitas log yang sesuai dalam (8.19). Ini sama persis dengan pengaturan yang sama dengan (8.15) kecuali untuk komponen exposure. Oleh karena itu, dengan menggabungkan eksposure dalam setiap kelas risiko, model regresi Poisson untuk model tarif multiplicative ini akhirnya menjadi \\[ \\begin{array}{ll} \\log \\mu_i &amp;=\\log \\lambda_{i}+\\log m_i= \\log m_i+ \\beta_0+ \\beta_1 \\, x_{i1} + \\beta_{2} \\, x_{i2} +\\beta_3 \\, x_{i3}\\\\ &amp;=\\log m_i+\\mathbf{ x}^{\\prime}_i \\boldsymbol \\beta, \\end{array} \\] untuk i=1,…,n. Sebagai hasilnya, relativitas diberikan : \\[ \\begin{equation} {f}_0=e^{\\beta_0}, \\quad {f}_{12}=e^{\\beta_1}, \\quad {f}_{22}=e^{\\beta_2}, \\quad \\text{and}\\quad {f}_{23}=e^{\\beta_3}, \\tag{8.20} \\end{equation} \\] dengan \\(f_{11}=1\\) dan \\(f_{21}=1\\) dari konstruksi asli. Untuk dataset sebenarnya, \\(β_i\\) , i=0,1,2,3, digantikan dengan mle bi menggunakan metode dalam tambahan teknis di akhir bab ini (Bagian 8.A). 8.3.4 Numerical Examples Kami menyajikan dua contoh numerik dari regresi Poisson. Pada contoh pertama, kami membuat model regresi Poisson dari Tabel 8.2, yang merupakan dataset dari perusahaan asuransi mobil hipotetis. Contoh kedua menggunakan dataset industri yang sebenarnya dengan lebih banyak faktor risiko. Tujuan kami adalah menunjukkan bagaimana model regresi Poisson dapat digunakan berdasarkan aturan klasifikasi yang diberikan, sehingga kami tidak memperhatikan kualitas kesesuaian model Poisson dalam bab ini. Example 8.1: Poisson regression for the illustrative auto insurer Pada beberapa subbab terakhir, kami mempertimbangkan dataset dari perusahaan asuransi mobil hipotetis dengan dua faktor risiko, seperti yang tercantum dalam Tabel 8.2. Sekarang kami menerapkan model regresi Poisson pada dataset ini. Seperti yang dilakukan sebelumnya, kami menetapkan (j,k)=(1,1) sebagai sel tarif dasar, sehingga \\(f_{11}=f_{21}=1\\). Hasil dari regresi memberikan perkiraan koefisien \\((b_0, b_1,b_2,b_3)=(-2.3359, -0.3004, -0.7837, -1.0655)\\), yang pada gilirannya menghasilkan perkiraan relativitas yang sesuai \\({f}_0=0.0967, \\quad {f}_{12}= 0.7405, \\quad {f}_{22}=0.4567 \\quad \\text{and}\\quad {f}_{23}=0.3445,\\) dari hubungan yang diberikan dalam (8.20). Skrip R dan outputnya adalah sebagai berikut. mydat1&lt;- read.csv(&quot;eg1_v1a.csv&quot;) mydat1 VtypeF &lt;- relevel(factor(Vtype), ref=&quot;1&quot;) # treat Vtype as factors with 1 as base. AgebndF &lt;- relevel(factor(Agebnd), ref=&quot;1&quot;) # treat Age band as factors. Pois_reg1 = glm(Claims ~ VtypeF + AgebndF, data = mydat1, family = poisson(link = log), offset = log(Expsr) ) Pois_reg1 Example 8.2. Poisson regression for Singapore insurance claims data Contoh 8.2. Regresi Poisson untuk data klaim asuransi Singapura Dataset sebenarnya ini merupakan subset dari data yang digunakan oleh Frees dan Valdez (2008). Data ini berasal dari General Insurance Association of Singapore, sebuah organisasi yang terdiri dari perusahaan asuransi non-jiwa di Singapura. Data ini berisi jumlah kecelakaan mobil untuk n=7.483 kebijakan asuransi mobil dengan beberapa variabel penjelas kategorikal dan paparan untuk setiap kebijakan. Variabel penjelas meliputi empat faktor risiko: jenis kendaraan yang diasuransikan (mobil (A) atau lainnya (O), ditandai sebagai Vtype), usia kendaraan dalam tahun (Vage), jenis kelamin pemegang kebijakan (Sex), dan usia pemegang kebijakan (dalam tahun, dikelompokkan menjadi tujuh kategori, ditandai sebagai Age). Berdasarkan deskripsi data, ada beberapa hal yang perlu dipertimbangkan sebelum membuat model. Pertama, terdapat 3.842 kebijakan dengan jenis kendaraan A (mobil) dan 3.641 kebijakan dengan jenis kendaraan lainnya. Namun, informasi usia dan jenis kelamin hanya tersedia untuk kebijakan kendaraan tipe A saja; pengemudi dari semua jenis kendaraan lainnya tercatat berusia 21 tahun atau kurang dengan jenis kelamin tidak spesifik, kecuali satu kebijakan, yang menunjukkan bahwa tidak ada informasi pengemudi yang dikumpulkan untuk kendaraan non-mobil. Kedua, kendaraan tipe A semuanya diklasifikasikan sebagai kendaraan pribadi dan semua jenis lainnya tidak. Ketika kita memasukkan faktor risiko ini, kita berasumsi semua jenis kelamin yang tidak ditentukan menjadi laki-laki. Karena informasi usia hanya berlaku untuk kendaraan tipe A, kita mengatur model sesuai dengan itu. Artinya, kita hanya menerapkan variabel usia pada kendaraan tipe A. Kami juga menggunakan lima kelompok usia kendaraan, menyederhanakan tujuh kelompok usia asli, dengan menggabungkan usia kendaraan 0, 1, dan 2; kelompok yang digabungkan ditandai sebagai level 211 dalam file data. Dengan demikian, model Poisson kami memiliki bentuk eksplisit berikut \\[ \\begin{align*} \\log \\mu_i= \\mathbf{ x}^{\\prime}_i\\beta+&amp;\\log m_i=\\beta_0+\\beta_1 I(Sex_i=M)+ \\sum_{t=2}^6 \\beta_t\\, I(Vage_i=t) \\\\ &amp;+ \\sum_{t=7}^{13} \\beta_t \\,I(Vtype_i=A)\\times I(Age_i=t-7)+\\log m_i. \\end{align*} \\] Hasil pemodelan ditampilkan dalam Tabel 8.3, yang memiliki beberapa komentar. Frekuensi klaim lebih tinggi untuk pria sebesar 17,3%, ketika faktor-faktor penilaian lainnya tetap. Namun, hal ini mungkin dipengaruhi oleh fakta bahwa semua jenis kelamin yang tidak ditentukan telah dianggap sebagai laki-laki. Mengenai usia kendaraan, frekuensi klaim secara bertahap menurun seiring bertambahnya usia kendaraan, ketika faktor-faktor penilaian lainnya tetap. Level dimulai dari 2 untuk variabel ini, tetapi sekali lagi, penomoran tersebut bersifat nominal dan tidak mempengaruhi hasil numerik. Variabel usia pemegang kebijakan hanya berlaku untuk kendaraan tipe A (mobil), dan tidak ada kebijakan di kelompok usia pertama. Kita dapat berspekulasi bahwa pengemudi yang lebih muda berusia kurang dari 21 tahun mengemudikan mobil orang tua mereka daripada memiliki mobil sendiri karena premi asuransi yang tinggi atau peraturan terkait. Relativitas yang hilang dapat diestimasi melalui interpolasi atau penilaian profesional dari aktuaris. Frekuensi klaim paling rendah untuk kelompok usia 3 dan 4, tetapi meningkat secara signifikan untuk kelompok usia yang lebih tua, pola yang wajar terlihat dalam banyak dataset kerugian asuransi mobil. Kami juga mencatat bahwa tidak ada tingkat dasar dalam variabel usia pemegang kebijakan, dalam arti bahwa tidak ada relativitas yang sama dengan 1. Hal ini karena variabel tersebut hanya berlaku untuk kendaraan tipe A. Ini tidak menyebabkan masalah secara numerik, tetapi jika diperlukan untuk tujuan lain, kita dapat menetapkan relativitas dasar sebagai berikut. Karena tidak ada kebijakan di kelompok usia 0, kita anggap kelompok usia 1 sebagai kasus dasar. Secara khusus, kita memperlakukan relativitasnya sebagai hasil perkalian antara 0,918 dan 1, di mana yang pertama adalah relativitas umum (yaitu, pengurangan premi umum) yang diterapkan pada semua kebijakan dengan kendaraan tipe A dan yang terakhir adalah nilai dasar untuk kelompok usia 1. Kemudian relativitas untuk kelompok usia 2 dapat dianggap sebagai 0,917 = 0,918 × 0,999, di mana 0,999 dipahami sebagai relativitas untuk kelompok usia 2. Kelompok usia lainnya dapat diperlakukan dengan cara yang serupa. Table 8.3. Singapore Insurance Claims Data \\[ {\\small \\begin{matrix} \\begin{array}{clcc} \\hline \\text{Rating factor} &amp; \\text{Level} &amp; \\text{Relativity in the tariff} &amp; \\text{Note}\\\\ \\hline\\hline \\text{Base value} &amp; &amp; 0.167 &amp; f_0\\\\ \\hline \\text{Sex} &amp; 1 (F) &amp; 1.000 &amp; \\text{Base level}\\\\ &amp; 2 (M) &amp; 1.173 &amp;\\\\\\hline \\text{Vehicle age} &amp; 2 (0-2\\text{ yrs}) &amp; 1.000 &amp; \\text{Base level}\\\\ &amp; 3 (3-5\\text{ yrs}) &amp; 0.843 \\\\ &amp; 4 (6-10\\text{ yrs}) &amp; 0.553 \\\\ &amp; 5 (11-15\\text{ yrs}) &amp; 0.269 \\\\ &amp; 6 (16+\\text{ yrs}) &amp; 0.189 &amp;\\\\\\hline \\text{Policyholder age} &amp; 0 (0-21) &amp; \\text{N/A} &amp; \\text{No policy} \\\\ \\text{(Only applicable to} &amp; 1 (22-25) &amp; 0.918 \\\\ \\text{vehicle type A)} &amp; 2 (26-35) &amp; 0.917 \\\\ &amp; 3 (36-45) &amp; 0.758 \\\\ &amp; 4 (46-55) &amp; 0.632 \\\\ &amp; 5 (56-65) &amp; 1.102\\\\ &amp; 6 (65+) &amp; 1.179\\\\ \\hline \\hline \\end{array} \\end{matrix} } \\] Mari kita mencoba beberapa contoh berdasarkan Tabel 8.3. Misalkan seorang pemegang kebijakan pria berusia 40 tahun yang memiliki kendaraan tipe A berusia 7 tahun. Frekuensi klaim yang diharapkan untuk pemegang kebijakan ini kemudian diberikan oleh \\(λ = 0,167 × 1,173 × 0,553 × 0,758 = 0,082\\) Sebagai contoh lain, pertimbangkan seorang pemegang kebijakan wanita berusia 60 tahun yang memiliki kendaraan tipe O berusia 3 tahun. Frekuensi klaim yang diharapkan untuk pemegang kebijakan ini adalah \\(λ = 0,167 × 1 × 0,843 = 0,141\\) Perhatikan bahwa untuk kebijakan ini, variabel kelompok usia tidak digunakan karena jenis kendaraannya bukan A. Skrip R diberikan sebagai berikut. mydat &lt;- read.csv(&quot;SingaporeAuto.csv&quot;, quote = &quot;&quot;, header = TRUE) attach(mydat) # create vehicle type as factor TypeA = 1 * (VehicleType == &quot;A&quot;) table(VehicleType) VtypeF &lt;- as.character(VehicleType) VtypeF[VtypeF != &quot;A&quot;] &lt;- &quot;O&quot; VtypeF = relevel(factor(VtypeF), ref=&quot;A&quot;) # create gender as factor Female = 1 * (SexInsured == &quot;F&quot; ) Sex = as.character(SexInsured) Sex[Sex != &quot;F&quot;] &lt;- &quot;M&quot; SexF = relevel(factor(Sex), ref = &quot;F&quot;) # create driver age as factor AgeCat = pmax(AgeCat - 1, 0) AgeCatF = relevel(factor(AgeCat), ref = &quot;0&quot;) table(AgeCatF) # No policy in the first age band # create vehicle age as factor VAgeCatF = relevel( factor(VAgeCat), ref = &quot;0&quot; ) VAgecat1 = factor(VAgecat1, labels = c(&quot;Vage0-2&quot;, &quot;Vage3-5&quot;, &quot;Vage6-10&quot;, &quot;Vage11-15&quot;, &quot;Vage15+&quot;) ) VAgecat1F = relevel( factor(VAgecat1), ref = &quot;Vage0-2&quot; ) # Poisson reg model Pois_reg2 = glm(Clm_Count ~ SexF + TypeA:AgeCatF + VAgecat1F, offset = LNWEIGHT, poisson(link = log) ) summary(Pois_reg2) # compute relativities exp(Pois_reg2$coefficients) detach(mydat) Sebagai catatan penutup, kami ingin menyebutkan bahwa regresi Poisson bukan satu-satunya model regresi hitung yang mungkin. Sebenarnya, distribusi Poisson dapat menjadi pembatasan karena memiliki satu parameter saja, serta rata-ratanya dan variansinya selalu sama. Ada model regresi hitung lain yang memungkinkan struktur distribusi yang lebih fleksibel, seperti regresi binomial negatif dan regresi zero-inflated (ZI); rincian mengenai regresi alternatif ini dapat ditemukan dalam teks lain yang tercantum dalam bagian berikutnya. 8.4 Further Resources and Contributors Bacaan dan Referensi Lanjutan Regresi Poisson merupakan anggota khusus dari kelas model regresi yang lebih umum dikenal sebagai generalized linear model (GLM). GLM mengembangkan kerangka regresi yang bersatu untuk dataset ketika variabel responsnya bersifat kontinu, biner, atau diskrit. Model regresi linear klasik dengan kesalahan yang terdistribusi secara normal juga merupakan anggota dari GLM. Terdapat banyak teks statistik standar yang membahas GLM, termasuk McCullagh dan Nelder (1989). Teks yang lebih mudah dipahami adalah Dobson dan Barnett (2008), Agresti (1996), dan Faraway (2016). Untuk aplikasi GLM dalam bidang aktuaria dan asuransi, lihat Frees (2009), De Jong dan Heller (2008). Selain itu, Ohlsson dan Johansson (2010) membahas GLM dalam konteks penetapan harga asuransi non-hidup dengan analisis tarif. "],["experience-rating-using-credibility-theory.html", "Bab 9 Experience Rating Using Credibility Theory 9.1 Pengantar Aplikasi Teori Kredibilitas 9.2 Limited Fluctuation Credibility 9.3 Bühlmann Credibility 9.4 Bühlmann-Straub Credibility", " Bab 9 Experience Rating Using Credibility Theory 9.1 Pengantar Aplikasi Teori Kredibilitas Berapa premi yang harus dibebankan untuk menyediakan asuransi? Jawabannya tergantung pada eksposur risiko kerugian. Metode yang umum digunakan untuk menghitung premi asuransi adalah dengan menilai tertanggung menggunakan rencana peringkat klasifikasi. Rencana klasifikasi digunakan untuk memilih tarif asuransi berdasarkan karakteristik peringkat tertanggung seperti wilayah geografis, usia, dll. Semua rencana pemeringkatan klasifikasi menggunakan seperangkat kriteria terbatas untuk mengelompokkan tertanggung ke dalam “kelas” dan akan ada variasi risiko kerugian di antara tertanggung di dalam kelas tersebut. Rencana pemeringkatan pengalaman mencoba untuk menangkap beberapa variasi dalam risiko kerugian di antara tertanggung dalam kelas pemeringkatan dengan menggunakan pengalaman kerugian tertanggung sendiri untuk melengkapi tingkat dari rencana pemeringkatan klasifikasi. Salah satu cara untuk melakukan hal ini adalah dengan menggunakan bobot kredibilitas \\(Z\\) dengan \\(0\\leq Z \\leq 1\\) untuk menghitung \\[\\hat{R}=Z\\bar{X}+(1-Z)M,\\] \\[\\begin{eqnarray*} \\hat{R}&amp;=&amp;\\textrm{tingkat bobot kredibilitas untuk risiko,}\\\\ \\bar{X}&amp;=&amp;\\textrm{kerugian rata-rata untuk risiko selama periode waktu tertentu,}\\\\ M&amp;=&amp;\\textrm{tingkat untuk kelompok klasifikasi, sering disebut tingkat manual.}\\\\ \\end{eqnarray*}\\] Untuk risiko yang pengalaman kerugiannya stabil dari tahun ke tahun, \\(Z\\) mungkin mendekati \\(1\\). Untuk risiko yang kerugiannya sangat bervariasi dari tahun ke tahun, \\(Z\\) mungkin mendekati \\(0\\). Teori kredibilitas juga digunakan untuk menghitung tingkat untuk masing-masing kelas dalam rencana peringkat klasifikasi. Ketika tingkat rencana klasifikasi sedang ditentukan, beberapa atau banyak kelompok mungkin tidak memiliki data yang cukup untuk menghasilkan tingkat yang stabil dan dapat diandalkan. Pengalaman kerugian aktual untuk suatu kelompok akan diberi bobot kredibilitas \\(Z\\) dan komplemen kredibilitas \\(1-Z\\) dapat diberikan pada pengalaman rata-rata untuk risiko di seluruh kelas. Atau, jika rencana pemeringkatan kelas sedang diperbarui, komplemen kredibilitas dapat diberikan pada tingkat kelas saat ini. Teori kredibilitas juga dapat diterapkan pada perhitungan frekuensi dan tingkat keparahan yang diharapkan. Menghitung nilai numerik untuk \\(Z\\) membutuhkan analisis dan pemahaman data. Apa saja varians dalam jumlah kerugian dan ukuran kerugian untuk risiko? Berapa varians antara nilai yang diharapkan di seluruh risiko? 9.2 Limited Fluctuation Credibility Di bagian ini, kita akan mempelajari cara: Hitung standar kredibilitas penuh untuk jumlah klaim, ukuran rata-rata klaim, dan kerugian agregat. Pelajari bagaimana hubungan antara sarana dan varians distribusi yang mendasari mempengaruhi standar kredibilitas penuh. Menentukan bobot kredibilitas \\(Z\\) menggunakan rumus kredibilitas parsial akar kuadrat. Kredibilitas fluktuasi terbatas, juga disebut “kredibilitas klasik” dan “kredibilitas Amerika”, diberi nama ini karena metode ini secara eksplisit mencoba untuk membatasi fluktuasi dalam estimasi frekuensi klaim, tingkat keparahan, atau kerugian. Sebagai contoh, anggaplah Anda ingin memperkirakan jumlah klaim yang diharapkan sebanyak \\(N\\) untuk sekelompok risiko dalam suatu kelas peringkat asuransi. Berapa banyak risiko yang diperlukan dalam kelas tersebut untuk memastikan bahwa tingkat akurasi tertentu dapat dicapai dalam estimasi? Pertama, pertanyaan ini akan dipertimbangkan dari perspektif berapa banyak klaim yang dibutuhkan. 9.2.1 Kredibilitas Penuh untuk Frekuensi Klaim Misalkan N adalah variabel acak yang mewakili jumlah klaim untuk sekelompok risiko, misalnya, risiko dalam klasifikasi peringkat tertentu. Jumlah klaim yang teramati akan digunakan untuk mengestimasi \\(\\mu_N=\\mathrm{E}[N]\\), jumlah klaim yang diharapkan. Seberapa besar \\(μ_N\\) yang dibutuhkan untuk mendapatkan estimasi yang baik? Salah satu cara untuk mengukur keakuratan estimasi adalah dengan pernyataan seperti: “Nilai \\(N\\) yang teramati harus berada dalam rentang 5% dari μN setidaknya 90% dari waktu.” Menuliskan ini sebagai ekspresi matematis akan menghasilkan \\(\\Pr[0.95 \\mu_N \\leq N \\leq 1.05 \\mu_N] \\geq 0.90\\). Menggeneralisasi pernyataan ini dengan membiarkan parameter rentang k menggantikan 5% dan tingkat probabilitas \\(p\\) menggantikan 0,90 memberikan persamaan \\[\\begin{equation} \\Pr[(1-k) \\mu_N \\leq N \\leq (1+k) \\mu_N] \\geq p . \\tag{9.1} \\end{equation}\\] Jumlah klaim yang diharapkan yang diperlukan agar probabilitas di sisi kiri (9.1) sama dengan \\(p\\) disebut standar kredibilitas penuh. Jika jumlah klaim yang diharapkan lebih besar atau sama dengan standar kredibilitas penuh maka kredibilitas penuh dapat diberikan pada data sehingga \\(Z = 1\\) . Biasanya nilai yang diharapkan \\(μ_N\\) tidak diketahui sehingga kredibilitas penuh akan diberikan pada data jika jumlah klaim aktual yang diamati \\(n\\) lebih besar atau sama dengan standar kredibilitas penuh. Nilai \\(k\\) dan \\(p\\) harus dipilih dan aktuaris dapat mengandalkan pengalaman, penilaian, dan faktor-faktor lain dalam membuat pilihan. Mengurangkan \\(μ_N\\) dari setiap suku dalam (9.1) dan membaginya dengan deviasi standar \\(σ_N\\) dari \\(N\\) memberikan \\[\\begin{equation} \\Pr\\left[\\frac{-k\\mu_N}{\\sigma_N}\\leq \\frac{N-\\mu_N}{\\sigma_N} \\leq \\frac{k\\mu_N}{\\sigma_N}\\right] \\geq p. \\tag{9.2} \\end{equation}\\] Dalam kredibilitas fluktuasi terbatas, distribusi normal standar digunakan untuk mendekati distribusi \\((N-\\mu_N)/\\sigma_N\\) . Jika \\(N\\) adalah jumlah dari banyak klaim dari sekelompok besar risiko yang sama dan klaim-klaim tersebut independen, maka perkiraannya mungkin masuk akal. Biarkan \\(y_p\\) adalah nilai yang sedemikian rupa sehingga \\[\\Pr[-y_p\\leq \\frac{N-\\mu_N}{\\sigma_N} \\leq y_p]=\\Phi(y_p)-\\Phi(-y_p)=p\\] di mana \\(Φ()\\) adalah fungsi distribusi kumulatif dari normal standar. Karena \\(\\Phi(-y_p)=1-\\Phi(y_p)\\) persamaan tersebut dapat ditulis ulang sebagai \\(2\\Phi(y_p)-1=p\\) . Penyelesaian untuk \\(y_p\\) memberikan \\(y_p=\\Phi^{-1}((p+1)/2)\\) dimana \\(\\Phi^{-1}( )\\) adalah kebalikan dari \\(Φ()\\) . Persamaan (9.2) akan terpenuhi jika \\(k\\mu_N/\\sigma_N \\geq y_p\\) dengan mengasumsikan aproksimasi normal. Pertama, kita akan mempertimbangkan ketidaksamaan ini untuk kasus ketika \\(N\\) memiliki distribusi Poisson: \\(\\Pr[N=n] = \\lambda^n\\textrm{e}^{-\\lambda}/n!\\) . Karena \\(\\lambda=\\mu_N=\\sigma_N^2\\) untuk Poisson, mengambil akar kuadrat menghasilkan \\(\\mu_N^{1/2}=\\sigma_N\\) . Jadi, \\(k\\mu_N/\\mu_N^{1/2} \\geq y_p\\) yang setara dengan \\(\\mu_N \\geq (y_p/k)^2\\) . Mari kita definisikan \\(\\lambda_{kp}\\) sebagai nilai dari \\(μ_N\\) yang mana kesetaraan berlaku. Maka standar kredibilitas penuh untuk distribusi Poisson adalah \\[\\begin{equation} \\lambda_{kp} = \\left(\\frac{y_p}{k}\\right)^2 \\textrm{with } y_p=\\Phi^{-1}((p+1)/2). \\tag{9.3} \\end{equation}\\] Jika jumlah klaim yang diharapkan \\(μ_N\\) lebih besar atau sama dengan \\(\\lambda_{kp}\\) maka persamaan (9.1) diasumsikan berlaku dan kredibilitas penuh dapat diberikan pada data. Sebagaimana disebutkan sebelumnya, karena \\(μ_N\\) biasanya tidak diketahui, kredibilitas penuh diberikan jika jumlah klaim yang diamati \\(n\\) memenuhi \\(n≥\\lambda_{kp}\\). Contoh 9.2.1. Standar kredibilitas penuh ditetapkan sehingga jumlah klaim yang teramati berada dalam kisaran 5% dari nilai yang diharapkan dengan probabilitas \\(p = 0.95\\) . Jika jumlah klaim berdistribusi Poisson, tentukan jumlah klaim yang dibutuhkan untuk kredibilitas penuh. Solusi. Mengacu pada tabel distribusi normal standar, \\(y_p=\\Phi^{-1}((p+1)/2)=\\Phi^{-1}((0.95+1)/2)\\)\\(\\Phi^{-1}(0.975)=1.960\\). Dengan menggunakan nilai ini dan \\(k=.05\\) lalu \\(\\lambda_{kp} = (y_p/k)^{2}=(1.960/0.05)^{2}=1,536.64\\). Setelah dibulatkan, standar kredibilitas penuhnya adalah 1.537. 9.2.2 Kredibilitas Penuh untuk Kerugian Agregat dan Premi Murni Kerugian agregat adalah total dari semua jumlah kerugian untuk risiko atau kelompok risiko. Membiarkan \\(S\\) mewakili kerugian agregat \\[S=X_1+X_2+\\cdots+X_N.\\] Variabel acak \\(N\\) mewakili jumlah kerugian dan variabel acak \\(X_1, X_2,\\ldots,X_N\\) adalah jumlah kerugian individu. Pada bagian ini diasumsikan bahwa \\(N\\) tidak bergantung pada jumlah kerugian dan bahwa \\(X_1, X_2,\\ldots,X_N\\) adalah Independen dan berdistribusi identik. Rata-rata dan varians dari \\(S\\) adalah \\[\\mu_S=\\mathrm{E}(S)=\\mathrm{E}(N)\\mathrm{E}(X)=\\mu_N\\mu_X\\] dan \\[\\sigma^{2}_S=\\mathrm{Var}(S)=\\mathrm{E}(N)\\mathrm{Var}(X)+[\\mathrm{E}(X)]^{2}\\mathrm{Var}(N)=\\mu_N\\sigma^{2}_X+\\mu^{2}_X\\sigma^{2}_N ,\\] dimana \\(X\\) adalah jumlah kerugian tunggal. Lihat diskusi tentang model risiko kolektif Kerugian yang teramati \\(S\\) akan digunakan untuk mengestimasi kerugian yang diharapkan \\(μ_S = E(S)\\) . Seperti halnya model frekuensi pada bagian sebelumnya, kerugian yang teramati harus mendekati kerugian yang diharapkan seperti yang dikuantifikasikan dalam persamaan \\[\\Pr[(1-k)\\mu_S\\leq S \\leq(1+k)\\mu_S] \\geq p.\\] Setelah mengurangi rata-rata dan membaginya dengan deviasi standar, \\[\\Pr\\left[\\frac{-k\\mu_S}{\\sigma_S}\\leq (S-\\mu_S)/\\sigma_S \\leq \\frac{k\\mu_S}{\\sigma_S}\\right] \\geq p .\\] Seperti yang dilakukan pada bagian sebelumnya, diasumsikan bahwa distribusi \\((S-\\mu_S)/\\sigma_S\\) adalah standar normal dan \\(k\\mu_S/\\sigma_S=y_p=\\Phi^{-1}((p+1)/2)\\). Persamaan ini dapat ditulis ulang sebagai \\(\\mu_S^2=(y_p/k)^2\\sigma_S^2\\). Dengan menggunakan rumus sebelumnya untuk \\(μ_S\\) dan \\(\\sigma_{S}^2\\), maka didapatkan \\((\\mu_N\\mu_X)^2=(y_p/k)^2(\\mu_N\\sigma^{2}_X+\\mu^{2}_X\\sigma^{2}_N)\\). Dengan membagi kedua sisi dengan \\(\\mu_N\\mu_X^2\\) dan mengurutkan sisi kanan, maka didapatkan standar kredibilitas penuh \\(n_S\\) untuk kerugian agregat. \\[\\begin{equation} n_S=\\left(\\frac{y_p}{k}\\right)^2\\left[\\left(\\frac{\\sigma_N^2}{\\mu_N}\\right)+\\left(\\frac{\\sigma_X}{\\mu_X}\\right)^2\\right]=\\lambda_{kp}\\left[\\left(\\frac{\\sigma_N^2}{\\mu_N}\\right)+\\left(\\frac{\\sigma_X}{\\mu_X}\\right)^2\\right]. \\tag{9.5} \\end{equation}\\] Contoh 9.2.5. Jumlah klaim memiliki distribusi Poisson. Jumlah kerugian individu didistribusikan secara independen dan identik dengan distribusi Pareto \\(F(x)=1-[\\theta/(x+\\theta)]^{\\alpha}\\). Jumlah klaim dan jumlah kerugian adalah independen. Jika kerugian agregat yang diamati harus berada dalam 5% dari nilai yang diharapkan dengan probabilitas \\(p=0.95\\), berapa banyak kerugian yang diperlukan untuk kredibilitas penuh? Solusi. Karena jumlah klaim berdistribusi Poisson, maka \\((\\sigma_N^2/\\mu_N)=1\\). Rata-rata dari distribusi Pareto adalah \\(\\mu_X=\\theta/(\\alpha-1)\\) dan variansinya adalah \\(\\sigma_X^2=\\theta^{2}\\alpha/[(\\alpha-1)^{2}(\\alpha-2)]\\), sehingga \\((\\sigma_X/\\mu_X)^2=\\alpha/(\\alpha-2)\\). Menggabungkan istilah frekuensi dan severity memberikan \\([(\\sigma_N^2/\\mu_N)+(\\sigma_X/\\mu_X)^2]=2(\\alpha-1)/(\\alpha-2)\\). Dari tabel distribusi normal standar, didapatkan \\(y_p=\\Phi^{-1}((0.95+1)/2)=1.960\\). Standar kredibilitas penuh adalah \\(n_S=(1.96/0.05)^{2}[2(\\alpha-1)/(\\alpha-2)]=3,073.28(\\alpha-1)/(\\alpha-2)\\). Jika \\(α=3\\) maka \\(n_S=6,146.56\\) untuk standar kredibilitas penuh sebesar 6.147. Perlu diperhatikan bahwa jumlah klaim yang jauh lebih banyak diperlukan untuk kredibilitas penuh untuk kerugian agregat dibandingkan dengan frekuensi saja. 9.2.3 Kredibilitas Penuh untuk Tingkat Keparahan Misalkan X adalah variabel acak yang merepresentasikan besarnya satu klaim. Severity klaim adalah \\(\\mu_X=\\mathrm{E}(X)\\). Anggaplah \\({X_1,X_2, \\ldots, X_n}\\) adalah sampel acak dari n klaim yang akan digunakan untuk mengestimasi severity klaim \\(μ_X\\). Klaim-klaim tersebut diasumsikan iid. Nilai rata-rata dari sampel adalah \\[\\bar{X}=\\frac{1}{n}\\left(X_1+X_2+\\cdots+X_n\\right).\\] Seberapa besar nilai n yang diperlukan untuk mendapatkan estimasi yang baik? Perhatikan bahwa n bukanlah variabel acak sedangkan di model kerugian agregat ia adalah variabel acak. Pada Bagian 9.2.1, akurasi sebuah estimator untuk frekuensi didefinisikan dengan menentukan agar jumlah klaim berada di dalam interval tertentu sekitar rata-rata jumlah klaim dengan probabilitas tertentu. Untuk severity, persyaratan ini adalah \\[\\Pr[(1-k)\\mu_X\\leq \\bar{X} \\leq(1+k)\\mu_X ]\\geq p ,\\] dimana \\(k\\) dan \\(p\\) harus ditentukan. Dengan mengikuti langkah-langkah pada Bagian 9.2.1, rata-rata severity klaim \\(μ_X\\) dikurangi dari setiap termin dan simpangan baku estimator severity klaim \\(\\sigma_{\\bar{X}}\\) dibagi ke dalam setiap termin sehingga diperoleh \\[\\Pr\\left[\\frac{-k~\\mu_X}{\\sigma_{\\bar{X}}}\\leq (\\bar{X}-\\mu_X)/\\sigma_{\\bar{X}} \\leq \\frac{k~\\mu_X}{\\sigma_{\\bar{X}}}\\right] \\geq p .\\] Seperti pada bagian sebelumnya, diasumsikan bahwa \\((\\bar{X}-\\mu_X)/\\sigma_{\\bar{X}}\\) secara kasar terdistribusi normal dan persamaan sebelumnya terpenuhi jika \\(k\\mu_X/\\sigma_{\\bar{X}}\\geq y_p\\) dengan \\(y_p=\\Phi^{-1}((p+1)/2)\\). Karena \\(\\bar{X}\\) adalah rata-rata klaim individual \\(X_1, X_2,\\dots, X_n\\), simpangan baku X¯ sama dengan simpangan baku klaim individual dibagi \\(\\sigma_{\\bar{X}}=\\sigma_X/\\sqrt{n}\\). Sehingga, \\(k\\mu_X/(\\sigma_X/\\sqrt{n})\\geq y_p\\) dan dengan sedikit aljabar, persamaan ini dapat dituliskan ulang sebagai \\(n \\geq (y_p/k)^2(\\sigma_X/\\mu_X)^2\\). Standar kredibilitas penuh untuk keparahan adalah \\[\\begin{equation} n_X=\\left(\\frac{y_p}{k}\\right)^2\\left(\\frac{\\sigma_X}{\\mu_X}\\right)^2=\\lambda_{kp}\\left(\\frac{\\sigma_X}{\\mu_X}\\right)^2. \\tag{9.6} \\end{equation}\\] Perhatikan bahwa istilah \\(\\sigma_X/\\mu_X\\) adalah koefisien variasi untuk klaim individual. Meskipun \\(\\lambda_{kp}\\) adalah standar kredibilitas penuh untuk frekuensi dengan diasumsikan distribusi Poisson, tidak ada asumsi tentang distribusi untuk jumlah klaim. Contoh 9.2.6. Besaran klaim individual didistribusikan secara independen dan identik dengan distribusi Pareto Tipe \\(F(x)=1-[\\theta/(x+\\theta)]^{\\alpha}\\). Berapa banyak klaim yang dibutuhkan agar rata-rata keparahan klaim yang diamati berada dalam 5% dari nilai harapan dengan probabilitas \\(p=0.95\\)? Solusi. Rata-rata Pareto adalah \\(\\mu_X=\\theta/(\\alpha-1)\\) dan variansnya adalah \\(\\sigma_X^2=\\theta^{2}\\alpha/[(\\alpha-1)^{2}(\\alpha-2)]\\) sehingga \\((\\sigma_X/\\mu_X)^2=\\alpha/(\\alpha-2)\\). Dari tabel distribusi normal standar, kita dapat menggunakan \\(y_p=\\Phi^{-1}((0.95+1)/2)=1.960\\). Standar kredibilitas penuh adalah \\(n_X=(1.96/0.05)^{2}[\\alpha/(\\alpha-2)]=1,536.64\\alpha/(\\alpha-2)\\). Misalkan \\(α=3\\) maka \\(n_X=4,609.92\\) untuk standar kredibilitas penuh sebesar 4.610. 9.2.4 Kredibilitas parsial Pada bagian sebelumnya, standar kredibilitas penuh dihitung untuk memperkirakan frekuensi (\\(n_f\\)), premi murni (\\(n_{PP}\\)), dan tingkat keparahan (\\(n_X\\)) - pada bagian ini, standar kredibilitas penuh ini akan ditandai dengan \\(n_0\\). Dalam setiap kasus, standar kredibilitas penuh adalah jumlah klaim yang diharapkan untuk mencapai tingkat akurasi tertentu saat menggunakan data empiris untuk memperkirakan nilai yang diharapkan. Jika jumlah klaim yang diamati lebih besar atau sama dengan standar kredibilitas penuh, maka bobot kredibilitas penuh \\(Z = 1\\) diberikan pada data. Dalam kredibilitas fluktuasi terbatas, bobot kredibilitas \\(Z\\) yang ditugaskan pada data adalah: \\[Z= \\left\\{ \\begin{array}{ll} \\sqrt{n /n_{0}} &amp;\\textrm{if } n &lt; n_{0} \\\\ 1 &amp; \\textrm{if } n \\ge n_{0} , \\end{array} \\right.\\] Di mana \\(n_0\\) merupakan standar kredibilitas penuh. Jumlah klaim \\(n\\) merupakan jumlah klaim untuk data yang digunakan untuk memperkirakan frekuensi yang diharapkan, tingkat keparahan, atau premi murni. Contoh 9.2.7. Jumlah klaim memiliki distribusi Poisson. Jumlah kerugian individu didistribusikan secara independen dan identik dengan distribusi Pareto Tipe II \\(F(x)=1-[\\theta/(x+\\theta)]^{\\alpha}\\). Dalam hal ini, \\(α=3\\). Jumlah klaim dan jumlah kerugian adalah independen. Standar kredibilitas penuh adalah bahwa premi murni yang diamati harus berada dalam 5% dari nilai yang diharapkan dengan probabilitas \\(p=0.95\\). Berapa kredibilitas \\(Z\\) yang diberikan untuk premi murni yang dihitung dari 1.000 klaim? Solusi. Karena jumlah klaim adalah Poisson, \\[\\frac{\\mathrm{E}(X^2)}{[\\mathrm{E}~(X)]^2} =\\frac{\\sigma_N^2}{\\mu_N}+\\left(\\frac{\\sigma_X}{\\mu_X}\\right)^2.\\] Rata-rata dari Pareto adalah \\(μX=θ/(α−1)\\) dan momen kedua adalah \\(\\mathrm{E}(X^2)=2\\theta^{2}/[(\\alpha-1)(\\alpha-2)]\\) sehingga \\(\\mathrm{E}(X^2)/[\\mathrm{E}~(X)]^2=2(\\alpha-1)/(\\alpha-2)\\). Dari tabel distribusi normal standar, \\(y_p=\\Phi^{-1}((0.95+1)/2)=1.960\\). Standar kredibilitas penuh adalah \\[n_{PP}=(1.96/0.05)^{2}[2(\\alpha-1)/(\\alpha-2)]=3,073.28(\\alpha-1)/(\\alpha-2)\\] dan jika \\(α=3\\), maka \\(n_0=n_{PP}=6,146.56\\) atau 6.147 jika dibulatkan ke atas. Kredibilitas yang diberikan untuk 1.000 klaim adalah \\(Z=(1,000/6,147)^{1/2}=0.40\\). Kredibilitas fluktuasi terbatas menggunakan rumus \\(Z=\\sqrt{n/n_0}\\) untuk membatasi fluktuasi dalam perkiraan yang diboboti kredibilitas untuk sesuai dengan fluktuasi yang diizinkan untuk data dengan jumlah klaim yang diharapkan pada standar kredibilitas penuh. Varians atau simpangan baku digunakan sebagai ukuran fluktuasi. Selanjutnya, kami akan menunjukkan contoh untuk menjelaskan mengapa rumus akar kuadrat digunakan. Misalkan tingkat keparahan klaim rata-rata sedang diestimasi dari sampel ukuran \\(n\\) yang lebih kecil dari standar kredibilitas penuh \\(n_0=n_X\\). Dengan menerapkan teori kredibilitas, perkiraan \\(\\hat{\\mu}_X\\) akan menjadi: \\[\\hat{\\mu}_X=Z\\bar{X}+(1-Z)M_X ,\\] dengan \\(\\bar{X}=(X_1+X_2+\\cdots+X_n)/n\\) dan variabel acak iid \\(X_i\\) yang mewakili ukuran klaim individu. Kredibilitas komplementer diterapkan pada \\(M_X\\) yang bisa menjadi perkiraan tingkat keparahan rata-rata tahun lalu yang disesuaikan dengan inflasi, rata-rata tingkat keparahan untuk kumpulan risiko yang jauh lebih besar, atau kuantitas relevan lainnya yang dipilih oleh aktuaris. Diasumsikan bahwa varians dari \\(M_X\\) adalah nol atau bisa diabaikan. Dengan asumsi ini, \\[\\mathrm{Var}(\\hat{\\mu}_X)=\\mathrm{Var}(Z\\bar{X})=Z^2\\mathrm{Var}(\\bar{X})=\\frac{n}{n_0}\\mathrm{Var}(\\bar{X}).\\] Karena \\(\\bar{X}=(X_1+X_2+\\cdots+X_n)/n\\) maka berlaku bahwa \\(\\mathrm{Var}(\\bar{X})=\\mathrm{Var}(X_i)/n\\) di mana variabel acak \\(X_i\\) adalah satu klaim. Oleh karena itu, \\[\\mathrm{Var}(\\hat{\\mu}_X)=\\frac{n}{n_0}\\mathrm{Var}(\\bar{X})=\\frac{n}{n_0}\\frac{\\mathrm{Var}(X_i)}{n}=\\frac{\\mathrm{Var}(X_i)}{n_0}.\\] Term terakhir adalah varians tepat dari rata-rata sampel \\(\\bar{X}\\) ketika ukuran sampel sama dengan standar kredibilitas penuh \\(n_0=n_X\\). 9.3 Bühlmann Credibility Dalam bagian ini, kita akan mempelajari: Menghitung perkiraan yang ditimbang kredibilitas untuk kerugian yang diharapkan untuk suatu risiko atau kelompok risiko. Menentukan kredibilitas \\(Z\\) yang diberikan kepada pengamatan. Menghitung nilai yang diperlukan dalam kredibilitas Bühlmann, termasuk Nilai Harapan Variansi Proses \\(( EPV )\\), Variansi Rata-rata Hipotetis \\(( VHM )\\) dan rata-rata kolektif \\(μ\\) . Mengenali situasi di mana model Bühlmann sesuai. Rencana peringkat klasifikasi mengelompokkan pemegang polis ke dalam kelas berdasarkan karakteristik risiko. Meskipun pemegang polis dalam satu kelas memiliki kesamaan, mereka tidak identik dan kerugian yang diharapkan tidak akan sama persis. Rencana peringkat pengalaman dapat melengkapi rencana peringkat kelas dengan menimbang kredibilitas pengalaman kerugian individu pemegang polis dengan tarif kelas untuk menghasilkan tarif yang lebih akurat bagi pemegang polis. Dalam penyajian kredibilitas Bühlmann, disarankan untuk menetapkan parameter risiko \\(θ\\) untuk setiap pemegang polis. Kerugian \\(X\\) untuk pemegang polis akan memiliki fungsi distribusi yang umum \\(Fθ(x)\\) dengan rata-rata \\(μ(θ)=E(X|θ)\\) dan varians \\(σ2(θ)=Var(X|θ)\\). Kerugian \\(X\\) dapat mewakili premi murni, kerugian agregat, jumlah klaim, keparahan klaim, atau ukuran kerugian lainnya untuk periode waktu, seringkali selama satu tahun. Parameter risiko \\(θ\\) dapat bersifat kontinu atau diskrit dan dapat multivariat tergantung pada model yang digunakan. Jika seorang pemegang polis dengan parameter risiko \\(θ\\) mengalami kerugian \\(X1,...,Xn\\) selama \\(n\\) periode waktu, maka tujuannya adalah untuk menemukan \\(E(μ(θ)|X1,...,Xn)\\), yaitu ekspektasi bersyarat dari \\(μ(θ)\\) yang diberikan \\(X1,...,Xn\\). Perkiraan yang ditimbang kredibilitas Bühlmann untuk \\(E(μ(θ)|X1,...,Xn)\\) untuk pemegang polis adalah sebagai berikut: \\[\\begin{equation}\\hat{\\mu}(\\theta)=Z\\bar{X}+(1-Z)\\mu \\tag{9.7}\\end{equation}\\] Dengan : \\[\\begin{eqnarray*} \\theta&amp;=&amp;\\textrm{a risk parameter that identifies a policyholder&#39;s risk level}\\\\ \\hat{\\mu}(\\theta)&amp;=&amp;\\textrm{estimated expected loss for a policyholder with parameter }\\theta\\\\ &amp; &amp; \\textrm{and loss experience } \\bar{X}\\\\ \\bar{X}&amp;=&amp;(X_1+\\cdots+X_n)/n \\textrm{ is the average of $n$ observations of the policyholder } \\\\ Z&amp;=&amp;\\textrm{credibility assigned to $n$ observations } \\\\ \\mu&amp;=&amp;\\textrm{the expected loss for a randomly chosen policyholder in the class.}\\\\ \\end{eqnarray*}\\] Untuk pemegang polis yang dipilih, asumsi variabel acak \\(Xj\\) dianggap iid untuk \\(j=1,...,n\\) karena diasumsi8kan bahwa paparan pemegang polis terhadap kerugian tidak berubah dari waktu ke waktu. Kuantitas \\(\\bar{X}\\) adalah rata-rata dari \\(n\\) pengamatan dan \\(E(\\bar{X}|θ)=E(Xj|θ)=μ(θ)\\). Jika seorang pemegang polis dipilih secara acak dari kelas dan tidak ada informasi kerugian tentang risiko, maka kerugian yang diharapkan adalah \\(μ=E(μ(θ))\\) di mana harapan diambil dari semua \\(θ\\) dalam kelas. Dalam situasi ini, \\(Z=0\\) dan kerugian yang diharapkan adalah \\(μ^(θ)=μ\\) untuk risiko tersebut. Kuantitas \\(μ\\) juga dapat ditulis sebagai \\(μ=E(Xj)\\) atau \\(μ=E(\\bar{X})\\) dan sering disebut sebagai mean keseluruhan atau collective mean. Perhatikan bahwa \\(E(Xj)\\) dievaluasi dengan hukum total ekspektasi: \\(E(Xj)=E(E[Xj|θ])\\). Example 9.3.1 Jumlah klaim \\(X\\) untuk seorang tertanggung dalam suatu kelas memiliki distribusi Poisson dengan mean \\(θ&gt;0\\). Parameter risiko \\(θ\\) didistribusikan secara eksponensial di dalam kelas dengan pdf \\(f(θ)=e−θ\\). Berapakah jumlah klaim yang diharapkan untuk seorang tertanggung yang dipilih secara acak dari kelas tersebut? Solusi Variabel acak \\(X\\) memiliki distribusi Poisson dengan parameter \\(θ\\) dan \\(E(X|θ)=θ\\). Jumlah klaim yang diharapkan untuk seorang tertanggung yang dipilih secara acak adalah \\(μ=E(μ(θ))=E(E(X|θ))=E(θ)=∫∞0θe−θdθ=1\\). Pada contoh di atas, parameter risiko \\(θ\\) adalah variabel acak dengan distribusi eksponensial. Pada contoh berikutnya, terdapat tiga jenis risiko dan parameter risiko memiliki distribusi diskrit. Example 9.3.2 Untuk setiap risiko (pemegang polis) dalam populasi, jumlah kerugian \\(N\\) dalam setahun memiliki distribusi Poisson dengan parameter \\(λ\\). Jumlah kerugian individu \\(Xi\\) untuk sebuah risiko independen dari \\(N\\) dan identik dan memiliki distribusi Pareto Tipe II dengan \\(F(x) = 1 - [θ / (x + θ)]α\\). Ada tiga jenis risiko dalam populasi sebagai berikut: \\[\\small{ \\begin{array}{|c|c|c|c|} \\hline \\text{Risk } &amp; \\text{Percentage} &amp; \\text{Poisson} &amp; \\text{Pareto} \\\\ \\text{Type} &amp; \\text{of Population} &amp; \\text{Parameter} &amp; \\text{Parameters} \\\\ \\hline A &amp; 50\\% &amp; \\lambda=0.5 &amp; \\theta=1000, \\alpha=2.0 \\\\ B &amp; 30\\% &amp; \\lambda=1.0 &amp; \\theta=1500, \\alpha=2.0 \\\\ C &amp; 20\\% &amp; \\lambda=2.0 &amp; \\theta=2000, \\alpha=2.0 \\\\ \\hline \\end{array} }\\] Jika sebuah risiko dipilih secara acak dari populasi tersebut, berapa kerugian total yang diharapkan dalam setahun? Solusi Untuk suatu risiko, jumlah klaim yang diharapkan adalah \\(E(N|λ) = λ\\). Nilai harapan dari suatu variabel acak yang didistribusikan Pareto adalah \\(E(X|θ,α) = θ/(α-1)\\). Nilai harapan dari variabel acak kehilangan agregat \\(S = X1 +⋯+XN\\) untuk risiko dengan parameter \\(λ\\), \\(α\\), dan \\(θ\\) adalah \\(E(S) = E(N)E(X) = λθ/(α-1)\\). Kerugian agregat yang diharapkan untuk suatu risiko jenis A adalah \\(E(SA) = (0,5)(1000)/(2-1) = 500\\). Kerugian agregat yang diharapkan untuk suatu risiko yang dipilih secara acak dari populasi adalah \\(E(S) = 0,5[(0,5)(1000)]+0,3[(1,0)(1500)]+0,2[(2,0)(2000)] = 1500\\). Berapa parameter risiko untuk suatu risiko (pemegang polis) pada contoh sebelumnya? Dapat dikatakan bahwa parameter risiko memiliki tiga komponen \\((λ, θ, α)\\) dengan nilai mungkin (0,5, 1000, 2,0), (1,0, 1500, 2,0), dan (2,0, 2000, 2,0) tergantung pada jenis risiko. Perlu diperhatikan bahwa pada kedua contoh tersebut, parameter risiko adalah kuantitas acak dengan distribusi probabilitasnya sendiri. Kita tidak tahu nilai parameter risiko untuk risiko yang dipilih secara acak. Meskipun formula (9.7) diperkenalkan dengan menggunakan rating pengalaman sebagai contoh, model kredibilitas Bühlmann memiliki aplikasi yang lebih luas. Misalkan ada rencana rating dengan beberapa kelas. Formula kredibilitas (9.7) dapat digunakan untuk menentukan tarif kelas individu. Rata-rata keseluruhan \\(μ\\) akan menjadi rata-rata kerugian untuk semua kelas yang digabungkan, \\(\\bar{X}\\) akan menjadi pengalaman untuk kelas individu, dan \\(μ^(θ)\\) akan menjadi perkiraan kerugian untuk kelas tersebut. 9.3.1 Credibility Z, EPV, and VHM Ketika menghitung estimasi kredibilitas \\(μ^(θ)=Z\\bar{X}+(1−Z)μ\\), berapa bobot \\(Z\\) yang harus diberikan pada pengalaman \\(\\bar{X}\\) dan berapa bobot \\((1−Z)\\) pada rata-rata keseluruhan \\(μ\\)? Dalam kredibilitas Bühlmann, terdapat tiga faktor yang perlu dipertimbangkan: Berapa variasi dalam satu pengamatan \\(Xj\\) untuk risiko yang dipilih? Dengan \\(\\bar{X} = (X1 + ⋯ + Xn) / n\\) dan dengan asumsi bahwa pengamatan adalah iid kondisional pada \\(θ\\), maka mengikuti bahwa \\(Var(\\bar{X}|θ) = Var(Xj|θ) / n\\). Untuk \\(Var(\\bar{X}|θ)\\) yang lebih besar, bobot kredibilitas \\(Z\\) yang lebih kecil harus diberikan pada pengalaman \\(\\bar{X}\\). Nilai Harapan dari Varians Proses, disingkat \\(EPV\\), adalah nilai harapan dari \\(Var(Xj|θ)\\) di seluruh risiko: \\(EPV = \\mathrm{E}(\\mathrm{Var}(X_j|\\theta)).\\) karena \\(Var(\\bar{X}|θ) = Var(Xj|θ)/n)\\) maka berlaku bahwa \\(E(Var(\\bar{X}|θ))=EPV/n\\). Seberapa homogen populasi risiko yang pengalaman kerugiannya digabungkan untuk menghitung rata-rata keseluruhan \\(μ\\)? Jika semua risiko memiliki potensi kerugian yang serupa, maka bobot yang lebih besar \\((1-Z)\\) diberikan pada rata-rata keseluruhan \\(μ\\) karena \\(μ\\) adalah rata-rata untuk sekelompok risiko yang serupa dan rata-rata \\(μ(θ)\\) tidak terlalu jauh. Homogenitas atau heterogenitas populasi diukur dengan Variance of the Hypothetical Means dengan singkatan \\(VHM\\): \\[VHM=\\mathrm{Var}(\\mathrm{E}(X_j|\\theta))=\\mathrm{Var}(\\mathrm{E}(\\bar{X}|\\theta)).\\] Perhatikan bahwa kita menggunakan \\(E(\\bar{X}|θ)=E(Xj|θ)\\) untuk kesamaan kedua. Berapa banyak pengamatan n yang digunakan untuk menghitung \\(\\bar{X}\\)? Sampel yang lebih besar akan menghasilkan \\(Z\\) yang lebih besar. Example 9.3.3 Jumlah klaim \\(N\\) dalam setahun untuk suatu risiko dalam populasi memiliki distribusi Poisson dengan mean \\(λ&gt;0\\). Parameter risiko \\(λ\\) didistribusikan secara seragam di selang (0,2). Hitung \\(EPV\\) dan \\(VHM\\) untuk populasi. Solusi Variabel acak N berdistribusi Poisson dengan parameter λ sehingga Var(N|λ)=λ . Nilai harapan dari varian proses adalah EPV=E(Var(N|λ)) = E(λ)=∫20λ12dλ=1 . Varians dari rata-rata hipotetis adalah \\(VHM=Var(E(N|λ)) = Var(λ)=E(λ^2)−(E(λ))^2 = \\int_{0}^{2}\\lambda^2 \\frac{1}{2} d\\lambda-(1)^2=\\frac{1}{3}\\) Formula kepercayaan Bühlmann meliputi nilai untuk \\(n\\), \\(EPV\\), dan \\(VHM\\): \\[\\begin{equation} Z=\\frac{n}{n+K} \\quad , \\quad K =\\frac{EPV}{VHM}. \\tag{9.8} \\end{equation}\\] Jika \\(VHM\\) meningkat maka \\(Z\\) juga meningkat. Jika \\(EPV\\) meningkat maka \\(Z\\) menjadi lebih kecil. Berbeda dengan kredibilitas fluktuasi terbatas di mana \\(Z=1\\) ketika jumlah klaim yang diharapkan lebih besar dari standar kredibilitas penuh, \\(Z\\) dapat mendekati tetapi tidak sama dengan 1 ketika jumlah pengamatan n mendekati tak hingga. Jika Anda mengalikan pembilang dan penyebut rumus \\(Z\\) dengan \\(( VHM/n )\\), maka \\(Z\\) dapat ditulis kembali sebagai: \\[Z=\\frac{VHM}{VHM+(EPV/n)} .\\] Jumlah pengamatan \\(n\\) tertangkap dalam istilah \\((EPV/n)\\). Seperti yang ditunjukkan di bullet (1) di awal bagian, \\(E(Var(\\bar{X}|θ)) = EPV/n\\). Seiring dengan bertambahnya jumlah pengamatan, varians yang diharapkan dari \\(\\bar{X}\\) menjadi lebih kecil dan kredibilitas \\(Z\\) meningkat sehingga lebih banyak bobot diberikan pada \\(\\bar{X}\\) dalam perkiraan yang dibobotkan kredibilitas \\(μ^(θ)\\). 9.4 Bühlmann-Straub Credibility Di bagian ini, Anda akan belajar cara: Menghitung perkiraan bobot kredibilitas untuk kerugian yang diharapkan untuk risiko atau kelompok risiko menggunakan model Bühlmann-Straub. Menentukan kredibilitas \\(Z\\) yang diberikan kepada pengamatan. Menghitung nilai yang dibutuhkan termasuk Expected Value of the Process Variance \\((EPV)\\), Variance of the Hypothetical Means \\((VHM)\\), dan mean kolektif \\(μ\\). Mengenali situasi di mana model Bühlmann-Straub sesuai. Dengan kredibilitas Bühlmann standar atau least-squares seperti yang dijelaskan pada bagian sebelumnya, kerugian \\(X1,…,Xn\\) yang timbul dari pemegang polis yang dipilih diasumsikan sebagai iid. Jika subskrip menunjukkan tahun 1, tahun 2, dan seterusnya hingga tahun \\(n\\), maka asumsi iid berarti bahwa pemegang polis memiliki paparan kerugian yang sama setiap tahun. Misalkan ada pemegang polis komersial yang menggunakan armada kendaraan dalam bisnisnya. Pada tahun 1, ada \\(m1\\) kendaraan dalam armada, \\(m2\\) kendaraan pada tahun 2, .., dan \\(mn\\) kendaraan pada tahun n. Paparan kerugian dari kepemilikan dan penggunaan armada ini akan tidak konstan dari tahun ke tahun. Kerugian tahunan untuk armada tersebut tidak dapat didefinisikan sebagai iid. Maka untuk permisalan tersebut \\(Yjk\\) didefinisikan sebagai kerugian untuk kendaraan ke-k dalam armada untuk tahun ke-j. Kemudian, total kerugian untuk armada pada tahun ke-j adalah \\(Yj1+⋯+Yjmj\\), di mana kita menambahkan kerugian untuk masing-masing dari \\(mj\\) kendaraan. Sedangkan dalam model Bühlmann-Straub, diasumsikan bahwa variabel acak \\(Yjk\\), iid di semua kendaraan dan tahun untuk pemegang polis. Dengan asumsi ini, rata-rata \\(E(Yjk|θ)=μ(θ)\\) dan variansi \\(Var(Yjk|θ)=σ2(θ)\\) sama untuk semua kendaraan dan tahun. Jumlah \\(μ(θ)\\) adalah kerugian yang diharapkan dan \\(σ2(θ)\\) adalah varians pada kerugian untuk satu tahun untuk satu kendaraan untuk pemegang polis dengan parameter risiko \\(θ\\). Jika \\(Xj\\) adalah kerugian rata-rata per unit paparan pada tahun ke-j, \\(Xj=(Yj1+⋯+Yjmj)/mj\\), maka \\(E(Xj|θ)=μ(θ)\\) dan \\(Var(Xj|θ)=σ^2(θ)/mj\\) untuk pemegang polis dengan parameter risiko \\(θ\\). Kerugian rata-rata per kendaraan untuk seluruh periode \\(n\\) tahun adalah: \\[\\begin{equation*} \\bar{X}= \\frac{1}{m} \\sum_{j=1}^{n} m_j X_{j} \\quad , \\quad m=\\sum_{j=1}^{n} m_j. \\end{equation*}\\] Maka berikutnya \\(E (\\bar{X}|θ)=μ(θ)\\) dan \\(Var(\\bar{X}|θ)=σ2(θ)/m\\) di mana \\(μ(θ)\\) dan \\(σ^2(θ)\\) adalah rata-rata dan varians untuk satu kendaraan selama satu tahun untuk pemegang polis. Example 9.4.1 Prove that \\(Var(\\bar{X}|θ)=σ^2(θ)/m\\) for a risk with risk parameter \\(θ\\). Solusi \\[\\begin{eqnarray*} \\mathrm{Var}(\\bar{X}|\\theta)&amp;=&amp;\\mathrm{Var}\\left(\\frac{1}{m} \\sum_{j=1}^{n} m_j X_j|\\theta \\right)\\\\ &amp;=&amp;\\frac{1}{m^2}\\sum_{j=1}^{n} \\mathrm{Var}(m_j X_{j}|\\theta)=\\frac{1}{m^2}\\sum_{j=1}^{n} m_j^2 \\mathrm{Var}(X_j|\\theta)\\\\ &amp;=&amp;\\frac{1}{m^2}\\sum_{j=1}^{n} m_j^2 (\\sigma^2(\\theta)/m_j)=\\frac{\\sigma^2(\\theta)}{m^2}\\sum_{j=1}^{n} m_j=\\sigma^2(\\theta)/m.\\\\ \\end{eqnarray*}\\] Dimana Buhlmann-Straub credibility adalah: \\[\\begin{equation}\\hat{\\mu}(\\theta)=Z\\bar{X}+(1-Z)\\mu \\tag{9.9} \\end{equation}\\] Dengan : \\[\\begin{eqnarray*} \\theta&amp;=&amp;\\textrm{a risk parameter that identifies a policyholder&#39;s risk level}\\\\ \\hat{\\mu}(\\theta)&amp;=&amp;\\textrm{estimated expected loss for one exposure for the policyholder}\\\\ &amp; &amp; \\textrm{with loss experience } \\bar{X}\\\\ \\bar{X}&amp;=&amp; \\frac{1}{m} \\sum_{j=1}^{n} m_j X_j \\textrm{ is the average loss per exposure for $m$ exposures.}\\\\ &amp; &amp; \\textrm{$X_j$ is the average loss per exposure and $m_j$ is the number of exposures in year $j$.} \\\\ Z&amp;=&amp;\\textrm{credibility assigned to $m$ exposures } \\\\ \\mu&amp;=&amp;\\textrm{expected loss for one exposure for randomly chosen}\\\\ &amp; &amp; \\textrm{ policyholder from population.}\\\\ \\end{eqnarray*}\\] Perlu diperhatikan bahwa \\(μ^(θ)\\) merupakan estimator untuk kerugian yang diharapkan untuk satu paparan. Jika pemegang polis memiliki mj paparan maka kerugian yang diharapkan adalah \\(mjμ^(θ)\\). "],["insurance-portfolio-management-including-reinsurance.html", "Bab 10 Insurance Portfolio Management including Reinsurance 10.1 Introduction to Insurance Portfolios 10.2 Tails of Distributions 10.3 Risk Measures 10.4 Reinsurance", " Bab 10 Insurance Portfolio Management including Reinsurance Portofolio asuransi adalah kumpulan kontrak asuransi. Untuk membantu mengelola ketidakpastian portofolio, bab ini akan membahas mengenai: Menghitung kewajiban yang luar biasa besar dengan memeriksa bagian ekor dari distribusi, Menghitung risiko secara keseluruhan dengan memperkenalkan ringkasan yang dikenal sebagai ukuran risiko, dan Membahas opsi-opsi penyebaran risiko portofolio melalui reasuransi, yaitu pembelian proteksi asuransi oleh perusahaan asuransi. 10.1 Introduction to Insurance Portfolios Kontrak merupakan perjanjian antara pemegang polis dan perusahaan asuransi. Penanggung, dan mengelola, portofolio yang merupakan kumpulan kontrak. Seperti di bidang keuangan lainnya, ada pilihan pengambilan keputusan manajemen yang hanya terjadi di tingkat portofolio. Misalnya, pengambilan keputusan strategis tidak terjadi di tingkat kontrak. Itu terjadi di ruang konferensi, di mana manajemen meninjau data yang tersedia dan mungkin mengarahkan arah baru. Dari perspektif portofolio, perusahaan asuransi ingin melakukan perencanaan kapasitas, menetapkan kebijakan manajemen, dan menyeimbangkan bauran produk yang dipesan untuk meningkatkan pendapatan sambil mengendalikan volatilitas. Secara konseptual bahwa perusahaan asuransi tidak lebih dari sebuah kumpulan atau portofolio, kontrak asuransi. Pada Bab 5 telah mempelajari tentang pemodelan portofolio asuransi sebagai jumlah kontrak individu berdasarkan asumsi independensi antar kontrak. Karena pentingnya hal tersebut, bab ini berfokus langsung pada distribusi portofolio. Portofolio asuransi (Kumpulan, atau agregasi, kontrak asuransi) mewakili kewajiban perusahaan asuransi dengan membahas probabilitas hasil yang besar dengan menggunakan gagasan distribusi heavy-tail Portofolio asuransi mewakili kewajiban perusahaan sehingga perusahaan asuransi menyimpan aset dalam jumlah yang setara untuk memenuhi kewajiban tersebut. 10.2 Tails of Distributions Pada subab ini akan membahas mengenai: Menggambarkan distribusi ekor berat secara intuitif. Mengklasifikasikan berat ekor distribusi berdasarkan momen. Membandingkan ekor dari dua distribusi. Dalam statistik, distribusi tails adalah distribusi yang memiliki “ekor” panjang yang perlahan mengecil menuju akhir distribusi. Distribusi tails sangat penting untuk dipahami dalam bisnis karena mereka menghadirkan peluang untuk model bisnis yang berputar di sekitar menghasilkan sedikit penjualan untuk banyak produk. “Ekor panjang” dari distribusi adalah istilah yang menunjukkan seberapa cepat ekstrem ini mendekati nol. Untuk distribusi berekor pendek, ekor mendekati nol dengan sangat cepat. Distribusi seperti itu biasanya memiliki tampilan terpotong (“digergaji”). Distribusi berekor pendek klasik adalah distribusi seragam (persegi panjang) di mana probabilitasnya konstan pada rentang tertentu dan kemudian turun menjadi nol di mana pun - kita akan membicarakannya sebagai tidak memiliki ekor, atau ekor yang sangat pendek. Untuk distribusi berekor sedang, ekor menurun ke nol secara moderat. Distribusi berekor sedang klasik adalah distribusi normal (Gaussian). Untuk distribusi berekor panjang, ekor menurun ke nol dengan sangat lambat—dan karenanya orang cenderung melihat probabilitas jauh dari badan distribusi. Distribusi ekor panjang klasik adalah distribusi Cauchy. Dalam hal panjang ekor, histogram yang ditunjukkan di atas akan menjadi karakteristik distribusi “berekor pendek”. Estimator optimal (tidak bias dan paling tepat) untuk lokasi pusat distribusi sangat bergantung pada panjang ekor distribusi. Dalam konteks asuransi, beberapa kerugian besar yang menimpa portofolio dan kemudian dikonversi menjadi klaim biasanya mewakili bagian terbesar dari ganti rugi yang dibayarkan oleh perusahaan asuransi. Kerugian juga disebut ‘ekstrem’, dimodelkan secara kuantitatif oleh ekor dari distribusi probabilitas terkait. Misalnya, periode tekanan pada keuangan dapat muncul dengan frekuensi yang lebih tinggi dari yang diharapkan, dan kerugian asuransi dapat terjadi dengan tingkat keparahan yang lebih buruk. Oleh karena itu, studi tentang perilaku probabilistik pada bagian ekor model aktuaria sangat penting dalam kerangka kerja modern manajemen risiko kuantitatif. Untuk alasan ini, bagian ini dikhususkan untuk pengenalan beberapa gagasan matematika yang mencirikan bobot ekor variabel acak. Secara formal, definisikan X sebagai kewajiban (acak) yang muncul dari kumpulan (portofolio) kontrak asuransi. (Pada bab-bab sebelumnya telah menggunakan S untuk kerugian agregat). Pada bagian ini mempelajari ekor kanan dari distribusi X yang merepresentasikan terjadinya kerugian besar. Secara informal, sebuah variabel acak dikatakan berekor berat jika probabilitas tinggi diberikan pada nilai yang besar. Perhatikan bahwa ini tidak berarti bahwa densitas probabilitas/fungsi massa meningkat ketika nilai X menuju tak terhingga. Memang untuk variabel acak bernilai riil, pdf/pmf harus berkurang hingga tak terhingga untuk menjamin probabilitas total sama dengan satu. Namun, yang menjadi perhatian adalah laju peluruhan pdf/pmf. Hasil yang tidak diinginkan lebih mungkin terjadi pada portofolio asuransi yang digambarkan oleh variabel acak kerugian yang memiliki ekor yang lebih berat (kanan). Bobot ekor dapat berupa konsep absolut atau relatif. Khususnya, untuk yang pertamadapat menganggap variabel acak memiliki ekor yang berat jika sifat matematis tertentu dari distribusi probabilitas terpenuhi. Maka dapat dikatakan ekor dari satu distribusi lebih berat/ringan dari yang lain jika beberapa ukuran ekor lebih besar/kecil. Beberapa pendekatan kuantitatif telah diusulkan untuk mengklasifikasikan dan membandingkan bobot ekor. Di antara sebagian besar pendekatan ini, fungsi kelangsungan hidup berfungsi sebagai blok bangunan. Berikut ini merupakan memperkenalkan dua metode klasifikasi ekor yang sederhana namun berguna, yang keduanya didasarkan pada perilaku fungsi kelangsungan hidup X. 10.2.1 Classification Based on Moments Salah satu cara untuk mengklasifikasikan bobot ekor dari suatu distribusi adalah dengan menilai keberadaan momen-momen sesaae. Karena tujuan utama terletak pada ekor kanan distribusi, maka mengasumsikan variabel acak kewajiban atau kerugian \\(X\\) bernilai positif. Pada awalnya, momen sesaat ke-k dari peubah acak kontinu \\(X\\) yang diperkenalkan pada Bagian 3.1, dapat dihitung sebagai berikut. \\[\\mu_k&#39; = \\int_0^{\\infty} x^k f(x) ~dx = k \\int_0^{\\infty} x^{k-1} S(x) ~dx, \\\\\\] di mana \\(S(\\cdot)\\) menyatakan fungsi survival dari \\(X\\) . Ungkapan ini menekankan bahwa keberadaan momen mentah bergantung pada perilaku asimtotik dari fungsi survival di tak terhingga. Yakni, semakin cepat fungsi survival meluruh ke nol, semakin tinggi orde momen berhingga \\((k)\\) yang dimiliki oleh variabel acak terkait. Anda dapat menafsirkan \\(k^{\\ast}\\) sebagai nilai terbesar dari \\(k\\) sehingga momennya terbatas. Secara formal, definisikan \\(k^{\\ast}=\\sup\\{k &gt; 0:\\mu_k&#39;&lt;\\infty \\}\\) , dimana sup mewakili supremum. Definisi 10.1. Pertimbangkan variabel acak kerugian non-negatif \\(X\\) . Jika semua momen baku positif ada, yaitu orde maksimal dari momen berhingga \\(k^{\\ast}=\\infty\\) , maka \\(X\\) dikatakan berekor ringan berdasarkan metode momen. Jika \\(k^{\\ast} &lt; \\infty\\), maka \\(X\\) dikatakan berekor berat (dikatakan berekor berat jika probabilitas tinggi diberikan pada nilai yang besar) berdasarkan metode momen. Selain itu, untuk dua variabel acak rugi positif \\(X_1\\) dan \\(X_2\\) dengan orde maksimal momen masing-masing \\(k^{\\ast}_1\\) dan \\(k^{\\ast}_1\\), dengan mengatakan \\(X_1\\) memiliki ekor (kanan) yang lebih berat daripada \\(X_2\\) jika \\(k^{\\ast}_1\\leq k^{\\ast}_2\\). bagian pertama dari Definisi 10.1 adalah konsep absolut dari bobot ekor, sedangkan bagian kedua adalah konsep relatif dari bobot ekor yang membandingkan ekor (kanan) di antara dua distribusi. Selanjutnya, kami menyajikan beberapa contoh yang mengilustrasikan aplikasi metode berbasis momen untuk membandingkan bobot ekor. contoh 10.2.1. Sifat ekor ringan dari distribusi gamma. Misalkan \\(X\\sim gamma(\\alpha,\\theta)\\), dengan \\(\\alpha&gt;0\\) dan \\(\\theta&gt;0\\) , maka untuk semua \\(k&gt;0\\) , tunjukkan bahwa \\(\\mu_k&#39; &lt; \\infty\\). \\[\\begin{eqnarray*} \\mu_k&#39; &amp;=&amp; \\int_0^{\\infty} x^k \\frac{x^{\\alpha-1} e^{-x/\\theta}}{\\Gamma(\\alpha) \\theta^{\\alpha}} dx \\\\ &amp;=&amp; \\int_0^{\\infty} (y\\theta)^k \\frac{(y\\theta)^{\\alpha-1} e^{-y}}{\\Gamma(\\alpha) \\theta^{\\alpha}} \\theta dy \\\\ &amp;=&amp; \\frac{\\theta^k}{\\Gamma(\\alpha)} \\Gamma(\\alpha+k) &lt; \\infty. \\end{eqnarray*}\\] karena semua momen positif ada, yaitu \\(k^{\\ast}=\\infty\\), sesuai dengan metode klasifikasi berbasis momen pada Definisi 10.1, maka distribusi gamma berekor ringan Contoh 10.2.2. Sifat ekor ringan dari distribusi Weibull. Misalkan \\(X\\sim Weibull(\\theta,\\tau)\\), dengan \\(\\theta&gt;0\\) dan \\(\\tau&gt;0\\) , maka untuk semua \\(k&gt;0\\) , tunjukkan bahwa \\(\\mu_k&#39; &lt; \\infty\\). \\[\\begin{eqnarray*} \\mu_k&#39; &amp;=&amp; \\int_0^{\\infty} x^k \\frac{\\tau x^{\\tau-1} }{\\theta^{\\tau}} e^{-(x/\\theta)^{\\tau}}dx \\\\ &amp;=&amp; \\int_0^{\\infty} \\frac{ y^{k/\\tau} }{\\theta^{\\tau}} e^{-y/\\theta^{\\tau}}dy \\\\ &amp;=&amp; \\theta^{k} \\Gamma(1+k/\\tau) &lt; \\infty. \\end{eqnarray*}\\] Sekali lagi, karena adanya semua momen positif, distribusi Weibull berekor ringan. distribusi gamma dan Weibull digunakan secara luas dalam praktik aktuaria. Aplikasi dari kedua distribusi ini sangat luas, termasuk, namun tidak terbatas pada, pemodelan tingkat keparahan klaim asuransi, penilaian solvabilitas, pencadangan kerugian, perkiraan risiko agregat, rekayasa keandalan, dan analisis kegagalan. Sejauh ini kami telah melihat dua contoh penggunaan metode berbasis momen untuk menganalisis distribusi ekor ringan. Kami mendokumentasikan contoh distribusi ekor berat sebagai berikut. Contoh 10.2.3. Sifat ekor yang berat dari distribusi Pareto. Misalkan \\(X\\sim Pareto(\\alpha,\\theta)\\) , dengan \\(\\alpha&gt;0\\) dan \\(\\theta&gt;0\\) , maka untuk \\(k&gt;0\\) \\[\\begin{eqnarray*} \\mu_k^{&#39;} &amp;=&amp; \\int_0^{\\infty} x^k \\frac{\\alpha \\theta^{\\alpha}}{(x+\\theta)^{\\alpha+1}} dx \\\\ &amp;=&amp; \\alpha \\theta^{\\alpha} \\int_{\\theta}^{\\infty} (y-\\theta)^k {y^{-(\\alpha+1)}} dy. \\end{eqnarray*}\\] mempertimbangkan integrasi serupa: \\[\\begin{eqnarray*} g_k=\\int_{\\theta}^{\\infty} {y^{k-\\alpha-1}} dy=\\left\\{ \\begin{array}{ll} &lt;\\infty, &amp; \\hbox{for } k&lt;\\alpha;\\\\ =\\infty, &amp; \\hbox{for } k\\geq \\alpha. \\end{array} \\right. \\end{eqnarray*}\\] \\[\\lim_{y\\rightarrow \\infty} \\frac{(y-\\theta)^k {y^{-(\\alpha+1)}}}{y^{k-\\alpha-1}}=\\lim_{y\\rightarrow \\infty} (1-\\theta/y)^{k}=1.\\] Penerapan teorema perbandingan limit untuk integral tak tentu menghasilkan μ′k terbatas jika dan hanya jika gk terbatas. Oleh karena itu, kita dapat menyimpulkan bahwa momen mentah dari variabel acak Pareto hanya ada sampai k &lt; α , yaitu, k∗ = α , dan dengan demikian distribusinya berekor berat. Terlebih lagi, orde maksimal dari momen berhingga hanya bergantung pada parameter bentuk α dan merupakan fungsi yang meningkat dari α . Dengan kata lain, berdasarkan metode momen, bobot ekor dari variabel acak Pareto hanya dimanipulasi oleh α - semakin kecil nilai α , semakin berat bobot ekornya. Karena k∗&lt;∞ , ekor dari distribusi Pareto lebih berat dibandingkan dengan distribusi gamma dan Weibull. 10.2.2 Comparison Based on Limiting Tail Behavior Untuk mengatasi masalah-masalah yang disebutkan di atas pada metode klasifikasi berbasis momen, sebuah pendekatan alternatif untuk membandingkan bobot ekor adalah dengan secara langsung mempelajari perilaku pembatas dari fungsi-fungsi survival. Definisi 10.2. Untuk dua variabel acak \\(X\\) dan \\(Y\\) , misalkan \\[\\gamma=\\lim_{t\\rightarrow \\infty}\\frac{S_X(t)}{S_Y(t)}.\\] Dengan : \\(X\\) memiliki ekor kanan yang lebih berat daripada \\(Y\\) jika \\(\\gamma=\\infty\\); \\(X\\) dan \\(Y\\) secara proporsional ekuivalen pada ekor kanan jika \\(\\gamma =c \\in (0, \\infty)\\); \\(X\\) memiliki ekor kanan yang lebih ringan daripada \\(Y\\) jika \\(\\gamma=0\\). Contoh 10.2.4. Perbandingan distribusi Pareto dan distribusi Weibull. Misalkan \\(X\\sim Pareto(\\alpha, \\theta)\\) dan \\(Y\\sim Weibull(\\tau, \\theta)\\), untuk \\(\\alpha&gt;0\\), \\(\\tau&gt;0\\), dan \\(\\theta&gt;0\\). Tunjukkan bahwa Pareto memiliki ekor kanan yang lebih berat daripada Weibull. \\[\\begin{eqnarray*} \\lim_{t\\rightarrow \\infty}\\frac{S_X(t)}{S_Y(t)} &amp;=&amp; \\lim_{t\\rightarrow \\infty}\\frac{(1+t/\\theta)^{-\\alpha}}{\\exp\\{-(t/\\theta)^{\\tau}\\}} \\\\ &amp;=&amp; \\lim_{t\\rightarrow \\infty}\\frac{\\exp\\{t/\\theta^{\\tau} \\}}{(1+t^{1/\\tau}/\\theta)^{\\alpha}} \\\\ &amp;=&amp; \\lim_{t\\rightarrow \\infty}\\frac{\\sum_{i=0}^{\\infty}\\left(\\frac{t}{\\theta^{\\tau}}\\right)^{i}/i!}{(1+t^{1/\\tau}/\\theta)^{\\alpha}}\\\\ &amp;=&amp; \\lim_{t\\rightarrow \\infty} \\sum_{i=0}^{\\infty} \\left(t^{-i/\\alpha}+\\frac{t^{(1/\\tau-i/\\alpha)}}{\\theta} \\right)^{-\\alpha}/\\theta^{\\tau i}i!\\\\ &amp;=&amp; \\infty. \\end{eqnarray*}\\] Oleh karena itu, distribusi Pareto memiliki ekor yang lebih berat daripada distribusi Weibull. Kita juga dapat menyadari bahwa eksponensial mencapai tak terhingga lebih cepat daripada polinomial, oleh karena itu, batas yang disebutkan di atas haruslah tak terhingga. untuk beberapa distribusi yang fungsi-fungsi kelangsungan hidupnya tidak dapat diekspresikan secara eksplisit, kita dapat menggunakan rumus alternatif berikut ini: \\[\\begin{eqnarray*} \\lim_{t\\to \\infty} \\frac{S_X(t)}{S_Y(t)} &amp;=&amp; \\lim_{t \\to \\infty} \\frac{S_X^{&#39;}(t)}{S_Y^{&#39;}(t)} \\\\ &amp;=&amp; \\lim_{t \\to \\infty} \\frac{-f_X(t)}{-f_Y(t)}\\\\ &amp;=&amp; \\lim_{t\\to \\infty} \\frac{f_X(t)}{f_Y(t)}. \\end{eqnarray*}\\] mengingat bahwa fungsi kepadatannya ada. Ini adalah aplikasi dari Aturan L’Hôpital dari kalkulus Contoh 10.2.5. Perbandingan distribusi Pareto dengan distribusi gamma. Misalkan \\(X\\sim Pareto(\\alpha, \\theta)\\) dan \\(Y\\sim gamma(\\alpha, \\theta)\\), untuk \\(\\alpha&gt;0\\) dan \\(\\theta&gt;0\\) . Tunjukkan bahwa Pareto memiliki ekor kanan yang lebih berat daripada gamma. \\[\\begin{eqnarray*} \\lim_{t\\to \\infty} \\frac{f_{X}(t)}{f_{Y}(t)} &amp;=&amp; \\lim_{t \\to \\infty} \\frac{\\alpha \\theta^{\\alpha} (t+ \\theta)^{-\\alpha-1}}{t^{\\tau-1} e^{-t/\\lambda} \\lambda^{-\\tau} \\Gamma(\\tau)^{-1}} \\\\ &amp;\\propto&amp; \\lim_{t\\to \\infty} \\frac{e^{t/\\lambda}}{(t+\\theta)^{\\alpha+1} t^{\\tau-1}} \\\\ &amp;=&amp; \\infty, \\end{eqnarray*}\\] karena eksponensial menuju tak terhingga lebih cepat daripada polinomial. 10.3 Risk Measures Materi ini akan mempelajari : Mendefinisikan ide koherensi dan menentukan apakah suatu ukuran risiko koheren atau tidak. Mendefinisikan nilai-at-risiko dan menghitung kuantitas ini untuk distribusi tertentu. Mendefinisikan nilai-at-risiko ekor dan menghitung besaran ini untuk distribusi tertentu. Dapat menyatakan bahwa risiko yang terkait dengan satu distribusi lebih berbahaya (secara asimtotik) dibandingkan distribusi lainnya jika ekornya lebih berat. Namun, mengetahui bahwa satu risiko lebih berbahaya (secara asimtotik) daripada risiko lainnya mungkin tidak memberikan informasi yang cukup untuk tujuan manajemen risiko yang canggih, dan sebagai tambahan, kita juga tertarik untuk mengukur seberapa besar risiko tersebut. Faktanya, besarnya risiko yang terkait dengan distribusi kerugian yang diberikan merupakan input penting untuk banyak aplikasi asuransi, seperti penentuan harga aktuaria, pemesanan, lindung nilai, pengawasan peraturan asuransi, dan sebagainya. Menurut pemahaman saya berdasarkan apa yang saya baca dalam beberapa laman mengenai ukuran resiko, Ukuran risiko adalah ukuran statistik yang merupakan prediktor historis risiko dan volatilitas investasi , dan juga merupakan komponen utama dalam teori portofolio modern (MPT). MPT adalah metodologi keuangan dan akademik standar untuk menilai kinerja saham atau dana saham dibandingkan dengan indeks tolok ukurnya . 10.3.1 Coherent Risk Measures Untuk membandingkan besarnya risiko dengan cara yang praktis, kami mencari fungsi yang memetakan variabel acak kerugian yang diminati ke nilai numerik yang menunjukkan tingkat risiko, yang disebut ukuran risiko. Secara matematis, ukuran risiko secara sederhana meringkas fungsi distribusi variabel acak sebagai satu angka. Dua ukuran risiko sederhana adalah rata-rata \\(\\mathrm{E}[X]\\) dan deviasi standar \\(\\mathrm{SD}(X)=\\sqrt{\\mathrm{Var}(X)}\\). Contoh klasik lain dari ukuran risiko termasuk prinsip deviasi standar \\[ \\begin{equation} H_{\\mathrm{SD}}(X)=\\mathrm{E}[X]+\\alpha \\mathrm{SD}(X),\\text{ for } \\alpha\\geq 0, \\tag{10.1} \\end{equation} \\] dan prinsip varians \\[H_{\\mathrm{Var}}(X)=\\mathrm{E}[X]+\\alpha \\mathrm{Var}(X),\\text{ for } \\alpha\\geq 0.\\] Untuk memeriksa bahwa semua fungsi yang disebutkan di atas dapat menggunakan ukuran risiko di mana dengan memasukkan variabel acak kerugian dan fungsi-fungsi tersebut menghasilkan nilai numerik. Dengan catatan yang berbeda, fungsi \\(H^{\\ast}(X)=\\alpha X^{\\beta}\\) untuk setiap \\(\\alpha,\\beta\\neq 0\\) yang bernilai riil, β≠0 bukan merupakan ukuran risiko karena \\(H^{\\ast}\\) menghasilkan variabel acak lain dan bukan nilai numerik tunggal. Karena ukuran risiko adalah ukuran skalar yang bertujuan untuk menggunakan nilai numerik tunggal untuk menggambarkan sifat stokastik dari variabel acak kerugian, maka tidak mengherankan jika tidak ada ukuran risiko yang dapat menangkap semua informasi risiko dari variabel acak yang terkait. Oleh karena itu, ketika mencari ukuran risiko yang berguna, dalam mengingat bahwa ukuran tersebut setidaknya harus dapat ditafsirkan secara praktis; dapat dihitung dengan mudah; dan mampu merefleksikan informasi risiko yang paling penting yang mendasari distribusi kerugian. Beberapa ukuran risiko telah dikembangkan dalam literatur. Namun, tidak ada ukuran risiko terbaik yang dapat mengungguli yang lain, dan pemilihan ukuran risiko yang tepat sangat bergantung pada pertanyaan aplikasi yang dihadapi. Dalam hal ini, sangat penting untuk menekankan bahwa risiko adalah konsep yang subyektif, dan dengan demikian, bahkan dengan masalah yang sama, ada berbagai pendekatan untuk menilai risiko. Namun, untuk banyak aplikasi manajemen risiko, ada kesepakatan luas bahwa ukuran risiko yang masuk akal secara ekonomi harus memenuhi empat aksioma utama yang akan kami jelaskan secara rinci selanjutnya. Ukuran risiko yang memenuhi aksioma-aksioma ini disebut sebagai ukuran risiko yang koheren. Pertimbangkan sebuah ukuran risiko \\(H(\\cdot)\\). Ukuran ini dikatakan sebagai ukuran risiko yang koheren untuk dua variabel acak \\(X\\) dan \\(Y\\) jika aksioma-aksioma berikut ini terpenuhi. Aksioma 1. Subaditifitas: \\(H(X+Y)\\leq H(X)+H(Y)\\) Implikasi ekonomi dari aksioma ini adalah bahwa manfaat diversifikasi ada jika risiko-risiko yang berbeda digabungkan. Aksioma 2. Monotonisitas: jika \\(Pr[X≤Y]=1\\). maka H(X)≤H(Y). Ingat bahwa X dan Y adalah variabel acak yang mewakili kerugian, implikasi ekonomi yang mendasarinya adalah bahwa kerugian yang lebih tinggi pada dasarnya mengarah ke tingkat risiko yang lebih tinggi. Aksioma 3. Homogenitas positif: \\(H(cX) = cH(X)\\) untuk setiap konstanta positif c. Implikasi ekonomi yang potensial dari aksioma ini adalah bahwa ukuran risiko harus independen dari unit moneter yang digunakan untuk mengukur risiko. Sebagai contoh, misalkan c adalah nilai tukar mata uang antara dolar AS dan dolar Kanada, maka risiko kerugian acak yang diukur dalam satuan dolar AS (yaitu, X) dan dolar Kanada (yaitu, \\(cX\\)) seharusnya hanya berbeda sampai dengan nilai tukar c (yaitu, \\(cH(x)=H(cX)\\)). Aksioma 4. Ketidakvariasian terjemahan: \\(H(X + c) = H(X) + c\\) untuk setiap konstanta positif c. Jika konstanta c diinterpretasikan sebagai uang tunai bebas risiko dan X adalah portofolio asuransi, maka penambahan uang tunai ke dalam portofolio hanya meningkatkan risiko portofolio sebesar jumlah uang tunai. Memverifikasi sifat koheren untuk beberapa ukuran risiko bisa sangat mudah, tetapi terkadang sangat menantang. Sebagai contoh, adalah hal yang mudah untuk memeriksa apakah rata-rata adalah ukuran risiko yang koheren. SPECIAL CASE Rata-rata adalah ukuran risiko yang koheren. Untuk setiap pasangan variabel acak X dan Y yang memiliki mean berhingga dan konstanta \\(c&gt;0\\) validasi subaditifitas: \\(E[X+Y]=E[X]+E[Y]\\) validasi monotonitas: jika \\(Pr[X≤Y]=1\\) maka \\(E[X]≤E[Y]\\) validasi homogenitas positif: \\(E[cX]=cE[X]\\) validasi invariansi penerjemahan: \\(E[X+c]=E[X]+c\\) Untuk melihat bahwa deviasi standar bukanlah ukuran risiko yang koheren, mulailah dengan memeriksa apakah deviasi standar memenuhi Verification of the Special Case Untuk melihat bahwa deviasi standar bukanlah ukuran risiko yang koheren, mulailah dengan memeriksa apakah deviasi standar memenuhi validasi subaditifitas: \\[ \\begin{eqnarray*} \\mathrm{SD}[X+Y]&amp;=&amp;\\sqrt{\\mathrm{Var}(X)+\\mathrm{Var}(Y)+2\\mathrm{Cov}(X,Y)}\\\\ &amp;\\leq&amp; \\sqrt{\\mathrm{SD}(X)^2+\\mathrm{SD}(Y)^2+2\\mathrm{SD}(X)\\mathrm{SD}(Y)}\\\\ &amp;=&amp; \\mathrm{SD}(X)+\\mathrm{SD}(Y); \\end{eqnarray*} \\] validasi homogenitas positif: \\(\\mathrm{SD}[cX]=c~\\mathrm{SD}[X]\\) Namun, deviasi standar tidak memenuhi sifat invariansi terjemahan karena untuk setiap konstanta positif c \\(\\mathrm{SD}(X+c)=\\mathrm{SD}(X)&lt;\\mathrm{SD}(X)+c.\\) Selain itu, deviasi standar juga tidak memenuhi sifat monotonitas. Untuk melihat hal ini, pertimbangkan dua variabel acak berikut: \\[ \\begin{eqnarray} X=\\left\\{ \\begin{array}{ll} 0, &amp; \\hbox{with probability $0.25$;} \\\\ 4, &amp; \\hbox{with probability $0.75$,} \\end{array} \\right. \\tag{10.2} \\end{eqnarray} \\] dan Y adalah variabel acak yang merosot sedemikian sehingga \\[ \\begin{eqnarray} \\Pr[Y = 4] = 1. \\tag{10.3} \\end{eqnarray} \\] Dapat Memeriksa \\(\\Pr[X\\leq Y]=1\\), tapi \\[ \\mathrm{SD}(X)=\\sqrt{4^2\\cdot 0.25\\cdot 0.75}=\\sqrt{3}&gt;\\mathrm{SD}(Y)=0 \\] Special Case. The Standard Deviation Principle (10.1) is a Coherent Risk Measure. Untuk tujuan ini, untuk sebuah \\(α&gt;0\\) dapat meriksa empat aksioma untuk \\(H_{SD}(X+Y)\\) satu per satu: validasi subaditifitas : \\[ \\begin{eqnarray*} H_{\\mathrm{SD}}(X+Y) &amp;=&amp; \\mathrm{E}[X+Y]+\\alpha \\mathrm{SD}(X+Y) \\\\ &amp;\\leq&amp; \\mathrm{E}[X]+\\mathrm{E}[Y]+\\alpha [\\mathrm{SD}(X) +\\mathrm{SD}(Y)]\\\\ &amp;=&amp; H_{\\mathrm{SD}}(X)+ H_{\\mathrm{SD}}(Y); \\end{eqnarray*} \\] - validasi homogenitas positif: \\(H_{\\mathrm{SD}}(cX)=c\\mathrm{E}[X]+c\\alpha\\mathrm{SD}(X)=cH_{\\mathrm{SD}}(X);\\) validasi invariansi terjemahan: \\(H_{\\mathrm{SD}}(X+c)=\\mathrm{E}[X]+c+\\alpha\\mathrm{SD}(X)=H_{\\mathrm{SD}}(X)+c.\\) Hanya untuk memverifikasi properti monotonitas, yang mungkin terpenuhi atau tidak, tergantung pada nilai α. Untuk melihat hal ini, mempertimbangkan rumus diatas di mana \\(Pr[X≤Y]=1\\). Dengan memisalkan \\(\\alpha=0.1\\cdot \\sqrt{3}\\) maka \\(H_{\\mathrm{SD}}(X)=3+0.3=3.3&lt; H_{\\mathrm{SD}}(Y)=4\\) dan kondisi monotonitas terpenuhi. Di sisi lain, misalkan \\(\\alpha=\\sqrt{3}\\). maka \\(H_{\\mathrm{SD}}(X)=3+3=6&gt; H_{\\mathrm{SD}}(Y)=4\\) dan kondisi monotonitas tidak terpenuhi. Lebih tepatnya, dengan menetapkan \\[ H_{\\mathrm{SD}}(X) = 3+\\alpha\\sqrt{3} \\leq4= H_{\\mathrm{SD}}(Y) \\] Dapt menemukan bahwa kondisi monotonitas hanya terpenuhi untuk \\(0\\leq\\alpha\\leq 1/\\sqrt{3}\\) dan dengan demikian prinsip deviasi standar \\(H_{SD}\\) adalah koheren. Hasil ini tampak sangat intuitif bagi kami karena prinsip deviasi standar \\(H_{SD}\\) adalah kombinasi linier dari dua ukuran risiko yang satu koheren dan yang lainnya tidak koheren. Jika \\(\\alpha\\leq 1/\\sqrt{3}\\) maka ukuran yang koheren mendominasi ukuran yang tidak koheren, sehingga ukuran yang dihasilkan \\(H_{SD}\\) yang dihasilkan adalah koheren dan sebaliknya. Perlu dicatat bahwa kesimpulan di atas tidak dapat digeneralisasi untuk setiap pasangan variabel acak X dan Y. Literatur mengenai ukuran risiko telah berkembang pesat dalam hal popularitas dan kepentingannya. Dalam dua subbab berikutnya, kami memperkenalkan dua indeks yang baru-baru ini mendapatkan perhatian yang belum pernah terjadi sebelumnya di antara para ahli teori, praktisi, dan regulator. Kedua indeks tersebut adalah Value-at-Risk (\\(VaR\\)) dan Tail Value-at-Risk (\\(TVaR\\)). Alasan ekonomi di balik dua ukuran risiko populer ini mirip dengan metode klasifikasi ekor yang diperkenalkan pada bagian sebelumnya, yang dengannya kami berharap dapat menangkap risiko kerugian ekstrem yang diwakili oleh ekor distribusi. 10.3.2 Value-at-Risk Dengan mempertimbangkan variabel acak kerugian asuransi \\(X\\) . Ukuran nilai-at-risiko dari \\(X\\) dengan tingkat kepercayaan \\(q∈(0,1)\\) dirumuskan sebagai \\[ \\begin{eqnarray} VaR_q[X]=\\inf\\{x:F_X(x)\\geq q\\}. \\tag{10.4} \\end{eqnarray} \\] Di sini, \\(inf\\) adalah operator infimum sehingga ukuran \\(VaR\\) menghasilkan nilai terkecil dari \\(X\\) sedemikian rupa sehingga cdf yang terkait pertama kali melebihi atau sama dengan q . Selanjutnya dapat menginterpretasikan VaR dalam konteks aplikasi aktuarial. VaR adalah ukuran dari ‘kerugian maksimal’ yang mungkin terjadi pada produk asuransi/portfolio atau investasi berisiko, terjadi sebesar q × 100% waktu, selama periode waktu tertentu (biasanya satu tahun). Misalnya, jika X adalah variabel acak kerugian tahunan dari produk asuransi, VaR0.95 [X] = 100 juta berarti tidak lebih dari 5% peluang bahwa kerugian akan melebihi 100 juta selama satu tahun tertentu. Karena interpretasi yang bermakna ini, VaR telah menjadi standar industri untuk mengukur risiko keuangan dan asuransi sejak tahun 1990-an. Konglomerasi keuangan, regulator, dan akademisi sering menggunakan VaR untuk mengukur modal risiko, memastikan kepatuhan dengan aturan regulasi, dan mengungkapkan posisi keuangan. 10.3.2.1 Example 10.3.1. VaR for the exponential distribution Dengan mempertimbangkan variabel acak kerugian asuransi X dengan distribusi eksponensial yang memiliki parameter \\(θ\\) untuk \\(θ&gt;0\\), maka cdf dari \\(X\\) diberikan oleh \\[ F_X(x)=1-e^{-x/\\theta}, \\text{ for } x&gt;0. \\] Mencari ekspresi bentuk tertutup untuk VaR JAWAB Karena distribusi eksponensial adalah distribusi kontinu, nilai terkecil di mana cdf pertama kali melebihi atau sama dengan \\(q ∈ (0,1)\\) harus berada pada titik xq yang memenuhi. \\(q=F_X(x_q)=1-\\exp\\{-x_q/\\theta \\}.\\) Maka \\(VaR_q[X]=F_X^{-1}(q)=-\\theta[\\log(1-q)].\\) Hasil yang didapat pada rumus diatas dapat digeneralisasikan untuk variabel acak kontinu apa pun yang memiliki cdf yang ketat meningkat. Secara khusus, \\(VaR\\) dari variabel acak kontinu mana pun adalah kebalikan dari cdf yang sesuai. Mari kita pertimbangkan contoh lain dari variabel acak kontinu yang memiliki dukungan dari negatif tak terhingga hingga positif tak terhingga. 10.3.3 Example 10.3.2. VaR for the normal distribution. Dengan mempertimbangkan variabel acak kerugian asuransi \\(X\\sim Normal(\\mu,\\sigma^2)\\) dengan \\(σ&gt;0\\). Dalam kasus ini, seseorang dapat menginterpretasikan nilai negatif dari X sebagai keuntungan atau pendapatan. Berikan ekspresi bentuk tertutup untuk VaR. Karena distribusi normal adalah distribusi kontinu, maka VaR dari X harus memenuhi \\[ \\begin{eqnarray*} q &amp;=&amp; F_X(VaR_q[X])\\\\ &amp;=&amp;\\Pr\\left[(X-\\mu)/\\sigma\\leq (VaR_q[X]-\\mu)/\\sigma\\right]\\\\ &amp;=&amp;\\Phi((VaR_q[X]-\\mu)/\\sigma). \\end{eqnarray*} \\] Maka didapat \\[ VaR_q[X]=\\Phi^{-1}(q)\\ \\sigma+\\mu. \\] Dalam banyak aplikasi asuransi, kita harus menangani transformasi dari variabel acak. Misalnya, pada Contoh 10.3.2, variabel acak kerugian \\(X\\sim Normal(\\mu, \\sigma^2)\\) dapat dilihat sebagai transformasi linier dari variabel acak normal standar \\(Z\\sim Normal(0,1)\\), yaitu \\(X=Z\\sigma+\\mu\\). Dengan mengatur \\(μ = 0\\) dan \\(σ = 1\\), Dapar mempermudah untuk memeriksa \\(VaR_q[Z]=\\Phi^{-1}(q).\\). Transformasi linier dari variabel acak normal setara dengan transformasi linier dari VaR dari variabel acak asli. Temuan ini dapat digeneralisasikan lebih lanjut ke variabel acak mana pun selama transformasinya ketat meningkat. 10.3.3.1 Example 10.3.3. VaR for transformed variables. Pertimbangkan variabel acak kerugian asuransi Y dengan distribusi lognormal dengan parameter \\(μ∈R\\) dan \\(σ^2&gt;0\\) Mencari ekspresi \\(VaR\\) dari Y dalam hal invers cdf normal standar. Dengan memperhatikan bahwa \\(\\log Y\\sim Normal(\\mu,\\sigma^2)\\), atau setara dengan membiarkan \\(X\\sim Normal(\\mu,\\sigma^2)\\), maka \\(Y\\overset{d}{=}e^{X}\\) yang merupakan transformasi ketat meningkat. Di sini, notasi \\(\\overset{d}{=}\\) berarti kesamaan dalam distribusi. \\(VaR\\) dari\\(Y\\) diberikan oleh transformasi eksponensial dari VaR dari X. Secara tepat, untuk \\(q∈(0,1)\\), \\(VaR_{q}[Y]= e^{VaR_q[X]}=\\exp\\{\\Phi^{-1}(q)\\ \\sigma+\\mu\\}.\\) Sejauh ini telah melihat beberapa contoh tentang VaR untuk variabel acak kontinu, selanjutnya dengan mempertimbangkan contoh mengenai VaR untuk variabel acak diskrit. 10.3.3.2 Example 10.3.4. VaR for a discrete random variable Dengan mempertimbangkan variabel acak kerugian asuransi dengan distribusi probabilitas sebagai berikut: \\[ {\\small \\Pr[X=x] = \\left\\{ \\begin{array}{ll} 0.75, &amp; \\text{for }x=1 \\\\ 0.20, &amp; \\text{for }x=3 \\\\ 0.05, &amp; \\text{for }x=4. \\end{array} \\right. } \\] Menemukan \\(VaR\\) pada \\(q = 0.6, 0.9, 0.95, 0.95001\\) JAWAB Nilai cdf yang sesuai dari X adalah \\[ F_X(x)=\\left\\{ \\begin{array}{ll} 0, &amp; \\hbox{ $x&lt;1$;} \\\\ 0.75, &amp; \\hbox{ $1\\leq x&lt;3$;} \\\\ 0.95, &amp; \\hbox{ $3\\leq x&lt;4$;} \\\\ 1, &amp; \\hbox{ $4\\leq x$.} \\end{array} \\right. \\] Berdasarkan definisi VaR dengan demikian, maka memiliki \\(VaR_{0.6}[X]=1\\) \\(VaR_{0.9}[X]=3\\) \\(VaR_{0.95}[X]=3\\) \\(VaR_{0.950001}[X]=4\\) Selanjutnya adalah ringkasan dari bagian tentang ukuran VaR. Beberapa keuntungan dalam menggunakan VaR antara lain: memiliki interpretasi yang bermakna secara praktis; relatif mudah untuk dihitung untuk banyak distribusi dengan fungsi distribusi tertutup; tidak ada asumsi tambahan yang diperlukan untuk penghitungan VaR. Namun, ada beberapa keterbatasan dalam penggunaan VaR dalam praktik manajemen risiko, di antaranya adalah: pemilihan tingkat kepercayaan \\(q∈(0,1)\\) sangat subjektif, sementara \\(VaR\\) dapat sangat sensitif terhadap pilihan q skenario/informasi kerugian yang berada di atas \\((1-q)\\times 100\\%\\) peristiwa terburuk, sepenuhnya diabaikan; VaR bukanlah ukuran risiko yang koheren (terutama, ukuran VaR tidak memenuhi aksioma subadditivitas, artinya manfaat diversifikasi mungkin tidak sepenuhnya tercermin). 10.3.4 Tail Value-at-Risk Dengan mengingat bahwa VaR mewakili kerugian maksimal dengan peluang \\((1-q)\\times 100\\%\\). Seperti yang disebutkan pada bagian sebelumnya, satu kelemahan utama dari pengukuran VaR adalah tidak mencerminkan kerugian ekstrem yang terjadi di luar skenario terburuk dengan peluang \\((1-q)\\times 100\\%\\). Untuk tujuan ilustrasi, mari kita pertimbangkan contoh yang sedikit tidak realistis namun inspiratif berikut. 10.3.4.1 Example 10.3.5 Dengan mempertimbangkan dua variabel kerugian, \\(X\\sim Uniform [0,100]\\), dan \\(Y\\) dengan distribusi eksponensial yang memiliki parameter \\(θ = 31,71\\). Kami menggunakan \\(VaR\\) pada tingkat kepercayaan \\(95%\\) untuk mengukur tingkat risiko dari \\(X\\) dan \\(Y\\). Perhitungan sederhana memberikan \\[ VaR_{0.95}[X]=VaR_{0.95}[Y]=95, \\] dan dengan demikian kedua distribusi kerugian ini memiliki tingkat risiko yang sama menurut \\(VaR_{0.95}\\). Namun, Y lebih berisiko daripada X jika kerugian ekstrem menjadi masalah utama karena X dibatasi di atas sedangkan Y tidak dibatasi. Hanya memperkirakan risiko dengan menggunakan VaR pada tingkat kepercayaan tertentu bisa menyesatkan dan mungkin tidak mencerminkan sifat sebenarnya dari risiko. Sebagai solusinya, Tail Value-at-Risk (\\(TVaR\\)) diusulkan untuk mengukur kerugian ekstrem yang berada di atas suatu tingkat \\(VaR\\) tertentu sebagai rata-rata. Kami mendokumentasikan definisi TVaR dalam apa yang mengikuti. Untuk kesederhanaan, kami akan membatasi diri pada variabel acak positif kontinu saja, yang lebih sering digunakan dalam konteks manajemen risiko asuransi. Kami merujuk pembaca yang tertarik ke Hardy (2006) untuk diskusi yang lebih komprehensif tentang TVaR untuk variabel acak diskrit dan kontinu. Menetapkan \\(q ∈ (0,1)\\), nilai Tail Value-at-Risk dari variabel acak (kontinu) X dirumuskan sebagai \\[ \\begin{eqnarray*} TVaR_q[X] &amp;=&amp; \\mathrm{E}[X|X&gt;VaR_q[X]], \\end{eqnarray*} \\] yang diasumsikan harapan eksistensi. Perhitungan \\(TVaR\\) biasanya terdiri dari dua komponen utama - \\(VaR\\) dan rata-rata kerugian yang berada di atas \\(VaR\\). \\(TVaR\\) dapat dihitung melalui sejumlah formula. Pertimbangkan variabel acak positif kontinu X, untuk kenyamanan notional, mari kita tulis \\(\\pi_q=VaR_q[X]\\). Sesuai definisi, \\(TVaR\\) dapat dihitung melalui \\[ \\begin{eqnarray} TVaR_{q}[X]=\\frac{1}{(1-q)}\\int_{\\pi_q}^{\\infty}xf_X(x)dx. \\tag{10.5} \\end{eqnarray} \\] 10.3.4.2 Example 10.3.6. TVaR for a normal distribution Dengan mempertimbangkan variabel acak kerugian asuransi \\(X\\sim Normal (\\mu,\\sigma^2)\\) dengan μ ∈ R dan \\(σ &gt; 0\\). Mencari ekspresi \\(TVaR\\) Biarkan Z adalah variabel acak normal standar. Untuk \\(q∈(0,1)\\) , maka \\(TVaR\\) dari \\(X\\) dapat dihitung melalui \\[ \\begin{eqnarray*} TVaR_q[X] &amp;=&amp; \\mathrm{E}[X|X&gt;VaR_q[X]]\\\\ &amp;=&amp;\\mathrm{E}[\\sigma Z+\\mu|\\sigma Z+\\mu&gt;VaR_q[X]]\\\\ &amp;=&amp; \\sigma\\mathrm{E}[Z|Z&gt;(VaR_q[X]-\\mu)/\\sigma]+\\mu\\\\ &amp;\\overset{(1)}{=}&amp; \\sigma\\mathrm{E}[Z|Z&gt;VaR_q[Z]]+\\mu, \\end{eqnarray*} \\] dimana \\(\\overset{(1)}{=}\\) berlaku karena hasil yang dilaporkan pada contoh diatas. Selanjutnya, kita beralih untuk mempelajari \\(TVaR_q[Z]=\\mathrm{E}[Z|Z&gt;VaR_q[Z]]\\) dengan \\(\\omega(q)=(\\Phi^{-1}(q))^2/2\\), maka kami dapat \\[ \\begin{eqnarray*} (1-q)\\ TVaR_q[Z] &amp;=&amp; \\int_{\\Phi^{-1}(q)}^{\\infty} z \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}dz\\\\ &amp;=&amp; \\int_{\\omega(q)}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-x}dx\\\\ &amp;=&amp; \\frac{1}{\\sqrt{2\\pi}} e^{-\\omega(q)}\\\\ &amp;=&amp; \\phi(\\Phi^{-1}(q)). \\end{eqnarray*} \\] Maka \\(TVaR_q[X]=\\sigma\\frac{\\phi(\\Phi^{-1}(q))}{1-q}+\\mu.\\) Sebelumya telah diseebutkan pada subseksi sebelumnya bahwa VaR dari fungsi acak yang ketat meningkat sama dengan fungsi VaR dari variabel acak asli.Seseorang dapat menunjukkan bahwa TVaR dari transformasi linier variabel acak yang ketat meningkat sama dengan fungsi VaR dari variabel acak asli. Hal ini disebabkan oleh sifat linearitas dari harapan. Namun, temuan tersebut tidak dapat diperluas ke fungsi non-linear. Contoh variabel acak lognormal berikut menjadi contoh yang berlawanan. 10.3.4.3 Example 10.3.7. TVaR of a lognormal distribution Dengan memertimbangkan sebuah variabel acak kerugian asuransi \\(X\\) dengan distribusi lognormal dengan parameter \\(μ∈R\\) dan \\(σ&gt;0\\). dapat menunjukkan \\(TVaR_q[X] = \\frac{e^{\\mu+\\sigma^2/2}}{(1-q)} \\Phi(\\Phi^{-1}(q)-\\sigma).\\) JAWAB Dengan mengingat bahwa pdf dari distribusi lognormal dirumuskan sebagai \\[ f_X(x)=\\frac{1}{\\sigma\\sqrt{2\\pi} x}\\exp\\{-(\\log x-\\mu )^2/2\\sigma^2 \\}, \\text{ for } x&gt;0. \\] Menetapkan \\(q∈(0,1)\\), maka \\(TVaR\\) dari \\(X\\) dapat dihitung melalui \\[ \\begin{eqnarray} TVaR_q[X] &amp;=&amp; \\frac{1}{(1-q)} \\int_{\\pi_q}^{\\infty} x f_X(x)dx \\nonumber\\\\ &amp;=&amp;\\frac{1}{(1-q)} \\int_{\\pi_q}^{\\infty} \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left\\{ -\\frac{(\\log x-\\mu)^2}{2\\sigma^2} \\right\\}dx\\nonumber\\\\ &amp;\\overset{(1)}{=}&amp;\\frac{1}{(1-q)} \\int_{\\omega(q)}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{ -\\frac{1}{2}w^2+\\sigma w+\\mu}dw\\nonumber\\\\ &amp;=&amp;\\frac{e^{\\mu+\\sigma^2/2}}{(1-q)} \\int_{\\omega(q)}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{ -\\frac{1}{2}(w-\\sigma)^2}dw\\nonumber\\\\ &amp;=&amp;\\frac{e^{\\mu+\\sigma^2/2}}{(1-q)} \\Phi(\\omega(q)-\\sigma), \\tag{10.6} \\end{eqnarray} \\] Di sini, \\(\\overset{(1)}{=}\\) terpenuhi dengan menerapkan perubahan variabel \\(w=(logx−μ)/σ\\), dan \\(ω(q)=(logπq−μ)/σ\\). Dengan memanggil rumus \\(VaR\\) untuk variabel acak lognormal dapat menyederhanakan menjadi \\[ \\begin{eqnarray*} TVaR_q[X] &amp;=&amp; \\frac{e^{\\mu+\\sigma^2/2}}{(1-q)} \\Phi(\\Phi^{-1}(q)-\\sigma). \\end{eqnarray*} \\] \\(TVaR\\) dari variabel acak lognormal bukanlah eksponensial dari \\(TVaR\\) dari variabel acak normal. Untuk distribusi di mana fungsi distribusi kelangsungan hidupnya lebih mudah untuk dikerjakan, dapat menerapkan teknik integrasi dengan bagian (asumsikan rata-ratanya terbatas) untuk menulis ulang persamaan (10.5) sebagai \\[ \\begin{eqnarray*} TVaR_{q}[X]&amp;=&amp;\\left[-x S_X(x)\\big |_{\\pi_q}^{\\infty}+\\int_{\\pi_q}^{\\infty}S_X(x)dx\\right]\\frac{1}{(1-q)}\\\\ &amp;=&amp; \\pi_q +\\frac{1}{(1-q)}\\int_{\\pi_q}^{\\infty}S_X(x)dx. \\end{eqnarray*} \\] 10.3.4.4 Example 10.3.8. TVaR of an exponential distribution Pertimbangkan sebuah variabel acak kerugian asuransi X dengan distribusi eksponensial yang memiliki parameter θ untuk \\(θ&gt;0\\). Mencari suatu ekspresi untuk TVaR. JAWAB Dari subseksi sebelumnya, telah melihat bahwa \\(\\pi_q=-\\theta[\\log(1-q)].\\) Dengan mempertimbangkan \\(TVaR\\): \\[ \\begin{eqnarray*} TVaR_q[X] &amp;=&amp; \\pi_q+\\int_{\\pi_q}^{\\infty} e^{-x/\\theta}dx/(1-q)\\\\ &amp;=&amp; \\pi_q+\\theta e^{-\\pi_q/\\theta}/(1-q)\\\\ &amp;=&amp; \\pi_q+\\theta. \\end{eqnarray*} \\] Pengukuran berikut erat kaitannya dengan TVaR Juga dapat membantu untuk menyatakan \\(TVaR\\) dalam bentuk nilai harapan terbatas. Secara khusus, kita memiliki \\[ \\begin{eqnarray} TVaR_q[X] &amp;=&amp; \\int_{\\pi_q}^{\\infty} (x-\\pi_q+\\pi_q)f_X(x)dx/(1-q) \\nonumber\\\\ &amp;=&amp; \\pi_q+\\frac{1}{(1-q)}\\int_{\\pi_q}^{\\infty} (x-\\pi_q)f_X(x)dx\\nonumber\\\\ &amp;=&amp; \\pi_q+e_X(\\pi_q)\\nonumber\\\\ &amp;=&amp; \\pi_q +\\frac{\\left({\\mathrm{E}[X]-\\mathrm{E}[X\\wedge\\pi_q]}\\right)}{(1-q)}, \\tag{10.7} \\end{eqnarray} \\] Dimana \\(e_X(d)=\\mathrm{E}[X-d|X&gt;d]\\) untuk \\(d&gt;0\\) menyatakan fungsi kerugian berlebih rata-rata. Untuk banyak distribusi parametrik yang umum digunakan, rumus-rumus untuk menghitung \\(E[X\\)] dan \\(E[X∧π_q]\\) dapat ditemukan dalam tabel distribusi. 10.3.4.5 Example 10.3.9. TVaR of a Pareto distribution Pertimbangkan sebuah variabel acak kerugian \\(X\\sim Pareto(\\theta,\\alpha)\\) dengan \\(θ&gt;0\\) dan \\(α&gt;0\\). Fungsi distribusi kumulatif (cdf) dari X diberikan oleh \\(F_X(x)=1-\\left(\\frac{\\theta}{\\theta+x} \\right)^{\\alpha}, \\text{ for } x&gt;0 .\\) menetapkan \\(q∈(0,1)\\) dan atur \\(F_X(π_q)=q\\) , maka kita dapat dengan mudah memperoleh \\[ \\begin{eqnarray} \\pi_q=\\theta\\left[(1-q)^{-1/\\alpha}-1 \\right]. \\tag{10.8} \\end{eqnarray} \\] Sebelumnya diketahui bahwa \\[ \\mathrm{E}[X]=\\frac{\\theta}{\\alpha-1}, \\] dan \\[ \\mathrm{E}[X\\wedge \\pi_q]=\\frac{\\theta}{\\alpha-1}\\left[ 1-\\left(\\frac{\\theta}{\\theta+\\pi_q}\\right)^{\\alpha-1} \\right]. \\] Dengan memanfaatkan persamaan sebelumnya menghasilkan \\[ \\begin{eqnarray*} TVaR_q[X] &amp;=&amp; \\pi_q+\\frac{\\theta}{\\alpha-1} \\frac{(\\theta/(\\theta+\\pi_q))^{\\alpha-1}} {(\\theta/(\\theta+\\pi_q))^{\\alpha}}\\\\ &amp;=&amp;\\pi_q +\\frac{\\theta}{\\alpha-1}\\left( \\frac{\\pi_q+\\theta}{\\theta} \\right)\\\\ &amp;=&amp; \\pi_q+\\frac{\\pi_q+\\theta}{\\alpha-1}, \\end{eqnarray*} \\] Menyesuaikan \\(q∈(0,1)\\), nilai risiko bersyarat untuk sebuah variabel acak X diformulasikan sebagai \\[CVaR_q[X] = \\frac{1}{1-q}\\int_{q}^{1} VaR_{\\alpha}[X]\\ d\\alpha .\\] Nilai risiko bersyarat juga dikenal sebagai rata-rata nilai risiko (\\(AVaR\\)) dan kegagalan yang diharapkan (\\(ES\\)). Dapat ditunjukkan bahwa \\(CVaR_q[X] = TVaR_q[X]\\) ketika \\(\\Pr(X=VaR_q[X])=0\\), yang berlaku untuk variabel acak kontinu. Artinya, jika X kontinu, maka melalui perubahan variabel, kita dapat menulis ulang persamaan sebagai \\[ \\begin{eqnarray} TVaR_{q}[X] &amp;=&amp; \\frac{1}{1-q}\\int_{q}^{1} VaR_{\\alpha}[X]\\ d\\alpha. \\tag{10.9} \\end{eqnarray} \\] Formula alternatif (10.9) ini memberitahu bahwa \\(TVaR\\) adalah rata-rata dari \\(VaR_α[X]\\) dengan tingkat kepercayaan yang bervariasi di atas \\(α∈[q,1]\\). Oleh karena itu, \\(TVaR\\) secara efektif menyelesaikan sebagian besar keterbatasan dari VaR yang diuraikan pada subbab sebelumnya. Pertama, karena efek rata-rata, TVaR mungkin kurang sensitif terhadap perubahan tingkat kepercayaan dibandingkan dengan VaR. Kedua, semua kerugian ekstrem yang berada di atas peristiwa terburuk \\((1-q)\\times 100\\%\\) yang paling mungkin dihitung. Dalam hal ini, seseorang dapat melihat bahwa untuk setiap \\(q∈(0,1)\\) \\(TVaR_q[X]\\geq VaR_q[X].\\) Ketiga dan mungkin yang paling penting, TVaR adalah ukuran risiko koheren dan dengan demikian mampu menangkap efek diversifikasi dari portofolio asuransi dengan lebih akurat. 10.4 Reinsurance reasuransi adalah asuransi yang dibeli oleh perusahaan asuransi. Berbeda dengan asuransi yang dibeli oleh individu, reasuransi biasanya dirancang khusus untuk pembeli dan memiliki fleksibilitas kontrak yang lebih besar. Ada dua jenis reasuransi, yaitu reasuransi proporsional dan non-proporsional. Reasuransi proporsional melibatkan persentase tertentu dari kerugian dan premi yang diambil oleh perusahaan reasuransi. Sedangkan reasuransi non-proporsional mencakup kontrak stop-loss dan excess of loss. semua jenis kontrak reasuransi membagi risiko total menjadi dua bagian, yaitu risiko yang ditanggung oleh perusahaan reasuransi dan risiko yang ditahan oleh perusahaan asuransi. Dalam hal ini, \\(X\\) adalah risiko total, \\(Y_{reinsurer}\\) adalah risiko yang ditanggung oleh perusahaan reasuransi, dan \\(Y_{insurer}\\) adalah risiko yang ditahan oleh perusahaan asuransi. Dinyatakan dalam \\(X = Y_{insurer}+Y_{reinsurer}\\) struktur matematika dasar dari sebuah perjanjian reasuransi sama dengan modifikasi cakupan dalam asuransi personal yang diperkenalkan dalam Bab 3. Dalam reasuransi proporsional, transformasi \\(Y_{insurer} = cX\\) identik dengan penyesuaian co-insurance dalam asuransi personal. Dalam reasuransi stop-loss, transformasi \\(Y_{reinsurer} = max(0, X-M)\\) sama dengan pembayaran asuransi dengan nilai retensi (deductible) \\(M\\), sedangkan \\(Y_{insurer} = min(X, M)\\) setara dengan apa yang dibayarkan oleh pemegang polis dengan nilai retensi \\(M\\). Namun, dalam aplikasi praktis matematika, fokus dalam asuransi personal umumnya pada harapan sebagai bahan utama yang digunakan dalam penetapan harga. Sedangkan dalam reasuransi, fokusnya adalah pada seluruh distribusi risiko, karena peristiwa ekstrim menjadi perhatian utama untuk stabilitas keuangan perusahaan asuransi dan reasuransi. 10.4.1 Proportional Reinsurance Jumlah yang dibayarkan oleh perusahaan asuransi utama dan perusahaan reasuransi dituliskan sebagai \\[\\begin{equation} Y_{insurer} = c X \\ \\ \\text{and} \\ \\ \\ Y_{reinsurer} = (1-c) X, \\end{equation}\\] dimana \\(c\\in (0,1)\\) menunjukkan proporsi yang disimpan oleh perusahaan asuransi. Perhatikan bahwa \\(Y_{insurer}+Y_{reinsurer}=X\\) contoh 10.4.1 akan menunjukkan bagaimana perjanjian quota-share mempengaruhi distribusi kerugian melalui demonstrasi \\(r\\) singkat menggunakan simulasi. Gambar yang disertakan memberikan bentuk relatif dari distribusi kerugian total, bagian yang ditahan oleh perusahaan asuransi, dan bagian yang ditanggung oleh perusahaan reasuransi. set.seed(2018) theta = 1000 alpha = 3 nSim = 10000 library(actuar) ## ## Attaching package: &#39;actuar&#39; ## The following objects are masked from &#39;package:VGAM&#39;: ## ## dgumbel, dlgamma, dpareto, pgumbel, plgamma, ppareto, qgumbel, ## qlgamma, qpareto, rgumbel, rlgamma, rpareto ## The following objects are masked from &#39;package:stats&#39;: ## ## sd, var ## The following object is masked from &#39;package:grDevices&#39;: ## ## cm X &lt;- rpareto(nSim, shape = alpha, scale = theta) par(mfrow=c(1,3)) plot(density(X), xlim=c(0,3*theta), ylim=c(0,0.008), main=&quot;Total Loss&quot;, xlab=&quot;Losses&quot;) plot(density(0.75*X), xlim=c(0,3*theta), ylim=c(0,0.008), main=&quot;Insurer (75%)&quot;, xlab=&quot;Losses&quot;) plot(density(0.25*X), xlim=c(0,3*theta), ylim=c(0,0.008), main=&quot;Reinsurer (25%)&quot;, xlab=&quot;Losses&quot;) 10.4.1.1 Quota Share is Desirable for Reinsurers Kontrak bagian kuota (quota share) sangat diinginkan bagi reinsurer. Untuk melihat ini, asumsikan bahwa perusahaan asuransi dan reinsurer ingin memasuki kontrak untuk berbagi total kerugian \\(X\\) \\[\\begin{equation} Y_{insurer}=g(X) \\ \\ \\ \\text{and} \\ \\ \\ \\ Y_{reinsurer}=X-g(X), \\end{equation}\\] Dalam kontrak quota share, diasumsikan ada sebuah fungsi generik \\(g(\\cdot)\\) (dikenal sebagai fungsi retensi) yang digunakan untuk membagi kerugian antara perusahaan asuransi dan reinsurer. Fungsi retensi tersebut harus memastikan bahwa perusahaan asuransi tidak mempertahankan lebih banyak kerugian daripada yang sebenarnya terjadi, sehingga hanya fungsi yang memenuhi \\(g(x) ≤ x\\) yang dianggap. Selanjutnya, diasumsikan bahwa perusahaan asuransi hanya peduli dengan variabilitas klaim yang dipertahankan dan tidak memperdulikan pilihan fungsi \\(g\\) selama \\(variansi (Y_{insurer})\\) tetap sama dan sama dengan \\(Q\\), sebagai contoh. Kemudian, hasil berikut menunjukkan bahwa kontrak reasuransi quota share meminimalkan ketidakpastian reinsurer sebagaimana diukur dengan \\(Var(Y_{reinsurer})\\). Proposisi. Misalkan \\(Var(Y_{insurer}) =Q\\) maka \\(Var((1-c)X) \\le Var (g(X))\\) untuk semua \\(g(\\cdot)\\) sehingga \\(E[g(X)]=K\\) dimana \\(c=Q/Var(X)\\) Proposisi ini memiliki daya tarik secara intuitif - dengan asuransi quota share, reinsurer membagi tanggung jawab untuk klaim yang sangat besar pada ekor distribusi. Ini berbeda dengan perjanjian non-proportional di mana reinsurer bertanggung jawab atas klaim yang sangat besar. 10.4.1.2 Optimizing Quota Share Agreements for Insurers Dalam kasus di mana ada \\(n\\) risiko dalam suatu portofolio, di mana setiap risiko dinyatakan oleh \\(X_i\\), kita dapat mempertimbangkan variasi dari kesepakatan kuota bersama di mana jumlah yang ditahan oleh perusahaan asuransi dapat bervariasi dengan setiap risiko, disebut ci. Dalam hal ini, bagian perusahaan asuransi dari risiko portofolio adalah \\(Y_{insurer} = \\sum_{i=1}^n c_i X_i\\). Kami mencari nilai-nilai \\(c_i\\) yang meminimalkan \\(Var(Y_{insurer})\\) dengan batasan bahwa \\(E(Y_{insurer}) = K\\). Ini berarti bahwa perusahaan asuransi ingin mempertahankan pendapatan setidaknya sebesar konstanta \\(K\\) dan sejalan dengan batasan ini, ingin meminimalkan ketidakpastian risiko yang ditahan dalam hal varians. Kasus ini dapat diterapkan pada berbagai aplikasi di mana risiko individu dapat didefinisikan sebagai risiko kebijakan atau klaim individu atau subportofolio, tergantung pada aplikasi spesifik. Sebagai contoh, perusahaan asuransi dapat membagi portofolionya menjadi subportofolio yang terdiri dari beberapa jenis bisnis, seperti (1) mobil pribadi, (2) mobil komersial, (3) pemilik rumah, (4) kompensasi pekerja, dan lain-lain. Dari hasil perhitungan matematika, diketahui bahwa konstanta untuk risiko ke-\\(i\\), \\(c_i\\), berkorelasi dengan rasio \\(\\frac{E(X_i)}{Var(X_i)}\\). Secara intuitif, jika \\(E(X_i)\\) lebih besar, maka nilai \\(c_i\\) akan semakin besar pula, dan sebaliknya, jika \\(Var(X_i)\\) semakin besar, maka nilai \\(c_i\\) akan semakin kecil. Faktor pengali proporsional ditentukan oleh persyaratan pendapatan \\(E(Y_{insurer})=K\\). Contoh yang diberikan membantu memahami hubungan ini. Contohnya, terdapat tiga risiko yang masing-masing memiliki distribusi Pareto dengan parameter yang berbeda. \\(\\alpha_1 =3\\), _1=1000 untuk resiko pertama \\(X_1\\), \\(\\alpha_2 =3\\), _2=2000 untuk resiko kedua \\(X_2\\), dan \\(\\alpha_3 =4\\), _3=3000 untuk resiko ketiga \\(X_3\\). Grafik disediakan untuk menunjukkan nilai \\(c_1\\), \\(c_2\\), dan \\(c_3\\) untuk pendapatan yang dibutuhkan sebesar \\(K\\). Perlu diperhatikan bahwa nilai-nilai ini meningkat secara linear dengan \\(K\\). solusi: theta1 = 1000; theta2 = 2000; theta3 = 3000; alpha1 = 3; alpha2 = 3; alpha3 = 4; library(actuar) propnfct &lt;- function(alpha,theta){ mu &lt;- mpareto(shape=alpha, scale=theta, order=1) var &lt;- mpareto(shape=alpha, scale=theta, order=2) - mu^2 mu/var } temp &lt;- propnfct(alpha1, theta1)*mpareto(shape=alpha1, scale=theta1, order=1)+ propnfct(alpha2, theta2)*mpareto(shape=alpha2, scale=theta2, order=1)+ propnfct(alpha3, theta3)*mpareto(shape=alpha3, scale=theta3, order=1) KVec &lt;- seq(100, 2500, length.out=20) Lambdavec &lt;- 2*KVec/temp c1 &lt;- propnfct(alpha1, theta1) c2 &lt;- propnfct(alpha2, theta2) c3 &lt;- propnfct(alpha3, theta3) c1Vec &lt;- c2Vec &lt;- c3Vec &lt;- 0*KVec for (j in 1:20) { c1Vec[j] &lt;- (Lambdavec[j]/2) * propnfct(alpha1, theta1) c2Vec[j] &lt;- (Lambdavec[j]/2) * propnfct(alpha2, theta2) c3Vec[j] &lt;- (Lambdavec[j]/2) * propnfct(alpha3, theta3) } plot(KVec, c1Vec, type=&quot;l&quot;, ylab=&quot;proportion&quot;, xlab=&quot;required revenue (K)&quot;, ylim=c(0,1)) lines(KVec, c2Vec) lines(KVec, c3Vec) text(1200,0.80, expression(c[1])) text(2000,0.75, expression(c[2])) text(1500,0.30, expression(c[3])) 10.4.2 Non-Proportional Reinsurance 10.4.2.1 The Optimality of Stop-Loss Insurance Dalam sebuah perjanjian stop-loss, asuransi menetapkan level retensi \\(M (&gt; 0)\\) dan membayar seluruh klaim untuk nilai \\(X ≤ M\\). Selanjutnya, untuk klaim dengan nilai \\(X &gt; M\\), asuransi primer membayar \\(M\\) dan reasuransi membayar sisanya, yaitu \\(X - M\\). Oleh karena itu, asuransi menanggung risiko sebesar \\(M\\). Singkatnya, jumlah yang dibayar oleh asuransi primer dan reasuransi adalah sebagai berikut: \\[\\begin{equation} Y_{insurer} = \\begin{cases} X &amp; \\text{for } X \\le M\\\\ M &amp; \\text{for } X &gt;M \\\\ \\end{cases} \\ \\ \\ \\ = \\min(X,M) = X \\wedge M \\end{equation}\\] dan \\[\\begin{equation} Y_{reinsurer} = \\begin{cases} 0 &amp; \\text{for } X \\le M\\\\ X- M &amp; \\text{for } X &gt;M \\\\ \\end{cases} \\ \\ \\ \\ = \\max(0,X-M) \\end{equation}\\] sama seperti sebelumnya \\(Y_{insurer}+Y_{reinsurer}=X\\) Kontrak tipe stop-loss sangat diinginkan oleh perusahaan asuransi. Dalam hal ini, perusahaan asuransi dan reasuransi ingin memasuki kontrak sehingga \\(Y_{insurer} = g(X)\\) dan \\(Y_{reinsurer} = X - g(X)\\) untuk beberapa fungsi retensi generik \\(g(\\cdot)\\). Dengan asumsi bahwa perusahaan asuransi hanya peduli tentang variabilitas klaim yang disimpan dan tidak peduli dengan pilihan \\(g\\) selama \\(Var(Y_{insurer})\\) dapat diminimalkan. Kembali, kita mengenakan batasan bahwa \\(E(Y_{insurer}) = K\\) ; perusahaan asuransi perlu mempertahankan pendapatan \\(K\\). Dalam rangka untuk memenuhi batasan ini, perusahaan asuransi ingin meminimalkan ketidakpastian risiko yang disimpan (sebagaimana diukur oleh varians). Kemudian, hasil berikut menunjukkan bahwa perjanjian reasuransi stop-loss meminimalkan ketidakpastian reinsurer sebagaimana diukur oleh \\(Var(Y_{reinsurer})\\). Proposisi. Anggaplah \\(E(Y_{insurer})=K\\). Maka, \\(Var(X\\wedge M)\\le Var(g(X))\\) untuk semua \\(g(\\cdot)\\), di mana \\(M\\) adalah nilai sedemikian rupa sehingga \\(E(X\\wedge M)=K\\). 10.4.2.2 Excess of Loss Dalam reasuransi non-proporsional, terdapat jenis polis excess of loss. Dalam polis ini, risiko total \\(X\\) diasumsikan terdiri dari \\(n\\) risiko terpisah \\(X_1,...,X_n\\), dimana masing-masing risiko tersebut memiliki batas atas, misalnya \\(M_i\\). Pada polis ini, perusahaan asuransi menahan risiko \\[\\begin{equation} Y_{insurer} = \\sum_{i=1}^n Y_{i,insurer}, \\ \\ \\ \\ \\text{dengan} \\ \\ \\ \\ \\ Y_{i,insurer} = X_i \\wedge M_i. \\end{equation}\\] Sedangkan, untuk bagian risiko yang melebihi batas tersebut, reinsurer bertanggung jawab menanggungnya, yaitu \\(Y_{reinsurer}=X−Y_{insurer}\\). Batas retensi dapat bervariasi untuk setiap risiko atau dapat sama untuk semua risiko, yaitu \\(M_i=M\\) untuk semua \\(i\\). 10.4.2.3 Optimal Choice for Excess of Loss Retention Limits apa pilihan terbaik dari batas retensi excess of loss \\(M_i\\)? Untuk menjawab pertanyaan ini, kita perlu mencari nilai-nilai \\(M_i\\) yang meminimalkan \\(Var(Y_{insurer})\\) dengan mempertahankan konstrain bahwa \\(E(Y_{insurer})=K\\). Dalam rangka mempertahankan konstrain pendapatan ini, perusahaan asuransi ingin meminimalkan ketidakpastian risiko yang dipertahankan (yang diukur dengan variansinya). Dari perhitungan matematika, terungkap bahwa selisih antara batas retensi dan klaim rata-rata yang diharapkan oleh insurer, \\(M_i−E(X_i\\wedge M_i)\\), adalah sama untuk semua risiko. Hal ini secara intuitif menarik. Contoh 10.4.3. Excess of loss untuk tiga risiko Pareto. Pertimbangkan tiga risiko yang memiliki distribusi Pareto, masing-masing memiliki set parameter yang berbeda (sehingga mereka independen tetapi tidak identik). Gunakan set parameter yang sama seperti pada Contoh 10.4.2. Untuk contoh ini: A. Tunjukkan secara numerik bahwa batas retensi optimal \\(M_1\\), \\(M_2\\), dan \\(M_3\\) (dikurangi klaim rata-rata yang diharapkan oleh insurer, \\(M_i−E(X_i\\wedge M_i))\\) adalah sama untuk semua risiko, seperti yang kita turunkan secara teoritis. B. Selanjutnya, bandingkan secara grafis distribusi total risiko dengan risiko yang disimpan oleh insurer dan oleh reinsurer. solusi: A. mengoptimasi Lagrangian menggunakan paket R alabama untuk Algoritma Minimasi Barrier Adaptif Lagrangian. theta1 = 1000;theta2 = 2000;theta3 = 3000; alpha1 = 3; alpha2 = 3; alpha3 = 4; Pmin &lt;- 2000 library(actuar) VarFct &lt;- function(M){ M1=M[1];M2=M[2];M3=M[3] mu1 &lt;- levpareto(limit=M1,shape=alpha1, scale=theta1, order=1) var1 &lt;- levpareto(limit=M1,shape=alpha1, scale=theta1, order=2)-mu1^2 mu2 &lt;- levpareto(limit=M2,shape=alpha2, scale=theta2, order=1) var2 &lt;- levpareto(limit=M2,shape=alpha2, scale=theta2, order=2)-mu2^2 mu3 &lt;- levpareto(limit=M3,shape=alpha3, scale=theta3, order=1) var3 &lt;- levpareto(limit=M3,shape=alpha3, scale=theta3, order=2)-mu3^2 varFct &lt;- var1 +var2+var3 meanFct &lt;- mu1+mu2+mu3 c(meanFct,varFct) } f &lt;- function(M){VarFct(M)[2]} h &lt;- function(M){VarFct(M)[1] - Pmin} library(alabama) ## Loading required package: numDeriv par0=rep(1000,3) op &lt;- auglag(par=par0,fn=f,hin=h,control.outer=list(trace=FALSE)) Batas retensi optimal \\(M_1\\), \\(M_2\\), dan \\(M_3\\) yang menghasilkan batas retensi dikurangi klaim yang diharapkan dari perusahaan asuransi, \\(M_i-E(X_i\\wedge M_i)\\), sama untuk semua risiko, seperti yang kita dapatkan secara teoritis. M1star = op$par[1];M2star = op$par[2];M3star = op$par[3] M1star -levpareto(M1star,shape=alpha1, scale=theta1,order=1) ## [1] 1344.135 M2star -levpareto(M2star,shape=alpha2, scale=theta2,order=1) ## [1] 1344.133 M3star -levpareto(M3star,shape=alpha3, scale=theta3,order=1) ## [1] 1344.133 B. membandingkan secara grafis distribusi risiko total dengan yang dipertahankan oleh perusahaan asuransi dan perusahaan reasuransi. set.seed(2018) nSim = 10000 library(actuar) Y1 &lt;- rpareto(nSim, shape = alpha1, scale = theta1) Y2 &lt;- rpareto(nSim, shape = alpha2, scale = theta2) Y3 &lt;- rpareto(nSim, shape = alpha3, scale = theta3) YTotal &lt;- Y1 + Y2 + Y3 Yinsur &lt;- pmin(Y1,M1star)+pmin(Y2,M2star)+pmin(Y3,M3star) Yreinsur &lt;- YTotal - Yinsur par(mfrow=c(1,3)) plot(density(YTotal), xlim=c(0,10000), main=&quot;Total Loss&quot;, xlab=&quot;Losses&quot;) plot(density(Yinsur), xlim=c(0,10000), main=&quot;Insurer&quot;, xlab=&quot;Losses&quot;) plot(density(Yreinsur), xlim=c(0,10000), main=&quot;Reinsurer&quot;, xlab=&quot;Losses&quot;) 10.4.3 Additional Reinsurance Treaties 10.4.3.1 Surplus Share Proportional Treaty Jenis perjanjian reasuransi proposional lainnya adalah surplus share, yang umum digunakan dalam asuransi properti komersial. Perjanjian surplus share memungkinkan reinsured untuk membatasi eksposurnya pada risiko dengan jumlah tertentu (retained line). Reinsurer mengambil bagian dari risiko secara proporsional terhadap jumlah nilai yang diasuransikan melebihi retained line, hingga batas tertentu (dinyatakan sebagai kelipatan dari retained line, atau jumlah line). Sebagai contoh, jika retained line adalah 100.000 dan batas yang diberikan adalah 4 line (400.000), maka jika \\(X\\) adalah kerugian, bagian dari reinsurer adalah \\(min(400000,(X−100000)_+)\\). 10.4.3.2 Layers of Coverage Dalam reasuransi non-proporsional stop-loss, kita dapat memperluas kontrak dengan menambahkan pihak lain ke dalam kontrak. Sebagai contoh, selain hanya ada perusahaan asuransi dan reasuransi atau perusahaan asuransi dan pemegang polis, kita bisa mempertimbangkan situasi di mana ada tiga pihak, yaitu pemegang polis, perusahaan asuransi, dan reasuransi, yang sepakat untuk berbagi risiko. Secara umum, kita dapat mempertimbangkan keberadaan \\(k\\) pihak. Jika \\(k = 3\\), maka bisa jadi perusahaan asuransi dan dua reasuransi yang berbeda. Contoh 10.4.4. Lapisan perlindungan untuk tiga pihak. Misalkan ada \\(k = 3\\) pihak. Pihak pertama bertanggung jawab atas klaim pertama senilai 100, pihak kedua bertanggung jawab atas klaim dari 100 hingga 3000, dan pihak ketiga bertanggung jawab atas klaim di atas 3000. Jika ada empat klaim masing-masing senilai 50, 600, 1800, dan 4000, maka klaim tersebut akan dialokasikan ke pihak-pihak sebagai berikut: library(kableExtra) tabel &lt;- data.frame( Buyer = c(&quot;0-100&quot;, &quot;100-3000&quot;, &quot;3000-∞&quot;, &quot;total&quot;), claim_1 = c(50, 0, 0, 50), claim_2 = c(100, 500, 0, 600), claim_3 = c(100, 1700, 0, 1800), claim_4 = c(100, 2900, 1000,4000), total = c(350, 5100, 1000, 6450) ) kable(tabel, align = &quot;c&quot;, caption = &quot;Tabel Klaim&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 10.1: Tabel Klaim Buyer claim_1 claim_2 claim_3 claim_4 total 0-100 50 100 100 100 350 100-3000 0 500 1700 2900 5100 3000-∞ 0 0 0 1000 1000 total 50 600 1800 4000 6450 Untuk menangani situasi umum dengan \\(k\\) kelompok, partisi garis bilangan real positif menjadi \\(k\\) interval menggunakan titik potong \\[\\begin{equation} 0 = M_0 &lt; M_1 &lt; \\cdots &lt; M_{k-1} &lt; M_k = \\infty. \\end{equation}\\] Perhatikan bahwa interval ke-\\(j\\) adalah \\((M_{j−1},M_j]\\). Sekarang biarkan \\(Y_j\\) menjadi jumlah risiko yang dibagi oleh pihak ke-\\(j\\). Untuk mengilustrasikan, jika kerugian \\(x\\) adalah sehingga \\(M_{j−1}&lt;x≤M_j\\), maka \\[\\begin{equation} \\left(\\begin{array}{c} Y_1\\\\ Y_2 \\\\ \\vdots \\\\ Y_j \\\\Y_{j+1} \\\\ \\vdots \\\\Y_k \\end{array}\\right) =\\left(\\begin{array}{c} M_1-M_0 \\\\ M_2-M_1 \\\\ \\vdots \\\\ x-M_{j-1} \\\\ 0 \\\\ \\vdots \\\\0 \\end{array}\\right) \\end{equation}\\] kita dapat menuliskannya sebagai \\[\\begin{equation} Y_j = \\min(X,M_j) - \\min(X,M_{j-1}) . \\end{equation}\\] Dengan ungkapan \\(Y_j=min(X,M_j)−min(X,M_{j−1})\\), kita dapat melihat bahwa pihak ke-\\(j\\) bertanggung jawab atas klaim dalam interval \\((M_{j−1},M_j]\\). Dengan ini, Anda dapat memeriksa bahwa \\(X=Y_1+Y_2+⋯+Y_k\\). Seperti yang ditekankan dalam contoh berikut, kami juga mencatat bahwa pihak-pihak yang terlibat tidak harus berbeda. 10.4.3.3 Portfolio Management Example Banyak variasi kontrak yang mendasar mungkin terjadi. Untuk satu ilustrasi lagi, pertimbangkan yang berikut ini. Contoh 10.4.6. Manajemen Portofolio. Anda adalah Kepala Petugas Risiko dari sebuah perusahaan telekomunikasi. Perusahaan Anda memiliki beberapa risiko properti dan tanggung jawab. Kami akan mempertimbangkan: \\(X_1\\) - gedung, dimodelkan menggunakan distribusi gamma dengan rata-rata 200 dan parameter skala 100. \\(X_2\\) - kendaraan bermotor, dimodelkan menggunakan distribusi gamma dengan rata-rata 400 dan parameter skala 200. \\(X_3\\) - risiko direktur dan pejabat eksekutif, dimodelkan menggunakan distribusi Pareto dengan rata-rata 1000 dan parameter skala 1000. \\(X_4\\) - risiko siber, dimodelkan menggunakan distribusi Pareto dengan rata-rata 1000 dan parameter skala 2000. Sebutkan total risiko sebagai \\(X = X_1 + X_2 + X_3 + X_4\\). Untuk kesederhanaan, Anda mengasumsikan bahwa risiko ini independen. (Nanti, di Bagian 14.6, kami akan mempertimbangkan kasus yang lebih kompleks dari ketergantungan.) Untuk mengelola risiko, Anda mencari perlindungan asuransi. Anda ingin mengelola jumlah bangunan dan kendaraan bermotor yang kecil secara internal, hingga M1 dan M2, masing-masing. Anda mencari asuransi untuk menutupi semua risiko lain. Secara khusus, bagian dari perusahaan asuransi adalah \\[\\begin{equation} Y_{insurer} = (X_1 - M_1)_+ + (X_2 - M_2)_+ + X_3 + X_4 , \\end{equation}\\] sehingga risiko yang Anda tanggung adalah \\(Y_{retained} = X-Y_{insurer} = min(X_1,M_1)+min(X_2,M_2)\\). Menggunakan deductible \\(M_1=100\\) dan \\(M_2=200\\): A. Tentukan jumlah klaim yang diharapkan dari (i) yang ditanggung, (ii) yang diterima oleh asuransi, dan (iii) jumlah keseluruhan. B. Tentukan persentil ke-80, ke-90, ke-95, dan ke-99 untuk (i) yang ditanggung, (ii) yang diterima oleh asuransi, dan (iii) jumlah keseluruhan. C. Bandingkan distribusi dengan memplotting densitas untuk (i) yang ditanggung, (ii) yang diterima oleh asuransi, dan (iii) jumlah keseluruhan. Solusi menyiapkan parameter # For the gamma distributions, use alpha1 &lt;- 2; theta1 &lt;- 100 alpha2 &lt;- 2; theta2 &lt;- 200 # For the Pareto distributions, use alpha3 &lt;- 2; theta3 &lt;- 1000 alpha4 &lt;- 3; theta4 &lt;- 2000 # Limits M1 &lt;- 100 M2 &lt;- 200 dengan parameter ini, selanjutnya mensimulasikan realisasi dari risiko-risiko dalam portofolio # Simulate the risks nSim &lt;- 10000 #number of simulations set.seed(2017) #set seed to reproduce work X1 &lt;- rgamma(nSim,alpha1,scale = theta1) X2 &lt;- rgamma(nSim,alpha2,scale = theta2) # For the Pareto Distribution, use library(actuar) X3 &lt;- rpareto(nSim,scale=theta3,shape=alpha3) X4 &lt;- rpareto(nSim,scale=theta4,shape=alpha4) # Portfolio Risks X &lt;- X1 + X2 + X3 + X4 Yretained &lt;- pmin(X1,M1) + pmin(X2,M2) Yinsurer &lt;- X - Yretained A. hasil dari jumlah ekspektasi klaim # Expected Claim Amounts ExpVec &lt;- t(as.matrix(c(mean(Yretained),mean(Yinsurer),mean(X)))) colnames(ExpVec) &lt;- c(&quot;Retained&quot;, &quot;Insurer&quot;,&quot;Total&quot;) round(ExpVec,digits=2) ## Retained Insurer Total ## [1,] 269.05 2274.41 2543.46 B. hasil untuk kuantilnya # Quantiles quantMat &lt;- rbind( quantile(Yretained, probs=c(0.80, 0.90, 0.95, 0.99)), quantile(Yinsurer, probs=c(0.80, 0.90, 0.95, 0.99)), quantile(X , probs=c(0.80, 0.90, 0.95, 0.99))) rownames(quantMat) &lt;- c(&quot;Retained&quot;, &quot;Insurer&quot;,&quot;Total&quot;) round(quantMat,digits=2) ## 80% 90% 95% 99% ## Retained 300.00 300.00 300.00 300.00 ## Insurer 3075.67 4399.80 6172.69 11859.02 ## Total 3351.35 4675.04 6464.20 12159.02 C. Berikut adalah hasil plot densitas risiko yang dipertahankan, diasuransikan, dan total portofolio. par(mfrow=c(1,3)) plot(density(Yretained), xlim=c(0,500), main=&quot;Retained Portfolio Risk&quot;, xlab=&quot;Loss (Note the different horizontal scale)&quot;, ylab = &quot;Density (Note different vertical scale)&quot;) plot(density(Yinsurer), xlim=c(0,15000), main=&quot;Insurer Portfolio Risk&quot;, xlab=&quot;Loss&quot;) plot(density(X), xlim=c(0,15000), main=&quot;Total Portfolio Risk&quot;, xlab=&quot;Loss&quot;) "],["loss-reserving.html", "Bab 11 Loss Reserving 11.1 Motivation 11.2 Loss Reserve Data 11.3 The Chain-Ladder Method 11.4 GLMs and Bootstrap for Loss Reserves", " Bab 11 Loss Reserving Pratinjau Bab. Bab ini memperkenalkan pereservasi kerugian (juga dikenal sebagai pereservasi klaim) untuk produk asuransi harta benda dan kecelakaan (P&amp;C, atau umum, non-hidup). Secara khusus, bab ini menggambarkan beberapa alat analitik dasar, meskipun penting, untuk menilai cadangan pada portofolio produk asuransi P&amp;C. Pertama, Bagian 11.1 memberikan motivasi tentang perlunya pereservasi kerugian, kemudian Bagian 11.2 mempelajari sumber data yang tersedia dan memperkenalkan beberapa notasi formal untuk menangani pereservasi kerugian sebagai tantangan prediksi. Selanjutnya, Bagian 11.3 membahas metode tangga rantai (chain-ladder method) dan model tangga rantai tanpa distribusi Mack. Bagian 11.4 kemudian mengembangkan pendekatan yang sepenuhnya stokastik untuk menentukan cadangan yang belum diselesaikan dengan menggunakan model linear generalisasi (GLM), termasuk teknik bootstrapping untuk mendapatkan distribusi prediktif cadangan yang belum diselesaikan melalui simulasi. 11.1 Motivation Titik awal adalah masa hidup klaim asuransi P&amp;C. Gambar 11.1 menggambarkan perkembangan klaim tersebut dari waktu ke waktu dan mengidentifikasi peristiwa yang menarik perhatian: Kejadian yang diasuransikan atau kecelakaan terjadi pada waktu tocc. Kejadian ini dilaporkan kepada perusahaan asuransi pada waktu trep, setelah beberapa keterlambatan. Jika klaim yang diajukan diterima oleh perusahaan asuransi, pembayaran akan dilakukan untuk mengganti kerugian keuangan pemegang polis. Dalam contoh ini, perusahaan asuransi mengganti kerugian yang terjadi dengan pembayaran kerugian pada waktu t1, t2, dan t3. Pada akhirnya, klaim diselesaikan atau ditutup pada waktu tset. Seringkali klaim tidak langsung diselesaikan karena adanya keterlambatan dalam pelaporan klaim, keterlambatan dalam proses penyelesaian, atau keduanya. Keterlambatan pelaporan adalah waktu yang berlalu antara terjadinya kejadian yang diasuransikan dan pelaporan kejadian ini kepada perusahaan asuransi. Waktu antara pelaporan dan penyelesaian klaim dikenal sebagai keterlambatan penyelesaian. Misalnya, secara intuitif dapat dipahami bahwa klaim kerusakan material atau properti diselesaikan lebih cepat daripada klaim cedera tubuh yang melibatkan jenis cedera yang kompleks. Klaim yang ditutup juga dapat dibuka kembali karena perkembangan baru, misalnya cedera yang membutuhkan perawatan tambahan. Secara keseluruhan, perkembangan klaim biasanya membutuhkan waktu. Adanya keterlambatan ini dalam penyelesaian klaim membutuhkan perusahaan asuransi untuk memiliki modal guna menyelesaikan klaim-kilaim ini di masa depan. 11.1.1 Closed, IBNR, and RBNS Claims Berdasarkan status run-off klaim, kami membedakan tiga jenis klaim dalam catatan perusahaan asuransi. Jenis pertama adalah klaim yang ditutup. Untuk klaim-klaim ini, perkembangan lengkap telah diamati. Dengan garis merah pada Gambar 11.2 menunjukkan saat ini, semua peristiwa dalam perkembangan klaim terjadi sebelum saat ini. Oleh karena itu, peristiwa-peristiwa ini diamati pada saat ini. Untuk kenyamanan, kami akan mengasumsikan bahwa klaim yang ditutup tidak dapat dibuka kembali. Klaim RBNS adalah klaim yang Sudah Dilaporkan, Tetapi Belum sepenuhnya Diselesaikan pada saat ini atau saat evaluasi (tanggal penilaian), yaitu saat di mana cadangan harus dihitung dan dicatat oleh perusahaan asuransi. Kejadian, pelaporan, dan mungkin beberapa pembayaran kerugian terjadi sebelum saat ini, tetapi penyelesaian klaim terjadi di masa depan, setelah saat ini. Klaim IBNR adalah klaim yang Telah Terjadi di masa lalu, Tetapi Belum Dilaporkan. Untuk klaim seperti ini, kejadian yang diasuransikan telah terjadi, tetapi perusahaan asuransi belum menyadari adanya klaim terkait. Klaim ini akan dilaporkan di masa depan dan perkembangannya yang lengkap (dari pelaporan hingga penyelesaian) terjadi di masa depan. Perusahaan asuransi akan melakukan reservasi modal untuk memenuhi kewajiban masa depan mereka terkait dengan klaim RBNS dan juga klaim IBNR. Perkembangan masa depan dari klaim-klaim tersebut tidak pasti, dan teknik pemodelan prediktif akan digunakan untuk menghitung cadangan yang tepat, berdasarkan data perkembangan historis yang diamati pada klaim-klaim serupa. 11.1.2 Why Reserving? Siklus produksi terbalik dalam pasar asuransi dan dinamika klaim yang digambarkan dalam Bagian 11.1.1 menjadi motivasi untuk perlunya pereservasi dan perancangan alat pemodelan prediktif untuk mengestimasi cadangan. Dalam asuransi, pendapatan premi mendahului biaya. Seorang perusahaan asuransi akan mengenakan premi kepada klien sebelum sebenarnya mengetahui seberapa mahal kebijakan atau kontrak asuransi tersebut akan menjadi. Hal ini berbeda dengan industri manufaktur yang biasanya seorang produsen mengetahui - sebelum menjual produk - berapa biaya produksi produk tersebut. Pada saat evaluasi yang ditentukan τ, perusahaan asuransi akan memprediksi kewajiban yang belum diselesaikan terkait dengan kontrak yang terjual di masa lalu. Ini adalah cadangan klaim atau cadangan kerugian; ini adalah modal yang diperlukan untuk menyelesaikan klaim terbuka dari paparan masa lalu. Ini adalah elemen yang sangat penting dalam neraca perusahaan asuransi, lebih khususnya pada sisi kewajiban neraca perusahaan. 11.2 Loss Reserve Data 11.2.1 From Micro to Macro Sekarang kami akan menjelaskan data yang tersedia untuk mengestimasi cadangan yang belum diselesaikan untuk portofolio kontrak asuransi P&amp;C. Perusahaan asuransi biasanya mencatat data mengenai perkembangan klaim individual seperti yang digambarkan dalam garis waktu di sebelah kiri Gambar 11.5. Kami menyebut data yang tercatat pada tingkat ini sebagai data granular atau mikro-level. Biasanya, seorang aktuaris menggabungkan informasi yang tercatat mengenai perkembangan klaim individual dari seluruh klaim dalam sebuah portofolio. Penggabungan ini menghasilkan data yang terstruktur dalam format segitiga seperti yang ditunjukkan di sebelah kanan Gambar 11.5. Data seperti ini disebut data agregat atau makro-level karena setiap sel di segitiga menampilkan informasi yang diperoleh dengan menggabungkan perkembangan dari beberapa klaim. Tampilan segitiga yang digunakan dalam pereservasi kerugian disebut segitiga run-off atau segitiga perkembangan. Pada sumbu vertikal, segitiga tersebut mencantumkan tahun kejadian atau kecelakaan yang diikuti oleh sebuah portofolio. Pembayaran kerugian yang tercatat untuk klaim tertentu terhubung dengan tahun dimana kejadian yang diasuransikan terjadi. Sumbu horizontal mengindikasikan keterlambatan pembayaran sejak terjadinya kejadian yang diasuransikan. 11.2.2 Run-off Triangles Contoh pertama dari segitiga run-off dengan pembayaran bertambah ditampilkan dalam Gambar 11.6 (diambil dari Wüthrich dan Merz (2008), Tabel 2.2, juga digunakan dalam Wüthrich dan Merz (2015), Tabel 1.4). Tahun kecelakaan (atau tahun kejadian) ditampilkan pada sumbu vertikal dan mulai dari tahun 2004 hingga 2013. Ini merujuk pada tahun dimana kejadian yang diasuransikan terjadi. Sumbu horizontal mengindikasikan keterlambatan pembayaran dalam tahun sejak terjadinya kejadian yang diasuransikan. Keterlambatan 0 digunakan untuk pembayaran yang dilakukan pada tahun terjadinya kecelakaan atau kejadian yang diasuransikan. Keterlambatan satu tahun digunakan untuk pembayaran yang dilakukan pada tahun setelah terjadinya kecelakaan. Sementara segitiga pada Gambar 11.6 menampilkan data pembayaran bertambah, Gambar 11.7 menunjukkan informasi yang sama dalam format akumulatif. Sekarang, sel (2004,1) menampilkan total jumlah klaim yang dibayarkan hingga keterlambatan pembayaran 1 untuk semua klaim yang terjadi pada tahun 2004. Oleh karena itu, itu merupakan jumlah dari jumlah yang dibayarkan pada tahun 2004 dan jumlah yang dibayarkan pada tahun 2005 atas kecelakaan yang terjadi pada tahun 2004. Berbagai informasi dapat disimpan dalam segitiga run-off seperti yang ditunjukkan dalam Gambar 11.6 dan Gambar 11.7. Tergantung pada jenis data yang disimpan, segitiga akan digunakan untuk mengestimasi jumlah yang berbeda. Sebagai contoh, dalam format bertambah, sebuah sel dapat menampilkan: Pembayaran klaim, seperti yang dijelaskan sebelumnya. Jumlah klaim yang terjadi pada tahun tertentu dan dilaporkan dengan keterlambatan tertentu, ketika tujuannya adalah untuk mengestimasi jumlah klaim IBNR. Perubahan jumlah yang terjadi, di mana jumlah klaim yang terjadi adalah jumlah dari klaim yang dibayarkan secara akumulatif dan estimasi kasus. Estimasi kasus adalah perkiraan ahli penangan klaim mengenai jumlah yang belum diselesaikan dalam suatu klaim. Dalam format akumulatif, sebuah sel dapat menampilkan: Jumlah pembayaran akumulatif, seperti yang dijelaskan sebelumnya. Total jumlah klaim dari tahun kejadian, dilaporkan hingga keterlambatan tertentu. Jumlah klaim yang terjadi. Sumber informasi lainnya juga mungkin tersedia, misalnya kovariat (seperti jenis klaim), informasi eksternal (seperti inflasi, perubahan dalam regulasi). Sebagian besar metode pereservasi klaim yang dirancang untuk segitiga run-off didasarkan pada satu sumber informasi, meskipun kontribusi terbaru fokus pada penggunaan data yang lebih terperinci untuk pereservasi kerugian. 11.2.3 Loss Reserve Notation Run-off Triangles Untuk memformalkan tampilan yang ditunjukkan dalam Gambar 11.6 dan 11.7, kita akan menggunakan notasi sebagai berikut. Kita mengasosiasikan i dengan tahun kejadian atau tahun kecelakaan, yaitu tahun di mana kejadian yang diasuransikan terjadi. Dalam notasi kita, tahun kecelakaan pertama yang dipertimbangkan dalam portofolio ditandai dengan 1, dan tahun kecelakaan terakhir yang paling baru ditandai dengan I . Selanjutnya, j merujuk pada keterlambatan pembayaran atau tahun perkembangan, di mana keterlambatan yang sama dengan 0 sesuai dengan tahun kecelakaan itu sendiri. Gambar 11.8 menunjukkan sebuah segitiga di mana jumlah tahun yang sama dipertimbangkan baik pada sumbu vertikal maupun sumbu horizontal, oleh karena itu j berjalan dari 0 hingga \\(J=I−1\\) . Variabel acak \\(X_{ij}\\) menunjukkan klaim bertambah yang dibayarkan dalam periode perkembangan \\(j\\) untuk klaim dari tahun kecelakaan \\(i\\) . Dengan demikian, \\(X_{ij}\\) adalah total jumlah yang dibayarkan pada tahun perkembangan \\(j\\) untuk semua klaim yang terjadi pada tahun kejadian \\(i\\) . Pembayaran ini sebenarnya dilakukan dalam tahun akuntansi atau tahun kalender \\(i+j\\) . Dalam perspektif akumulatif, \\(C_{ij}\\) adalah jumlah kumulatif yang dibayarkan hingga (dan termasuk) tahun perkembangan \\(j\\) untuk kecelakaan yang terjadi pada tahun \\(i\\) . Pada akhirnya, jumlah total \\(C_{ij}\\) dibayarkan pada tahun perkembangan akhir \\(J\\) untuk klaim yang terjadi pada tahun kecelakaan \\(i\\) . Dalam bab ini, waktu diungkapkan dalam tahun, meskipun unit waktu lainnya juga dapat digunakan, misalnya periode enam bulan atau kuartal. The Loss Reserve Pada saat evaluasi \\(\\tau\\) , data pada segitiga atas telah diamati, sedangkan segitiga bawah harus diprediksi. Di sini, saat evaluasi adalah akhir tahun kecelakaan \\(I\\) , yang berarti bahwa sel \\((i,j)\\) dengan \\(i+j \\leq I\\) diamati, dan sel \\((i,j)\\) dengan \\(i+j &gt; I\\) termasuk masa depan dan harus diprediksi. Oleh karena itu, untuk segitiga run-off kumulatif, tujuan metode pereservasi kerugian adalah memprediksi \\(C_i,I−1\\) , jumlah klaim akhir untuk tahun kejadian \\(i\\) , yang sesuai dengan periode perkembangan akhir \\(I−1\\) dalam Gambar 11.7. Kami mengasumsikan bahwa - setelah periode ini tidak akan ada pembayaran lebih lanjut, meskipun asumsi ini bisa dikendurkan. Karena \\(C_{i,I-1}\\) bersifat kumulatif, itu mencakup bagian yang diamati serta bagian yang harus diprediksi. Oleh karena itu, kewajiban yang belum diselesaikan atau cadangan kerugian untuk tahun kecelakaan \\(i\\) adalah \\[\\begin{eqnarray*} \\mathcal{R}^{(0)}_{i} = \\sum_{\\ell=I-i+1}^{I-1} X_{i\\ell} = C_{i,I}-C_{i,I-i}. \\end{eqnarray*}\\] Kami menyatakan cadangan baik sebagai jumlah data bertambah, \\(X_{i\\ell}\\) , maupun sebagai selisih antara angka kumulatif. Dalam kasus terakhir, jumlah yang belum diselesaikan adalah jumlah kumulatif akhir \\(C_{i,I}\\) dikurangi dengan jumlah kumulatif yang diamati paling baru \\(C_{i,I-i}\\) . Mengikuti Wüthrich dan Merz (2015), notasi \\(\\mathcal{R}^{(0)}_{i}\\) mengacu pada cadangan untuk tahun kejadian \\(i\\) di mana \\(i=1,\\ldots,I\\) . Superskrip (0) mengacu pada evaluasi cadangan pada saat ini, katakanlah \\(\\tau = 0\\) . Kami memahami \\(\\tau = 0\\) pada akhir tahun kejadian \\(I\\) , tahun kalender paling baru di mana data diamati dan terdaftar. 11.2.4 R Code to Summarize Loss Reserve Data Kami menggunakan paket ChainLadder (Gesmann et al. 2019) untuk mengimpor segitiga run-off di R dan untuk menjelajahi tren yang ada dalam segitiga tersebut. Vignette paket ini dengan baik mendokumentasikan fungsi-fungsi untuk bekerja dengan data segitiga. Pertama, kami menjelajahi dua cara untuk mengimpor sebuah segitiga. Long Format Data Dataset triangle_W_M_long.txt menyimpan segitiga run-off kumulatif dari Wüthrich dan Merz (2008) (Tabel 2.2) dalam format panjang. Artinya: setiap sel dalam segitiga merupakan satu baris dalam dataset ini, dan tiga fitur disimpan: ukuran pembayaran (kumulatif, dalam contoh ini), tahun kejadian ( \\(i\\) ) dan penundaan pembayaran ( \\(j\\) ). Kami mengimpor file .txt tersebut dan menyimpan data frame hasilnya sebagai my_triangle_long: my_triangle_long &lt;- read.table(&quot;Data/triangle_W_M_long.txt&quot;, header = TRUE) head(my_triangle_long) Kami menggunakan fungsi as.triangle dari paket ChainLadder untuk mengubah data frame menjadi tampilan segitiga. Objek hasilnya, my_triangle,` sekarang merupakan tipe segitiga. my_triangle &lt;- as.triangle(my_triangle_long, origin = &quot;origin&quot;, dev = &quot;dev&quot;, value = &quot;payment&quot;) str(my_triangle) Kami menampilkan segitiga dan mengenali angka-angka (dalam ribuan) dari Gambar 11.7. Sel-sel di segitiga bagian bawah ditandai sebagai tidak tersedia, NA. round(my_triangle/1000, digits = 0) Triangular Format Data Sebagai alternatif, segitiga dapat disimpan dalam file .csv dengan tahun kejadian di baris dan tahun perkembangan di sel-sel kolom. Kami mengimpor file .csv ini dan mengubah my_triangle_csv hasilnya menjadi matriks. my_triangle_csv &lt;- read.csv2(&quot;Data/triangle_W_M.csv&quot;, header = FALSE) my_triangle_matrix &lt;- as.matrix(my_triangle_csv) dimnames(my_triangle_matrix) &lt;- list(origin = 2004 : 2013, dev = 0:(ncol(my_triangle_matrix)-1)) my_triangle &lt;- as.triangle(my_triangle_matrix) round(my_triangle/1000, digits = 0) From Cumulative to Incremental, and vice versa Fungsi R cum2incr() dan incr2cum() memungkinkan kita untuk beralih dengan mudah antara tampilan kumulatif dan tampilan incremental, serta sebaliknya. plot(my_triangle) Sebagai alternatif, argumen lattice menciptakan satu plot per tahun kejadian. plot(my_triangle, lattice = TRUE) Daripada memplot segitiga kumulatif yang disimpan dalam my_triangle, kita dapat memplot segitiga run-off incremental. plot(my_triangle_incr) plot(my_triangle_incr, lattice = TRUE) 11.3 The Chain-Ladder Method Metode yang paling banyak digunakan untuk memperkirakan cadangan kerugian yang belum diselesaikan adalah metode chain-ladder. Asal-usul metode ini tidak jelas tetapi telah terbukti efektif dalam aplikasi praktis pada awal tahun 1970-an, Taylor (1986). Seperti yang akan kita lihat, nama ini merujuk pada penggabungan serangkaian faktor perkembangan (dari tahun ke tahun) menjadi tangga faktor; kerugian yang belum matang naik menuju kedewasaan ketika dikalikan dengan rangkaian rasio ini, maka muncullah deskriptor yang sesuai, yaitu metode chain-ladder. Kita akan memulai dengan menjelajahi metode chain-ladder dalam versi deterministik atau algoritma, tanpa membuat asumsi stokastik apa pun. Kemudian kita akan menggambarkan model chain-ladder distribusi bebas risiko dari Mack. 11.3.1 The Deterministic Chain-Ladder Metode chain-ladder deterministik berfokus pada run-off triangle dalam bentuk kumulatif. Ingatlah bahwa sel \\((i,j)\\) dalam segitiga ini menampilkan jumlah kumulatif yang dibayarkan hingga periode pengembangan \\(j\\) untuk klaim yang terjadi pada tahun \\(i\\). Metode chain-ladder mengasumsikan bahwa faktor pengembangan \\(f_j\\) (juga disebut faktor usia-ke-usia, rasio tautan, atau faktor chain-ladder) ada sehingga: \\[C_{i,j+1} = f_j \\times C_{i,j}.\\] Maka, faktor pengembangan memberitahu Anda bagaimana jumlah kumulatif dalam tahun pengembangan \\(j\\) tumbuh menjadi jumlah kumulatif dalam tahun \\(j+1\\). Kami menyoroti jumlah kumulatif dalam periode 0 dengan warna biru dan jumlah kumulatif dalam periode 1 dengan warna merah pada Gambar 11.10 yang diambil dari Wüthrich dan Merz (2008) (Tabel 2.2, juga digunakan dalam Wüthrich dan Merz (2015), Tabel 1.4). Metode chain-ladder kemudian menyajikan resep yang intuitif untuk memperkirakan atau menghitung faktor pengembangan ini. Karena faktor pengembangan pertama \\(f_0\\) menggambarkan perkembangan jumlah klaim kumulatif dari periode pengembangan 0 hingga periode pengembangan 1, faktor ini dapat diestimasi sebagai rasio dari jumlah kumulatif yang ditandai dengan warna merah dan jumlah kumulatif yang ditandai dengan warna biru, seperti yang terlihat pada Gambar 11.10. Dengan demikian, kita memperoleh perkiraan berikut untuk faktor pengembangan pertama \\(\\hat{f}_0^{CL}\\), dengan observasi \\(\\mathcal{D}_I\\). \\[\\hat{f}^{CL}_{\\color{magenta}{0}} = \\frac{\\sum_{i=1}^{10-\\color{magenta}{0}-1} \\color{red}{C_{i,\\color{magenta}{0}+1}}}{\\sum_{i=1}^{10-\\color{magenta}{0}-1} \\color{blue}{C_{i\\color{magenta}{0}}}}= 1.4925.\\] Perhatikan bahwa indeks i yang digunakan dalam penjumlahan pada pembilang dan penyebut berjalan dari periode kejadian pertama (1) hingga periode kejadian terakhir (9) di mana kedua periode pengembangan 0 dan 1 diamati. Sebagai hasilnya, faktor pengembangan ini mengukur bagaimana data yang ditandai dengan warna biru berkembang menjadi data yang ditandai dengan warna merah, dengan rata-rata di seluruh periode kejadian di mana kedua periode diamati. Metode chain-ladder kemudian menggunakan estimasi faktor pengembangan ini untuk memprediksi jumlah kumulatif \\(C_{10,1}\\) (yaitu jumlah kumulatif yang dibayarkan hingga dan termasuk tahun pengembangan 1 untuk kecelakaan yang terjadi pada tahun ke-10). Prediksi ini diperoleh dengan mengalikan jumlah klaim kumulatif terakhir yang diamati untuk periode kejadian 10 (yaitu \\(C_{10,0}\\) dengan periode pengembangan 0) dengan estimasi faktor pengembangan \\(\\hat{f}^{CL}_0\\). \\[\\hat{C}_{10, 1} = C_{10,0} \\cdot \\hat{f}^{CL}_0 = 5,676\\cdot 1.4925=8,471.\\] Melanjutkan pemikiran ini, faktor pengembangan berikutnya, \\(f_1\\), dapat diestimasi. Karena \\(f_1\\) mencerminkan perkembangan dari periode 1 ke periode 2, dapat diestimasi sebagai rasio angka-angka yang ditandai dengan warna merah dan biru seperti yang ditunjukkan dalam Gambar 11.11. Notasi matematis untuk perkiraan \\(\\hat{f}_1^{CL}\\) dari faktor pengembangan berikutnya \\(f_1\\), dengan mengingat observasi DI, adalah sebagai berikut: \\[\\hat{f}^{CL}_{\\color{magenta}{1}} = \\frac{\\sum_{i=1}^{10-\\color{magenta}{1}-1} \\color{red}{C_{i,\\color{magenta}{1}+1}}}{\\sum_{i=1}^{10-\\color{magenta}{1}-1} \\color{blue}{C_{i\\color{magenta}{1}}}}=1.0778.\\] Dengan demikian, faktor ini mengukur pertumbuhan jumlah yang dibayarkan secara kumulatif pada periode pengembangan 1 menjadi periode 2, dihitung rata-rata untuk semua periode kejadian di mana kedua periode tersebut diamati. Indeks i sekarang berjalan dari periode 1 hingga 8, karena ini adalah periode kejadian di mana kedua periode pengembangan 1 dan 2 diamati. Perkiraan untuk faktor pengembangan kedua ini kemudian digunakan untuk memprediksi sel-sel yang hilang dan tidak teramati pada periode pengembangan 2: \\[\\begin{array}{rl} \\hat{C}_{10,2} &amp;= C_{10,0} \\cdot \\hat{f}^{CL}_0 \\cdot \\hat{f}_1^{CL} = \\hat{C}_{10,1} \\cdot \\hat{f}_1^{CL} = 8,471 \\cdot 1.0778 = 9,130 \\\\ \\hat{C}_{9,2} &amp;= C_{9,1} \\cdot \\hat{f}^{CL}_1 = 7,649 \\cdot 1.0778 = 8,244. \\end{array}\\] Perlu diperhatikan bahwa untuk \\(\\hat{C}_{10,2}\\), Anda sebenarnya menggunakan perkiraan \\(\\hat{C}_{10,1}\\) dan mengalikannya dengan perkiraan faktor pengembangan \\(\\hat{f}_1^{CL}\\). Kita melanjutkannya secara analog dan mendapatkan prediksi berikut, dicetak miring pada Gambar 11.12: Pada akhirnya, kita perlu memperkirakan nilai-nilai pada kolom terakhir. Faktor perkembangan terakhir, f8 , mengukur pertumbuhan dari periode perkembangan 8 ke periode perkembangan 9 dalam segitiga. Karena hanya baris pertama dalam segitiga yang memiliki kedua sel yang diamati, faktor terakhir ini diperkirakan sebagai rasio antara nilai yang berwarna merah dan nilai yang berwarna biru pada Gambar 11.13. Diberikan observasi \\(\\mathcal{D}_I\\), perkiraan faktor ini \\(\\hat{f}^{CL}_{8}\\) adalah sama dengan: \\[\\hat{f}^{CL}_{\\color{magenta}{8}} = \\frac{\\sum_{i=1}^{10-\\color{magenta}{8}-1} \\color{red}{C_{i,\\color{magenta}{8}+1}}}{\\sum_{i=1}^{10-\\color{magenta}{8}-1} \\color{blue}{C_{i\\color{magenta}{8}}}}=1.001.\\] Biasanya, faktor perkembangan terakhir ini mendekati 1 dan oleh karena itu arus kas yang dibayarkan dalam periode pengembangan terakhir relatif kecil. Dengan menggunakan perkiraan faktor pengembangan ini, kita sekarang dapat memperkirakan jumlah klaim kumulatif yang tersisa dalam kolom dengan mengalikan nilai-nilai untuk tahun pengembangan 8 dengan faktor ini. Notasi matematika umum untuk prediksi tangga rantai untuk segitiga bawah \\(( i+j&gt;I )\\) adalah sebagai berikut: \\[\\begin{array}{rl} \\hat{C}_{ij}^{CL} &amp;= C_{i,I-i} \\cdot \\prod_{l=I-i}^{j-1} \\hat{f}_l^{CL} \\\\ \\hat{f}_j^{CL} &amp;= \\frac{\\sum_{i=1}^{I-j-1} C_{i,j+1}}{\\sum_{i=1}^{I-j-1} C_{ij}}, \\end{array}\\] di mana \\(C_{i,I-i}\\) adalah pada diagonal terakhir yang diamati. Jelas bahwa asumsi penting dari metode chain-ladder adalah bahwa perkembangan proporsional klaim dari satu periode pengembangan ke periode berikutnya serupa untuk semua tahun kejadian. Ini menghasilkan Figure 11.14 berikut: angka-angka dalam kolom terakhir menunjukkan perkiraan jumlah klaim akhir. Estimasi jumlah klaim yang tertunda \\(\\hat{\\mathcal{R}}_i^{CL}\\) untuk periode kejadian tertentu \\(i=I-J+1,\\ldots, I\\) kemudian diberikan oleh selisih antara jumlah klaim akhir dan jumlah kumulatif yang diamati pada diagonal terbaru: \\[\\hat{\\mathcal{R}}_i^{CL} =\\hat{C}_{iJ}^{CL}-C_{i,I-i}.\\] Ini adalah estimasi chain-ladder untuk cadangan yang diperlukan untuk memenuhi kewajiban di masa depan terkait klaim yang terjadi dalam periode kejadian ini. Cadangan per periode kejadian dan total yang dijumlahkan dari semua periode kejadian dirangkum dalam Figure 11.15. 11.3.2 Mack’s Distribution-Free Chain-Ladder Model Pada tahap ini, metode chain-ladder tradisional memberikan estimasi titik \\(\\hat{C}^{CL}_{iJ}\\) untuk ramalan \\(C_{iJ}\\) , menggunakan informasi \\(\\mathcal{D}_I\\) . Karena metode chain-ladder adalah algoritma yang sepenuhnya deterministik dan intuitif untuk melengkapi segitiga run-off, kita tidak dapat menentukan seberapa dapat diandalkan estimasi titik tersebut atau memodelkan variasi pembayaran di masa depan. Untuk menjawab pertanyaan-pertanyaan tersebut, diperlukan sebuah model stokastik yang mendasari yang dapat mereproduksi estimasi cadangan chain-ladder. Dalam bagian ini, kami akan fokus pada model chain-ladder bebas distribusi sebagai model stokastik yang mendasar, diperkenalkan dalam Mack (1993). Metode ini memungkinkan kita untuk mengestimasi kesalahan standar dari prediksi chain-ladder. Pada Bagian berikutnya, yaitu Bagian 11.4, model linear umum digunakan untuk mengembangkan pendekatan yang sepenuhnya stokastik untuk memprediksi cadangan yang belum terbayar. Dalam pendekatan Mack, berlaku kondisi-kondisi berikut (tanpa mengasumsikan distribusi): Klaim Kumulatif \\((C_{ij})_{j=0,\\ldots,J}\\) adalah independen di antara periode kejadian yang berbeda i . Terdapat konstanta tetap \\(f_0, \\ldots, f_{J-1}\\) dan \\(\\sigma^2_0,\\ldots, \\sigma^2_{J-1}\\) yang memenuhi untuk semua \\(i=1,\\ldots, I\\) dan \\(j=0,\\ldots,J-1\\): \\[\\begin{array}{rl} E[C_{i,j+1}|C_{i0},\\ldots,C_{ij}] &amp;= f_j \\cdot C_{ij} \\\\ \\text{Var}(C_{i,j+1}|C_{ij}) &amp;= \\sigma^2_j \\cdot C_{ij}. \\end{array}\\] Ini berarti klaim kumulatif \\((C_{ij})_{j=0,\\ldots,J}\\) adalah proses Markov (di periode pengembangan j) dan oleh karena itu masa depan hanya bergantung pada masa sekarang. Dengan asumsi ini, nilai harapan dari jumlah klaim akhir \\(C_{i,J}\\), dengan data yang tersedia di segitiga atas, adalah jumlah kumulatif pada diagonal terbaru \\(C_{i, I-1}\\) dikali dengan faktor pengembangan yang sesuai \\(f_j\\) . Dalam notasi matematika, kita mendapatkan dengan faktor pengembangan yang diketahui \\(f_j\\)dan observasi \\(\\mathcal{D}_I\\) : \\[E[C_{iJ}|\\mathcal{D}_I] = C_{i,I-i} \\prod_{j=I-i}^{J-1} f_j\\] Ini persis apa yang dilakukan metode chain-ladder deterministik, seperti yang dijelaskan di Bagian 11.3.1. Dalam praktiknya, faktor pengembangan tidak diketahui dan perlu diestimasi dari data yang tersedia di segitiga atas. Dalam pendekatan Mack, kita mendapatkan persis ekspresi yang sama untuk mengestimasi faktor pengembangan \\(f_j\\) pada saat \\(I\\) seperti dalam algoritma chain-ladder deterministik: \\[\\hat{f}_j^{CL} =\\frac{\\sum_{j=1}^{I-j-1} C_{i,j+1}}{\\sum_{i=1}^{I-j-1} C_{ij}}.\\] Prediksi untuk sel-sel dalam segitiga bawah (yaitu sel-sel \\(C_{i,j}\\) dimana \\(i+j&gt;I\\)) kemudian diperoleh dengan menggantikan faktor-faktor yang tidak diketahui \\(f_j\\) dengan perkiraan mereka yang sesuai \\(\\hat{f}_j^{CL}\\) : \\[\\hat{C}^{CL}_{ij} = C_{i,I-i}\\prod_{l=I-i}^{j-1} \\hat{f}_l^{CL}.\\] Untuk mengkuantifikasi kesalahan prediksi yang muncul dengan prediksi chain-ladder, Mack juga memperkenalkan parameter-varian \\(\\sigma^2_j\\). Untuk mendapatkan wawasan dalam estimasi parameter-varian ini, diperkenalkan faktor-faktor perkembangan individu \\(f_{i,j}\\) (yang spesifik untuk periode kejadian i ) \\[f_{i,j} = \\frac{C_{i,j+1}}{C_{ij}}.\\] Faktor-faktor perkembangan individu ini juga menggambarkan bagaimana jumlah akumulasi tumbuh dari periode $j $ ke periode \\(j+1\\) , tetapi mereka hanya mempertimbangkan rasio dua sel (daripada mengambil rasio dua jumlah selama semua periode kejadian yang tersedia). Perhatikan bahwa faktor-faktor perkembangan dapat ditulis sebagai rata-rata tertimbang dari faktor-faktor perkembangan individu: \\[\\hat{f}_j^{CL} = \\sum_{i=1}^{I-j-1} \\frac{C_{ij}}{\\sum_{i=1}^{I-j-1} C_{ij}} f_{i,j},\\] Mari kita sekarang mengestimasi parameter-varian \\(\\sigma^2\\) dengan menulis asumsi varians Mack dalam bentuk yang setara. Pertama, varians dari rasio \\(C_{i,j+1}\\) dan \\(c_{i,j}\\) yang bersyarat pada \\(C_{i,0},\\ldots, C_{i,j}\\) berbanding terbalik dengan \\(C_{i,j}\\): \\[\\text{Var}[C_{i,j+1}/C_{ij}|C_{i0},\\ldots,C_{ij}] ~ \\propto ~ \\frac{1}{C_{ij}}.\\] Ini mengingatkan kita pada pengaturan kuadrat terkecil berbobot yang khas di mana bobotnya adalah kebalikan dari variabilitas respons. Oleh karena itu, variabel respons yang lebih volatil atau tidak presisi akan diberi bobot lebih rendah. \\(C_{i,j}\\) berperan sebagai bobotnya. Dengan menggunakan parameter-varian yang tidak diketahui \\(\\sigma^2_j\\) , asumsi varians ini dapat ditulis sebagai: \\[\\text{Var}[C_{i,j+1}|C_{i0},\\ldots,C_{ij}] = \\sigma^2_j \\cdot C_{ij},\\] Koneksi dengan kuadrat terkecil berbobot kemudian secara langsung menghasilkan estimasi tak bias untuk parameter-varian yang tidak diketahui \\(\\sigma^2_j\\) dalam bentuk jumlah kuadrat residu yang diboboti: \\[\\hat{\\sigma}^2_j = \\frac{1}{I-j-2}\\sum_{i=1}^{I-j-1} C_{ij}\\left(\\frac{C_{i,j+1}}{C_{ij}}-\\hat{f}_j^{CL}\\right)^2.\\] 11.3.3 R code for Chain-Ladder Predictions Kami menggunakan objek my_triangle dengan tipe triangle yang dibuat pada Bagian 11.2.4. Model chain-ladder bebas distribusi dari Mack (1993) diimplementasikan dalam paket ChainLadder (Gesmann et al. 2019) (sebagai bentuk khusus dari kuadrat terkecil berbobot) dan dapat diterapkan pada data my_triangle untuk memprediksi jumlah klaim yang belum diselesaikan dan mengestimasi kesalahan standar di sekitar ramalan tersebut. CL &lt;- MackChainLadder(my_triangle) CL round(CL$f,digits = 4) Kita juga dapat mencetak seluruh run-off triangle (termasuk prediksi). MSEP (Mean Squared Error of Prediction) untuk total cadangan melintasi semua periode kejadian diberikan oleh: CL$Total.Mack.S.E^2 Disarankan untuk memvalidasi asumsi Mack dengan memeriksa bahwa tidak ada tren dalam plot residu. Empat plot terakhir yang kita peroleh dengan perintah berikut menunjukkan masing-masing residu standar terhadap nilai yang cocok, periode asal, periode kalender, dan periode pengembangan. plot(CL) Plot bagian kiri atas adalah grafik batang posisi klaim terbaru ditambah IBNR dan kesalahan standar Mack berdasarkan periode kejadian. Plot bagian kanan atas menunjukkan pola perkembangan yang diprediksi untuk semua periode kejadian (dimulai dari 1 untuk periode kejadian tertua). Ketika mengatur argumen lattice=TRUE, kita akan mendapatkan plot perkembangan, termasuk prediksi dan perkiraan kesalahan standar berdasarkan periode kejadian. plot(CL, lattice=TRUE) 11.4 GLMs and Bootstrap for Loss Reserves Bagian ini membahas model regresi untuk menganalisis segitiga run-off. Ketika menganalisis data dalam segitiga run-off dengan model regresi, alat standar untuk pembangunan model, estimasi, dan prediksi tersedia. Dengan menggunakan alat-alat ini, kita dapat melampaui estimasi titik dan kesalahan standar seperti yang dijelaskan di Bagian 11.3. Secara khusus, kita membangun model linier generalisasi (GLM) untuk pembayaran inkremental \\(X_{ij}\\) dalam Gambar 11.6. Sementara metode chain-ladder bekerja dengan data kumulatif, GLM khas mengasumsikan variabel respons menjadi independen dan oleh karena itu bekerja dengan segitiga run-off inkremental. 11.4.1 Model Specification Misalkan \\(X_{ij}\\) menyatakan pembayaran inkremental dalam sel \\((i, j)\\) dari segitiga run-off. Kami mengasumsikan bahwa \\(X_{ij}\\) saling independen dengan kepadatan \\(f(x_{ij};\\theta_{ij},\\phi)\\) dari keluarga distribusi eksponensial. Kami mengidentifikasi: \\(\\mu_{ij}=E[X_{ij}]\\) sebagai nilai harapan dari sel \\(X_{ij}\\), \\(\\phi\\)sebagai parameter dispersi, dan \\(\\text{Var}[X_{ij}]=\\phi \\cdot V(\\mu_{ij})\\) , di mana \\(V(.)\\) adalah fungsi varians \\(\\eta_{ij}\\) sebagai prediktor linear sehingga \\(\\eta_{ij}=g(\\mu_{ij})\\) dengan \\(g\\) sebagai fungsi link. Distribusi dari keluarga eksponensial dan fungsi link default-nya tercantum di http://stat.ethz.ch/R-manual/R-patched/library/stats/html/family.html. Sekarang kami akan membahas tiga GLM khusus yang banyak digunakan untuk penyisihan kerugian. Pertama, model regresi Poisson diperkenalkan dalam Bagian 8.2. Dalam model ini, kami mengasumsikan bahwa Xij memiliki distribusi Poisson dengan parameter \\[\\mu_{ij} = \\pi_i \\cdot \\gamma_j,\\] struktur yang terdiri dari persilangan kelas yang mencakup efek multiplicative dari tahun kejadian \\(i\\) dan periode perkembangan \\(j\\). Struktur model yang diusulkan tidak dapat diidentifikasi tanpa batasan tambahan pada parameter, misalnya \\(\\sum_{j=0}^J \\gamma_j=1\\). Batasan ini memberikan interpretasi eksplisit terhadap \\(\\pi_i\\) (dengan \\(i=1,\\ldots,I\\)) sebagai ukuran paparan atau volume untuk tahun kejadian \\(i\\) dan \\(γ_j\\) sebagai fraksi dari total volume yang dibayarkan dengan penundaan \\(j\\). Namun, saat melakukan kalibrasi GLM di R, batasan alternatif seperti \\(\\pi_1=1\\) atau \\(\\gamma_1=1\\), atau reparametrisasi di mana \\(\\mu_{ij} = \\exp{(\\mu+\\alpha_i+\\beta_j)}\\) lebih mudah diimplementasikan. Kami melanjutkan dengan spesifikasi terakhir tersebut, termasuk \\(\\alpha_1 = \\beta_0 = 0\\), yang dikenal sebagai batasan sudut. GLM ini memperlakukan tahun kejadian dan penundaan pembayaran sebagai variabel faktor dan cocok dengan parameter per tingkat, disamping intercept \\(\\mu\\). Batasan sudut menjadikan efek tingkat pertama variabel faktor sama dengan nol. Asumsi Poisson sangat berguna untuk segitiga run-off dengan jumlah klaim yang dilaporkan, sering digunakan dalam estimasi jumlah klaim IBNR (lihat Bagian 11.2). Kedua, modifikasi menarik dari model regresi Poisson dasar adalah model regresi Poisson yang terdispersi berlebihan di mana \\(Z_{ij}\\) memiliki distribusi Poisson dengan parameter \\(\\mu_{ij}/\\phi\\) dan \\[\\begin{array}{rl} X_{ij} &amp;\\sim \\phi \\cdot Z_{ij} \\\\ \\mu_{ij} &amp;= \\exp{(\\mu + \\alpha_i + \\beta_j)}. \\end{array}\\] sebagai akibatnya, \\(X_{ij}\\) memiliki spesifikasi yang sama untuk rata-ratanya seperti dalam model regresi Poisson dasar, tetapi sekarang \\[\\text{Var}[X_{ij}] = \\phi^2 \\cdot \\text{Var}[Z_{ij}] = \\phi \\cdot \\exp{(\\mu + \\alpha_i + \\beta_j)}.\\] konstruksi ini memungkinkan untuk adanya di bawah (ketika \\(\\phi &lt;1\\)) dan over-dispersion (dengan \\(\\phi &gt;1\\)). Karena \\(X_{ij}\\) tidak lagi mengikuti distribusi yang terkenal, pendekatan ini disebut quasi-likelihood. Ini sangat berguna untuk memodelkan segitiga run-off dengan pembayaran bertambah, karena biasanya mengungkapkan over-dispersion. Ketiga, model regresi gamma relevan untuk memodelkan segitiga run-off dengan pembayaran klaim. Ingat dari Bagian 3.2.1 (lihat juga Lampiran Bab 18) bahwa distribusi gamma memiliki parameter bentuk \\(\\alpha\\) dan parameter skala \\(\\theta\\). Dari ini, kita melakukan reparameterisasi dan mendefinisikan parameter baru \\(\\mu = \\alpha \\cdot \\theta\\) sambil tetap mempertahankan parameter skala \\(\\theta\\). Selanjutnya, anggap bahwa \\(X_{ij}\\) memiliki distribusi gamma dan memperbolehkan \\(\\phi\\) bervariasi berdasarkan \\(ij\\) sehingga \\[\\mu_{ij} = \\exp{(\\mu + \\alpha_i + \\beta_j)}.\\] 11.4.2 Model Estimation and Prediction kami sekarang mengestimasi parameter regresi \\(\\phi\\), \\(\\alpha_{i}\\), dan \\(\\beta_j\\) dalam GLM yang diusulkan. Di R, fungsi glm tersedia untuk mengestimasi parameter-parameter ini melalui estimasi maximum likelihood (mle) atau estimasi quasi-likelihood (dalam kasus Poisson over-dispersed). Dengan adanya estimasi parameter \\(\\hat{\\phi}\\), \\(\\hat{\\alpha_i}\\), dan\\(\\hat{\\beta}_j\\), kita dapat menghasilkan estimasi titik untuk setiap sel dalam segitiga atas. \\[\\hat{X}_{ij} =\\hat{E[X_{ij}]} = \\exp{(\\hat{\\mu}+\\hat{\\alpha}_i+\\hat{\\beta}_j)},\\ \\text{with}\\ i+j\\leq I.\\] \\[\\hat{X}_{ij} = \\hat{E[X_{ij}]} = \\exp{(\\hat{\\mu}+\\hat{\\alpha}_i+\\hat{\\beta}_j)},\\ \\text{with}\\ i+j&gt; I.\\] "],["experience-rating-using-bonus-malus.html", "Bab 12 Experience Rating using Bonus-Malus 12.1 Introduction 12.2 Sistem NCD di Beberapa Negara 12.3 Model Rantai BMS dan Markov 12.4 BMS dan Distribusi Stasioner 12.5 BMS dan Peringkat Premium", " Bab 12 Experience Rating using Bonus-Malus 12.1 Introduction Bonus-malus system, yang digunakan secara bergantian sebagai “diskon tanpa kesalahan”, “peringkat prestasi”, “peringkat pengalaman” atau “diskon tanpa klaim” di berbagai negara, didasarkan pada pemberian sanksi kepada tertanggung yang bertanggung jawab atas satu atau beberapa klaim dengan biaya tambahan premi (malus), dan memberi penghargaan kepada tertanggung dengan diskon premi (bonus) jika tidak memiliki klaim. Sistem bonus-malus dapat dianggap sebagai implementasi komersial dari premi kredibilitas yang ideal seperti yang dibahas dalam Bab 9, di mana premi yang harus dibayar oleh tertanggung dihitung secara individual. Perusahaan asuransi menggunakan sistem bonus-malus untuk dua tujuan utama: untuk mendorong pengemudi agar mengemudi lebih hati-hati dalam satu tahun tanpa klaim, dan untuk memastikan tertanggung membayar premi yang sebanding dengan risiko mereka berdasarkan pengalaman klaim mereka melalui mekanisme peringkat pengalaman. No Claim Discount (NCD) (NCD) adalah sistem peringkat pengalaman yang biasa digunakan dalam asuransi kendaraan bermotor. Sistem ini merupakan upaya untuk mengkategorikan tertanggung ke dalam kelompok-kelompok homogen yang membayar premi berdasarkan pengalaman klaim mereka. Tergantung pada aturan dalam skema, pemegang polis baru mungkin diminta untuk membayar premi penuh pada awalnya, dan mendapatkan diskon di tahun-tahun berikutnya sebagai hasil dari tahun-tahun bebas klaim. Sistem NCD memberikan penghargaan kepada pemegang polis yang tidak mengajukan klaim selama satu tahun. Dengan kata lain, sistem ini memberikan bonus kepada pengemudi yang berhati-hati. Prinsip bonus ini dapat mempengaruhi keputusan pemegang polis untuk mengklaim atau tidak mengklaim, terutama jika melibatkan kecelakaan dengan kerusakan kecil, yang dikenal sebagai fenomena ‘lapar akan bonus’. Fenomena ‘lapar akan bonus’ dalam sistem NCD dapat mengurangi biaya klaim perusahaan asuransi, dan mungkin dapat mengimbangi penurunan pendapatan premi yang diharapkan. 12.2 Sistem NCD di Beberapa Negara 12.2.1 Sistem NCD di Malaysia Sebelum liberalisasi Tarif Motor pada 1 Juli 2017, peringkat asuransi motor di Malaysia diatur oleh Tarif Motor. Di bawah Tarif, tarif yang dikenakan tidak boleh lebih rendah dari tarif yang ditentukan di bawah kelas risiko, untuk memastikan bahwa persaingan harga di antara Penanggung tidak akan pergi di bawah tingkat ekonomi negara. Dasar Faktor peringkat yang dipertimbangkan adalah ruang lingkup asuransi, kapasitas kubik kendaraan dan perkiraan nilai kendaraan (atau uang pertanggungan, mana saja lebih rendah). Di bawah Tarif Motor, premi akhir yang harus dibayar disesuaikan oleh pengalaman klaim pemegang polis, atau setara, hak NCD-nya. Efektif 1 Juli 2017, tarif premi asuransi kendaraan bermotor adalah diliberalisasi, atau dikurangi tarifnya. Harga premi sekarang ditentukan oleh perusahaan asuransi perorangan dan Takaful operator, dan konsumen mampu Nikmati pilihan produk asuransi motor yang lebih luas dengan harga bersaing. Karena liberalisasi tarif mendorong inovasi dan persaingan di antara Penanggung dan operator Takaful, premi didasarkan pada risiko yang lebih luas faktor selain dua faktor peringkat yang ditentukan dalam Tarif Motor, yaitu uang pertanggungan dan kapasitas kubik kendaraan. Faktor peringkat lainnya mungkin didefinisikan dalam profil risiko tertanggung, seperti usia kendaraan, usia pengemudi, fitur keselamatan dan keamanan kendaraan, geografis lokasi kendaraan dan pelanggaran lalu lintas pengemudi. Berbeda Penanggung dan operator takaful memiliki cara berbeda untuk mendefinisikan risiko Profil Tertanggung, harga polis mungkin berbeda dari satu penanggung ke yang lain. Namun, struktur NCD dari Tarif Motor tetap ada ‘tidak berubah’ dan terus ada, dan ‘dapat dipindahtangankan’ dari satu Penanggung, atau dari satu operator Takaful, ke operator Takaful lainnya. Diskon dalam sistem NCD Malaysia dibagi menjadi enam kelas, Mulai dari kelas awal diskon 0%, diikuti oleh kelas Diskon 25%, 30%, 38.3%, 45% dan 55%. Gambar 12.1 menyediakan kelas-kelas Sistem NCD di Malaysia. Tahun bebas klaim menunjukkan bahwa pemegang polis berhak untuk maju selangkah ke kelas diskon berikutnya, seperti dari diskon 0% hingga diskon 25% di tahun perpanjangan. Jika a Pemegang Polis sudah berada di kelas tertinggi, yaitu sebesar 55% diskon, tahun bebas klaim menunjukkan bahwa pemegang polis tetap di kelas yang sama. Di sisi lain, jika satu atau lebih klaim dibuat dalam tahun ini, NCD akan hangus dan pemegang polis harus mulai dari Diskon 0% di tahun perpanjangan. Seperangkat aturan transisi ini juga bisa diringkas sebagai aturan -1 / Top, yaitu, kelas bonus untuk bebas klaim tahun, dan pindah ke kelas tertinggi setelah memiliki satu atau lebih klaim. Untuk tujuan ilustrasi, Gambar 12.1 menunjukkan kelas dan diagram transisi untuk sistem NCD Malaysia. Figure 12.1: (ref:TranMalaysia) 12.2.2 Sistem NCD di Negara Lain Sistem NCD di Brasil dibagi menjadi tujuh kelas, dengan mengikuti tingkat premium (Lemaire dan Zi 1994): 100, 90, 85, 80, 75, 70, dan 65. Tingkat premium ini juga setara dengan diskon berikut Kelas: 0%, 10%, 15%, 20%, 25%, 30% dan 45%. Pemegang polis baru harus: Mulai dari diskon 0%, atau pada tingkat premium 100, dan tahun bebas klaim menunjukkan bahwa pemegang polis dapat bergerak maju dengan diskon satu kelas. Jika satu atau Lebih banyak klaim yang terjadi dalam setahun, pemegang polis harus pindah satu kelas mundur untuk setiap klaim. Gambar 12.2 menunjukkan kelas dan diagram transisi untuk sistem NCD di Brasil. Seperangkat aturan transisi ini juga dapat diringkas sebagai aturan dari -1/+1, yaitu, kelas bonus untuk tahun bebas klaim, dan kelas malus untuk setiap klaim yang dilaporkan. Figure 12.2: (ref:TransitionBrazil) Sistem NCD di Swiss dibagi menjadi dua puluh dua kelas, dengan level premium berikut: 270, 250, 230, 215, 200, 185, 170, 155, 140, 130, 120, 110, 100, 90, 80, 75, 70, 65, 60, 55, 50 dan 45 (Lemaire dan Zi 1994). Level-level ini adalah Juga setara dengan pemuatan berikut (malus): 170%, 150%, 130%, 115%, 100%, 85%, 70%, 55%, 40%, 30%, 20%, dan 10%, dan diskon berikut: 0%, 10%, 20%, 25%, 30%, 35%, 40%, 45%, 50% dan 55%. Pemegang polis baru memiliki untuk memulai dengan diskon 0%, atau pada tingkat premium 100, dan tahun bebas klaim menunjukkan bahwa pemegang polis dapat bergerak maju satu kelas. Jika satu atau lebih Klaim yang terjadi dalam tahun ini, pemegang polis harus pindah empat kelas mundur untuk setiap klaim. Tabel 12.1 dan Gambar 12.3 masing-masing menunjukkan kelas dan diagram transisi untuk sistem NCD di Swiss. Seperangkat aturan transisi ini dapat diringkas sebagai aturan -1/+4. Table 12.1. Classes of NCD (Switzerland) \\[ \\small{ \\begin{array}{*{20}c} \\hline \\text{Classes} &amp; \\text{Loadings } (\\%) &amp; \\text{Classes} &amp; \\text{Discounts } (\\%)\\\\ \\hline {0} &amp; {170} &amp; {12} &amp; {0}\\\\ {1} &amp; {150} &amp; {13} &amp; {10}\\\\ {2} &amp; {130} &amp; {14} &amp; {20}\\\\ {3} &amp; {115} &amp; {15} &amp; {25}\\\\ {4} &amp; {100} &amp; {16} &amp; {30}\\\\ {5} &amp; {85} &amp; {17} &amp; {35}\\\\ {6} &amp; {70} &amp; {18} &amp; {40}\\\\ {7} &amp; {55} &amp; {19} &amp; {45}\\\\ {8} &amp; {40} &amp; {20} &amp; {50}\\\\ {9} &amp; {30} &amp; {21} &amp; {55}\\\\ {10} &amp; {20} &amp;&amp; \\\\ {11} &amp; {10} &amp;&amp; \\\\ \\hline \\end{array} } \\] Figure 12.3: Transition Diagram for NCD Classes (Switzerland) 12.3 Model Rantai BMS dan Markov BMS dapat diwakili oleh waktu diskrit Rantai Markov. Proses stokastik dikatakan memiliki sifat Markov jika evolusi dari proses di masa depan hanya bergantung pada keadaan sekarang tetapi tidak pada masa lalu. Waktu diskrit Markov Chain adalah proses Markov dengan ruang keadaan diskrit. 12.3.1 Probabilitas Transisi Sebuah Rantai Markov ditentukan oleh probabilitas transisinya. Probabilitas transisi dari keadaan \\(i\\) (pada waktu \\(n\\)) ke keadaan \\(j\\) (pada waktu \\(n+1\\)) disebut probabilitas transisi satu langkah, dan dilambangkan dengan \\(p_{ij}(n,n+1) = Pr (X_{n + 1} = j|X_n = i)\\), \\(i = 1,2,\\ldots,k\\), \\(j = 1,2,\\ldots,k\\). Untuk transisi umum dari waktu \\(m\\) ke waktu \\(n\\), untuk \\(m&lt;n\\), dengan mengkondisikan \\(X_{o}\\) untuk \\(m\\le o\\le n\\), kita memiliki persamaan Chapman-Kolmogorov dari \\[\\begin{equation} p_{ij}(m,n)=\\sum_{l\\in S} p_{il}(m,o)p_{lj}(o,n). \\end{equation}\\] Sebuah Rantai Markov yang homogen waktu memenuhi properti \\(p_{ij}(n,n+t)=p_{ij}^{(t)}\\) untuk semua \\(n\\). Sebagai contoh, kita memiliki \\(p_{ij}(n,n+1)=p_{ij}^{(1)}\\equiv p_{ij}\\). Dalam kasus ini, persamaan Chapman-Kolmogorov dapat ditulis sebagai \\[\\begin{equation} p_{ij}(0,m+n)=\\sum_{l\\in S} p_{il}(0,m)p_{lj}(m,m+n)=\\sum_{l\\in S}p_{il}^{(m)}p_{lj}^{(n)}. \\end{equation}\\] Dalam konteks BMS, transisi kelas NCD diatur oleh probabilitas transisi pada tahun tertentu. Transisi kelas NCD juga merupakan Rantai Markov yang homogen waktu karena seperangkat aturan transisi tetap dan tidak bergantung pada waktu. Kita dapat mewakili probabilitas transisi satu langkah dengan a \\(k \\times k\\) transition matrix \\({\\bf P}=(p_{ij})\\) that corresponds to NCD classes \\(0,1,2,\\ldots,k-1\\). \\[ \\small{ {\\bf P} = \\left[ {\\begin{array}{*{20}c} p_{00} &amp; p_{01} &amp; \\ldots &amp; &amp; &amp; p_{0k-1} \\\\ p_{10} &amp; p_{11} &amp; \\ldots &amp; &amp; &amp; p_{1k-1} \\\\ \\vdots &amp; \\ddots &amp; &amp; &amp; &amp; \\vdots \\\\ p_{k-10} &amp; p_{k-11} &amp; \\cdots &amp; &amp; &amp; p_{k-1k-1} \\end{array} } \\right] } \\] Di sini, elemen ke-\\((i,j)\\) adalah probabilitas transisi dari state \\(i\\) ke state \\(j\\). Dengan kata lain, setiap baris dari mewakili transisi mengalir keluar dari state, sedangkan setiap kolom merepresentasikan transisi mengalir ke state. Penjumlahan dari transisi probabilitas mengalir keluar dari keadaan harus sama dengan 1, atau setiap baris dari matriks harus berjumlah 1, yaitu \\(\\sum_jp_{ij} = 1\\). Semua probabilitas juga harus tidak boleh negatif (karena mereka adalah probabilitas), yaitu $ p_{ij} $. Pertimbangkan sistem NCD Malaysia. Misalkan \\(\\{X_{t}:t=0,1,2,\\ldots\\}\\) adalah kelas NCD yang ditempati oleh pemegang polis pada waktu \\(t\\) dengan ruang keadaan \\(S=\\{0,1,2,3,4,5\\}\\). Oleh karena itu, probabilitas transisi pada tahun tanpa klaim sama dengan probabilitas transisi dari state \\(i\\) ke state \\(i+1\\), yaitu \\(p_{ii+1}\\). Jika seorang tertanggung memiliki satu atau lebih klaim dalam satu tahun, probabilitas transisi kembali ke keadaan 0 diwakili oleh \\(p_{i0}=1-p_{ii+1}\\). Oleh karena itu, sistem NCD Malaysia dapat diwakili oleh matriks transisi \\(6\\times 6\\) berikut ini: \\[ \\small{ {\\bf P} = \\left[ {\\begin{array}{*{20}c} p_{00}&amp;p_{01}&amp;0&amp;0&amp;0&amp;0\\\\ p_{10}&amp;0&amp;p_{12}&amp;0&amp;0&amp;0\\\\ p_{20}&amp;0&amp;0&amp;p_{23}&amp;0&amp;0\\\\ p_{30}&amp;0&amp;0&amp;0&amp;p_{34}&amp;0\\\\ p_{40}&amp;0&amp;0&amp;0&amp;0&amp;p_{45}\\\\ p_{50}&amp;0&amp;0&amp;0&amp;0&amp;p_{55} \\end{array} }\\right] = \\left[ {\\begin{array}{*{20}c} {1 - p_{01}}&amp;p_{01}&amp;0&amp;0&amp;0&amp;0\\\\ {1 - p_{12}}&amp;0&amp;p_{12}&amp;0&amp;0&amp;0\\\\ {1 - p_{23}}&amp;0&amp;0&amp;p_{23}&amp;0&amp;0\\\\ {1 - p_{34}}&amp;0&amp;0&amp;0&amp;p_{34}&amp;0\\\\ {1 - p_{45}}&amp;0&amp;0&amp;0&amp;0&amp;p_{45}\\\\ {1 - p_{55}}&amp;0&amp;0&amp;0&amp;0&amp;p_{55} \\end{array} }\\right] } \\] Example 12.3.1. Provide the transition matrix for the NCD system in Brazil. Solusi Solusi Berdasarkan kelas NCD dan diagram transisi yang ditunjukkan pada [Gambar 12.2], probabilitas tahun tanpa klaim sama dengan probabilitas bergerak maju satu kelas, sedangkan probabilitas memiliki satu atau lebih klaim dalam satu tahun sama dengan probabilitas bergerak mundur satu kelas untuk setiap klaim. Oleh karena itu, setiap baris dapat berisi dua atau lebih probabilitas transisi; satu probabilitas untuk maju ke berikutnya, dan satu atau lebih probabilitas untuk mundur satu kelas. Matriks transisi matriks transisi adalah: \\[ \\small{ {\\bf P} = \\left[ {\\begin{array}{*{20}{c}} {1 - p_{01}}&amp;p_{01}&amp;0&amp;0&amp;0&amp;0&amp;0\\\\ {1 - p_{12}}&amp;0&amp;p_{12}&amp;0&amp;0&amp;0&amp;0\\\\ {1 - \\sum_j p_{2j}}&amp;p_{21}&amp;0&amp;p_{23}&amp;0&amp;0&amp;0\\\\ {1 - \\sum_j p_{3j}}&amp;p_{31}&amp;p_{32}&amp;0&amp;p_{34}&amp;0&amp;0\\\\\\ {1 - \\sum_j p_{4j}}&amp;p_{41}&amp;p_{42}&amp;p_{43}&amp;0&amp;p_{45}&amp;0\\\\ {1 - \\sum_j p_{5j}}&amp;p_{51}&amp;p_{52}&amp;p_{53}&amp;p_{54}&amp;0&amp;p_{56}\\\\ {1 - \\sum_j p_{6j}}&amp;p_{61}&amp;p_{62}&amp;p_{63}&amp;p_{64}&amp;p_{65}&amp;p_{66} \\end{array} } \\right] } \\] 12.4 BMS dan Distribusi Stasioner 12.4.1 Distribusi Stasioner Distribusi stasioner dari Rantai Markov adalah distribusi probabilitas yang tetap tidak berubah seiring berjalannya waktu ke masa depan. Ini diwakili oleh vektor baris \\(\\boldsymbol \\pi =(\\pi_{1},\\pi_{2},\\ldots,\\pi_{k})\\) dengan properti berikut: \\[\\begin{align} 0\\le \\pi_{j}\\le 1,\\notag\\\\ \\sum\\limits_{j}\\pi_{j}=1,\\notag\\\\ \\pi_{j}=\\sum\\limits_{i}\\pi_{i}p_{ij}. \\end{align}\\] Persamaan terakhir dapat ditulis sebagai \\(\\boldsymbol\\pi \\bf P=\\boldsymbol\\pi\\). Dua kondisi pertama adalah diperlukan untuk distribusi probabilitas sedangkan sifat terakhir menunjukkan bahwa vektor baris \\(\\boldsymbol\\pi\\) tidak berubah (yaitu tidak berubah) oleh transisi satu langkah matriks. Dengan kata lain, setelah Rantai Markov mencapai kondisi stasioner, distribusi probabilitasnya akan tetap stasioner dari waktu ke waktu. Secara matematis, vektor stasioner \\(\\boldsymbol\\pi\\) juga dapat diperoleh dengan mencari vektor eigen kiri dari matriks transisi satu langkah. Contoh 12.4.1. Temukan distribusi stasioner untuk sistem NCD di Malaysia dengan asumsi bahwa probabilitas tahun tanpa klaim untuk semua kelas NCD adalah \\(p_0\\). Solusi Matriks transisi dapat ditulis ulang sebagai: \\[ \\small{ {\\bf P} = \\left[ {\\begin{array}{*{20}c} {1 - p_{0}}&amp;p_{0}&amp;0&amp;0&amp;0&amp;0\\\\ {1 - p_{0}}&amp;0&amp;p_{0}&amp;0&amp;0&amp;0\\\\ {1 - p_{0}}&amp;0&amp;0&amp;p_{0}&amp;0&amp;0\\\\ {1 - p_{0}}&amp;0&amp;0&amp;0&amp;p_{0}&amp;0\\\\ {1 - p_{0}}&amp;0&amp;0&amp;0&amp;0&amp;p_{0}\\\\ {1 - p_{0}}&amp;0&amp;0&amp;0&amp;0&amp;p_0 \\end{array} }\\right] } \\] Distribusi stasioner dapat dihitung dengan menggunakan \\(\\pi_{j}=\\sum\\limits{i}\\pi_{i}p_{ij}\\). Solusinya adalah: \\[ \\small{ \\begin{array}{lll} {\\pi _0} = \\sum\\limits_i {\\pi_i}p_{i0} = (1 - {p_0})\\sum\\limits_i {{\\pi _i}} = 1 - {p_0}\\\\ {\\pi _1} = \\sum\\limits_i {\\pi _i}p_{i1} = {\\pi_0}{p_{01}} = (1 - {p_0}){p_0}\\\\ {\\pi _2} = \\sum\\limits_i {\\pi _i}p_{i2} = {\\pi _1}{p_{12}} = (1 - {p_0}){p_0}^2\\\\ {\\pi _3} = \\sum\\limits_i {\\pi _i}p_{i3} = {\\pi _2}{p_{23}} = (1 - {p_0}){p_0}^3\\\\ {\\pi _4} = \\sum\\limits_i {\\pi _i}p_{i4} = {\\pi _3}{p_{34}} = (1 - {p_0}){p_0}^4\\\\ {\\pi _5} = \\sum\\limits_i {\\pi _i}p_{i5} = {\\pi _4}{p_{45}} + {\\pi _5}{p_{55}} = (1 - {p_0}){p_0}^5 + {\\pi _5}{p_0}\\\\ \\therefore {\\pi _5} = \\frac{(1 - {p_0}){p_0}^5}{{(1 - {p_0})}} = {p_0}^5 \\end{array} } \\] Distribusi stasioner yang ditunjukkan pada Contoh 12.4.1 mewakili distribusi asimtotik dari sistem PTM, atau distribusi dalam jangka panjang. Sebagai contoh, dengan asumsi bahwa probabilitas tahun tanpa klaim adalah \\(p_0=0,90,\\) Probabilitas stasioner adalah: \\[ \\small{ \\begin{array}{l} {\\pi _0} = 1 - {p_0} = 0.1000\\\\ {\\pi _1} = (1 - {p_0}){p_0} = 0.0900\\\\ {\\pi _2} = (1 - {p_0}){p_0}^2 = 0.0810\\\\ {\\pi _3} = (1 - {p_0}){p_0}^3 = 0.0729\\\\ {\\pi _4} = (1 - {p_0}){p_0}^4 = 0.0656\\\\ {\\pi _5} = {p_0}^5 = 0.5905 \\end{array} } \\] Dengan kata lain, \\({\\pi_0} = 0.10\\) menunjukkan bahwa 10% tertanggung pada akhirnya akan masuk ke dalam kelas 0, \\({\\pi_1} = 0.09\\) menunjukkan bahwa 9% tertanggung pada akhirnya akan masuk ke kelas 1, dan seterusnya, hingga \\({\\pi _5} = 0.59\\), yang menunjukkan bahwa 59% dari tertanggung pada akhirnya akan masuk ke dalam kelas 5. 12.4.2 Kode untuk Distribusi Stasioner R Kita dapat menggunakan vektor eigen kiri dari matriks transisi untuk menghitung distribusi stasioner. Kode berikut dapat digunakan untuk Hitung vektor eigen kiri: R 1. Membuat Matriks Transisi #create transition matrix entries = c(0.1,0.9,0,0,0,0, 0.1,0,0.9,0,0,0, 0.1,0,0,0.9,0,0, 0.1,0,0,0,0.9,0, 0.1,0,0,0,0,0.9, 0.1,0,0,0,0,0.9) (TP &lt;- matrix(entries,nrow=6,byrow=TRUE) ) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.1 0.9 0.0 0.0 0.0 0.0 ## [2,] 0.1 0.0 0.9 0.0 0.0 0.0 ## [3,] 0.1 0.0 0.0 0.9 0.0 0.0 ## [4,] 0.1 0.0 0.0 0.0 0.9 0.0 ## [5,] 0.1 0.0 0.0 0.0 0.0 0.9 ## [6,] 0.1 0.0 0.0 0.0 0.0 0.9 2. Hitung nilai eigen dan vektor eigen menggunakan fungsi eigen #hint -- left eigenvector is the same as right eigenvector of transpose #of transition matrix eigenTP &lt;- eigen(t(TP)) signif(eigenTP$values, digits = 3) ## [1] 1.00e+00+0.00e+00i -4.88e-04+3.55e-04i -4.88e-04-3.55e-04i ## [4] 1.87e-04+5.74e-04i 1.87e-04-5.74e-04i 6.03e-04+0.00e+00i signif(eigenTP$vectors, digits = 3) ## [,1] [,2] [,3] [,4] ## [1,] 0.162+0i 1.15e-13+8.40e-14i 1.15e-13-8.40e-14i -4.40e-14+1.36e-13i ## [2,] 0.145+0i -6.60e-11-2.03e-10i -6.60e-11+2.03e-10i 1.72e-10+1.25e-10i ## [3,] 0.131+0i -9.80e-08+3.02e-07i -9.80e-08-3.02e-07i 2.57e-07-1.87e-07i ## [4,] 0.118+0i 3.84e-04-2.79e-04i 3.84e-04+2.79e-04i -1.47e-04-4.51e-04i ## [5,] 0.106+0i -7.07e-01+0.00e+00i -7.07e-01+0.00e+00i -7.07e-01+0.00e+00i ## [6,] 0.954+0i 7.07e-01+0.00e+00i 7.07e-01+0.00e+00i 7.07e-01+0.00e+00i ## [,5] [,6] ## [1,] -4.40e-14-1.36e-13i 1.43e-13+0i ## [2,] 1.72e-10-1.25e-10i 2.13e-10+0i ## [3,] 2.57e-07+1.87e-07i 3.17e-07+0i ## [4,] -1.47e-04+4.51e-04i 4.74e-04+0i ## [5,] -7.07e-01+0.00e+00i 7.07e-01+0i ## [6,] 7.07e-01+0.00e+00i -7.07e-01+0i 3. Hitung vektor eigen kiri #divide entry of first column by sum of elements, so that entries sum to 1 #provide answers in 4 decimal places signif(eigen(t(TP))$vectors[,1]/sum(eigen(t(TP))$vectors[,1]), digits = 4) ## [1] 0.10000+0i 0.09000+0i 0.08100+0i 0.07290+0i 0.06561+0i 0.59050+0i Solusi. Seperti yang diberikan di Bagian Lampiran ??, ketika jumlah klaim terdistribusi Poisson dengan parameter \\(\\lambda=0.10\\), probabilitas klaim \\(k\\) adalah \\(p_k = \\frac{e^{ - 0.1}{(0.1)}^k}{{k!}}, {\\rm{ }}k = 0,1,2, \\ldots.\\) 12.4.3 Evolusi Premium Kita mungkin tertarik untuk mengetahui evolusi premi rata-rata setelah \\(n\\) tahun (atau \\(n\\) langkah). Dalam sistem NCD, probabilitas transisi n langkah, \\(p_{ij}^{(n)}=\\Pr(X_{n}=j|X_{0}=i)\\), dapat digunakan untuk menghitung evolusi premi rata-rata. Probabilitas \\(p_{ij}^{(n)}\\) dapat diperoleh sebagai elemen \\((i,j)\\) ke- dari pangkat \\(n\\) ke- dari matriks transisi \\({\\bf P}\\), yaitu, \\({\\bf P}^{n}\\) Contoh 12.4.4. Amati premi dalam 20 tahun di bawah sistem NCD di Malaysia, dengan asumsi bahwa probabilitas klaim adalah Poisson didistribusikan dengan parameter \\(λ=0,10\\) dan premi sebelum menerapkan PTM adalah \\(m=100\\). **Solusi. Di bawah NCD Malaysia, kami menggunakan probabilitas Poisson \\(p_k = \\frac{e^{ - 0.1}{(0.1)}^k}{k!}\\), hanya untuk \\(k=0,1\\). Oleh karena itu, matriks transisi pada tahun pertama adalah: \\[ \\small{ {\\bf{P}^{(1)}} = \\left[{\\begin{array}{*{20}{c}} {0.0952}&amp;{0.9048}&amp;0&amp;0&amp;0&amp;0\\\\ {0.0952}&amp;0&amp;{0.9048}&amp;0&amp;0&amp;0\\\\ {0.0952}&amp;0&amp;0&amp;{0.9048}&amp;0&amp;0\\\\ {0.0952}&amp;0&amp;0&amp;0&amp;{0.9048}&amp;0\\\\ {0.0952}&amp;0&amp;0&amp;0&amp;0&amp;{0.9048}\\\\ {0.0952}&amp;0&amp;0&amp;0&amp;0&amp;{0.9048} \\end{array}} \\right] } \\] Premi pada tahun pertama, setelah menerapkan PTM, adalah: \\[ \\begin{aligned} &amp;= \\sum\\limits_j \\text{(premium)} \\times \\text{(average proportion in class } j) \\times \\text{(1 - *NCD* in class }j) \\\\ &amp;= m\\left[ \\frac{\\sum\\limits_i {p_{i0}}}{6}{(1)} + \\frac{\\sum\\limits_i {p_{i1}}}{6}{(1 - 0.25)} + \\ldots + \\frac{\\sum\\limits_i {p_{i5}}}{6}{(1 - 0.55)} \\right]\\\\ &amp;= 100[0.0952(1) + 0.1508(0.75) + \\cdots + 0.3016(0.45)]\\\\ &amp;= 62.55. \\end{aligned} \\] Menggunakan langkah-langkah serupa, premi di \\(n\\)-th tahun untuk \\(n = 2,...,20\\) dapat diamati. Dari , premi dalam 20 tahun adalah: R 62.55, 59.87, 58.06, 57.06, 56.58, 56.58, 56.58, 56.58, 56.58, 56.58, 56.58, 56.58, 56.58, 56.58, 56.58, 56.58, 56.58, 56.58, 56.58, 56.58. 12.4.4 Program untuk Evolusi Premium R Kode berikut dapat digunakan untuk menemukan premi dalam tahun ke-n dan premi dalam 20 tahun di bawah sistem NCD di Malaysia (untuk temukan solusinya dalam Contoh 12.4.4).R 1. Create a Transition Matrix #create transition matrix entries = c(0.0952,0.9048,0,0,0,0, 0.0952,0,0.9048,0,0,0, 0.0952,0,0,0.9048,0,0, 0.0952,0,0,0,0.9048,0, 0.0952,0,0,0,0,0.9048, 0.0952,0,0,0,0,0.9048) (TP &lt;- matrix(entries,nrow=6,byrow=TRUE) ) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.0952 0.9048 0.0000 0.0000 0.0000 0.0000 ## [2,] 0.0952 0.0000 0.9048 0.0000 0.0000 0.0000 ## [3,] 0.0952 0.0000 0.0000 0.9048 0.0000 0.0000 ## [4,] 0.0952 0.0000 0.0000 0.0000 0.9048 0.0000 ## [5,] 0.0952 0.0000 0.0000 0.0000 0.0000 0.9048 ## [6,] 0.0952 0.0000 0.0000 0.0000 0.0000 0.9048 2. Create a function for the \\(n\\)th power of a square matrix #create function for nth power of square matrix powA &lt;- function(n) { if (n==1) return (TP) if (n==2) return (TP%*%TP) if (n&gt;2) return ( TP%*%powA(n-1))} #example for n=3 signif(powA(3), digits = 3) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.0952 0.0861 0.0779 0.741 0.000 0.000 ## [2,] 0.0952 0.0861 0.0779 0.000 0.741 0.000 ## [3,] 0.0952 0.0861 0.0779 0.000 0.000 0.741 ## [4,] 0.0952 0.0861 0.0779 0.000 0.000 0.741 ## [5,] 0.0952 0.0861 0.0779 0.000 0.000 0.741 ## [6,] 0.0952 0.0861 0.0779 0.000 0.000 0.741 3. create function for premium in nth year #define NCD percentage NCD = c(1,.75,.7,.6167,.55,.45) #create function for premium in nth year p = numeric(0) prem &lt;- function(n){ for (j in 1:length(NCD)) p[j] = mean(powA(n)[,j]) 100*sum(p*NCD) } #example for n=3 signif(prem(3), digits = 4) ## [1] 58.06 4. Provide Premiums for 20 years premium=numeric(0) for (n in 1:20) {premium[n] = prem(n) } signif(premium, digits = 4) ## [1] 62.55 59.87 58.06 57.06 56.58 56.58 56.58 56.58 56.58 56.58 56.58 56.58 ## [13] 56.58 56.58 56.58 56.58 56.58 56.58 56.58 56.58 Contoh 12.4.5. Amati premi dalam 20 tahun di bawah sistem NCD di Brasil, dengan asumsi bahwa probabilitas \\(k\\) klaim adalah \\(p_k = \\frac{e^{-0.1}{(0.1)}^k}{k!}, \\ \\ k = 0,1,2,\\ldots\\), dan premi sebelum menerapkan PTM adalah \\(m=100\\). Solusi. Matriks transisi untuk sistem NCD di Brasil adalah: \\[ {\\bf P} = \\left[ {\\begin{array}{*{20}{c}} {0.0952}&amp;{0.9048}&amp;0&amp;0&amp;0&amp;0&amp;0\\\\ {0.0952}&amp;0&amp;{0.9048}&amp;0&amp;0&amp;0&amp;0\\\\ {0.0047}&amp;{0.0905}&amp;0&amp;{0.9048}&amp;0&amp;0&amp;0\\\\ {0.0002}&amp;{0.0045}&amp;{0.0905}&amp;0&amp;{0.9048}&amp;0&amp;0\\\\ {0.0000}&amp;{0.0002}&amp;{0.0045}&amp;{0.0905}&amp;0&amp;{0.9048}&amp;0\\\\ {0.0000}&amp;{0.0000}&amp;{0.0002}&amp;{0.0045}&amp;{0.0905}&amp;0&amp;{0.9048}\\\\ {0.0000}&amp;{0.0000}&amp;{0.0000}&amp;{0.0002}&amp;{0.0045}&amp;{0.0905}&amp;{0.9048} \\end{array}} \\right] \\] Using R, the premiums in 20 years are: 76.69, 73.76, 71.31, 69.38, 67.92, 66.93, 66.40, 66.05, 65.88, 65.78, 65.72, 65.69, 65.67, 65.66, 65.66, 65.66, 65.66, 65.65, 65.65, 65.65. Hasil dalam Contoh 12.4-5 memungkinkan kita untuk mengamati evolusi premi untuk sistem NCD di Malaysia dan Brasil dengan asumsi bahwa jumlah klaim didistribusikan dengan parameter \\(λ=0,10\\), dan premi sebelum menerapkan PTM adalah \\(m=100\\). Evolusi premi untuk kedua negara disediakan pada Tabel 12.4, dan ditunjukkan secara grafis pada Gambar 12.4. Table 12.4. Evolution of Premium (Malaysia and Brazil) \\[ \\small{ \\begin{array}{ccc|ccc} \\hline \\text{Year} &amp; \\text{Premium} &amp; \\text{Premium} &amp; \\text{Year} &amp; \\text{Premium} &amp; \\text{Premium} \\\\ &amp; \\text{Malaysia} &amp; \\text{Brazil} &amp; &amp; \\text{Malaysia} &amp; \\text{Brazil} \\\\ \\hline 0 &amp; 100 &amp; 100 &amp; 11 &amp; 56.58 &amp; 65.72 \\\\ 1 &amp; 62.55 &amp; 76.69 &amp; 12 &amp; 56.58 &amp; 65.69 \\\\ 2 &amp; 59.87 &amp; 73.76 &amp; 13 &amp; 56.58 &amp; 65.67 \\\\ 3 &amp; 58.06 &amp; 71.31 &amp; 14 &amp; 56.58 &amp; 65.66 \\\\ 4 &amp; 57.06 &amp; 69.38 &amp; 15 &amp; 56.58 &amp; 65.66 \\\\ 5 &amp; 56.58 &amp; 67.92 &amp; 16 &amp; 56.58 &amp; 65.66 \\\\ 6 &amp; 56.58 &amp; 66.93 &amp; 17 &amp; 56.58 &amp; 65.66 \\\\ 7 &amp; 56.58 &amp; 66.40 &amp; 18 &amp; 56.58 &amp; 65.65 \\\\ 8 &amp; 56.58 &amp; 66.05 &amp; 19 &amp; 56.58 &amp; 65.65 \\\\ 9 &amp; 56.58 &amp; 65.88 &amp; 20 &amp; 56.58 &amp; 65.65 \\\\ \\hline \\end{array} } \\] Figure 12.4: Evolution of Premium (Malaysia and Brazil) 12.4.5 Tingkat Konvergensi Kami mungkin juga tertarik untuk menentukan variasi antara probabilitas pada tahun ke-\\(n\\) \\(p_{ij}^{(n)}\\) dan probabilitas stasioner, \\(\\pi _j\\). Variasi antara probabilitas dapat diukur menggunakan: \\[ \\left| {average(p_{ij}^{(n)}) - {\\pi _j}} \\right|. \\] Oleh karena itu, variasi total dapat diukur dengan jumlah variasi Di semua kelas: \\[\\begin{equation} \\sum\\limits_j {\\left| {average(p_{ij}^{(n)}) - {\\pi _j}} \\right|}. \\end{equation}\\] Variasi total juga disebut Tingkat konvergensi karena mengukur tingkat konvergensi setelah \\(n\\) tahun (atau \\(n\\) transisi). Lebih rendah Variasi total menyiratkan tingkat konvergensi yang lebih baik antara \\(n\\)-langkah probabilitas transisi dan distribusi stasioner. Contoh 12.4.6. Memberikan variasi total (tingkat konvergensi) dalam 20 tahun di bawah sistem NCD di Malaysia, dengan asumsi bahwa probabilitas klaim adalah Poisson didistribusikan dengan parameter \\(λ=0,10\\). Solution. Using R, probabilitas stasioner adalah: \\[ \\left[ {\\begin{array}{*{20}{c}} {{\\pi _0}}\\\\{{\\pi_1}}\\\\{{\\pi _2}}\\\\ {{\\pi _3}}\\\\{{\\pi _4}}\\\\{{\\pi_5}} \\end{array}} \\right] = \\left[ {\\begin{array}{*{20}{c}} {0.0952}\\\\{0.0861}\\\\{0.0779}\\\\ {0.0705}\\\\{0.0638}\\\\{0.6064} \\end{array}} \\right] \\] Matriks transisi pada tahun pertama adalah: \\[ {\\bf{P}}^{(1)} = \\left[ {\\begin{array}{*{20}{c}} {0.0952}&amp;{0.9048}&amp;0&amp;0&amp;0&amp;0\\\\ {0.0952}&amp;0&amp;{0.9048}&amp;0&amp;0&amp;0\\\\ {0.0952}&amp;0&amp;0&amp;{0.9048}&amp;0&amp;0\\\\ {0.0952}&amp;0&amp;0&amp;0&amp;{0.9048}&amp;0\\\\ {0.0952}&amp;0&amp;0&amp;0&amp;0&amp;{0.9048}\\\\ {0.0952}&amp;0&amp;0&amp;0&amp;0&amp;{0.9048} \\end{array}} \\right] \\] Variasi dapat dihitung sebagai: \\[ \\begin{array}{l} \\left| {\\sum\\limits_i {\\frac{p_{i0}^{}}{6}} - {\\pi _0}} \\right| = 0\\\\ \\left|{\\sum\\limits_i {\\frac{p_{i1}^{}}{6}} - {\\pi _1}} \\right| = 0.0647\\\\ \\vdots \\\\ \\left| {\\sum\\limits_i \\frac{p_{i5}}{6} - {\\pi _5}} \\right| = .3048 \\end{array} \\] Oleh karena itu, variasi total pada tahun pertama adalah \\[ \\sum\\limits_j {\\left| {\\sum\\limits_i {\\frac{p_{ij}}{6} - {\\pi _j}} } \\right|} = 0.6096 . \\] Using R, variasi total (atau tingkat konvergensi) dalam 20 tahun adalah: 0.6096, 0.3941, 0.2252, 0.0958, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0. 12.4.6 R Program untuk Tingkat Konvergensi Kode berikut dapat digunakan untuk menghitung variasi total dalam R \\(n\\)-th tahun, dan total variasi (tingkat konvergensi) di 20 tahun di bawah sistem NCD di Malaysia (solusinya dalam Contoh 12.4.6). 1. Recall the Transition Matrix TP ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.0952 0.9048 0.0000 0.0000 0.0000 0.0000 ## [2,] 0.0952 0.0000 0.9048 0.0000 0.0000 0.0000 ## [3,] 0.0952 0.0000 0.0000 0.9048 0.0000 0.0000 ## [4,] 0.0952 0.0000 0.0000 0.0000 0.9048 0.0000 ## [5,] 0.0952 0.0000 0.0000 0.0000 0.0000 0.9048 ## [6,] 0.0952 0.0000 0.0000 0.0000 0.0000 0.9048 2. Create stationary probabilities SP &lt;- eigen(t(TP))$vectors[,1]/sum(eigen(t(TP))$vectors[,1]) signif(SP, digits = 3) ## [1] 0.0952+0i 0.0861+0i 0.0779+0i 0.0705+0i 0.0638+0i 0.6060+0i 3. Create a function for total variation in \\(n\\)th year TV=function(n){ dif =numeric(0) for (j in 1:length(SP)) dif[j]=abs(mean(powA(n)[,j])-SP[j]) sum(dif) } #example for n=1 signif(TV(1), digits = 4) ## [1] 0.6096 4. Provide total variations (convergence rate) in 20 years tot.var=numeric(0) for (n in 1:20) {tot.var[n] = TV(n)} signif(tot.var,4) ## [1] 6.096e-01 3.941e-01 2.252e-01 9.580e-02 1.943e-15 2.068e-15 2.193e-15 ## [8] 2.304e-15 2.415e-15 2.415e-15 2.415e-15 2.415e-15 2.415e-15 2.415e-15 ## [15] 2.415e-15 2.415e-15 2.415e-15 2.415e-15 2.415e-15 2.415e-15 Contoh 12.4.7. Memberikan variasi total (atau tingkat konvergensi) dalam 20 tahun di bawah sistem NCD di Brasil, dengan asumsi bahwa jumlah klaim didistribusikan sebagai Poisson dengan parameter \\(λ=0,10\\). Solution. Using R code, variasi total (atau tingkat konvergensi) dalam 20 tahun untuk sistem NCD di Brasil adalah: 1.2617, 1.0536, 0.8465, 0.6412, 0.4362, 0.2316, 0.1531, 0.0747, 0.0480, 0.0232, 0.0145, 0.0071, 0.0043, 0.0021, 0.0013, 0.0006, 0.0004, 0.0002, 0.0001, 0.0001. Contoh 12.4.6-7 memberikan tingkat konvergensi untuk dua BMS yang berbeda (dua negara yang berbeda). BMS Malaysia mencapai stasioner penuh hanya setelah lima tahun, sementara BMS di Brasil membutuhkan waktu lebih lama. Seperti disebutkan dalam Lemaire (1998), BMS yang lebih canggih akan bertemu lebih lambat, dan dianggap sebagai kelemahan karena membutuhkan waktu lebih lama untuk stabil. Tujuan utama BMS adalah untuk memisahkan yang baik driver dari driver yang buruk, dan dengan demikian, diinginkan untuk memiliki proses klasifikasi yang dapat diselesaikan (atau distabilkan) segera setelah mungkin. 12.5 BMS dan Peringkat Premium 12.5.1 Peringkat Premium Dalam pembuatan tarif asuransi motor, BMS adalah bentuk mekanisme penilaian posteriori untuk melengkapi penggunaan klasifikasi risiko apriori seperti yang dijelaskan dalam Bab 8. Segmentasi risiko a priori yang diperkenalkan dalam Bagian 8.1 membagi portofolio driver menjadi sejumlah kelas risiko homogen berdasarkan faktor peringkat yang dapat diamati (lihat Bagian 7.4.2), sehingga pemegang polis di kelas risiko yang sama membayar premi apriori yang sama. Alasan mendasar untuk menggunakan BMS yang bergantung pada informasi pengalaman klaim adalah untuk menangani heterogenitas residual dalam setiap kelas risiko homogen (misalnya, lihat diskusi sebelum Contoh 2.6.2) karena variabel yang dapat diamati jauh dari sempurna dalam memprediksi risiko perilaku mengemudi. Mekanisme a posteriori yang ideal adalah kerangka kerja premi kredibilitas yang dikembangkan dalam Bab 9 dan khususnya Bagian 9.5 (see also Dionne &amp; Vanasse (1989)), di mana premi diturunkan secara individual untuk setiap pemegang polis dengan memasukkan a priori dan a posteriori informasi. Namun, penentuan premi individu seperti itu terlalu kompleks dari sudut pandang komersial untuk implementasi praktis oleh perusahaan asuransi motor. Untuk alasan ini, BMS adalah solusi yang disukai dan terdiri dari tiga blok bangunan berikut: (a) kelas BMS; (b) aturan transisi; (c) tingkat premi (juga dikenal sebagai relativitas premi atau koefisien penyesuaian premi). Dua blok bangunan pertama telah ditentukan sebelumnya dan telah dibahas di bagian sebelumnya, sedangkan penentuan (bukan ditentukan sebelumnya seperti yang dibahas dalam kasus sistem Malaysia, Brasil dan Swiss) dari relativitas premi penting bagi perusahaan asuransi motor justru karena sifatnya yang saling melengkapi dan koreksi untuk memperhitungkan ketidaksempurnaan atau ketidakakuratan dalam a priori klasifikasi risiko. Perhatikan bahwa relativitas premi di bawah BMS berbeda dari ukuran relativitas yang didefinisikan dalam Bagian 7.4.2, yang merupakan rasio antara risiko yang diharapkan dari kelas risiko tertentu ke kelas risiko dasar, yang keduanya dihitung dari faktor peringkat yang dapat diamati. Dalam subbagian berikut, kami secara singkat memperkenalkan pengaturan pemodelan yang diperlukan untuk mempelajari penentuan relativitas optimal. Kami merujuk pembaca yang tertarik ke Denuit, Maréchal, Pitrebois, &amp; Walhin (2007) untuk diskusi yang lebih lengkap tentang rincian teknis. 12.5.2 Klasifikasi Risiko Apriori Mari kita pertimbangkan sebuah portofolio dengan \\(n\\) polis, di mana eksposur risiko (lihat Bagian 7.4.1) dari driver \\(i\\) dilambangkan sebagai \\(m_{i}\\). dilambangkan sebagai \\(m_{i}\\) dan jumlah klaim yang dilaporkan diwakili oleh \\(Y_{i}\\), mengikuti notasi yang digunakan di Bagian 8.2.). Biarkan \\({\\bf x}_{i}^{T}=(x_{i1},x_{i2},\\ldots, x_{iq})\\) menjadi vektor variabel teramati variabel yang dapat diamati untuk \\(i=1,2,\\ldots, n\\). Regresi Poisson seperti yang dikembangkan di Bagian ?? biasanya dipilih untuk memodelkan \\(Y_{i}\\) di bawah kerangka kerja generalized linear models (GLM), lihat Bagian 13.3.2.2 dan juga McCullagh &amp; Nelder (1989). Kita kemudian dapat menyatakan prediksi frekuensi klaim yang diharapkan untuk pemegang polis \\(i\\) sebagai where \\(\\hat{\\beta}_{0},\\hat{\\beta}_{1},\\ldots,\\hat{\\beta}_{q}\\) are the estimated regression coefficients. In other words, \\(\\lambda_{i}=\\frac{\\mu_{i}}{m_{i}}\\) adalah frekuensi klaim yang diharapkan per unit eksposur, yang merupakan fokus utama dari klasifikasi risiko apriori. 12.5.3 Pemodelan Heterogenitas Residual Karena faktor-faktor yang tidak teramati yang dapat mempengaruhi perilaku mengemudi tidak diperhitungkan dalam memperkirakan frekuensi klaim yang diharapkan, perusahaan asuransi harus memperhitungkan untuk heterogenitas residual dalam setiap kelas risiko a priori dengan memperkenalkan komponen efek acak \\(\\Theta_{i}\\) ke dalam distribusi bersyarat \\(Y_{i}\\). Dengan \\(\\Theta_{i}=\\theta\\), \\(Y_{i}\\) mengikuti distribusi Poisson dengan rata-rata \\(\\lambda_{i}\\theta\\), yaitu, \\[\\begin{equation} \\Pr(Y_{i}=k|\\Theta_{i}=\\theta)=\\exp(-\\lambda_{i}\\theta)\\frac{(\\lambda_{i}\\theta)^{k}}{k!},k=0,1,2,\\ldots. \\end{equation}\\] Mengikuti dari pengaturan model gamma-Poisson di Bagian 9.5.1, kami selanjutnya mengasumsikan bahwa semua \\(\\Theta_{i}\\)’s independen dan mengikuti \\((a,a)\\) distribusi dengan fungsi densitas berikut seperti yang diperkenalkan dalam Lampiran 18.2 \\[\\begin{equation} f(\\theta)=\\frac{1}{\\Gamma(a)}a^{a}\\theta^{a-1}\\exp(-a\\theta), \\quad \\theta &gt; 0, \\end{equation}\\] di mana penggunaan campuran gamma-Poisson menghasilkan distribusi binomial negatif untuk \\(Y_i\\) (lihat Bagian 2.6). Dengan spesifikasi tersebut, kami memperoleh \\(\\mathrm{E}(\\Theta_{i})=1\\) dan karenanya \\(\\mathrm{E}(Y_{i})=\\mathrm{E}(\\mathrm{E}(Y_{i}|\\Theta_{i}))=\\mathrm{E}(\\lambda_{i}\\Theta_{i})=\\lambda_{i}\\). menurut hukum harapan yang diulangi dalam Lampiran 16.2.1. Selanjutnya, dapat ditunjukkan bahwa distribusi posterior \\(\\Theta| y_{1}=k_{1},y_{2}=k_{2},\\ldots,y_{n}=k_{n}\\) adalah gamma didistribusikan dengan Parameter \\(a+\\sum_{j=1}^{n} k_{j}\\) and \\(a+n\\lambda_{i}\\) dan karena itu Bayesian Premi diberikan sebagai \\[\\begin{equation} \\mathrm{E}(\\lambda_{i}\\Theta| y_{1}=k_{1},\\ldots,y_{n}=k_{n})=\\lambda_{i}\\times\\frac{a+\\sum_{j=1}^{n} k_{j}}{a+n\\lambda_{i}}. \\end{equation}\\] Di sisi lain, menerapkan estimasi tertimbang kredibilitas Bühlmann di Bagian 9.3 ke model gamma-Poisson di Bagian 9.5.1, kami memperoleh \\[\\begin{align*} EPV&amp;=\\mathrm{E}(\\mathrm{Var}(Y| \\lambda_{i}))=\\mathrm{E}(\\lambda_{i}\\Theta)=\\lambda_{i},\\\\ VHM&amp;=\\mathrm{Var}(\\mathrm{E}(Y| \\lambda_{i}))=\\mathrm{Var}(\\lambda_{i}\\Theta)=\\frac{\\lambda_{i}^{2}}{a},\\\\ K&amp;=\\frac{EPV}{VHM}=\\frac{\\lambda_{i}}{\\frac{\\lambda_{i}^{2}}{a}}=\\frac{a}{\\lambda_{i}},\\\\ Z&amp;=\\frac{n}{n+K}=\\frac{n\\lambda_{i}}{n\\lambda_{i}+a},\\\\ \\bar{Y}&amp;=\\frac{\\sum_{j=1}^{n} y_{j}}{n}=\\frac{\\sum_{j=1}^{n} k_{j}}{n},\\\\ \\mu&amp;=\\mathrm{E}(\\mathrm{E}(Y_{i}| \\lambda_{i}))=\\mathrm{E}(\\lambda_{i}\\Theta)=\\lambda_{i}, \\end{align*}\\] dan karenanya perkiraan tertimbang kredibilitas sebagai \\[\\begin{align} \\mathrm{E}(\\mathrm{E}(Y| \\lambda_{i})| y_{1}=k_{1},\\ldots,y_{n}=k_{n})&amp;=\\mathrm{E}(\\lambda_{i}\\Theta| y_{1}=k_{1},\\ldots,y_{n}=k_{n})\\notag\\\\ &amp;=Z\\times \\bar{Y}+ (1-Z)\\times \\mu\\notag\\\\ &amp;=\\frac{n\\lambda_{i}}{n\\lambda_{i}+a}\\times \\frac{\\sum_{j=1}^{n} k_{j}}{n}+\\frac{a}{n\\lambda_{i}+a}\\times\\lambda_{i}\\notag\\\\ &amp;=\\frac{\\lambda_{i}(a+\\sum_{j=1}^{n} k_{j})}{a+n\\lambda_{i}}, \\end{align}\\] that is, premi kredibilitas Bühlmann sama persis dengan premi Bayesian. Terlepas dari kenyataan bahwa premi kredibilitas yang diperoleh secara individual di atas merupakan premi a posteriori yang ideal, dalam praktiknya perusahaan asuransi menggunakan BMS sebagai pendekatan diskrit untuk premi Bayesian, karena struktur BMS yang relatif lebih sederhana dibandingkan dengan perhitungan individual premi kredibilitas. 12.5.4 Distribusi Stasioner yang Memungkinkan Heterogenitas Residual Misalkan seorang pengemudi dipilih secara acak dari portofolio yang telah diklasifikasikan ke dalam kelas risiko \\(h\\) melalui penggunaan variabel a priori yang diamati. Frekuensi klaim yang diharapkan sebenarnya untuk driver ini diberikan oleh \\(\\Lambda\\Theta\\), di mana \\(\\Lambda\\) adalah frekuensi klaim yang diharapkan yang tidak diketahui a priori dan \\(\\Theta\\) adalah heterogenitas residual acak. Selanjutnya, mari kita nyatakan \\(w_{g}\\) sebagai proporsi dari pengemudi di kelas risiko \\(g\\) ke-, yaitu, \\(w_{g}=\\Pr(\\Lambda=\\lambda_{g})=\\frac{n_{g}}{n}\\) di mana \\(n_{g}\\) adalah jumlah pengemudi yang diklasifikasikan dalam kelas risiko ke-g. Catatan bahwa karena ada dua konsep kelas risiko yang berbeda (dari klasifikasi risiko a priori) dan kelas BMS (atau NCD) (untuk mekanisme pemeringkatan a posteriori), untuk selanjutnya selanjutnya dalam bab ini kami akan menyebut kelas BMS sebagai tingkat BMS sebagai gantinya untuk menghindari kebingungan yang tidak perlu. Misalkan \\(p_{ij}^{\\lambda}(\\lambda\\theta)\\) adalah probabilitas transisi dari Tingkat BMS \\(i\\) ke tingkat \\(j\\) untuk pengemudi dengan frekuensi klaim yang diharapkan \\(\\lambda\\theta\\) yang termasuk dalam kelas risiko dengan prediksi frekuensi klaim \\(\\lambda\\). Dengan kata lain,matriks transisi satu langkah dapat dituliskan sebagai \\({\\bf P}(\\lambda\\theta; \\lambda)=\\{p_{ij}^{\\lambda}(\\lambda\\theta)\\}\\). Vektor baris dari distribusi stasioner distribusi \\(\\boldsymbol \\pi=(\\pi_{0}^{\\lambda}(\\lambda\\theta),\\pi_{1}^{\\lambda}(\\lambda\\theta),\\ldots,\\pi_{k-1}^{\\lambda}(\\lambda\\theta))\\) dapat diperoleh dengan menyelesaikan kondisi berikut: \\[\\begin{align*} \\boldsymbol\\pi(\\lambda\\theta;\\lambda)&amp;=\\boldsymbol\\pi(\\lambda\\theta;\\lambda){\\bf P}(\\lambda\\theta;\\lambda)\\\\ \\boldsymbol\\pi(\\lambda\\theta;\\lambda){\\bf 1}&amp;=1, \\end{align*}\\] di mana \\(\\bf 1\\) adalah vektor kolom dari \\(\\bf 1\\) dan \\(\\pi_{\\ell}^{\\lambda}(\\lambda\\theta)\\) adalah probabilitas stasioner untuk pengemudi dengan frekuensi klaim yang diharapkan sebenarnya dari \\(\\lambda\\theta\\) untuk berada di level \\(\\ell\\) ketika kondisi keseimbangan stabil tercapai dalam jangka panjang. Perhatikan bahwa dengan memasukkan efek acak parameter \\(\\theta\\), ekspresi (bukan nilai numerik) untuk \\(\\pi_{\\ell}^{\\lambda}(\\lambda\\theta)\\) dapat ditemukan dengan menggunakan MATLAB, tetapi tidak dengan R. Dengan pengaturan ini, probabilitas driver untuk tetap berada di level BMS \\(L=\\ell\\) untuk \\(\\ell=0,1,\\ldots,k-1\\) dalam konteks seluruh portofolio dapat diperoleh sebagai \\[\\begin{align} \\Pr(L=\\ell)&amp;=\\sum\\limits_{g=1}^{h}\\Pr(L=\\ell|\\Lambda=\\lambda_{g})\\Pr(\\Lambda=\\lambda_{g})\\notag\\\\ &amp;=\\sum\\limits_{g=1}^{h}\\Pr(\\Lambda=\\lambda_{g})\\int_{0}^{\\infty}\\Pr(L=\\ell|\\Lambda=\\lambda_{g},\\Theta=\\theta)f(\\theta)d\\theta\\notag\\\\ &amp;=\\sum\\limits_{g=1}^{h}w_{g}\\int_{0}^{\\infty}\\pi_{\\ell}^{\\lambda_{g}}(\\lambda_{g}\\theta)f(\\theta)d\\theta. \\end{align}\\] 12.5.5 Penentuan Relativitas Optimal Relativitas optimal untuk setiap tingkat BMS pertama kali diturunkan oleh Norberg (1976) melalui minimalisasi fungsi objektif berikut, yang lebih umum dikenal sebagai kriteria Norberg: \\[\\begin{equation*} \\min \\mathrm{E}\\left((\\bar{\\lambda}\\Theta-\\bar{\\lambda}r_{L})^{2}\\right)\\equiv\\min\\mathrm{E}\\left((\\Theta-r_{L})^{2}\\right), \\end{equation*}\\] di mana \\(\\hat{\\lambda}\\) adalah frekuensi klaim yang diharapkan secara konstan untuk semua pemegang polis tanpa adanya klasifikasi risiko apriori dan \\(r_{L}\\) adalah relativitas premi untuk Tingkat BMS \\(L\\). Pitrebois, Denuit, &amp; Walhin (2003) kemudian memasukkan informasi dari *a priori klasifikasi risiko ke dalam optimasi fungsi tujuan yang sama dari \\[\\begin{equation*} \\min\\mathrm{E}\\left((\\Theta-r_{L})^{2}\\right) \\end{equation*}\\] untuk mendapatkan \\(r_{L}\\) secara analitis. Tan, Li, Li, &amp; Balasooriya (2015) lebih lanjut mengusulkan minimalisasi dari fungsi objektif berikut \\[\\begin{equation} \\min\\mathrm{E}\\left((\\Lambda\\Theta-\\Lambda r_{L})^{2}\\right), \\text{ subject to } \\mathrm{E}(r_{L})=1, \\end{equation}\\] di bawah kendala keseimbangan keuangan (yaitu, relativitas premi yang diharapkan sama dengan 1) untuk menentukan relativitas optimal dari BMS yang diberikan tingkat BMS yang telah ditentukan sebelumnya dan aturan transisi, di mana \\[\\begin{align*} \\min\\mathrm{E}\\left((\\Lambda\\Theta-\\Lambda r_{L})^{2}\\right)&amp;=\\sum\\limits_{\\ell=0}^{k-1}\\mathrm{E}\\left((\\Lambda\\Theta-\\Lambda r_{L})^{2}|L=\\ell\\right)\\Pr(L=\\ell)\\\\ &amp;=\\sum\\limits_{\\ell=0}^{k-1}\\mathrm{E}\\left(\\mathrm{E}\\left((\\Lambda\\Theta-\\Lambda r_{L})^{2}|L=\\ell,\\Lambda\\right)|L=\\ell\\right)\\Pr(L=\\ell)\\\\ &amp;=\\sum\\limits_{\\ell=0}^{k-1}\\sum\\limits_{g=1}^{h}\\mathrm{E}\\left((\\Lambda\\Theta-\\Lambda r_{L})^{2}|L=\\ell,\\Lambda=\\lambda_{g}\\right)\\times\\Pr\\left(\\Lambda=\\lambda_{g}|L=\\ell\\right)\\Pr(L=\\ell)\\\\ &amp;=\\sum\\limits_{\\ell=0}^{k-1}\\sum\\limits_{g=1}^{h}\\int_{0}^{\\infty}(\\lambda_{g}\\theta-\\lambda_{g}r_{\\ell})^{2}\\pi_{\\ell}(\\lambda_{g}\\theta)w_{g}f(\\theta)d\\theta\\\\ &amp;=\\sum\\limits_{g=1}^{h}w_{g}\\int_{0}^{\\infty}\\sum\\limits_{\\ell=0}^{k-1}(\\lambda_{g}\\theta-\\lambda_{g}r_{\\ell})^{2}\\pi_{\\ell}(\\lambda_{g}\\theta)f(\\theta)d\\theta. \\end{align*}\\] Sangatlah penting bahwa relativitas optimal memiliki rata-rata 100%, sehingga bonus dan minus saling mengimbangi satu sama lain untuk menghasilkan kondisi keseimbangan keuangan. Perhatikan bahwa pendekatan yang dipertimbangkan oleh Pitrebois et al. (2003) tidak membutuhkan kendala keseimbangan finansial karena solusi analitis untuk fungsi objektifnya diberikan oleh \\(r_{\\ell}=\\mathrm{E}(\\Theta|L=\\ell)\\), jadi mengikuti bahwa \\(\\mathrm{E}(r_{L})=\\mathrm{E}\\left(\\mathrm{E}(\\Theta|L)\\right) =\\mathrm{E}(\\Theta)=1\\) dengan pilihan spesifik distribusi gamma \\((a,a)\\) untuk distribusi acak komponen efek $$. Dalam hal ini, masalah optimasi dapat diselesaikan dengan menentukan Lagrangian sebagai \\[\\begin{align} \\mathcal{L}({\\bf r}, \\alpha)&amp;=\\mathrm{E}\\left((\\Lambda\\Theta-\\Lambda r_{L})^{2}\\right)+\\alpha(\\mathrm{E}(r_{L})-1)\\notag\\\\ &amp;=\\sum\\limits_{\\ell=0}^{k-1} \\mathrm{E}\\left((\\Lambda\\Theta-\\Lambda r_{L})^{2}|L=\\ell\\right)\\Pr(L=\\ell)+\\alpha\\left(\\sum\\limits_{\\ell=0}^{k-1}r_{\\ell}\\Pr(L=\\ell)-1\\right),\\notag \\end{align}\\] where \\({\\bf r}=(r_{0},r_{1},\\ldots,r_{k-1})^{T}\\). Kondisi urutan pertama yang diperlukan diberikan sebagai berikut \\[\\begin{align*} \\Pr(L=\\ell)\\times \\left(2\\mathrm{E}\\left(\\Lambda^{2}\\Theta-\\Lambda^{2}r_{L}|L=\\ell\\right)-\\alpha\\right)&amp;=0, \\qquad \\ell=0,1,\\ldots, k-1,\\\\ \\sum\\limits_{\\ell=0}^{k-1}r_{\\ell}\\Pr(L=\\ell)-1&amp;=0. \\end{align*}\\] Finally, solusi yang ditetapkan untuk \\(\\alpha\\) and \\(r_{\\ell}, \\ell=0,1,\\ldots,k-1\\) diperoleh sebagai \\[\\begin{align*} \\alpha&amp;=\\frac{\\left(\\sum\\limits_{\\ell=0}^{k-1}\\frac{\\mathrm{E}(\\Lambda^{2}\\Theta|L=\\ell)\\Pr(L=\\ell)}{\\mathrm{E}(\\Lambda^{2}|L=\\ell)}\\right)-1}{\\sum\\limits_{\\ell=0}^{k-1}\\frac{\\Pr(L=\\ell)}{2\\mathrm{E}(\\Lambda^{2}|L=\\ell)}},\\\\ r_{\\ell}&amp;=\\frac{\\mathrm{E}(\\Lambda^{2}\\Theta|L=\\ell)}{\\mathrm{E}(\\Lambda^{2}|L=\\ell)}-\\frac{\\alpha}{2\\mathrm{E}(\\Lambda^{2}|L=\\ell)}, \\end{align*}\\] dimana \\[\\begin{align*} \\Pr(L=\\ell)&amp;=\\sum\\limits_{g=1}^{h}w_{g}\\int_{0}^{\\infty}\\pi_{\\ell}^{\\lambda_{g}}(\\lambda_{g}\\theta)f(\\theta)d\\theta,\\\\ \\mathrm{E}(\\Lambda^{2}\\Theta|L=\\ell)&amp;=\\frac{\\sum\\limits_{g=1}^{h}w_{g}\\int_{0}^{\\infty}\\lambda_{g}^{2}\\theta\\pi_{\\ell}^{\\lambda_{g}}(\\lambda_{g}\\theta)f(\\theta)d\\theta}{\\sum\\limits_{g=1}^{h}w_{g}\\int_{0}^{\\infty}\\pi_{\\ell}^{\\lambda_{g}}(\\lambda_{g}\\theta)f(\\theta)d\\theta},\\\\ \\mathrm{E}(\\Lambda^{2}|L=\\ell)&amp;=\\frac{\\sum\\limits_{g=1}^{h}w_{g}\\int_{0}^{\\infty}\\lambda_{g}^{2}\\pi_{\\ell}^{\\lambda_{g}}(\\lambda_{g}\\theta)f(\\theta)d\\theta}{\\sum\\limits_{g=1}^{h}w_{g}\\int_{0}^{\\infty}\\pi_{\\ell}^{\\lambda_{g}}(\\lambda_{g}\\theta)f(\\theta)d\\theta}. \\end{align*}\\] Jika kita melakukan optimasi tanpa kendala keseimbangan keuangan, maka kita memperoleh \\[\\begin{align*} \\alpha^{\\text{unconstrained}}&amp;=0,\\\\ r_{\\ell}^{\\text{unconstrained}}&amp;=\\frac{\\mathrm{E}(\\Lambda^{2}\\Theta|L=\\ell)}{\\mathrm{E}(\\Lambda^{2}|L=\\ell)}. \\end{align*}\\] 12.5.6 Ilustrasi Numerik Pada bagian ini, kami menyajikan dua ilustrasi numerik yang mengintegrasikan informasi a priori ke dalam penentuan relativitas optimal. Kami mempertimbangkan mempertimbangkan tingkat BMS dan aturan transisi dari sistem Malaysia dan Brasil, tetapi memilih untuk menghitung himpunan relativitas optimal dan bukan tingkat premi tertentu yang diberikan sebelumnya. Dalam ilustrasi kami, kami juga mengasumsikan bahwa 3 nilai a priori frekuensi klaim yang diharapkan berikut ini diberikan \\(\\lambda_{1}=0.1\\),\\(\\lambda_{2}=0.3\\), and \\(\\lambda_{3}=0.5\\) dengan proporsi sebagai berikut \\[ \\Pr(\\Lambda=\\lambda_{1})=0.6, \\Pr(\\Lambda=\\lambda_{2})=0.3, \\Pr(\\Lambda=\\lambda_{3})=0.1. \\] Kami juga mengasumsikan bahwa parameter gamma ditetapkan pada \\(a = 1,5\\). Perhatikan bahwa meskipun asumsi-asumsi pemodelan ini sederhana, tujuannya di sini adalah untuk mendemonstrasikan penentuan relativitas optimal di bawah pengaturan yang relatif sederhana, dan bahwa prosedur optimasi untuk BMS tetap sama meskipun Klasifikasi risiko a priori dilakukan secara ekstensif. Kami merujuk pembaca yang tertarik pada data klaim kendaraan bermotor seperti yang didokumentasikan di De Jong &amp; Heller (2008) untuk melakukan segmentasi risiko a priori sebelum melanjutkan ke penentuan relativitas optimal. Lebih lanjut, seperti yang disebutkan sebelumnya, penyertaan parameter efek acak \\(\\theta\\) menyiratkan bahwa ekspresi yang diperlukan (bukan nilai numerik) untuk probabilitas stasioner \\(\\pi_{\\ell}^{\\lambda}(\\lambda\\theta)\\) yang akan digunakan untuk integral selanjutnya dapat ditemukan dengan menggunakan MATLAB tetapi tidak dengan R. Juga, karena bentuk probabilitas stasioner yang diperoleh agak rumit, dalam subbagian ini kita kami memilih untuk tidak menyertakan kode R untuk penentuan relativitas optimal. Lebih penting lagi, kami berharap bahwa kunci utama dari subbab ini adalah agar pembaca dapat mendapatkan pemahaman konseptual yang solid secara keseluruhan tentang bagaimana memperhitungkan semua informasi yang relevan dalam desain sistem bonus-malus. Untuk BMS Malaysia dengan 6 level dan aturan transisi -1/Top, nilai numerik yang diperoleh dari relativitas optimal disajikan dalam [Tabel 12.5]. bersama dengan probabilitas stasioner. Kami menemukan bahwa sekitar setengah dari pemegang polis akan menempati tingkat BMS tertinggi dengan relativitas premi terendah dalam jangka panjang jangka panjang ketika kondisi stasioner telah tercapai. Kami juga mengamati bahwa batasan relativitas optimal lebih tinggi daripada yang tidak dibatasi karena kebutuhan untuk memenuhi kendala keseimbangan keuangan. Table 12.5. Optimal Relativities with \\(k=6\\) levels and transition rule of -1/Top \\[ \\small{ \\begin{array}{*{20}c} \\hline \\text{Level }\\ell &amp; \\Pr(L=\\ell) &amp; r_{\\ell} &amp; r_{\\ell}^{\\text{unconstrained}}\\\\ \\hline {0} &amp; {16.22\\%} &amp; {131.99\\%} &amp; {126.33\\%}\\\\ {1} &amp; {11.29\\%} &amp; {127.33\\%} &amp; {120.66\\%}\\\\ {2} &amp; {8.49\\%} &amp; {120.64\\%} &amp; {113.03\\%}\\\\ {3} &amp; {6.69\\%} &amp; {113.93\\%} &amp; {105.44\\%}\\\\ {4} &amp; {5.44\\%} &amp; {107.79\\%} &amp; {98.47\\%}\\\\ {5} &amp; {51.87\\%} &amp; {78.06\\%} &amp; {63.15\\%}\\\\ {E(r_{L})} &amp; &amp; {100\\%} &amp; {88.87\\%} \\\\ \\hline \\end{array} } \\] Selain itu, kami melihat bahwa kecuali untuk level BMS tertinggi (level 5), level BMS lainnya akan membebankan biaya tambahan malus kepada pemegang polis yang menempati yang menempati level tersebut. Temuan ini tidak mengejutkan karena kerangka teori kami kerangka teori kami di sini adalah untuk menentukan relativitas optimal mengingat perhitungan premi dasar a priori dengan hanya mengandalkan informasi frekuensi klaim tetapi tidak dengan tingkat keparahan klaim. Dalam praktiknya, perusahaan asuransi dapat memperkenalkan Tingkat NCD hanya dengan diskon (bonus) tetapi tidak dengan beban (minus) karena premi dasar a priori telah dinaikkan dengan mempertimbangkan informasi frekuensi klaim dan tingkat keparahan klaim. Untuk BMS Brasil dengan 7 tingkat dan aturan transisi -1/+1, nilai numerik yang sesuai dari relativitas optimal ditunjukkan pada [Tabel 12.6]. Kami menemukan bahwa sekitar tiga perempat pemegang polis akan menempati level BMS tertinggi dengan relativitas premi terendah pada kondisi stasioner. Temuan ini adalah terutama disebabkan oleh penalti yang lebih ringan pada aturan transisi -1/+1 dibandingkan dengan aturan -1/Top, sehingga lebih banyak pemegang polis yang diharapkan menempati tingkat BMS tertinggi. Mirip dengan contoh sebelumnya, kami menemukan bahwa relativitas optimal yang tidak dibatasi yang tidak dibatasi lebih rendah dan menghasilkan nilai \\(\\mathrm{E}(r_{L})\\) yang lebih rendah. Table 12.6. Optimal Relativities with \\(k=7\\) levels and transition rule of -1/+1 \\[ \\small{ \\begin{array}{*{20}c} \\hline \\text{Level }\\ell &amp; \\Pr(L=\\ell) &amp; r_{\\ell} &amp; r_{\\ell}^{\\text{unconstrained}}\\\\ \\hline {0} &amp; {3.28\\%} &amp; {234.94\\%} &amp; {228.65\\%}\\\\ {1} &amp; {2.21\\%} &amp; {196.24\\%} &amp; {189.27\\%}\\\\ {2} &amp; {2.00\\%} &amp; {168.36\\%} &amp; {160.59\\%}\\\\ {3} &amp; {2.38\\%} &amp; {145.96\\%} &amp; {137.03\\%}\\\\ {4} &amp; {4.02\\%} &amp; {125.53\\%} &amp; {114.63\\%}\\\\ {5} &amp; {10.38\\%} &amp; {106.25\\%} &amp; {91.12\\%}\\\\ {6} &amp; {75.74\\%} &amp; {85.89\\%} &amp; {61.74\\%}\\\\ {E(r_{L})} &amp; &amp; {100\\%} &amp; {78.97\\%} \\\\ \\hline \\end{array} } \\] Perhatikan bahwa nilai yang diperoleh dari relativitas optimal mungkin tidak diinginkan untuk implementasi komersial karena kemungkinan perbedaan yang tidak teratur yang tidak beraturan di antara tingkat BMS yang berdekatan. Untuk mengatasi masalah ini, perusahaan asuransi dapat mempertimbangkan memberlakukan relativitas optimal linier dalam bentuk \\(r_{L}^{\\text{linear}}=a+bL\\) dengan menyelesaikan optimasi terkendala berikut ini dengan kendala ketidaksamaan \\[ \\min\\mathrm{E}\\left((\\Lambda\\Theta-\\Lambda a-\\Lambda bL)^{2}\\right) \\text{ subject to } a+b\\mathrm{E}(L)\\ge 1. \\] Kami merujuk pembaca yang tertarik ke Tan (2016) untuk berdiskusi tentang bagaimana memasukkan batasan komersial lebih lanjut dan juga solusi untuk masalah optimasi yang melibatkan kondisi Kuhn-Tucker. Bibliography De Jong, P., &amp; Heller, G. Z. (2008). Generalized linear models for insurance data. Cambridge University Press, Cambridge. Denuit, M., Maréchal, X., Pitrebois, S., &amp; Walhin, J.-F. (2007). Actuarial modelling of claim counts: Risk classification, credibility and bonus-malus systems. John Wiley &amp; Sons, Chichester. Dionne, G., &amp; Vanasse, C. (1989). A generalization of automobile insurance rating models: The negative binomial distribution with a regression component. ASTIN Bulletin, 19(2), 199–212. Lemaire, J. (1998). Bonus-malus systems: The european and asian approach to merit rating. North American Actuarial Journal, 2(1), 26–38. McCullagh, P., &amp; Nelder, J. A. (1989). Generalized linear models, second edition. Chapman &amp; Hall, London. Norberg, R. (1976). A credibility theory for automobile bonus system. Scandinavian Actuarial Journal, 2, 92–107. Pitrebois, S., Denuit, M., &amp; Walhin, J.-F. (2003). Setting a bonus-malus scale in the presence of other rating factors: Taylor’s work revisited. ASTIN Bulletin, 33(2), 419–436. Tan, C. I. (2016). Optimal design of a bonus-malus system: Linear relativities revisited. Annals of Actuarial Science, 10(1), 52–64. Tan, C. I., Li, J., Li, J. S.-H., &amp; Balasooriya, U. (2015). Optimal relativities and transition rules of a bonus-malus system. Insurance: Mathematics and Economics, 61, 255–263. "],["aggregate-loss-models-1.html", "Bab 13 Aggregate Loss Models", " Bab 13 Aggregate Loss Models "],["dependence-modeling.html", "Bab 14 Dependence Modeling 14.1 Variable Types 14.2 Classic Measures of Scalar Associations 14.3 Introduction to Copulas 14.4 Application Using Copulas 14.5 Types of Copulas 14.6 Why is Dependence Modeling Important? 14.7 Further Resources and Contributors 14.8 Referensi", " Bab 14 Dependence Modeling 14.1 Variable Types Pada bagian ini, akan mempelajari : Mengklasifikasikan variabel sebagai kualitatif atau kuantitatif. Mendeskripsikan variabel multivariat. Orang, perusahaan, dan entitas lain yang ingin kita pahami dijelaskan dalam dataset oleh karakteristik numerik. Karena karakteristik ini bervariasi berdasarkan entitasnya, mereka umumnya dikenal sebagai variabel. Untuk mengelola sistem asuransi, penting untuk memahami distribusi setiap variabel dan bagaimana mereka saling terkait. Umumnya, dataset memiliki banyak variabel (dimensi tinggi), sehingga berguna untuk memulainya dengan mengklasifikasikannya ke dalam jenis yang berbeda. Seperti yang akan terlihat, klasifikasi ini tidaklah ketat; ada tumpang tindih di antara kelompok-kelompok tersebut. Meskipun demikian, pengelompokan yang dirangkum dalam Tabel 14.1 dan dijelaskan dalam sisa bagian ini memberikan langkah pertama yang solid dalam merangkai sebuah dataset. Table 14.1. Variable Types \\[ {\\small \\begin{matrix} \\begin{array}{l|l} \\hline \\textbf{Variable Type} &amp; \\textbf{Example} \\\\\\hline Qualitative &amp; \\\\ \\text{Binary} &amp; \\text{Sex} \\\\ \\text{Categorical (Unordered, Nominal)} &amp; \\text{Territory (e.g., state/province) in which an insured resides} \\\\ \\text{Ordered Category (Ordinal)} &amp; \\text{Claimant satisfaction (five point scale ranging from 1=dissatisfied} \\\\ &amp; ~~~ \\text{to 5 =satisfied)} \\\\\\hline Quantitative &amp; \\\\ \\text{Continuous} &amp; \\text{Policyholder&#39;s age, weight, income} \\\\ \\text{Discrete} &amp; \\text{Amount of deductible (0, 250, 500, and 1000)} \\\\ \\text{Count} &amp; \\text{Number of insurance claims} \\\\ \\text{Combinations of} &amp; \\text{Policy losses, mixture of 0&#39;s (for no loss)} \\\\ ~~~ \\text{Discrete and Continuous} &amp; ~~~\\text{and positive claim amount} \\\\ \\text{Interval Variable} &amp; \\text{Driver Age: 16-24 (young), 25-54 (intermediate),} \\\\ &amp; ~~~\\text{55 and over (senior)} \\\\ \\text{Circular Data} &amp; \\text{Time of day measures of customer arrival} \\\\ \\hline Multivariate ~ Variable &amp; \\\\ \\text{High Dimensional Data} &amp; \\text{Characteristics of a firm purchasing worker&#39;s compensation} \\\\ &amp; ~~~\\text{insurance (location of plants, industry, number of employees,} \\\\ &amp;~~~\\text{and so on)} \\\\ \\text{Spatial Data} &amp; \\text{Longitude/latitude of the location an insurance hailstorm claim} \\\\ \\text{Missing Data} &amp; \\text{Policyholder&#39;s age (continuous/interval) and -99 for} \\\\ &amp;~~~ `\\text{not reported,&#39; that is, missing} \\\\ \\text{Censored and Truncated Data} &amp; \\text{Amount of insurance claims in excess of a deductible} \\\\ \\text{Aggregate Claims} &amp; \\text{Losses recorded for each claim in a motor vehicle policy.} \\\\ \\text{Stochastic Process Realizations} &amp; \\text{The time and amount of each occurrence of an insured loss} \\\\ \\hline \\end{array} \\end{matrix}} \\] Dalam analisis data, penting untuk memahami jenis variabel yang sedang Anda kerjakan. Sebagai contoh, pertimbangkan sepasang variabel acak (Coverage,Claim) dari data LGPIF yang diperkenalkan dalam Bagian 1.3 . Kita ingin tahu apakah distribusi Coverage bergantung pada distribusi Claim atau apakah keduanya saling independen secara statistik. Kita juga ingin tahu bagaimana distribusi Claim tergantung pada variabel EntityType . Karena variabel EntityType termasuk dalam kelas variabel yang berbeda, pemodelan ketergantungan antara Claim dan Coverage mungkin memerlukan teknik yang berbeda dari yang digunakan untuk Claim dan EntityType . ### Qualitative Variables Di sub-bagian ini, akan mempelajari: Mengklasifikasikan variabel kualitatif sebagai nominal atau ordinal Mendeskripsikan variabel biner Variabel kualitatif, atau kategorikal, adalah variabel yang pengukurannya menunjukkan keanggotaan dalam satu set grup atau kategori. Sebagai contoh, jika Anda sedang mengkodekan lokasi tempat tinggal tertanggung menjadi bagian utara (1), selatan (2), dan lainnya (3). Variabel lokasi ini adalah contoh variabel nominal, di mana level-levelnya tidak memiliki urutan alami. Analisis variabel nominal tidak harus bergantung pada label kategori. Sebagai contoh, jika saya menggunakan kode 1, 2, 3 untuk utara, selatan, dan lainnya, seharusnya saya mendapatkan set statistik ringkasan yang sama jika saya menggunakan kode 2, 1, 3, yaitu menukar urutan utara dan selatan. Di sisi lain, variabel ordinal adalah jenis variabel kategorikal di mana terdapat urutan. Sebagai contoh, dalam survei untuk melihat seberapa puas pelanggan dengan departemen pelayanan klaim kami, kita dapat menggunakan skala lima poin yang berkisar dari 1 yang berarti tidak puas hingga 5 yang berarti puas. Variabel ordinal menyediakan urutan yang jelas untuk level-level variabel, tetapi tingkat pemisahan antar level tidak diketahui. Variabel biner adalah jenis khusus variabel kategorikal di mana hanya ada dua kategori yang umumnya disimbolkan dengan 0 dan 1. Sebagai contoh, kita dapat mengkodekan variabel dalam dataset sebagai 1 jika tertanggung adalah perempuan dan 0 jika laki-laki. 14.1.1 Quantitative Variables Di sub-bagian ini, Anda akan belajar: Membedakan antara variabel kontinu dan variabel diskrit Menggunakan kombinasi variabel kontinu dan diskrit Mendeskripsikan data berbentuk lingkaran Berbeda dengan variabel kualitatif, variabel kuantitatif adalah variabel di mana setiap level numerik adalah hasil dari suatu skala sehingga jarak antara dua level skala memiliki makna. Variabel kontinu adalah variabel yang dapat mengambil nilai apa pun dalam interval tertentu. Misalnya, umur, berat badan, atau pendapatan tertanggung dapat direpresentasikan sebagai variabel kontinu. Sebaliknya, variabel diskrit adalah variabel yang hanya dapat mengambil sejumlah nilai yang terbatas dalam interval tertentu. Misalnya, saat memeriksa pilihan tertanggung mengenai jumlah potongan klaim, mungkin hanya ada nilai 0, 250, 500, dan 1000 sebagai hasil yang mungkin. Seperti variabel ordinal, nilai-nilai ini mewakili kategori yang berbeda yang memiliki urutan. Namun, berbeda dengan variabel ordinal, perbedaan numerik antar level memiliki makna ekonomi. Jenis khusus dari variabel diskrit adalah variabel hitungan, yang nilainya adalah bilangan bulat nonnegatif. Sebagai contoh, kita akan tertarik dengan jumlah klaim yang timbul dari suatu polis selama periode tertentu. Beberapa variabel pada dasarnya merupakan kombinasi dari komponen diskrit dan kontinu. Misalnya, ketika kita menganalisis kerugian tertanggung, kita akan menemui hasil diskrit di nol yang mewakili tidak ada kerugian tertanggung, dan jumlah kontinu untuk hasil positif yang mewakili jumlah kerugian tertanggung. Variasi menarik lainnya adalah variabel interval, yang memberikan rentang hasil yang mungkin. Data berbentuk lingkaran merupakan kategori menarik yang biasanya tidak dianalisis oleh perusahaan asuransi. Sebagai contoh data berbentuk lingkaran, misalkan Anda memantau panggilan ke pusat layanan pelanggan dan ingin mengetahui kapan waktu puncak panggilan tiba. Dalam konteks ini, waktu dalam sehari dapat dianggap sebagai variabel dengan realisasi pada sebuah lingkaran, misalnya bayangkan gambar analog dari jam. Untuk data berbentuk lingkaran, jarak antara pengamatan pada pukul 00:15 dan 00:45 sama dekatnya dengan pengamatan pada pukul 23:45 dan 00:15 (konvensi HH:MM mengacu pada jam dan menit). 14.1.2 Multivariate Variables Di sub-bagian ini, Anda akan belajar: Membedakan antara data univariat dan multivariat Menangani variabel yang hilang Data asuransi umumnya bersifat multivariat dalam arti bahwa kita dapat mengambil banyak pengukuran pada satu entitas. Misalnya, ketika mempelajari kerugian yang terkait dengan rencana kompensasi pekerja suatu perusahaan, kita mungkin ingin mengetahui lokasi pabrik manufaktur, industri di mana perusahaan beroperasi, jumlah karyawan, dan sebagainya. Strategi umum untuk menganalisis data multivariat adalah dengan memulai dengan memeriksa masing-masing variabel secara terpisah dari yang lain. Ini dikenal sebagai pendekatan univariat. Sebaliknya, untuk beberapa variabel, tidak masuk akal hanya melihat aspek satu dimensi saja. Misalnya, perusahaan asuransi biasanya mengorganisir data spasial berdasarkan garis bujur dan garis lintang untuk menganalisis lokasi klaim asuransi terkait cuaca akibat badai salju. Hanya memiliki satu angka, baik garis bujur atau garis lintang, memberikan sedikit informasi dalam memahami lokasi geografis. Kasus khusus lain dari variabel multivariat, yang kurang jelas, melibatkan pengkodean untuk data yang hilang. Secara historis, beberapa paket statistik menggunakan angka -99 untuk melaporkan saat variabel, seperti usia tertanggung, tidak tersedia atau tidak dilaporkan. Hal ini menyebabkan banyak analis yang tidak curiga memberikan statistik aneh saat merangkum sekumpulan data. Ketika data hilang, lebih baik untuk memikirkan variabel tersebut memiliki dua dimensi, satu untuk menunjukkan apakah variabel tersebut dilaporkan atau tidak, dan yang kedua menyediakan usia (jika dilaporkan). Demikian pula, data asuransi umumnya dicensored dan dipotong. Kami mengacu pada Bagian 4.3 untuk informasi lebih lanjut mengenai data yang dicensored dan dipotong. Klaim agregat, yang dijelaskan dalam Bab 5, juga dapat dikodekan sebagai jenis variabel multivariat khusus lainnya. Mungkin jenis variabel multivariat yang paling rumit adalah realisasi dari suatu proses stokastik. Anda akan mengingat bahwa proses stokastik hanya merupakan kumpulan variabel acak. Misalnya, dalam asuransi, kita mungkin memikirkan waktu klaim yang tiba di perusahaan asuransi dalam jangka waktu satu tahun. Ini adalah variabel berdimensi tinggi yang secara teoritis memiliki dimensi tak terhingga. Teknik khusus diperlukan untuk memahami realisasi dari proses stokastik ini, yang tidak akan dibahas di sini. 14.2 Classic Measures of Scalar Associations Pada bagian ini, akan mempelajari: -Mengestimasi korelasi menggunakan metode Pearson -Menggunakan ukuran berbasis peringkat seperti Spearman, Kendall untuk mengestimasi korelasi -Mengukur ketergantungan menggunakan rasio odds, Pearson chi-square, dan statistik uji likelihood ratio -Menggunakan korelasi berbasis normal untuk mengkuantifikasi hubungan yang melibatkan variabel ordinal 14.2.1 Association Measures for Quantitative Variables Untuk bagian ini, pertimbangkan sepasang variabel acak \\((X,Y)\\) dengan fungsi distribusi bersama \\(F(\\cdot)\\), dan sampel acak \\((X_i,Y_i)\\), i=1,…,n. Untuk kasus kontinu, anggap \\(F(\\cdot)\\) memiliki marginals yang mutlak kontinu dengan fungsi densitas marginal. 14.2.1.1 Pearson Correlation Definisikan fungsi kovariansi sampel \\(\\widehat{Cov}(X,Y) = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})\\), di mana \\(\\bar{X}\\) dan \\(\\bar{Y}\\) adalah rata-rata sampel dari X dan Y secara berturut-turut. Maka, korelasi momen-produk (Pearson) dapat ditulis sebagai \\[ r = \\frac{\\widehat{Cov}(X,Y)}{\\sqrt{\\widehat{Cov}(X,X)\\widehat{Cov}(Y,Y)}} = \\frac{\\widehat{Cov}(X,Y)}{\\sqrt{\\widehat{Var}(X)}\\sqrt{\\widehat{Var}(Y)}}. \\] Statistik korelasi r secara luas digunakan untuk menggambarkan hubungan linier antara variabel acak. Ini adalah pengestimasi (nonparametrik) dari parameter korelasi ρ, yang didefinisikan sebagai kovariansi dibagi dengan perkalian simpangan baku. Statistik ini memiliki beberapa fitur penting. Tidak seperti pengestimasi regresi, ia simetris antara variabel acak, sehingga korelasi antara X dan Y sama dengan korelasi antara Y dan X. Statistik ini tidak berubah oleh transformasi linear dari variabel acak (hingga perubahan tanda), sehingga kita dapat mengalikan variabel acak atau menambahkan konstanta seperti yang berguna untuk interpretasi. Rentang statistik ini adalah [-1,1] yang tidak bergantung pada distribusi X maupun Y. Selain itu, dalam kasus independensi, koefisien korelasi r adalah 0. Namun, diketahui bahwa korelasi nol secara umum tidak menyiratkan independensi, satu pengecualian adalah dalam kasus variabel acak yang terdistribusi secara normal. Statistik korelasi r juga merupakan pengestimasi (likelihood maksimum) dari parameter asosiasi untuk distribusi normal bivariat. Jadi, untuk data yang terdistribusi secara normal, statistik korelasi r dapat digunakan untuk menilai independensi. Untuk interpretasi tambahan dari statistik yang terkenal ini, pembaca dapat merujuk ke Lee Rodgers dan Nicewander (1998). ### Pearson correlation between Claim and Coverage r&lt;-cor(Claim,Coverage, method = c(&quot;pearson&quot;)) round(r,2) Output: [1] 0.31 ### Pearson correlation between Claim and log(Coverage) r&lt;-cor(Claim,log(Coverage), method = c(&quot;pearson&quot;)) round(r,2) Output: [1] 0.1 14.2.2 Rank Based Measures 14.2.2.1 Spearman’s Rho Koefisien korelasi Pearson memiliki kelemahan bahwa ia tidak invarian terhadap transformasi nonlinier dari data. Misalnya, korelasi antara X dan logY dapat sangat berbeda dari korelasi antara X dan Y. Seperti yang kita lihat dari kode R untuk statistik korelasi Pearson di atas, statistik korelasi r antara variabel penilaian Coverage dalam jutaan dolar logaritmik dan variabel jumlah Claim dalam dolar adalah 0,1 dibandingkan dengan 0,31 ketika kita menghitung korelasi antara variabel penilaian Coverage dalam jutaan dolar dan variabel jumlah Claim dalam dolar. Batasan ini adalah salah satu alasan untuk mempertimbangkan statistik alternatif. Ukuran alternatif korelasi didasarkan pada peringkat data. Misalkan R(Xj) menunjukkan peringkat Xj dari sampel X1,…,Xn dan begitu pula dengan R(Yj). Misalkan R(X)=(R(X1),…,R(Xn))′ menunjukkan vektor peringkat, dan begitu pula dengan R(Y). Misalnya, jika n=3 dan X=(24,13,109), maka R(X)=(2,1,3). Pengenalan komprehensif tentang statistik peringkat dapat ditemukan dalam, misalnya, Hettmansperger (1984). Selain itu, peringkat dapat digunakan untuk mendapatkan fungsi distribusi empiris, lihat Bagian 4.1.1 untuk informasi lebih lanjut tentang fungsi distribusi empiris. Dengan ini, ukuran korelasi Spearman (1904) adalah hanya korelasi momen-produk yang dihitung pada peringkat: \\[ r_S = \\frac{\\widehat{Cov}(R(X),R(Y))}{\\sqrt{\\widehat{Cov}(R(X),R(X))\\widehat{Cov}(R(Y),R(Y))}} = \\frac{\\widehat{Cov}(R(X),R(Y))}{(n^2-1)/12} . \\] Selanjutnya dapat memperoleh statistik korelasi Spearman rS menggunakan fungsi cor() di R dan memilih metode spearman. Dari hasil di bawah ini, korelasi Spearman antara variabel penilaian Coverage dalam jutaan dolar dan variabel jumlah Claim dalam dolar adalah 0,41. ### Spearman correlation between Claim and Coverage ### rs&lt;-cor(Claim,Coverage, method = c(&quot;spearman&quot;)) round(rs,2) ### Spearman correlation between Claim and log(Coverage) ### rs&lt;-cor(Claim,log(Coverage), method = c(&quot;spearman&quot;)) round(rs,2) Kita dapat menunjukkan bahwa statistik korelasi Spearman tidak berubah saat dilakukan transformasi yang secara ketat meningkat. Dari Kode R untuk statistik korelasi Spearman di atas, didapatkan rS=0,41 antara variabel penilaian Coverage dalam jutaan dolar dalam skala logaritmik dan variabel jumlah Claim dalam dolar. 14.2.2.2 Kendall’s Tau Salah satu ukuran alternatif yang menggunakan peringkat didasarkan pada konsep kesepakatan. Pasangan observasi (X, Y) dikatakan sesuai (tidak sesuai) jika observasi dengan nilai X yang lebih besar juga memiliki nilai Y yang lebih besar (lebih kecil). Kemudian \\(\\Pr(concordance) = \\Pr[ (X_1-X_2)(Y_1-Y_2) &gt;0 ]\\), \\(\\Pr(discordance) = \\Pr[ (X_1-X_2)(Y_1-Y_2) &lt;0 ]\\), \\(\\Pr(tie) = \\Pr[ (X_1-X_2)(Y_1-Y_2) =0 ]\\), dan \\[ \\begin{array}{rl} \\tau(X,Y) &amp;= \\Pr(concordance) - \\Pr(discordance) \\\\ &amp; = 2\\Pr(concordance) - 1 + \\Pr(tie). \\end{array} \\] Untuk mengestimasinya, pasangan \\((X_i, Y_i)\\) dan \\((X_j, Y_j)\\) dikatakan sesuai jika perkalian \\(sgn(X_j-X_i)sgn(Y_j-Y_i)\\) sama dengan 1 dan tidak sesuai jika perkalian tersebut sama dengan -1. Di sini, sgn(x) = 1, 0, -1 untuk x &gt; 0, x = 0, x &lt; 0, secara berturut-turut. Dengan ini, kita dapat mengungkapkan ukuran asosiasi Kendall (1938), yang dikenal sebagai tau Kendall, sebagai \\[ \\begin{array}{rl} \\hat{\\tau} &amp;= \\frac{2}{n(n-1)} \\sum_{i&lt;j} ~sgn(X_j-X_i) \\times sgn(Y_j-Y_i)\\\\ &amp;= \\frac{2}{n(n-1)} \\sum_{i&lt;j} ~sgn(R(X_j)-R(X_i)) \\times sgn(R(Y_j)-R(Y_i)) . \\end{array} \\] Menariknya, Hougaard (2000), halaman 137, mengatributkan penemuan awal dari statistik ini kepada Fechner (1897), mencatat bahwa penemuan Kendall adalah penemuan independen dan lebih lengkap daripada karya asli. Anda dapat memperoleh tau Kendall menggunakan fungsi cor() di R dan memilih metode kendall. Dari di bawah, τ^ = 0,32 antara variabel penilaian Coverage dalam jutaan dolar dan variabel jumlah Claim dalam dolar. Ketika ada seri dalam data, fungsi cor() menghitung tau_b Kendall seperti yang diusulkan oleh Kendall (1945). Kode R untuk Tau Kendall ### Kendall&#39;s tau correlation between Claim and Coverage ### tau&lt;-cor(Claim,Coverage, method = c(&quot;kendall&quot;)) round(tau,2) ### Kendall&#39;s tau correlation between Claim and log(Coverage) ### tau&lt;-cor(Claim,log(Coverage), method = c(&quot;kendall&quot;)) round(tau,2) Selain itu, untuk menunjukkan bahwa tau Kendall tidak berubah saat dilakukan transformasi yang meningkat secara ketat, kita melihat bahwa τ^ = 0,32 antara variabel penilaian Coverage dalam jutaan dolar dalam skala logaritmik dan variabel jumlah Claim dalam dolar. 14.2.3 Nominal Variables 14.2.3.1 Bernoulli Variables Untuk melihat mengapa ukuran ketergantungan untuk variabel kontinu mungkin tidak menjadi yang terbaik untuk variabel diskrit, mari kita fokus pada kasus variabel Bernoulli yang menghasilkan hasil biner sederhana, 0 dan 1. Untuk notasi, mari kita anggap \\(\\pi_{jk} = \\Pr(X=j, Y=k)\\) untuk j,k=0,1 dan πX = Pr(X=1) serta hal yang sama untuk πY. Maka, versi populasi dari korelasi momen-produk (Pearson) dapat dengan mudah dilihat sebagai \\[\\rho = \\frac{\\pi_{11} - \\pi_X \\pi_Y}{\\sqrt{\\pi_X(1-\\pi_X)\\pi_Y(1-\\pi_Y)}} .\\] Berbeda dengan kasus data kontinu, ukuran ini tidak mungkin mencapai batas-batas interval [-1,1]. Untuk melihat ini, mahasiswa probabilitas mungkin mengingat batas Fréchet-Höeffding untuk distribusi bersama yang ternyata adalah \\(\\max\\{0, \\pi_X+\\pi_Y-1\\} \\le \\pi_{11} \\le \\min\\{\\pi_X,\\pi_Y\\}\\) untuk probabilitas bersama ini. (Diskusi lebih lanjut tentang batas ini ada di Bagian 14.5.4.1.) Batasan pada probabilitas bersama ini memberlakukan pembatasan tambahan pada korelasi Pearson. Sebagai ilustrasi, anggaplah probabilitas yang sama \\(\\pi_X =\\pi_Y = \\pi &gt; 1/2\\). Maka, batas bawahnya adalah \\[ \\begin{eqnarray*} \\frac{2\\pi - 1 - \\pi^2}{\\pi(1-\\pi)} = -\\frac{1-\\pi}{\\pi} . \\end{eqnarray*} \\] Sebagai contoh, jika π = 0,8, maka nilai terkecil yang dapat dicapai oleh korelasi Pearson adalah -0,25. Secara umum, terdapat batasan pada ρ yang bergantung pada πX dan πY yang membuat interpretasi ukuran ini menjadi sulit. Seperti yang dicatat oleh Bishop, Fienberg, dan Holland (1975) (halaman 382), jika koefisien korelasi ini dipangkatkan dua, maka kita akan mendapatkan statistik chi-kuadrat Pearson (diperkenalkan di Bagian 2.7). Meskipun terdapat masalah batas yang dijelaskan di atas, fitur ini membuat koefisien korelasi Pearson menjadi pilihan yang baik untuk menggambarkan ketergantungan dengan data biner. Sebagai ukuran alternatif untuk variabel Bernoulli, rasio odds diberikan oleh \\[ \\begin{eqnarray*} OR(\\pi_{11}) = \\frac{\\pi_{11} \\pi_{00}}{\\pi_{01} \\pi_{10}} = \\frac{\\pi_{11} \\left( 1+\\pi_{11}-\\pi_X -\\pi_Y\\right)}{(\\pi_X-\\pi_{11})(\\pi_Y- \\pi_{11})} . \\end{eqnarray*} \\] Perhitungan yang menyenangkan menunjukkan bahwa OR(z) adalah 0 pada batas bawah Fréchet-Höeffding \\(z= \\max\\{0, \\pi_X+\\pi_Y-1\\}\\) dan tak terhingga pada batas atas \\(z=\\min\\{\\pi_X,\\pi_Y\\}\\). Dengan demikian, batasan pada ukuran ini tidak bergantung pada probabilitas marginal πX dan πY, sehingga lebih mudah untuk menginterpretasikan ukuran ini. Seperti yang dicatat oleh Yule (1900), rasio odds tidak berubah dengan pemberian label 0 dan 1. Selain itu, rasio odds tidak berubah terhadap margin dalam arti bahwa kita dapat menskalakan πX dan πY dengan konstanta positif dan rasio odds tetap tidak berubah. Secara khusus, misalkan ai, bj adalah set konstanta positif dan \\[ \\begin{eqnarray*} \\pi_{ij}^{new} &amp;=&amp; a_i b_j \\pi_{ij} \\end{eqnarray*} \\] dan \\(\\sum_{ij} \\pi_{ij}^{new}=1.\\) Maka, \\[ \\begin{eqnarray*} OR^{new} = \\frac{(a_1 b_1 \\pi_{11})( a_0 b_0 \\pi_{00})}{(a_0 b_1 \\pi_{01})( a_1 b_0\\pi_{10})} = \\frac{\\pi_{11} \\pi_{00}}{\\pi_{01} \\pi_{10}} =OR^{old} . \\end{eqnarray*} \\] Untuk bantuan tambahan dalam interpretasi, Yule mengusulkan dua transformasi untuk rasio odds, yang pertama dalam Yule (1900), \\[ \\begin{eqnarray*} \\frac{OR-1}{OR+1}, \\end{eqnarray*} \\] dan yang kedua dalam Yule (1912), \\[ \\begin{eqnarray*} \\frac{\\sqrt{OR}-1}{\\sqrt{OR}+1}. \\end{eqnarray*} \\] Meskipun statistik ini memberikan informasi yang sama dengan rasio odds asli OR, mereka memiliki keuntungan dapat mengambil nilai di interval [-1,1], sehingga lebih mudah untuk diinterpretasikan. Di bagian selanjutnya, kita juga akan melihat bahwa distribusi margin tidak memiliki efek pada Fréchet-Höeffding dari korelasi tetrakorik, ukuran asosiasi lainnya, lihat juga Joe (2014), halaman 48. Dari Tabel 14.2, \\(OR(\\pi_{11})=\\frac{1611(956)}{897(2175)}=0.79\\). Anda dapat memperoleh OR(π11) menggunakan fungsi oddsratio() dari pustaka epitools di R. Dari output di bawah ini, OR(π11) = 0,79 untuk variabel biner NoClaimCredit dan Fire5 dari data LGPIF. Table 14.2. 2 × 2 Table of Counts for Fire5 and NoClaimCredit \\[ {\\small \\begin{matrix} \\begin{array}{l|rr|r} \\hline &amp; \\text{Fire5} &amp; &amp; \\\\ \\text{NoClaimCredit} &amp; 0 &amp; 1 &amp; \\text{Total} \\\\ \\hline 0 &amp; 1611 &amp; 2175 &amp; 3786 \\\\ 1 &amp; 897 &amp; 956 &amp; 1853 \\\\ \\hline \\text{Total} &amp; 2508 &amp; 3131 &amp; 5639 \\\\ \\hline \\end{array} \\end{matrix}} \\] library(epitools) oddsratio(NoClaimCredit, Fire5,method = c(&quot;wald&quot;))$measure 14.2.3.2 Categorical Variables Secara umum, mari kita anggap \\((X,Y)\\) sebagai pasangan bivariat yang memiliki ncatX dan ncatY kategori, masing-masing. Untuk tabel dua arah dari frekuensi, mari njk menjadi jumlah pada baris j, kolom k. Mari nj⋅ menjadi total margin baris, dan n⋅k menjadi total margin kolom, dan \\(n=\\sum_{j,k} n_{j,k}\\). Definisikan statistik chi-square Pearson sebagai \\[ \\begin{eqnarray*} \\chi^2 = \\sum_{jk} \\frac{(n_{jk}- n_{j\\cdot}n_{\\cdot k}/n)^2}{n_{j\\cdot}n_{\\cdot k}/n} . \\end{eqnarray*} \\] Statistik uji rasio kemungkinan adalah \\[ \\begin{eqnarray*} G^2 = 2 \\sum_{jk} n_{jk} \\log\\frac{n_{jk}}{n_{j\\cdot}n_{\\cdot k}/n} . \\end{eqnarray*} \\] Dalam asumsi kemandirian, baik \\(χ^2\\) dan \\(G^2\\) memiliki distribusi chi-square asymptotic dengan derajat kebebasan \\((ncat_X-1)(ncat_Y-1)\\). Untuk membantu melihat apa yang diestimasikan oleh statistik ini, mari \\(\\pi_{jk} = \\Pr(X=j, Y=k)\\), dan \\(\\pi_{X,j}=\\Pr(X=j)\\), dan begitu pula untuk πY,k. Dengan mengasumsikan bahwa njk/n≈πjk untuk n yang besar, dan begitu pula untuk probabilitas marginal, kita memiliki \\[ \\begin{eqnarray*} \\frac{\\chi^2}{n} \\approx \\sum_{jk} \\frac{(\\pi_{jk}- \\pi_{X,j}\\pi_{Y,k})^2}{\\pi_{X,j}\\pi_{Y,k}} \\end{eqnarray*} \\] dan \\[ \\begin{eqnarray*} \\frac{G^2}{n} \\approx 2 \\sum_{jk} \\pi_{jk} \\log\\frac{\\pi_{jk}}{\\pi_{X,j}\\pi_{Y,k}} . \\end{eqnarray*} \\] Dalam hipotesis nol kemandirian, kita memiliki πjk=πX,jπY,k, dan jelas dari perkiraan ini bahwa kita mengharapkan statistik ini akan kecil di bawah hipotesis ini. Pendekatan klasik, seperti yang dijelaskan dalam Bishop, Fienberg, dan Holland (1975) (halaman 374), membedakan antara pengujian kemandirian dan ukuran asosiasi. Yang pertama dirancang untuk mendeteksi apakah hubungan ada, sedangkan yang terakhir dimaksudkan untuk menilai jenis dan tingkat hubungan. Kami mengakui perbedaan tujuan ini tetapi juga kurang peduli dengan perbedaan ini untuk aplikasi aktuaria. Table 14.3. Two-way Table of Counts for EntityType and NoClaimCredit \\[ {\\small \\begin{matrix} \\begin{array}{l|rr} \\hline &amp; \\text{NoClaimCredit} &amp; \\\\ \\text{EntityType} &amp; 0 &amp; 1 \\\\ \\hline \\text{City} &amp; 644 &amp; 149 \\\\ \\text{County} &amp; 310 &amp; 18 \\\\ \\text{Misc} &amp; 336 &amp; 273 \\\\ \\text{School} &amp; 1103 &amp; 494 \\\\ \\text{Town} &amp; 492 &amp; 479 \\\\ \\text{Village} &amp; 901 &amp; 440 \\\\ \\hline \\end{array} \\end{matrix}} \\] Anda dapat memperoleh statistik chi-square Pearson menggunakan fungsi chisq.test() dari perpustakaan MASS di R. Di sini, kita menguji apakah variabel EntityType independen dari variabel NoClaimCredit menggunakan Tabel 14.3. Kode R untuk Statistik Chi-square Pearson library(MASS) table = table(EntityType, NoClaimCredit) chisq.test(table) Karena nilai p kurang dari tingkat signifikansi 0,05, kita menolak hipotesis nol bahwa EntityType independen dari NoClaimCredit. Selain itu, Anda dapat memperoleh statistik uji rasio kemungkinan menggunakan fungsi likelihood.test() dari perpustakaan Deducer di R. Dari hasil di bawah, kita menguji apakah EntityType independen dari NoClaimCredit dari data LGPIF. Kesimpulan yang sama ditarik seperti pada uji chi-square Pearson. Kode R untuk Statistik Uji Rasio Kemungkinan library(Deducer) likelihood.test(EntityType, NoClaimCredit) 14.2.4 Ordinal Variables Ketika analis berpindah dari skala kontinu ke skala nominal, ada dua sumber utama kehilangan informasi (Bishop, Fienberg, dan Holland, 1975, halaman 343). Yang pertama adalah membagi pengukuran kontinu yang tepat menjadi kelompok-kelompok. Yang kedua adalah kehilangan urutan dari kelompok-kelompok tersebut. Oleh karena itu, masuk akal untuk menjelaskan apa yang dapat kita lakukan dengan variabel yang berada dalam kelompok-kelompok diskrit tetapi dengan urutan yang diketahui. Seperti yang dijelaskan di Bagian 14.1.1, variabel ordinal memberikan urutan yang jelas dari tingkat-tingkat variabel tetapi jarak antara tingkat-tingkat tersebut tidak diketahui. Hubungan asosiasi secara tradisional diukur secara parametrik menggunakan korelasi berbasis normal dan secara nonparametrik menggunakan korelasi Spearman dengan peringkat terikat. 14.2.4.1 Parametric Approach Using Normal Based Correlations Misalkan (y1, y2) adalah pasangan bivariat dengan nilai diskrit pada m1, …, m2. Untuk tabel dua arah dari hitungan ordinal, misalkan nst adalah jumlah dalam baris s dan kolom t. Misalkan \\((n_{m_1\\cdot}, \\ldots, n_{m_2\\cdot})\\) adalah total margin baris, \\((n_{\\cdot m_1}, \\ldots, n_{\\cdot m_2}))\\) adalah total margin kolom, dan \\(n=\\sum_{s,t} n_{s,t}\\) Misalkan \\(\\hat{\\xi}_{1s} = \\Phi^{-1}((n_{m_1}+\\cdots+n_{s\\cdot})/n)\\) untuk \\(s=m_1, ..., m_2\\) menjadi titik potong dan demikian pula untuk \\(\\hat{\\xi}_{2t}\\). Korelasi polikorik, berdasarkan prosedur estimasi dua langkah, adalah \\[ \\begin{array}{cr} \\hat{\\rho_N} &amp;=\\text{argmax}_{\\rho} \\sum_{s=m_1}^{m_2} \\sum_{t=m_1}^{m_2} n_{st} \\log\\left\\{ \\Phi_2(\\hat{\\xi}_{1s}, \\hat{\\xi}_{2t};\\rho) -\\Phi_2(\\hat{\\xi}_{1,s-1}, \\hat{\\xi}_{2t};\\rho) \\right.\\\\ &amp; \\left. -\\Phi_2(\\hat{\\xi}_{1s}, \\hat{\\xi}_{2,t-1};\\rho) +\\Phi_2(\\hat{\\xi}_{1,s-1}, \\hat{\\xi}_{2,t-1};\\rho) \\right\\} . \\end{array} \\] Hal ini disebut korelasi tetrakorik untuk variabel biner. Table 14.4. Two-way Table of Counts for Alarm Credit and NoClaimCredit \\[ {\\small \\begin{matrix} \\begin{array}{c|rr} \\hline &amp; \\text{NoClaimCredit} &amp; \\\\ \\text{AlarmCredit} &amp; 0 &amp; 1 \\\\ \\hline 1 &amp; 1669 &amp; 942 \\\\ 2 &amp; 121 &amp; 118 \\\\ 3 &amp; 195 &amp; 132 \\\\ 4 &amp; 1801 &amp; 661 \\\\ \\hline \\end{array} \\end{matrix}} \\] Anda dapat memperoleh korelasi polikorik atau tetrakorik menggunakan fungsi polychoric() atau tetrachoric() dari pustaka psych di R. Korelasi polikorik diilustrasikan menggunakan Tabel 14.4. Di sini, ρ^N=−0.14, yang berarti terdapat hubungan negatif antara AlarmCredit dan NoClaimCredit. Kode R untuk Korelasi Polikorik library(psych) AlarmCredit&lt;-as.numeric(ifelse(Insample$AC00==1,&quot;1&quot;, ifelse(Insample$AC05==1,&quot;2&quot;, ifelse(Insample$AC10==1,&quot;3&quot;, ifelse(Insample$AC15==1,&quot;4&quot;,0))))) x &lt;- table(AlarmCredit,NoClaimCredit) rhoN&lt;-polychoric(x,correct=FALSE)$rho round(rhoN,2) 14.2.5 Interval Variables Seperti yang dijelaskan di Bagian 14.1.2, variabel interval memberikan urutan yang jelas dari tingkat-tingkat variabel dan jarak numerik antara dua tingkat skala dapat dengan mudah diinterpretasikan. Misalnya, variabel kelompok usia pengemudi adalah variabel interval. Untuk mengukur asosiasi, kedua pendekatan variabel kontinu dan variabel ordinal masuk akal. Pendekatan pertama memanfaatkan pengetahuan tentang urutan meskipun mengasumsikan kelanjutan. Pendekatan kedua tidak bergantung pada kelanjutan tetapi juga tidak menggunakan informasi yang diberikan oleh jarak antara skala. 14.2.6 Discrete and Continuous Variables Korelasi poliseriat didefinisikan dengan cara yang sama ketika satu variabel (y1) bersifat kontinu dan variabel lainnya (y2) bersifat ordinal. Tentukan z sebagai skor normal dari y1. Korelasi poliseriat didefinisikan sebagai \\[ \\hat{\\rho_N} = \\text{argmax}_{\\rho} \\sum_{i=1}^n \\log\\left\\{ \\phi(z_{i1})\\left[ \\Phi\\left(\\frac{\\hat{\\xi}_{2,y_{i2}} - \\rho z_{i1}} {(1-\\rho^2)^{1/2}}\\right) -\\Phi\\left(\\frac{\\hat{\\xi}_{2,y_{i2-1}} - \\rho z_{i1}} {(1-\\rho^2)^{1/2}}\\right) \\right] \\right\\} . \\] Korelasi biserial didefinisikan dengan cara yang sama ketika satu variabel bersifat kontinu dan variabel lainnya bersifat biner. Table 14.5. Summary of Claim by NoClaimCredit \\[ {\\small \\begin{matrix} \\begin{array}{l|r|r} \\hline \\text{NoClaimCredit} &amp; \\text{Mean} &amp;\\text{Total} \\\\ &amp; \\text{Claim} &amp;\\text{Claim} \\\\ \\hline 0 &amp; 22,505 &amp; 85,200,483 \\\\ 1 &amp; 6,629 &amp; 12,282,618 \\\\ \\hline \\end{array} \\end{matrix}} \\] Anda dapat memperoleh korelasi poliseriat atau korelasi biserial menggunakan fungsi polyserial() atau biserial() dari pustaka psych di R. Tabel 14.5 memberikan ringkasan Klaim berdasarkan NoClaimCredit dan korelasi biserial diilustrasikan menggunakan kode R di bawah ini. Nilai ρ^N=−0.04 yang berarti terdapat korelasi negatif antara Klaim dan NoClaimCredit. Kode R untuk Korelasi Biserial library(psych) rhoN&lt;-biserial(Claim,NoClaimCredit) round(rhoN,2) 14.3 Introduction to Copulas Dalam bagian ini, Akan mempelajari: Menggambarkan fungsi distribusi multivariat dalam bentuk fungsi copula. Copula banyak digunakan dalam asuransi dan bidang lainnya untuk memodelkan ketergantungan antara hasil multivariat. Copula adalah fungsi distribusi multivariat dengan margin seragam. Secara khusus, misalkan \\(\\{U_1, \\ldots, U_p\\}\\) adalah p variabel acak seragam pada interval (0,1) Fungsi distribusi mereka \\({C}(u_1, \\ldots, u_p) = \\Pr(U_1 \\leq u_1, \\ldots, U_p \\leq u_p),\\) merupakan copula. Kita berusaha menggunakan copula dalam aplikasi yang berdasarkan pada data yang tidak hanya berdistribusi seragam. Oleh karena itu, pertimbangkan fungsi distribusi margin arbitrer \\(F_1(y_1),…, F_p(y_p)\\). Maka, kita dapat mendefinisikan fungsi distribusi multivariat menggunakan copula sebagai berikut \\[ \\begin{equation} {F}(y_1, \\ldots, y_p)= {C}({F}_1(y_1), \\ldots, {F}_p(y_p)). \\tag{14.1} \\end{equation} \\] Di sini, F adalah fungsi distribusi multivariat. Sklar (1959) menunjukkan bahwa setiap fungsi distribusi multivariat F, dapat ditulis dalam bentuk persamaan (14.1), yaitu dengan menggunakan representasi copula. Sklar juga menunjukkan bahwa, jika distribusi margin bersifat kontinu, maka terdapat representasi copula yang unik. Dalam bab ini, kita fokus pada pemodelan copula dengan variabel kontinu. Untuk kasus diskrit, pembaca dapat melihat Joe (2014) dan Genest dan Nešlohva (2007). Untuk kasus bivariat di mana p=2 , kita dapat menuliskan copula dan fungsi distribusi dari dua variabel acak sebagai berikut \\({C}(u_1, \\, u_2) = \\Pr(U_1 \\leq u_1, \\, U_2 \\leq u_2)\\) dan \\({F}(y_1, \\, y_2)= {C}({F}_1(y_1), {F}_p(y_2)).\\) Sebagai contoh, kita dapat melihat copula yang diperkenalkan oleh Frank (1979). Copula (fungsi distribusi) ini dinyatakan sebagai berikut: \\[ \\begin{equation} {C}(u_1,u_2) = \\frac{1}{\\gamma} \\log \\left( 1+ \\frac{ (\\exp(\\gamma u_1) -1)(\\exp(\\gamma u_2) -1)} {\\exp(\\gamma) -1} \\right). \\tag{14.2} \\end{equation} \\] Ini adalah fungsi distribusi bivariat dengan domainnya pada persegi unit [0,1]2. Di sini, γ adalah parameter ketergantungan, yang mengontrol rentang ketergantungan. Asosiasi positif meningkat seiring peningkatan nilai γ. Seperti yang akan kita lihat, asosiasi positif ini dapat dijelaskan dengan menggunakan koefisien korelasi Spearman (ρS) dan koefisien korelasi Kendall (τ). Copula Frank sering digunakan. Kita akan melihat fungsi copula lainnya di Bagian 14.5. 14.4 Application Using Copulas Dalam bagian ini, kita menganalisis data kerugian dan biaya asuransi menggunakan program statistik R. Kumpulan data ini diperkenalkan dalam penelitian oleh Frees dan Valdez pada tahun 1998 dan sekarang bisa dengan mudah diakses melalui paket copula. Proses penyesuaian model dimulai dengan mempelajari masing-masing variabel utama, yaitu LOSS (kerugian) dan ALAE (biaya tambahan). Selanjutnya, kita memodelkan cara kedua variabel ini saling berhubungan dalam distribusi keseluruhan. 14.4.1 Data Description Kita mulai dengan menggunakan sampel (n=1500) dari keseluruhan data. Kita fokus pada dua variabel pertama dalam data ini, yaitu kerugian (LOSS) dan biaya (ALAE). LOSS adalah klaim tanggung jawab umum dari Insurance Services Office, Inc. (ISO) ALAE adalah biaya khusus yang terkait dengan penyelesaian klaim individu (seperti biaya pengacara dan biaya penyelidikan klaim) Untuk melihat hubungan antara kerugian dan biaya, scatterplot dibuat dengan menggunakan skala dollar dan skala log dollar. Pada panel sebelah kiri, sulit melihat adanya hubungan antara kedua variabel tersebut. Namun, ketergantungan antara keduanya lebih terlihat saat dilihat dalam skala log, seperti yang ditunjukkan pada panel sebelah kanan. 14.4.2 Marginal Models Pertama-tama, kita melihat distribusi terpisah dari kerugian (LOSS) dan biaya (ALAE) sebelum melakukan pemodelan bersama. Dari histogram, terlihat bahwa baik LOSS maupun ALAE cenderung memiliki kecondongan ke kanan (right-skewed) dan ekor yang panjang (fat-tailed). Karena karakteristik ini, untuk kedua distribusi terpisah tersebut, kita menggunakan distribusi Pareto, yang memiliki bentuk fungsi distribusi : \\[ F(y)=1- \\left( 1 + \\frac{\\theta}{y+ \\theta} \\right) ^{\\alpha}. \\] Di sini, \\(θ\\) adalah parameter yang mengatur skala dan α adalah parameter yang mengatur bentuk distribusi. Distribusi terpisah dari kerugian (LOSS) dan biaya (ALAE) ditentukan menggunakan metode maximum likelihood. Khususnya, kami menggunakan fungsi vglm dari paket R VGAM. Pertama, kami menyesuaikan distribusi terpisah dari ALAE. Parameter-parameter yang digunakan dirangkum dalam Tabel 14.6. library(VGAM) fit.ALAE &lt;- vglm(ALAE ~ 1, paretoII(location=0, lscale=&quot;loge&quot;, lshape=&quot;loge&quot;)) # fit the model by vlgm function coef(fit.ALAE, matrix=TRUE) # extract fitted model coefficients, matrix=TRUE gives logarithm of estimated parameters instead of default normal scale estimates Kami mengulangi langkah ini untuk menyesuaikan distribusi terpisah dari variabel LOSS. Karena variabel kerugian juga terlihat memiliki data yang condong ke kanan dan ekor yang panjang, kami juga menggunakan distribusi Pareto untuk memodelkan distribusi marginalnya (walaupun dengan parameter yang berbeda). Table 14.6. Summary of Pareto Maximum Likelihood Fitted Parameters from the LGPIF Data \\[ {\\small \\begin{matrix} \\begin{array}{l|r|r} \\hline &amp; \\text{Shape } \\hat{\\theta} &amp;\\text{Scale } \\hat{\\alpha} \\\\ \\hline ALAE &amp; 15133.60360 &amp; 2.22304 \\\\ LOSS &amp; 16228.14797 &amp; 1.23766 \\\\ \\hline \\end{array} \\end{matrix}} \\] Untuk melihat visualisasi distribusi yang telah disesuaikan dari variabel LOSS dan ALAE, kita dapat menggunakan parameter yang diestimasi dan menggambar fungsi distribusi dan fungsi kepadatan yang sesuai. Untuk informasi lebih lanjut tentang pemilihan model terpisah, lihat Bab 4. 14.4.3 Probability Integral Transformation Ketika mempelajari simulasi, di Bagian 6.1.2 kita belajar tentang metode transformasi terbalik. Metode ini digunakan untuk mengubah variabel acak \\(U(0,1)\\) menjadi variabel acak \\(X\\) dengan menggunakan fungsi distribusi \\(F\\) secara terbalik, yaitu\\(X = F^{-1}(U)\\) Sebaliknya, transformasi integral probabilitas menyatakan bahwa F(X) = U. Namun, transformasi terbalik ini lebih berguna saat distribusi yang digunakan bersifat kontinu. Itu adalah fokus dari bab ini. Kita menggunakan transformasi integral probabilitas untuk dua tujuan: (1) sebagai alat diagnostik untuk memeriksa apakah kita telah dengan benar menentukan fungsi distribusi, dan (2) sebagai input dalam fungsi kopula dalam persamaan (14.1). Untuk tujuan pertama, kita dapat memeriksa apakah distribusi Pareto cocok untuk memodelkan distribusi marjinal kita. Dengan menggunakan distribusi Pareto yang telah disesuaikan, variabel ALAE diubah menjadi variabel u1, yang mengikuti distribusi seragam pada interval [0,1]: \\[ u_1 = \\hat{F}_{1}^{-1}(ALAE) = 1 - \\left( 1 + \\frac{ALAE}{\\hat{\\theta}} \\right)^{-\\hat{\\alpha}}. \\] Setelah melakukan transformasi integral probabilitas pada variabel ALAE, kita membuat histogram dari ALAE yang telah diubah. Plot ini terlihat cukup mendekati apa yang kita harapkan dari distribusi seragam, menunjukkan bahwa distribusi Pareto adalah spesifikasi yang sesuai. Dengan cara yang sama, variabel LOSS juga diubah menjadi variabel u2, yang mengikuti distribusi seragam pada rentang [0,1]. Panel sebelah menunjukkan histogram dari ALAE yang telah diubah, yang sekali lagi memperkuat spesifikasi distribusi Pareto. Untuk melihat data dari sudut pandang lain, variabel u2 dapat diubah menjadi skor normal dengan menggunakan fungsi kuantil dari distribusi normal standar. Seperti yang terlihat, skor normal dari variabel LOSS secara kasar memiliki distribusi normal standar secara marjinal. Gambar ini membantu karena para analis biasanya mencari pola-pola yang mendekati normalitas (yang terlihat jelas dalam gambar). Logikanya adalah, jika distribusi Pareto telah ditentukan dengan benar, maka kerugian yang telah diubah (u2) seharusnya secara kasar berdistribusi normal, dan skor normal \\(\\Phi^{-1}(u_2)\\) seharusnya juga secara kasar berdistribusi normal. (Di sini, \\(\\Phi\\) adalah fungsi distribusi kumulatif dari distribusi normal standar.) 14.4.4 Joint Modeling with Copula Function Sebelum memodelkan secara bersama kerugian dan biaya, kita membuat scatterplot dari variabel yang telah diubah \\((u_1, u_2)\\) Panel sebelah kiri merupakan plot \\(u_1 = \\hat{F}_1(ALAE)\\) dan \\(u_2=\\hat{F}_2(LOSS)\\). Kemudian kita mengubah masing-masing menggunakan fungsi distribusi normal standar terbalik \\(\\Phi^{-1}(\\cdot)\\) atau qnorm di R, untuk mendapatkan skor normal. Dengan penyesuaian skala, pola-pola tersebut menjadi jelas pada panel sebelah kanan. Untuk mempelajari lebih lanjut tentang skor normal dan aplikasinya dalam pemodelan copula, lihat Joe (2014). Panel sebelah kanan menunjukkan bahwa ada ketergantungan positif antara dua variabel acak ini. Hal ini dapat dijelaskan dengan menggunakan koefisien rho Spearman, yang nilainya adalah 0,451. Seperti yang kita pelajari dalam Bagian 14.2.2.1, statistik ini hanya bergantung pada urutan kedua variabel melalui peringkat masing-masing. Oleh karena itu, statistik ini tetap sama baik untuk (1) data asli, data yang telah diubah ke skala seragam pada panel kiri skor normal pada panel kanan. Langkah selanjutnya adalah menghitung estimasi parameter kopula. Salah satu pilihan adalah menggunakan metode maximum likelihood tradisional dan menentukan semua parameter secara bersamaan, namun hal ini dapat memakan waktu komputasi yang lama. Bahkan dalam contoh sederhana ini, hal ini berarti memaksimalkan fungsi (log) likelihood dengan lima parameter, yaitu dua parameter untuk distribusi marjinal ALAE, dua parameter untuk distribusi marjinal LOSS, dan satu parameter untuk kopula. Alternatif yang umum digunakan, yang disebut pendekatan inference for margins (IFM), adalah menggunakan distribusi marjinal yang telah ditentukan, yaitu u1 dan u2, sebagai input ketika menentukan kopula. Pendekatan ini digunakan dalam kasus ini. Pada kode berikutnya, Anda akan melihat bahwa parameter kopula yang cocok adalah \\(\\hat{\\gamma} = 3.114\\) 14.5 Types of Copulas Dalam bagian ini, akan mempelajari tentang: Mendefinisikan jenis-jenis dasar copula, termasuk copula normal, t, eliptis, dan Archimedean. Menafsirkan batas-batas yang membatasi fungsi distribusi copula saat tingkat ketergantungan bervariasi. Menghitung ukuran asosiasi untuk berbagai copula dan menafsirkan properti-properti mereka. Menafsirkan ketergantungan ekor untuk berbagai copula. Ada beberapa keluarga copula yang telah dijelaskan dalam literatur. Dua keluarga utama copula adalah copula Archimedean dan copula Eliptis. 14.5.1 Normal (Gaussian) Copulas Kami memulai studi kami dengan copula Frank dalam persamaan (14.2) karena dapat menangkap ketergantungan positif dan negatif serta memiliki bentuk analitik yang mudah dipahami. Namun, ekstensi ke kasus multivariat dengan p&gt;2 tidak mudah, oleh karena itu kami mencari alternatif lain. Secara khusus, distribusi normal atau Gaussian telah digunakan dalam banyak penelitian empiris sejak Gauss pada tahun 1887. Oleh karena itu, adalah wajar untuk mengacu pada distribusi ini sebagai pembanding untuk memahami ketergantungan multivariat. Untuk distribusi normal multivariat, bayangkan p variabel acak normal, masing-masing dengan rata-rata nol dan simpangan baku satu. Ketergantungan antara variabel-variabel ini dikendalikan oleh Σ , sebuah matriks korelasi dengan elemen-elemen satu di diagonal. Angka pada baris ke- i dan kolom ke- j , misalnya Σij , memberikan korelasi antara variabel acak normal ke- i dan ke- j . Kumpulan variabel acak ini memiliki distribusi normal multivariat dengan fungsi densitas probabilitas \\[ \\begin{equation} \\phi_N (\\mathbf{z})= \\frac{1}{(2 \\pi)^{p/2}\\sqrt{\\det \\boldsymbol \\Sigma}} \\exp\\left( -\\frac{1}{2} \\mathbf{z}^{\\prime} \\boldsymbol \\Sigma^{-1}\\mathbf{z}\\right). \\tag{14.3} \\end{equation} \\] Untuk mengembangkan versi copula yang sesuai, dimungkinkan untuk memulai dengan persamaan (14.1), mengevaluasinya menggunakan variabel normal, dan melakukan sedikit perhitungan kalkulus. Namun, kami hanya menyatakan sebagai definisi bahwa fungsi densitas copula normal (Gaussian) adalah \\[ c_N(u_1, \\ldots, u_p) = \\phi_N \\left(\\Phi^{-1}(u_1), \\ldots, \\Phi^{-1}(u_p) \\right) \\prod_{j=1}^p \\frac{1}{\\phi(\\Phi^{-1}(u_j))}. \\] Di sini, kami menggunakan \\(Φ\\) dan \\(ϕ\\) untuk menyatakan fungsi distribusi dan densitas normal standar. Berbeda dengan fungsi densitas probabilitas biasa \\(ϕN\\), fungsi densitas copula memiliki ranahnya pada hiper-kubus [0,1]p. par(mfrow=c(1, 2)) # BIVARIATE NORMAL DF WITH RHO = 0.25 norm.pdf &lt;- mvdc(normalCopula(0.25), c(&quot;norm&quot;, &quot;norm&quot;), list(list(mean = 0, sd =1), list(mean = 0, sd =1))) persp(norm.pdf, dMvdc, xlim = c(-3, 3), ylim=c(-3,3), zlab = expression(&quot;\\U03A6&quot;), xlab =&quot;x&quot;, ylab=&quot;y&quot;, cex.lab=0.8, cex.axis = 0.3) # BIVARIATE NORMAL COPULA WITH RHO = 0.25 norm.cop &lt;- ellipCopula(&quot;normal&quot;, param = c(0.25),dim = 2, dispstr = &quot;un&quot;) persp(norm.cop, dCopula, theta = 30, zlab=&quot;c(u,v)&quot;, xlab =&quot;u&quot;, ylab=&quot;v&quot;, cex.lab=0.8, cex.axis = 0.3) 14.5.2 t- and Elliptical Copulas Salah satu copula yang banyak digunakan dalam praktik adalah copula t. Baik copula t maupun copula normal adalah contoh khusus dari keluarga yang dikenal sebagai copula elliptical, oleh karena itu kami memperkenalkan keluarga umum ini terlebih dahulu, kemudian mengkhususkan pada kasus copula t. Distribusi normal dan distribusi t adalah contoh dari distribusi simetris. Lebih umumnya, distribusi elliptical adalah kelas distribusi yang simetris dan dapat bersifat multivariat. Singkatnya, distribusi elliptical adalah jenis distribusi multivariat yang simetris. Distribusi normal multivariat dan distribusi t multivariat adalah jenis khusus dari distribusi elliptical. Copula elliptical dibangun dari distribusi elliptical. Copula ini memecah (multivariat) distribusi elliptical menjadi distribusi marginal elliptical univariatnya melalui teorema Sklar. Sifat-sifat copula elliptical dapat diperoleh dari sifat-sifat distribusi elliptical yang sesuai, lihat contohnya Hofert et al. (2018). Secara umum, suatu vektor acak p-dimensi memiliki distribusi elliptical jika densitasnya dapat ditulis sebagai \\[ h_E (\\mathbf{z})= \\frac{k_p}{\\sqrt{\\det \\boldsymbol \\Sigma}} g_p \\left( \\frac{1}{2} (\\mathbf{z}- \\boldsymbol \\mu)^{\\prime} \\boldsymbol \\Sigma^{-1}(\\mathbf{z}- \\boldsymbol \\mu) \\right) , \\] untuk z∈Rp dan kp adalah konstanta, yang ditentukan agar densitas terintegrasi menjadi satu. Fungsi gp(⋅) disebut generator karena dapat digunakan untuk menghasilkan distribusi-distribusi yang berbeda. Tabel 14.7 merangkum beberapa pilihan yang digunakan dalam praktik aktuaria. Pilihan gp(x)=exp(−x) menghasilkan fungsi kepadatan probabilitas normal pada persamaan (14.3). Pilihan gp(x)=exp(−(1+2x/r)−(p+r)/2) menghasilkan distribusi t multivariat dengan r derajat kebebasan dan fungsi kepadatan probabilitasnya adalah. \\[ h_{t_r} (\\mathbf{z})= \\frac{k_p}{\\sqrt{\\det \\boldsymbol \\Sigma}} \\exp\\left[- \\left( 1+ \\frac{(\\mathbf{z}- \\boldsymbol \\mu)^{\\prime} \\boldsymbol \\Sigma^{-1}(\\mathbf{z}- \\boldsymbol \\mu)}{r} \\right)^{-(p+r)/2}\\right] . \\] Table 14.7. Generator Functions ( gp(⋅)) for Selected Elliptical Distributions \\[ \\small\\begin{array}{lc} \\hline &amp; Generator \\\\ Distribution &amp; g_p(x) \\\\ \\hline \\text{Normal distribution} &amp; e^{-x}\\\\ t-\\text{distribution with }r \\text{ degrees of freedom} &amp; (1+2x/r)^{-(p+r)/2}\\\\ \\text{Cauchy} &amp; (1+2x)^{-(p+1)/2}\\\\ \\text{Logistic} &amp; e^{-x}/(1+e^{-x})^2\\\\ \\text{Exponential power} &amp; \\exp(-rx^s)\\\\ \\hline \\end{array} \\] Ketika menghasilkan copula, kita dapat menggunakan distribusi elliptical. Karena copula lebih fokus pada hubungan, kita dapat membatasi pertimbangan kita pada kasus di mana μ=0 dan Σ adalah matriks korelasi. Dengan batasan ini, distribusi marginal dari copula elliptical multivariat adalah identik; kita menggunakan H untuk merujuk pada fungsi distribusi marginal ini, dan h adalah densitas yang sesuai. Densitas marginal ini adalah h(z)=k1g1(z2/2). Sebagai contoh, dalam kasus normal, kita memiliki H(⋅)=Φ(⋅) dan h(⋅)=ϕ(⋅). Sekarang kita siap untuk mendefinisikan fungsi kepadatan probabilitas (pdf) dari copula elliptical, suatu fungsi yang didefinisikan pada kubus unit [0,1]p sebagai \\[ {c}_E(u_1, \\ldots, u_p) = h_E \\left(H^{-1}(u_1), \\ldots, H^{-1}(u_p) \\right) \\prod_{j=1}^p \\frac{1}{h(H^{-1}(u_j))}. \\] Seperti yang telah disebutkan sebelumnya, sebagian besar karya empiris berfokus pada copula normal dan copula t. Secara khusus, copula t berguna untuk memodelkan ketergantungan pada ekor distribusi bivariat, terutama dalam aplikasi analisis risiko keuangan. Copula t dengan parameter asosiasi yang sama namun dengan variasi parameter derajat kebebasan menunjukkan struktur ketergantungan ekor yang berbeda. Untuk informasi lebih lanjut tentang copula t, pembaca dapat melihat Joe (2014) dan Hofert et al. (2018). 14.5.3 Archimedean Copulas Kelas copula ini juga dibangun dari fungsi generator. Untuk copula Archimedean, kita asumsikan bahwa g(⋅) adalah fungsi konveks, monoton menurun dengan domain [0,1] dan jangkauan \\([0,∞)\\) sehingga g(0)=0 . Gunakan g−1 untuk fungsi invers dari g . Maka fungsi \\(C_g(u_1, \\ldots, u_p) = g^{-1} \\left(g(u_1)+ \\cdots + g(u_p) \\right)\\) dikatakan sebagai fungsi distribusi copula Archimedean. Untuk kasus bivariat, p=2 , fungsi copula Archimedean dapat dituliskan oleh fungsi \\(C_{g}(u_1, \\, u_2) = g^{-1} \\left(g(u_1) + g(u_2) \\right).\\) Beberapa kasus khusus penting dari copula Archimedean meliputi copula Frank, Clayton/Cook-Johnson, dan copula Gumbel/Hougaard. Setiap kelas copula ini berasal dari fungsi generator yang berbeda. Sebagai contoh kasus khusus yang juga berguna, ingatlah copula Frank yang dijelaskan dalam Bagian 14.3 dan 14.4. Untuk memberikan ilustrasi, kami sekarang menyediakan ekspresi eksplisit untuk copula Clayton dan Gumbel/Hougaard. Copula Clayton Untuk p=2 , copula Clayton didefinisikan oleh parameter γ∈[−1,∞) sebagai berikut \\[ C_{\\gamma}^C(u)=\\max\\{u_1^{-\\gamma}+u_2^{-\\gamma}-1,0\\}^{1/\\gamma}, \\quad u \\in [0,1]^2. \\] Ini adalah fungsi distribusi bivariat yang didefinisikan pada kubus unit [0,1]2. Rentang ketergantungan dikendalikan oleh parameter γ , serupa dengan copula Frank. Copula Gumbel-Hougaard Copula Gumbel-Hougaard memiliki parameter γ∈[1,∞) dan didefinisikan sebagai berikut \\[ C_{\\gamma}^{GH}(u)=\\exp\\left(-\\left(\\sum_{i=1}^2 (-\\log u_i)^{\\gamma}\\right)^{1/\\gamma}\\right), \\quad u\\in[0,1]^2. \\] Untuk informasi lebih lanjut mengenai copula Archimedean, lihat Joe (2014), Frees and Valdez (1998), dan Genest and Mackay (1986). 14.5.4 Properties of Copulas Dengan banyak pilihan copula yang tersedia, penting bagi analis untuk memahami fitur umum dari perilaku alternatif-alternatif ini. 14.5.4.1 Bounds on Association Setiap fungsi distribusi dibatasi oleh nol dari bawah dan satu dari atas. Jenis batasan tambahan tersedia dalam konteks multivariat. Batasan-batasan ini berguna ketika mempelajari ketergantungan. Artinya, saat seorang analis mempertimbangkan variabel-variabel sebagai sangat tergantung, tersedia batasan-batasan yang tidak dapat dilampaui, terlepas dari tingkat ketergantungan. Batasan-batasan yang paling umum digunakan dalam pemodelan ketergantungan dikenal sebagai batasan Fréchet-Höeffding, dinyatakan sebagai berikut \\[ \\max( u_1 +\\cdots+ u_p + p -1, 0) \\leq C(u_1, \\ldots, u_p) \\leq \\min (u_1, \\ldots,u_p). \\] Untuk melihat bagian kanan persamaan ini, perhatikan bahwa \\[ C(u_1,\\ldots, u_p) = \\Pr(U_1 \\leq u_1, \\ldots, U_p \\leq u_p) \\leq \\Pr(U_j \\leq u_j), \\] untuk j=1,…,p. Batasan ini tercapai ketika \\(U_1 = \\cdots = U_p\\). Untuk melihat bagian kiri persamaan ini ketika p=2, pertimbangkan \\(U_2=1-U_1\\). Dalam kasus ini, jika \\(1−u_2&lt;u_1\\), maka \\[ \\Pr(U_1 \\leq u_1, U_2 \\leq u_2) = \\Pr ( 1-u_2 \\leq U_1 &lt; u_1) =u_1+u_2-1. \\] Lihat, misalnya, Nelson (1997) untuk diskusi tambahan. Untuk melihat bagaimana batasan-batasan ini berhubungan dengan konsep ketergantungan, pertimbangkan kasus p=2. Sebagai acuan, perhatikan bahwa copula produk adalah C(u1,u2)=u1⋅u2, yang mengasumsikan independensi antara variabel acak. Sekarang, dari pembahasan di atas, kita dapat melihat bahwa batas bawah tercapai ketika dua variabel acak tersebut memiliki hubungan negatif yang sempurna ( U2=1−U1). Selanjutnya, jelas bahwa batas atas tercapai ketika mereka memiliki hubungan positif yang sempurna ( U2=U1). 14.5.4.2 Measures of Association Hal menarik tentang ekspresi ini adalah bahwa ukuran ringkasan asosiasi ini hanya didasarkan pada peringkat setiap variabel. Dengan demikian, transformasi yang meningkatkan secara ketat tidak mempengaruhi ukuran-ukuran asosiasi ini. Secara khusus, pertimbangkan dua variabel acak, Y1 dan Y2, dan biarkan m1 dan m2 menjadi fungsi yang meningkatkan secara ketat. Maka, asosiasi, ketika diukur dengan Spearman’s rho atau tau Kendall, antara m1(Y1) dan m2(Y2) tidak berubah terlepas dari pilihan m1 dan m2. Sebagai contoh, ini memungkinkan analis untuk mempertimbangkan dolar, Euro, atau log dolar, dan masih mempertahankan ketergantungan yang sama. Seperti yang telah kita lihat di Bagian 14.2, hal ini tidak terjadi dengan ukuran korelasi Pearson. Schweizer, Wolff, dan yang lainnya (1981) membuktikan bahwa copula memperhitungkan seluruh ketergantungan dalam arti bahwa cara Y1 dan Y2 “bergerak bersama” ditangkap oleh copula, terlepas dari skala di mana setiap variabel diukur. Mereka juga menunjukkan bahwa (versi populasi) kedua ukuran asosiasi nonparametrik standar dapat diungkapkan semata-mata dalam hal fungsi copula. Koefisien korelasi Spearman diberikan oleh \\[ \\begin{equation} \\rho_S = 12 \\int_0^1 \\int_0^1 \\left\\{C(u,v) - uv \\right\\} du dv. \\tag{14.4} \\end{equation} \\] Kendall’s tau diberikan oleh \\(\\tau= 4 \\int_0^1 \\int_0^1 C(u,v)~dC(u,v) - 1 .\\) Untuk ekspresi-ekspresi ini, kita berasumsi bahwa Y1 dan Y2 memiliki fungsi distribusi bersama yang kontinu. Example. Loss versus Expenses Sebelumnya, di Bagian 14.4, kita melihat bahwa korelasi Spearman adalah 0.452, dihitung dengan fungsi rho. Kemudian, kita memasangkan copula Frank dengan data ini, dan memperkirakan parameter ketergantungan menjadi γ^=0.452. Sebagai alternatif, kode berikut menunjukkan bagaimana menggunakan versi empiris dari persamaan (14.4). Dalam kasus ini, koefisien korelasi Spearman adalah 0.462, yang mendekati koefisien korelasi Spearman sampel, yaitu 0.452. 14.5.4.3 Tail Dependency Ada aplikasi di mana berguna untuk membedakan bagian dari distribusi di mana asosiasi paling kuat terjadi. Sebagai contoh, dalam asuransi, penting untuk memahami asosiasi di antara kerugian terbesar, yaitu asosiasi di ekor kanan data. Untuk menangkap tipe ketergantungan ini, kita menggunakan fungsi konsentrasi ekor kanan, yang didefinisikan sebagai \\[ R(z) = \\frac{\\Pr(U_1 &gt;z, U_2 &gt; z)}{1-z} =\\Pr(U_1 &gt; z | U_2 &gt; z) =\\frac{1 - 2z + C(z,z)}{1-z} . \\] Sebagai pembanding, R(z) akan sama dengan z dalam kondisi independen. Joe (1997) menggunakan istilah “parameter ketergantungan ekor atas” untuk R=limz→1R(z). Dengan cara yang sama, kita dapat mendefinisikan fungsi konsentrasi ekor kiri sebagai \\[ L(z) = \\frac{\\Pr(U_1 \\leq z, U_2 \\leq z)}{z}=\\Pr(U_1 \\leq z | U_2 \\leq z) =\\frac{ C(z,z)}{z}, \\] dengan parameter ketergantungan ekor bawah \\(L = \\lim_{z \\rightarrow 0} L(z)\\). Fungsi konsentrasi ketergantungan ekor menangkap probabilitas dua variabel acak memiliki nilai ekstrem secara bersamaan. Menarik untuk melihat seberapa baik copula tertentu dapat menangkap ketergantungan ekor. Untuk tujuan ini, kita menghitung fungsi konsentrasi ekor kiri dan ekor kanan untuk empat jenis copula yang berbeda; Normal, Frank, Gumbel, dan t-copula. Hasilnya dirangkum dalam nilai fungsi konsentrasi untuk keempat copula ini dalam Tabel 14.8. Seperti dalam Venter (2002), kita menunjukkan L(z) untuk z≤0.5 dan R(z) untuk z&gt;0.5 dalam plot ketergantungan ekor pada Gambar 14.9. Kami menginterpretasikan plot ketergantungan ekor ini untuk berarti bahwa copula Frank dan Normal tidak menunjukkan ketergantungan ekor sedangkan copula t dan Gumbel melakukannya. Copula t simetris dalam perlakuan ekor atas dan ekor bawah. Table 14.8. Tail Dependence Parameters for Four Copulas \\[ {\\small \\begin{matrix} \\begin{array}{l|rr} \\hline \\text{Copula} &amp; \\text{Lower} &amp; \\text{Upper} \\\\ \\hline \\text{Frank} &amp; 0 &amp; 0 \\\\ \\text{Gumbel} &amp; 0 &amp; 0.74 \\\\ \\text{Normal} &amp; 0 &amp; 0 \\\\ t- &amp; 0.10 &amp; 0.10 \\\\ \\hline \\end{array} \\end{matrix}} \\] library(copula) ## ## Attaching package: &#39;copula&#39; ## The following objects are masked from &#39;package:VGAM&#39;: ## ## log1mexp, log1pexp, rlog U1 = seq(0,0.5, by=0.002) U2 = seq(0.5,1, by=0.002) U = rbind(U1, U2) TailFunction &lt;- function(Tailcop) { lowertail &lt;- pCopula(cbind(U1,U1), Tailcop)/U1 uppertail &lt;- (1-2*U2 +pCopula(cbind(U2,U2), Tailcop))/(1-U2) jointtail &lt;- rbind(lowertail,uppertail) } Tailcop1 &lt;- archmCopula(family = &quot;frank&quot;, param= c(0.05), dim = 2) Tailcop2 &lt;- archmCopula(family = &quot;gumbel&quot;,param = 3) Tailcop3 &lt;- ellipCopula(&quot;normal&quot;, param = c(0.25),dim = 2, dispstr = &quot;un&quot;) Tailcop4 &lt;- ellipCopula(&quot;t&quot;, param = c(0.25),dim = 2, dispstr = &quot;un&quot;, df=5) jointtail1 &lt;- TailFunction(Tailcop1) jointtail2 &lt;- TailFunction(Tailcop2) jointtail3 &lt;- TailFunction(Tailcop3) jointtail4 &lt;- TailFunction(Tailcop4) plot(U,jointtail1, cex=.2, xlim=c(0,1),ylab=&quot;Tail Dependence&quot;, ylim=c(0,1)) lines(U,jointtail2, type=&quot;p&quot;,lty=1, cex=.2) lines(U,jointtail3, type=&quot;p&quot;,lty=1, cex=.2) lines(U,jointtail4, type=&quot;p&quot;,lty=1, cex=.2) text(0.75, 0.1, &quot;Frank&quot;, cex=1.3) #1 text(0.1, 0.8, &quot;Gumbel&quot;, cex=1.3) #2 text(0.25, 0.1, &quot;normal&quot;, cex=1.3) #3 arrows(.17, 0.1, .07, 0.12,code=2, angle=20, length=0.1) text(0.9, 0.4, &quot;t with 5 df&quot;, cex=1.3) #4 14.6 Why is Dependence Modeling Important? Model ketergantungan sangat penting karena memungkinkan kita memahami struktur ketergantungan dengan mendefinisikan hubungan antara variabel dalam dataset. Dalam asuransi, mengabaikan pemodelan ketergantungan mungkin tidak berdampak pada penetapan harga tetapi dapat menyebabkan kesalahan dalam memperkirakan modal yang diperlukan untuk menutupi kerugian. Sebagai contoh, dari Bagian 14.4, terlihat adanya hubungan positif antara LOSS dan ALAE. Ini berarti bahwa jika terjadi kerugian besar, kita mengharapkan biaya juga akan besar dan mengabaikan hubungan ini dapat menyebabkan kesalahan dalam estimasi cadangan. Untuk mengilustrasikan pentingnya pemodelan ketergantungan, kami mengacu pada contoh manajemen portofolio di Bagian 10.4.3.3 yang mengasumsikan bahwa risiko properti dan risiko tanggung jawab adalah independen. Sekarang, kita memasukkan ketergantungan dengan memperbolehkan empat lini bisnis saling bergantung melalui copula Gaussian. Dalam Tabel 14.9, kami menunjukkan bahwa ketergantungan mempengaruhi kuantisasi portofolio (VaRq), meskipun tidak mempengaruhi nilai yang diharapkan. Misalnya, VaR0.99 untuk risiko total yang merupakan jumlah modal yang diperlukan untuk memastikan, dengan tingkat kepastian 99%, bahwa perusahaan tidak menjadi teknis insolven lebih tinggi ketika kita memasukkan ketergantungan. Hal ini menyebabkan alokasi modal yang lebih sedikit ketika ketergantungan diabaikan dan dapat menyebabkan masalah solvabilitas yang tidak terduga. Table 14.9. Results for Portfolio Expected Value and Quantiles (VaRq) \\[ {\\small \\begin{matrix} \\begin{array}{l|rrrr} \\hline \\text{Independent} &amp;\\text{Expected} &amp; VaR_{0.9} &amp; VaR_{0.95} &amp; VaR_{0.99} \\\\ &amp;\\text{Value} &amp; &amp; &amp; \\\\ \\hline \\text{Retained} &amp; 269 &amp; 300 &amp; 300 &amp; 300 \\\\ \\text{Insurer} &amp; 2,274 &amp; 4,400 &amp; 6,173 &amp; 11,859 \\\\ \\text{Total} &amp; 2,543 &amp; 4,675 &amp; 6,464 &amp; 12,159 \\\\ \\hline \\text{Gaussian Copula}&amp;\\text{Expected}&amp; VaR_{0.9} &amp; VaR_{0.95} &amp; VaR_{0.99} \\\\ &amp;\\text{Value} &amp; &amp; &amp; \\\\ \\hline \\text{Retained} &amp; 269 &amp; 300 &amp; 300 &amp; 300 \\\\ \\text{Insurer} &amp; 2,340 &amp; 4,988 &amp; 7,339 &amp; 14,905 \\\\ \\text{Total} &amp; 2,609 &amp; 5,288 &amp; 7,639 &amp; 15,205 \\\\ \\hline \\end{array} \\end{matrix}} \\] 14.7 Further Resources and Contributors TS 14.A. Other Classic Measures of Scalar Associations TS 14.A.1. Blomqvist’s Beta Blomqvist (1950) mengembangkan ukuran ketergantungan yang sekarang dikenal sebagai beta Blomqvist, juga disebut koefisien kesesuaian median dan koefisien korelasi medial. Dengan menggunakan fungsi distribusi, parameter ini dapat diungkapkan sebagai \\[ \\begin{equation*} \\beta_B = 4F\\left(F^{-1}_X(1/2),F^{-1}_Y(1/2) \\right) - 1. \\end{equation*} \\] Artinya, pertama-tama nilai setiap variabel margin pada median \\(F^{-1}_X(1/2)\\),\\(F^{-1}_Y(1/2)\\) masing-masing). Kemudian, nilai fungsi distribusi bivariat pada dua median tersebut. Setelah dilakukan penskalaan (dikalikan dengan 4 dan dikurangi 1), koefisien tersebut memiliki rentang [−1,1], di mana 0 terjadi pada kondisi independen. Seperti halnya rho Spearman dan tau Kendall, estimator berdasarkan peringkat mudah diberikan. Pertama, tuliskan \\(\\beta_B = 4C(1/2,1/2)-1 = 2\\Pr((U_1-1/2)(U_2-1/2))-1\\) di mana U1 dan U2 adalah variabel acak seragam. Kemudian, definisikan \\[ \\hat{\\beta}_B = \\frac{2}{n} \\sum_{i=1}^n I\\left( (R(X_{i})-\\frac{n+1}{2})(R(Y_{i})-\\frac{n+1}{2}) \\ge 0 \\right)-1 . \\] Lihat, misalnya, Joe (2014), halaman 57 atau Hougaard (2000), halaman 135, untuk lebih banyak detail. Karena parameter Blomqvist didasarkan pada pusat distribusi, ini sangat berguna ketika data tercemar; dalam kasus ini, informasi pada bagian ekstrem distribusi tidak selalu dapat diandalkan. Bagaimana ini mempengaruhi pilihan ukuran asosiasi? Pertama, ingat bahwa ukuran asosiasi didasarkan pada fungsi distribusi bivariat. Jadi, jika seseorang memiliki pengetahuan tentang perkiraan yang baik dari fungsi distribusi, maka perhitungan ukuran asosiasi cukup mudah secara prinsip. Kedua, untuk data yang tercemar, ekstensi bivariat dari estimator fungsi distribusi Kaplan-Meier univariat tersedia. Misalnya, versi yang diperkenalkan dalam Dabrowska (1988) menarik. Namun, karena terjadi kasus-kasus ketika sejumlah besar data muncul pada rentang atas data, estimator fungsi distribusi bivariat ini tidak dapat diandalkan. Ini berarti bahwa ukuran ringkasan dari perkiraan fungsi distribusi berdasarkan rho Spearman atau tau Kendall dapat tidak dapat diandalkan. Untuk situasi ini, beta Blomqvist tampaknya menjadi pilihan yang lebih baik karena fokus pada pusat distribusi. Hougaard (2000), Bab 14, memberikan diskusi tambahan. Anda dapat mendapatkan beta Blomqvist menggunakan fungsi betan() dari pustaka copula di R. Dari informasi di bawah ini, βB=0.3 antara variabel peringkat Coverage dalam juta dolar dan variabel jumlah Claim dalam dolar. ### Blomqvist&#39;s beta correlation between Claim and Coverage ### library(copula) n&lt;-length(Claim) U&lt;-cbind(((n+1)/n*pobs(Claim)),((n+1)/n*pobs(Coverage))) beta&lt;-betan(U, scaling=FALSE) round(beta,2) ### Blomqvist&#39;s beta correlation between Claim and log(Coverage) ### n&lt;-length(Claim) Fx&lt;-cbind(((n+1)/n*pobs(Claim)),((n+1)/n*pobs(log(Coverage)))) beta&lt;-betan(Fx, scaling=FALSE) round(beta,2) Selain itu, untuk menunjukkan bahwa beta Blomqvist tetap tidak berubah dalam transformasi yang ketat meningkat, βB=0.3 antara variabel peringkat Coverage dalam jutaan dolar logaritmik dan variabel jumlah Claim dalam dolar. TS 14.A.2. Nonparametric Approach Using Spearman Correlation with Tied Ranks Untuk variabel pertama, peringkat rata-rata pengamatan dalam baris s adalah \\[ \\begin{equation*} r_{1s} = n_{m_1\\cdot}+ \\cdots+ n_{s-1,\\cdot}+ \\frac{1}{2} \\left(1+ n_{s\\cdot}\\right) \\end{equation*} \\] dan demikian pula \\(r_{2t} = \\frac{1}{2} \\left[(n_{\\cdot m_1}+ \\cdots+ n_{\\cdot,s-1}+1)+ (n_{\\cdot m_1}+ \\cdots+ n_{\\cdot s})\\right]\\)] . Dengan ini, kita memiliki rho Spearman dengan peringkat terseret \\[ \\begin{equation*} \\hat{\\rho}_S = \\frac{\\sum_{s=m_1}^{m_2} \\sum_{t=m_1}^{m_2} n_{st}(r_{1s} - \\bar{r})(r_{2t} - \\bar{r})} {\\left[\\sum_{s=m_1}^{m_2}n_{s \\cdot}(r_{1s} - \\bar{r})^2 \\sum_{t=m_1}^{m_2} n_{\\cdot t}(r_{2t} - \\bar{r})^2 \\right]^2} \\end{equation*} \\] di mana rata-rata peringkat adalah \\(\\bar{r} = (n+1)/2\\) . . Untuk memperoleh statistik korelasi Spearman yang dikoreksi dengan tie menggunakan fungsi cor() di R dan memilih metode spearman. Dari informasi di bawah ini, ρ^S=−0.09. Kode R untuk Korelasi Spearman yang Dikoreksi dengan Tie. rs_ties&lt;-cor(AlarmCredit,NoClaimCredit, method = c(&quot;spearman&quot;)) round(rs_ties,2) &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 14.8 Referensi https://openacttexts.github.io/Loss-Data-Analytics/ChapFrequency-Modeling.html#S:goodness-of-fit 2b87845f7c3bc16ab57388bb56dc493594911fc0 "],["appendix-a-review-of-statistical-inference.html", "Bab 15 Appendix A: Review of Statistical Inference 15.1 konsep dasar 15.2 Estimasi Titik dan Properti 15.3 Estimasi Interval 15.4 Pengujian Hipotesis", " Bab 15 Appendix A: Review of Statistical Inference 15.1 konsep dasar Inferensi statistik adalah proses membuat kesimpulan tentang karakteristik dari sekumpulan besar item/individu (yaitu, populasi), menggunakan set data yang mewakili (misalnya, sampel acak) dari daftar item atau individu dari populasi yang dapat diambil sampel. Meskipun proses ini memiliki berbagai aplikasi di berbagai bidang termasuk ilmu pengetahuan, rekayasa, kesehatan, sosial, dan ekonomi, inferensi statistik penting bagi perusahaan asuransi yang menggunakan data dari pemegang polis mereka yang ada untuk membuat inferensi tentang karakteristik (misalnya, profil risiko) dari segmen target pelanggan tertentu (yaitu, populasi) yang tidak langsung diamati oleh perusahaan asuransi tersebut. 15.1.1 Sampel Acak Dalam statistika, kesalahan sampling terjadi ketika kerangka sampling, yaitu daftar dari mana sampel diambil, tidak cukup mewakili populasi yang menjadi tujuan. Sampel harus merupakan subset yang representatif dari populasi atau universum yang menjadi tujuan. Jika sampel tidak representatif, mengambil sampel yang lebih besar tidak akan menghilangkan bias, karena kesalahan yang sama terulang berulang kali. Oleh karena itu, kita memperkenalkan konsep sampling acak yang menghasilkan sampel acak sederhana yang mewakili populasi. Kita mengasumsikan bahwa variabel acak \\(X\\) mewakili pengambilan dari populasi dengan fungsi distribusi \\(F(\\cdot)\\) dengan rata-rata \\(\\mathrm{E}[X]=\\mu\\) dan varians \\(\\mathrm{Var}[X]=\\mathrm{E}[(X-\\mu)^2]\\), di mana \\(E(\\cdot)\\) menunjukkan ekspektasi dari variabel acak. Dalam sampling acak, kita melakukan total \\(n\\) pengambilan seperti yang direpresentasikan oleh \\(X_1,...,X_n\\), yang masing-masing tidak saling berhubungan (yaitu, statistik independen). Kita mengacu pada \\(X_1,...,X_n\\) sebagai sampel acak (dengan penggantian) dari \\(F(\\cdot)\\), baik dalam bentuk parametrik maupun nonparametrik. Alternatifnya, kita dapat mengatakan bahwa \\(X_1,...,X_n\\) secara identik dan independen didistribusikan (iid) dengan fungsi distribusi \\(F(\\cdot)\\). 15.1.2 Distribusi Sampel Dengan menggunakan sampel acak \\(X_1,...,X_n\\), kita tertarik untuk membuat kesimpulan tentang atribut tertentu dari distribusi populasi \\(F(\\cdot)\\). Misalnya, kita mungkin tertarik untuk membuat inferensi tentang rata-rata populasi yang ditandai dengan \\(\\mu\\). Alami untuk mempertimbangkan rata-rata sampel, \\(\\bar{X}=\\sum_{i=1}^nX_i\\), sebagai estimasi dari rata-rata populasi \\(\\mu\\). Kita menyebut rata-rata sampel sebagai statistik yang dihitung dari sampel acak \\(X_1,...,X_n\\). Statistik ringkasan lain yang umum digunakan termasuk simpangan baku sampel dan kuantil sampel. Ketika menggunakan statistik (misalnya, rata-rata sampel \\(\\bar{X}\\)) untuk membuat inferensi statistik tentang atribut populasi (misalnya, rata-rata populasi \\(\\mu\\)), kualitas inferensi ditentukan oleh bias dan ketidakpastian dalam estimasi, karena menggunakan sampel sebagai pengganti populasi. Oleh karena itu, penting untuk mempelajari distribusi statistik yang mengukur bias dan variabilitas statistik tersebut. Secara khusus, distribusi rata-rata sampel, \\(\\bar{X}\\) (atau statistik lainnya), disebut sebagai distribusi sampling. Distribusi sampling bergantung pada proses sampling, statistik, ukuran sampel \\(n\\), dan distribusi populasi \\(F(\\cdot)\\). Teorema limit sentral memberikan distribusi sampel (sampling) dari rata-rata sampel dalam sampel besar di bawah kondisi tertentu. 15.1.3 Teorema Limit Sentral Dalam statistika, terdapat variasi dari teorema limit sentral (Central Limit Theorem, CLT) yang menjamin bahwa, dalam kondisi tertentu, rata-rata sampel akan mendekati rata-rata populasi dengan distribusi samplingnya mendekati distribusi normal saat ukuran sampel mendekati tak hingga. Kami menyebutkan teorema limit sentral Lindeberg-Levy yang menetapkan distribusi sampling asimtotik dari rata-rata sampel \\(\\bar{X}\\) yang dihitung menggunakan sampel acak dari populasi universe dengan distribusi \\(F(\\cdot)\\). Teorema limit sentral Lindeberg-Levy. Misalkan \\(X_1,...,X_n\\) adalah sampel acak dari distribusi populasi \\(F(\\cdot)\\) dengan rata-rata \\(\\mu\\) dan varians \\(\\sigma^2&lt;\\infty\\). Selisih antara rata-rata sampel \\(\\bar{X}\\) dan \\(\\mu\\), ketika dikalikan dengan akar kuadrat \\(n\\), konvergen dalam distribusi menjadi distribusi normal saat ukuran sampel mendekati tak hingga. Artinya, \\[\\begin{equation} \\sqrt{n}(\\bar{X}-\\mu)\\xrightarrow[]{d}N(0,\\sigma). \\end{equation}\\] Perlu dicatat bahwa teorema limit sentral tidak memerlukan bentuk parametrik untuk \\(F(\\cdot)\\). Berdasarkan teorema limit sentral, kita dapat melakukan inferensi statistik pada rata-rata populasi (kita menyimpulkan, bukan menyimpulkan secara deduktif). Jenis inferensi yang dapat kita lakukan meliputi estimasi populasi, pengujian hipotesis tentang kebenaran suatu pernyataan nol, dan prediksi sampel masa depan dari populasi. 15.2 Estimasi Titik dan Properti Fungsi distribusi populasi \\(F(\\cdot)\\) biasanya dapat dikarakterisasi oleh sejumlah terbatas (terbatas) parameter yang disebut parameter, dalam hal ini kita mengacu pada distribusi sebagai distribusi parametrik. Sebaliknya, dalam analisis nonparametrik , atribut-atribut distribusi sampling tidak terbatas pada sejumlah kecil parameter. Untuk memperoleh karakteristik populasi, terdapat berbagai atribut yang terkait dengan distribusi populasi \\(F(\\cdot)\\). Ukuran-ukuran tersebut meliputi rata-rata, median, persentil (misalnya persentil ke-95), dan simpangan baku. Karena ukuran-ukuran ringkasan ini tidak bergantung pada referensi parametrik tertentu, mereka adalah ukuran ringkasan nonparametrik. Di sisi lain, dalam analisis parametrik, kita dapat mengasumsikan keluarga distribusi tertentu dengan parameter-parameter tertentu. Misalnya, orang biasanya menganggap bahwa logaritma jumlah klaim memiliki distribusi normal dengan rata-rata \\(\\mu\\) dan simpangan baku \\(\\sigma\\). Dengan kata lain, kita mengasumsikan bahwa klaim memiliki distribusi lognormal dengan parameter-parameter \\(\\mu\\) dan \\(\\sigma\\). Sebagai alternatif, perusahaan asuransi umumnya mengasumsikan bahwa tingkat keparahan klaim mengikuti distribusi gamma dengan parameter bentuk \\(\\alpha\\) dan parameter skala \\(\\boldsymbol{\\theta}\\). Di sini, distribusi normal, lognormal, dan gamma adalah contoh dari distribusi parametrik. Dalam contoh di atas, besaran \\(\\mu\\), \\(sigma\\), \\(alpha\\), dan \\(\\boldsymbol{\\theta}\\) dikenal sebagai parameter. Untuk keluarga distribusi parametrik yang diberikan, distribusi secara unik ditentukan oleh nilai-nilai parameter tersebut. Seringkali, kita menggunakan \\(\\boldsymbol{\\theta}\\) untuk menyatakan atribut ringkasan dari populasi. Dalam model-parametrik, \\(\\boldsymbol{\\theta}\\) dapat berupa parameter atau fungsi parameter dari suatu distribusi, seperti parameter rata-rata dan varians normal. Dalam analisis nonparametrik, \\(\\theta\\) dapat berbentuk ukuran ringkasan nonparametrik seperti rata-rata atau simpangan baku populasi. Misalkan \\(\\hat{\\theta} =\\hat{\\theta}(X_1, \\ldots, X_n)\\) adalah fungsi dari sampel yang memberikan representasi atau estimasi dari \\(\\theta\\). Ini disebut sebagai statistik, yaitu fungsi dari sampel \\(X_1,…,X_n\\). 15.2.1 Estimasi Metode Momen Sebelum mendefinisikan estimasi metode momen, kita akan mendefinisikan konsep momen terlebih dahulu. Momen adalah atribut populasi yang menggambarkan fungsi distribusi \\(F(\\cdot)\\). Diberikan pengambilan acak \\(X\\) dari \\(F(\\cdot)\\), ekspektasi \\(\\mu_k=\\mathrm{E}[X^k]\\) disebut sebagai momen ke-\\(k\\) dari \\(X\\), dengan \\(k=1,2,3,…\\) Sebagai contoh, rata-rata populasi \\(\\mu\\) adalah momen pertama. Selain itu, ekspektasi \\(\\mathrm{E}[(X-\\mu)^k]\\) disebut momen sentral ke-\\(k\\). Oleh karena itu, varians adalah momen sentral kedua. Dengan menggunakan sampel acak \\(X_1,…,X_n\\), kita dapat membuat momen sampel yang sesuai, \\(\\hat{\\mu}_k=(1/n)\\sum_{i=1}^n X_i^k\\), untuk mengestimasi atribut populasi \\(\\mu_k\\). Sebagai contoh, kita telah menggunakan rata-rata sampel \\(\\bar{X}\\) sebagai estimator untuk rata-rata populasi \\(\\mu\\). Demikian pula, momen sentral kedua dapat diestimasi sebagai \\((1/n)\\sum_{i=1}^n(X_i-\\bar{X})^2\\). Tanpa mengasumsikan bentuk parametrik untuk \\(F(\\cdot)\\), momen sampel merupakan estimasi nonparametrik dari atribut populasi yang sesuai. Estimator seperti itu yang didasarkan pada pencocokan momen sampel dan momen populasi yang sesuai disebut sebagai estimator metode momen (mme). Meskipun mme secara alami digunakan dalam model nonparametrik, itu juga dapat digunakan untuk mengestimasi parameter ketika diasumsikan sebuah keluarga distribusi parametrik tertentu untuk \\(F(\\cdot)\\). Misalkan \\(\\boldsymbol{\\theta}=(\\theta_1,\\cdots,\\theta_m)\\) merupakan vektor parameter yang sesuai dengan distribusi parametrik \\(F(\\cdot)\\). Dalam keluarga distribusi tersebut, biasanya kita mengetahui hubungan antara parameter dan momen. Khususnya, kita mengetahui bentuk spesifik dari fungsi-fungsi \\(h_1(\\cdot),h_2(\\cdot),\\cdots,h_m(\\cdot)\\) sehingga \\(\\mu_1=h_1(\\boldsymbol{\\theta}),\\,\\mu_2=h_2(\\boldsymbol{\\theta}),\\,\\cdots,\\,\\mu_m=h_m(\\boldsymbol{\\theta})\\). Diberikan mme \\(\\hat{\\mu}_1, \\ldots, \\hat{\\mu}_m\\) dari sampel acak, mme parameter \\(\\hat{\\theta}_1,\\cdots,\\hat{\\theta}_m\\) dapat diperoleh dengan memecahkan persamaan-persamaan dari \\[\\begin{equation} \\hat{\\mu}_1=h_1(\\hat{\\theta}_1,\\cdots,\\hat{\\theta}_m); \\end{equation}\\] \\[\\begin{equation} \\hat{\\mu}_2=h_2(\\hat{\\theta}_1,\\cdots,\\hat{\\theta}_m); \\end{equation}\\] \\[\\begin{equation} \\cdots \\end{equation}\\] \\[\\begin{equation} \\hat{\\mu}_m=h_m(\\hat{\\theta}_1,\\cdots,\\hat{\\theta}_m). \\end{equation}\\] 15.2.2 Estimasi Maksimum Likelihood Ketika \\(F(\\cdot)\\) mengambil bentuk parametrik, metode estimasi likelihood maksimum (maximum likelihood estimation) banyak digunakan untuk mengestimasi parameter populasi \\(\\boldsymbol{\\theta}\\). Estimasi likelihood maksimum didasarkan pada fungsi likelihood, yang merupakan fungsi dari parameter-parameter yang diberikan sampel yang diamati. Misalkan \\(f(x_i|\\boldsymbol{\\theta})\\) adalah fungsi probabilitas dari \\(X_i\\) yang dievaluasi pada \\(X_i=x_i\\) \\((i=1,2,\\cdots,n)\\); ini adalah fungsi massa probabilitas dalam kasus \\(X\\) yang diskrit dan fungsi densitas probabilitas dalam kasus \\(X\\) yang kontinu. Dengan asumsi independensi, fungsi likelihood dari \\(\\boldsymbol{\\theta}\\) yang terkait dengan pengamatan \\((X_1,X_2,\\cdots,X_n)=(x_1,x_2,\\cdots,x_n)=\\mathbf{x}\\) dapat ditulis sebagai \\[\\begin{equation} L(\\boldsymbol{\\theta}|\\mathbf{x})=\\prod_{i=1}^nf(x_i|\\boldsymbol{\\theta}), \\end{equation}\\] dengan fungsi log-likelihood yang sesuai diberikan oleh \\[\\begin{equation} l(\\boldsymbol{\\theta}|\\mathbf{x})=\\log(L(\\boldsymbol{\\theta}|\\mathbf{x}))=\\sum_{i=1}^n\\log f(x_i|\\boldsymbol{\\theta}). \\end{equation}\\] Estimator maximum likelihood (MLE) dari \\(\\boldsymbol{\\theta}\\) adalah himpunan nilai \\(\\boldsymbol{\\theta}\\) yang memaksimalkan fungsi likelihood (fungsi log-likelihood), dengan mempertimbangkan sampel yang diamati. Dengan demikian, MLE \\(\\hat{\\boldsymbol{\\theta}}\\) dapat ditulis sebagai \\[\\begin{equation} \\hat{\\boldsymbol{\\theta}}={\\mbox{argmax}}_{\\boldsymbol{\\theta}\\in\\Theta}l(\\boldsymbol{\\theta}|\\mathbf{x}), \\end{equation}\\] di mana \\(\\theta\\) adalah ruang parameter dari \\(\\boldsymbol{\\theta}\\), dan \\({\\mbox{argmax}}_{\\boldsymbol{\\theta}\\in\\Theta}l(\\boldsymbol{\\theta}|\\mathbf{x})\\) didefinisikan sebagai nilai \\(\\boldsymbol{\\theta}\\) di mana fungsi \\(l(\\boldsymbol{\\theta}|\\mathbf{x})\\) mencapai nilai maksimumnya. Diberikan bentuk analitis dari fungsi likelihood, mle dapat diperoleh dengan mengambil turunan pertama dari fungsi log-likelihood terhadap \\(\\boldsymbol{\\theta}\\), dan menetapkan nilai-nilai turunan parsial menjadi nol. Dengan kata lain, mle adalah solusi dari persamaan-persamaan: \\[\\begin{equation} \\frac{\\partial l(\\hat{\\boldsymbol{\\theta}}|\\mathbf{x})}{\\partial\\hat{\\theta}_1}=0; \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial l(\\hat{\\boldsymbol{\\theta}}|\\mathbf{x})}{\\partial\\hat{\\theta}_2}=0; \\end{equation}\\] \\[\\begin{equation} \\cdots \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial l(\\hat{\\boldsymbol{\\theta}}|\\mathbf{x})}{\\partial\\hat{\\theta}_m}=0, \\end{equation}\\] asalkan turunan parsial kedua negatif. Untuk model parametrik, mle dari parameter-parameter dapat diperoleh secara analitis (misalnya, dalam kasus distribusi normal dan estimasi linear), atau secara numerik melalui algoritma iteratif seperti metode Newton-Raphson dan versi adaptifnya (misalnya, dalam kasus model linier umum dengan variabel respons non-normal). Distribusi normal. Anggaplah \\((X_1,X_2,⋯,X_n)\\) sebagai sampel acak dari distribusi normal \\(N(\\mu, \\sigma^2)\\). Dengan sampel yang diamati \\((X_1,X_2,\\cdots,X_n)=(x_1,x_2,\\cdots,x_n)\\) kita dapat menulis fungsi likelihood dari \\(\\mu,\\sigma^2\\) sebagai \\[\\begin{equation} L(\\mu,\\sigma^2)=\\prod_{i=1}^n\\left[\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{\\left(x_i-\\mu\\right)^2}{2\\sigma^2}}\\right], \\end{equation}\\] dengan fungsi log-likelihood yang sesuai diberikan oleh \\[\\begin{equation} l(\\mu,\\sigma^2)=-\\frac{n}{2}[\\log(2\\pi)+\\log(\\sigma^2)]-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n\\left(x_i-\\mu\\right)^2. \\end{equation}\\] Dengan memecahkan \\[\\begin{equation} \\frac{\\partial l(\\hat{\\mu},\\sigma^2)}{\\partial \\hat{\\mu}}=0, \\end{equation}\\] kita mendapatkan \\(\\hat{\\mu}=\\bar{x}=(1/n)\\sum_{i=1}^nx_i\\). Mudah untuk diverifikasi bahwa \\(\\frac{\\partial l^2(\\hat{\\mu},\\sigma^2)}{\\partial \\hat{\\mu}^2}\\left|_{\\hat{\\mu}=\\bar{x}}\\right.&lt;0\\). Karena ini berlaku untuk \\(x\\) yang sembarang, \\(\\hat{\\mu}=\\bar{X}\\) adalah mle dari \\(\\mu\\). Demikian pula, dengan memecahkan \\[\\begin{equation} \\frac{\\partial l(\\mu,\\hat{\\sigma}^2)}{\\partial \\hat{\\sigma}^2}=0, \\end{equation}\\] kita mendapatkan \\(\\hat{\\sigma}^2=(1/n)\\sum_{i=1}^n(x_i-\\mu)^2\\). Dengan menggantikan \\(\\mu\\) oleh \\(\\hat{\\mu}\\), kita mendapatkan mle dari \\(\\sigma^2\\) sebagai \\(\\hat{\\sigma}^2=(1/n)\\sum_{i=1}^n(X_i-\\bar{X})^2\\). Dengan demikian, rata-rata sampel \\(\\bar{X}\\) dan \\(\\sigma^2\\) keduanya adalah mme dan MLE untuk rata-rata \\(\\mu\\) dan varian \\(\\sigma^2\\), dalam distribusi populasi normal \\(F(\\cdot)\\). Lebih banyak detail mengenai sifat-sifat fungsi likelihood diberikan dalam Lampiran Bagian 17.1. 15.3 Estimasi Interval Setelah kita memperkenalkan mme dan mle, kita dapat melakukan jenis pertama dari inferensi statistik, yaitu estimasi interval yang mengukur ketidakpastian akibat penggunaan sampel yang terbatas. Dengan mendapatkan distribusi sampling dari mle, kita dapat mengestimasi interval (interval kepercayaan) untuk parameter tersebut. Dalam pendekatan frequentist (misalnya, yang didasarkan pada estimasi maksimum likelihood), interval kepercayaan yang dihasilkan dari kerangka sampling yang sama akan mencakup nilai sebenarnya sebagian besar waktu (misalnya, 95% dari waktu), jika kita mengulangi proses sampling dan menghitung ulang interval berulang kali. Proses tersebut membutuhkan derivasi distribusi sampling untuk mle. 15.3.1 Distribusi Tepat untuk Rata-rata Sampel Normal Karena sifat aditivitas dari distribusi normal (yaitu, jumlah dari variabel acak normal yang mengikuti distribusi normal multivariat tetap mengikuti distribusi normal) dan distribusi normal termasuk dalam keluarga lokasi–skala (yaitu, transformasi lokasi dan/atau skala dari variabel acak normal menghasilkan distribusi normal), maka rata-rata sampel \\(\\bar{X}\\) dari sampel acak dari distribusi normal \\(F(\\cdot)\\) memiliki distribusi sampling normal untuk setiap \\(n\\) yang terbatas. Diberikan \\(X_i\\sim^{iid} N(\\mu,\\sigma^2)\\), mle dari \\(\\mu\\) memiliki distribusi yang tepat \\[\\begin{equation} \\bar{X}\\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right). \\end{equation}\\] Oleh karena itu, rata-rata sampel adalah estimator tidak bias dari \\(\\mu\\). Selain itu, ketidakpastian dalam estimasi dapat diukur dengan varian \\(\\sigma^2/n\\), yang akan berkurang seiring dengan pertambahan ukuran sampel \\(n\\). Ketika ukuran sampel mendekati tak terhingga, rata-rata sampel akan mendekati titik tunggal pada nilai sebenarnya. 15.3.2 Properti Sampel Besar dari Estimasi Maksimum Likelihood Untuk mle dari parameter mean dan parameter-parameter lain dari keluarga distribusi parametrik lainnya, kita biasanya tidak dapat mendapatkan distribusi sampling yang tepat untuk sampel yang terbatas. Untungnya, ketika ukuran sampel cukup besar, mle dapat diaproksimasi dengan distribusi normal. Berkat teori maximum likelihood secara umum, mle memiliki beberapa sifat yang baik dalam sampel yang cukup besar. MLE (Maximum Likelihood Estimator) \\(\\hat{\\theta}\\) dari parameter \\(\\theta\\) merupakan estimator yang konsisten. Artinya, \\(\\hat{\\theta}\\) konvergen dalam probabilitas ke nilai sebenarnya \\(\\theta\\), ketika ukuran sampel \\(n\\) mendekati tak hingga. MLE memiliki sifat asimptotik normalitas, yang berarti bahwa estimator akan konvergen dalam distribusi menjadi distribusi normal yang berpusat di sekitar nilai sebenarnya, ketika ukuran sampel mendekati tak hingga. Secara khusus, \\[\\begin{equation} \\sqrt{n}(\\hat{\\theta}-\\theta)\\rightarrow_d N\\left(0,\\,V\\right),\\quad \\mbox{as}\\quad n\\rightarrow \\infty, \\end{equation}\\] di mana \\(V\\) adalah invers dari Informasi Fisher. Dengan demikian, mle \\(\\hat{\\theta}\\) mengikuti secara aproksimatif distribusi normal dengan mean \\(\\theta\\) dan varian \\(V/n\\), ketika ukuran sampel besar. - MLE adalah efisien, yang berarti bahwa ia memiliki varian asimptotik terkecil \\(V\\), umumnya disebut sebagai batas bawah Cramer-Rao. Secara khusus, batas bawah Cramer-Rao adalah invers dari informasi Fisher yang didefinisikan sebagai \\(\\mathcal{I}(\\theta)=-\\mathrm{E}(\\partial^2\\log f(X;\\theta)/\\partial \\theta^2)\\). Oleh karena itu, \\(\\mathrm{Var}(\\hat{\\theta})\\) dapat diestimasi berdasarkan informasi Fisher yang diamati yang dapat ditulis sebagai \\(-\\sum_{i=1}^n \\partial^2\\log f(X_i;\\theta)/\\partial \\theta^2\\). Untuk banyak distribusi parametrik, informasi Fisher dapat dihitung secara analitik untuk mle dari parameter-parameter tersebut. Untuk model parametrik yang lebih kompleks, informasi Fisher dapat dievaluasi secara numerik menggunakan integrasi numerik untuk distribusi kontinu, atau penjumlahan numerik untuk distribusi diskrit. Informasi lebih lanjut tentang estimasi maximum likelihood dapat ditemukan di Bagian Lampiran 17.2. 15.3.3 Interval Kepercayaan Diberikan bahwa mle \\(\\hat{\\theta}\\) memiliki distribusi normal yang tepat atau mendekati dengan mean \\(\\theta\\) dan varian \\(\\mathrm{Var}(\\hat{\\theta})\\), kita dapat mengambil akar kuadrat dari varian dan memasukkan estimasi untuk mendefinisikan \\(se(\\hat{\\theta}) = \\sqrt{\\mathrm{Var}(\\hat{\\theta})}\\). Standar error adalah simpangan baku yang diestimasi yang mengukur ketidakpastian dalam estimasi yang dihasilkan dari penggunaan sampel terbatas. Dalam beberapa kondisi keberaturan yang mengatur distribusi populasi, kita dapat menunjukkan bahwa statistik \\[\\begin{equation} \\frac{\\hat{\\theta}-\\theta}{se(\\hat{\\theta})} \\end{equation}\\] konvergen dalam distribusi ke distribusi Student-\\(t\\) dengan derajat kebebasan (sebuah parameter dari distribusi) \\(n−p\\), di mana \\(p\\) adalah jumlah parameter dalam model selain varians. Sebagai contoh, untuk kasus distribusi normal, kita memiliki \\(p=1\\) untuk parameter \\(\\mu\\); untuk model regresi linear dengan satu variabel independen, kita memiliki \\(p=2\\) untuk parameter intercept dan variabel independen. Misalkan \\(t_{n-p}(1-\\alpha/2)\\) adalah persentil ke-\\(100\\times(1-\\alpha/2)\\) dari distribusi Student-\\(t\\) yang memenuhi \\(\\Pr\\left[t&lt; t_{n-p}\\left(1-{\\alpha}/{2}\\right) \\right]= 1-{\\alpha}/{2}\\). Maka, kita memiliki, \\[\\begin{equation} \\Pr\\left[-t_{n-p}\\left(1-\\frac{\\alpha}{2}\\right)&lt;\\frac{\\hat{\\theta}-\\theta}{se(\\hat{\\theta})}&lt; t_{n-p}\\left(1-\\frac{\\alpha}{2}\\right) \\right]= 1-{\\alpha}, \\end{equation}\\] dari mana kita dapat menentukan interval kepercayaan untuk \\(\\theta\\). Dari persamaan di atas, kita dapat mendapatkan sepasang statistik, \\(\\hat{\\theta}_1\\) dan \\(\\hat{\\theta}_2\\), yang memberikan interval dalam bentuk \\([\\hat{\\theta}_1, \\hat{\\theta}_2]\\). Interval ini adalah interval kepercayaan \\(1-\\alpha\\) untuk \\(\\theta\\), sehingga \\(\\Pr\\left(\\hat{\\theta}_1 \\le \\theta \\le \\hat{\\theta}_2\\right) = 1-\\alpha,\\), di mana probabilitas 1−α disebut tingkat kepercayaan. Perlu diperhatikan bahwa interval kepercayaan di atas tidak valid untuk sampel kecil, kecuali untuk kasus rata-rata normal. Dalam distribusi normal. Untuk rata-rata populasi normal \\(\\mu\\), mle memiliki distribusi sampling yang tepat \\(\\bar{X}\\sim N(\\mu,\\sigma/\\sqrt{n})\\), di mana kita dapat mengestimasi \\(se(\\hat{\\theta})\\) dengan \\(\\hat{\\sigma}/\\sqrt{n}\\). Berdasarkan Teorema Cochran, statistik yang dihasilkan memiliki distribusi Student-t yang tepat dengan derajat kebebasan \\(n−1\\). Oleh karena itu, kita dapat menentukan batas bawah dan batas atas dari interval kepercayaan sebagai \\[\\begin{equation} \\hat{\\mu}_1 = \\hat{\\mu} - t_{n-1}\\left(1-\\frac{\\alpha}{2}\\right)\\frac{ \\hat{\\sigma}}{\\sqrt{n}} \\end{equation}\\] dan \\[\\begin{equation} \\hat{\\mu}_2 = \\hat{\\mu} + t_{n-1}\\left(1-\\frac{\\alpha}{2}\\right)\\frac{ \\hat{\\sigma}}{\\sqrt{n}}. \\end{equation}\\] Ketika \\(\\alpha = 0.05\\), \\(t_{n-1}(1-\\alpha/2) \\approx 1.96\\) untuk nilai n yang besar. Berdasarkan Teorema Cochran, interval kepercayaan tetap valid tidak peduli dengan ukuran sampel. 15.4 Pengujian Hipotesis Untuk parameter \\(\\boldsymbol{\\theta}\\) dari distribusi parametrik, jenis lain dari inferensi statistik adalah pengujian hipotesis yang memverifikasi apakah hipotesis mengenai parameter tersebut benar, dengan menggunakan tingkat signifikansi tertentu yang disebut level of significance \\(\\alpha\\) (misalnya, 5%). Dalam pengujian hipotesis, kita menolak hipotesis nol, yaitu pernyataan yang membatasi mengenai parameter, jika probabilitas mengamati sampel acak seekstrem sampai dengan yang diamati lebih kecil daripada \\(\\alpha\\), jika hipotesis nol benar. 15.4.1 Konsep Dasar Pada pengujian statistik, biasanya kita tertarik untuk menguji apakah pernyataan mengenai beberapa parameter, hipotesis nol (dinyatakan sebagai \\(H_0\\)), benar berdasarkan data yang diamati. Hipotesis nol dapat memiliki bentuk umum \\(H_0:\\theta\\in\\Theta_0\\), di mana \\(\\Theta_0\\) adalah subset dari ruang parameter \\(Θ\\) dari \\(θ\\) yang mungkin berisi beberapa parameter. Untuk kasus dengan satu parameter \\(θ\\), hipotesis nol biasanya memiliki bentuk \\(H_0:\\theta=\\theta_0\\) atau \\(H_0:\\theta\\leq\\theta_0\\). Kebalikan dari hipotesis nol disebut hipotesis alternatif yang dapat dituliskan sebagai \\(H_a:\\theta\\neq\\theta_0\\) atau \\(H_a:\\theta&gt;\\theta_0\\). Uji statistik pada \\(H_0:\\theta=\\theta_0\\) disebut uji dua sisi karena hipotesis alternatif berisi dua ketidaksamaan, sedangkan uji dengan \\(H_0:\\theta\\leq\\theta_0\\) atau \\(H_0:\\theta\\geq\\theta_0\\) disebut uji satu sisi. Uji statistik biasanya dibangun berdasarkan statistik \\(T\\) dan distribusinya yang eksak atau berdasarkan sampel besar. Uji tersebut umumnya menolak uji dua sisi ketika \\(T&gt;c_1\\) atau \\(T&lt;c_2\\), di mana dua konstanta \\(c_1\\) dan \\(c_2\\) diperoleh berdasarkan distribusi sampel dari \\(T\\) pada tingkat probabilitas \\(\\alpha\\) yang disebut tingkat signifikansi. Secara khusus, tingkat signifikansi \\(\\alpha\\) memenuhi \\[\\begin{equation} \\alpha=\\Pr(\\mbox{reject }H_0|H_0\\mbox{ is true}), \\end{equation}\\] Artinya, jika hipotesis nol benar, kita hanya akan menolak hipotesis nol sebanyak 5% dari jumlah pengujian yang dilakukan, jika kita mengulangi proses sampling dan pengujian berulang kali. Oleh karena itu, tingkat signifikansi adalah probabilitas terjadinya kesalahan tipe I (kesalahan jenis pertama), yaitu kesalahan menolak hipotesis nol yang sebenarnya benar. Untuk alasan ini, tingkat signifikansi \\(\\alpha\\) juga disebut sebagai tingkat kesalahan tipe I. Jenis kesalahan lain yang dapat kita buat dalam pengujian hipotesis adalah kesalahan tipe II (kesalahan jenis kedua), yaitu kesalahan menerima hipotesis nol yang sebenarnya salah. Demikian pula, kita dapat mendefinisikan tingkat kesalahan tipe II sebagai probabilitas tidak menolak (menerima) hipotesis nol yang sebenarnya salah. Dengan kata lain, tingkat kesalahan tipe II diberikan oleh \\[\\begin{equation} \\Pr(\\mbox{accept }H_0|H_0\\mbox{ is false}). \\end{equation}\\] Jumlah lain yang penting mengenai kualitas uji statistik disebut daya uji (power of the test) \\(\\beta\\), yang didefinisikan sebagai probabilitas menolak hipotesis nol yang salah. Definisi matematis dari daya uji adalah sebagai berikut: \\[\\begin{equation} \\beta=\\Pr(\\mbox{reject }H_0|H_0\\mbox{ is false}). \\end{equation}\\] Perlu diperhatikan bahwa daya uji biasanya dihitung berdasarkan nilai alternatif tertentu \\(\\theta=\\theta_a\\), dengan distribusi sampling yang spesifik dan ukuran sampel yang spesifik pula. Dalam penelitian eksperimen sebenarnya, orang biasanya menghitung ukuran sampel yang diperlukan untuk memilih ukuran sampel yang akan memastikan peluang besar mendapatkan uji statistik yang signifikan secara statistik (misalnya, dengan daya uji yang telah ditentukan sebelumnya seperti 85%). 15.4.2 Uji Student-t berdasarkan Estimasi Maksimum Likelihood (MLE) Berdasarkan hasil dari Bagian 15.3.1, kita dapat mendefinisikan uji Student-\\(t\\) untuk menguji \\(H_0:\\theta=\\theta_0\\). Secara khusus, kita mendefinisikan statistik uji sebagai \\[\\begin{equation} t\\text{-stat}=\\frac{\\hat{\\theta}-\\theta_0}{se(\\hat{\\theta})}, \\end{equation}\\] distribusi besar-sampel dari distribusi student-\\(t\\) dengan derajat kebebasan \\(n−p\\), ketika hipotesis nol benar (yaitu, ketika \\(\\theta=\\theta_0\\). Dengan tingkat signifikansi tertentu \\(α\\), misalnya 5%, kita menolak hipotesis nol jika kejadian \\(t\\text{-stat}&lt;-t_{n-p}\\left(1-{\\alpha}/{2}\\right)\\) atau \\(t\\text{-stat}&gt; t_{n-p}\\left(1-{\\alpha}/{2}\\right)\\) terjadi (daerah penolakan). Dalam hipotesis nol \\(H_0\\), kita memiliki \\[\\begin{equation} \\Pr\\left[t\\text{-stat}&lt;-t_{n-p}\\left(1-\\frac{\\alpha}{2}\\right)\\right]=\\Pr\\left[t\\text{-stat}&gt; t_{n-p}\\left(1-\\frac{\\alpha}{2}\\right) \\right]= \\frac{\\alpha}{2}. \\end{equation}\\] Selain konsep daerah penolakan, kita dapat menolak uji berdasarkan nilai \\(p\\)-value yang didefinisikan sebagai \\(2\\Pr(T&gt;|t\\text{-stat}|)\\) untuk uji dua sisi yang disebutkan sebelumnya, di mana variabel acak \\(T\\sim T_{n-p}\\). Kita menolak hipotesis nol jika nilai \\(p\\)-value lebih kecil dari atau sama dengan \\(α\\). Untuk sampel yang diberikan, nilai \\(p\\)-value didefinisikan sebagai tingkat signifikansi terkecil di mana hipotesis nol akan ditolak. Demikian pula, kita dapat membuat uji satu sisi untuk hipotesis nol \\(H_0:\\theta\\leq\\theta_0\\) (atau \\(H_0:\\theta\\geq\\theta_0\\)). Dengan menggunakan statistik uji yang sama, kita menolak hipotesis nol ketika \\(t\\text{-stat}&gt; t_{n-p}\\left(1-{\\alpha}\\right)\\) (atau \\(t\\text{-stat}&lt;- t_{n-p}\\left(1-{\\alpha}\\right)\\) untuk uji pada \\(H_0:\\theta\\geq\\theta_0\\)). Nilai \\(p\\)-value yang sesuai didefinisikan sebagai \\(\\Pr(T&gt;|t\\text{-stat}|)\\) (atau \\(\\Pr(T&lt;|t\\text{-stat}|)\\)) untuk uji pada \\(H_0:\\theta\\geq\\theta_0\\)). Perlu diperhatikan bahwa uji ini tidak valid untuk sampel kecil, kecuali untuk kasus uji pada rata-rata normal. Uji \\(t\\) Satu-Sampel untuk Rata-Rata Normal. Untuk uji pada rata-rata normal dengan bentuk \\(H_0:\\mu=\\mu_0\\), \\(H_0:\\mu\\leq\\mu_0\\), atau \\(H_0:\\mu\\geq\\mu_0\\), kita dapat mendefinisikan statistik uji sebagai \\[\\begin{equation} t\\text{-stat}=\\frac{\\bar{X}-\\mu_0}{{\\hat{\\sigma}}/{\\sqrt{n}}}, \\end{equation}\\] untuk mana kita memiliki distribusi sampling yang tepat \\(t\\text{-stat}\\sim T_{n-1}\\) berdasarkan teorema Cochran, dengan \\(T_{n-1}\\) mengindikasikan distribusi Student-\\(t\\) dengan derajat kebebasan \\(n−1\\). Menurut teorema Cochran, uji ini valid baik untuk sampel kecil maupun besar. 15.4.3 Uji Rasio Kemungkinan (Likelihood Ratio Test) Pada subbagian sebelumnya, kami telah memperkenalkan uji Student-\\(t\\) pada satu parameter, berdasarkan sifat-sifat mle. Dalam bagian ini, kami mendefinisikan uji alternatif yang disebut uji rasio kemungkinan (likelihood ratio test, LRT). LRT dapat digunakan untuk menguji beberapa parameter dari model statistik yang sama. Diberikan fungsi kemungkinan \\(L(\\theta|\\mathbf{x})\\) dan \\(\\Theta_0 \\subset \\Theta\\), statistik uji rasio kemungkinan untuk menguji \\(H_0:\\theta\\in\\Theta_0\\) terhadap \\(H_a:\\theta\\notin\\Theta_0\\) diberikan oleh \\[\\begin{equation} L=\\frac{\\sup_{\\theta\\in\\Theta_0}L(\\theta|\\mathbf{x})}{\\sup_{\\theta\\in\\Theta}L(\\theta|\\mathbf{x})}, \\end{equation}\\] dan untuk menguji \\(H_0:\\theta=\\theta_0\\) versus \\(H_a:\\theta\\neq\\theta_0\\) adalah \\[\\begin{equation} L=\\frac{L(\\theta_0|\\mathbf{x})}{\\sup_{\\theta\\in\\Theta}L(\\theta|\\mathbf{x})}. \\end{equation}\\] LRT menolak hipotesis nol ketika \\(L &lt; c\\), dengan ambang batas tergantung pada tingkat signifikansi \\(α\\), ukuran sampel \\(n\\), dan jumlah parameter dalam \\(θ\\). Berdasarkan Lembaran Neyman-Pearson, LRT adalah uji paling kuat secara seragam untuk menguji \\(H_0:\\theta=\\theta_0\\) dengan \\(H_a:\\theta=\\theta_a\\). Artinya, LRT memberikan daya paling besar \\(β\\) untuk \\(α\\) dan nilai alternatif \\(\\theta_a\\) yang diberikan. Berdasarkan Teorema Wilks, statistik uji rasio kemungkinan \\(-2\\log(L)\\) konvergen dalam distribusi ke distribusi Chi-square dengan derajat kebebasan adalah selisih antara dimensionalitas ruang parameter \\(Θ\\) dan \\(\\Theta_0\\), ketika ukuran sampel mendekati tak terhingga dan ketika model nol tertanam dalam model alternatif. Dengan kata lain, ketika model nol adalah kasus khusus dari model alternatif yang berisi ruang sampel yang terbatas, kita dapat memperkirakan \\(c\\) dengan \\(\\chi^2_{p_1 - p_2}(1-\\alpha)\\), persentil ke-\\(100\\times(1-\\alpha)\\) dari distribusi Chi-square, dengan \\(p1−p2\\) sebagai derajat kebebasan, dan \\(p1\\) dan \\(p2\\) adalah jumlah parameter dalam model alternatif dan nol, masing-masing. Perlu diperhatikan bahwa LRT juga adalah uji berukuran besar yang tidak akan valid untuk sampel kecil. 15.4.4 Kriteria Informasi Dalam aplikasi kehidupan nyata, LRT (Likelihood Ratio Test) umumnya digunakan untuk membandingkan dua model yang saling bersarang. Namun, pendekatan LRT sebagai alat seleksi model memiliki dua kelemahan utama: 1) Biasanya membutuhkan model nol yang bersarang di dalam model alternatif; 2) Model yang dipilih dari LRT cenderung menghasilkan overfitting dalam sampel, sehingga mengakibatkan prediksi di luar sampel yang buruk. Untuk mengatasi masalah ini, pemilihan model berdasarkan kriteria informasi, yang berlaku untuk model non-bersarang sambil mempertimbangkan kompleksitas model, lebih banyak digunakan untuk pemilihan model. Di sini, kami memperkenalkan dua kriteria yang paling umum digunakan, yaitu kriteria informasi Akaike dan kriteria informasi Bayes. Secara khusus, kriteria informasi Akaike (\\(AIC\\)) didefinisikan sebagai \\[\\begin{equation} AIC = -2\\log L(\\hat{\\boldsymbol \\theta}) + 2p, \\end{equation}\\] dimana \\(\\hat{\\boldsymbol \\theta}\\) merupakan MLE dari \\({\\boldsymbol \\theta}\\), dan \\(p\\) adalah jumlah parameter dalam model. Istilah tambahan \\(2p\\) mewakili penalti untuk kompleksitas model. Dengan kata lain, dengan fungsi likelihood yang maksimum yang sama, \\(AIC\\) memilih model dengan jumlah parameter yang lebih sedikit. Perlu diperhatikan bahwa \\(AIC\\) tidak mempertimbangkan pengaruh dari ukuran sampel \\(n\\). Secara alternatif, orang menggunakan kriteria informasi Bayesian (\\(BIC\\)) yang mempertimbangkan ukuran sampel. \\(BIC\\) didefinisikan sebagai \\[\\begin{equation} BIC = -2\\log L(\\hat{\\boldsymbol \\theta}) + p\\,\\log(n). \\end{equation}\\] Kita dapat mengamati bahwa \\(BIC\\) cenderung memberikan bobot yang lebih tinggi pada jumlah parameter. Dengan fungsi likelihood yang dimaksimumkan yang sama, \\(BIC\\) akan menyarankan model yang lebih parsimonius dibandingkan dengan \\(AIC\\). "],["appendix-b-iterated-expectations.html", "Bab 16 Appendix B: Iterated Expectations 16.1 Conditional Distribution and Conditional Expectation 16.2 Iterated Expectations and Total Varians 16.3 Conjugate Distributions", " Bab 16 Appendix B: Iterated Expectations Apendiks ini memperkenalkan undang-undang yang terkait dengan harapan berulang.Dalam beberapa situasi, kami hanya mengamati satu hasil tetapi dapat membuat konsep hasil sebagai hasil dari proses dua (atau lebih) tahap. Jenis model statistik semacam itu disebut model dua tahap , atau hierarkis . Beberapa kasus khusus model hirarkis meliputi: model yang parameter distribusinya adalah variabel acak; distribusi campuran, di mana Tahap 1 mewakili undian sub-populasi dan Tahap 2 mewakili variabel acak dari distribusi yang ditentukan oleh sub-populasi yang ditarik pada Tahap 1; distribusi agregat, di mana Tahap 1 mewakili pengundian jumlah peristiwa dan Tahap dua mewakili jumlah kerugian yang terjadi per peristiwa. 16.1 Conditional Distribution and Conditional Expectation Ekspektasi iterasi adalah hukum mengenai perhitungan ekspektasi dan varian dari variabel acak menggunakan distribusi bersyarat dari variabel yang diberikan variabel lain. Oleh karena itu, pertama-tama kami memperkenalkan konsep yang terkait dengan distribusi bersyarat, dan perhitungan ekspektasi bersyarat dan variabel berdasarkan distribusi bersyarat yang diberikan. 16.1.1 Conditional Distribution memperkenalkan konsep distribusi bersyarat masing-masing untuk variabel acak diskrit dan kontinu. 16.1.1.1 Discrete Case Seandainya X Dan Y keduanya adalah variabel acak diskrit, yang berarti bahwa mereka dapat mengambil jumlah nilai yang mungkin terbatas atau dapat dihitung dengan probabilitas positif. Fungsi probabilitas gabungan (massa) dari ( X, Y) didefinisikan sebagai $$ p(x,y) = . $$ dimana X Dan Y independen (nilai dari X tidak tergantung pada itu Y), kita punya $$ p(x,y)=p(x)p(y), $$ dengan p ( x ) = Pr [ X= x ] Dan hal ( y) = Pr [ Y= y] menjadi fungsi probabilitas marjinal dari X Dan Y, masing-masing. Mengingat fungsi probabilitas bersama, kita dapat memperoleh fungsi probabilitas marjinal dari Y sebagai $$ p(y)=_x p(x,y), $$ di mana penjumlahannya melebihi semua nilai yang mungkin dari X, dan fungsi probabilitas marjinal dari X dapat diperoleh dengan cara yang serupa. Fungsi probabilitas bersyarat (massa) dari ( Y| X) didefinisikan sebagai \\[ p(y|x) =\\Pr[Y=y|X=x]= \\frac{p(x,y)}{\\Pr[X=x]}, \\] di mana kita dapat memperoleh fungsi probabilitas bersyarat dari ( X| Y) dengan cara yang serupa. Secara khusus, probabilitas bersyarat di atas mewakili probabilitas kejadian tersebut Y= y diberikan acara tersebut X= x. Oleh karena itu, bahkan dalam kasus di mana Pr [ X= x ] = 0, fungsi dapat diberikan sebagai bentuk tertentu, dalam aplikasi nyata. 16.1.1.2 Continuous Case Untuk variabel acak kontinu X Dan Y, kita dapat mendefinisikan fungsi probabilitas (kepadatan) gabungannya berdasarkan fungsi distribusi kumulatif bersama. Fungsi distribusi kumulatif gabungan dari ( X, Y) didefinisikan sebagai \\[ F(x,y) = \\Pr[X\\leq x, Y\\leq y]. \\] Dimana X dan Y independen, maka \\[ F(x,y)=F(x)F(y), \\] dengan F( x ) = Pr [ X≤ x ] Dan F( y) = Pr [ Y≤ y] menjadi fungsi distribusi kumulatif (cdf) dari X Dan Y, masing-masing. Variabel acak X disebut sebagai variabel acak kontinu jika cdfnya kontinu X. Ketika cdf F( x ) terus menerus X, lalu kita definisikan F( x ) = ∂F( x ) / ∂X sebagai fungsi kerapatan probabilitas (marginal) (pdf). X. Begitu pula jika bersama cdf F( x , y) kontinu pada keduanya X Dan y , kami mendefinisikan \\[ f(x,y)=\\frac{\\partial^2 F(x,y)}{\\partial x\\partial y} \\] sebagai fungsi kepadatan probabilitas gabungan dari (X,Y) dalam hal ini kita mengacu pada variabel acak sebagai kontinu bersama . DImana X dan Y independen, maka \\[ f(x,y)=f(x)f(y). \\] Mengingat fungsi kerapatan sendi, kita dapat memperoleh fungsi kerapatan marjinal dari Y sebagai \\[ f(y)=\\int_x f(x,y)\\,dx, \\] di mana integralnya melebihi semua nilai yang mungkin dari X, dan fungsi probabilitas marjinal dari X dapat diperoleh dengan cara yang serupa. Berdasarkan pdf gabungan dan pdf marginal, kami mendefinisikan fungsi kerapatan probabilitas bersyarat dari ( Y| X) sebagai \\[ f(y|x) = \\frac{f(x,y)}{f(x)}, \\] di mana kita dapat memperoleh fungsi probabilitas bersyarat dari ( X| Y) dengan cara yang serupa. Di sini, fungsi kerapatan bersyarat adalah fungsi kerapatan dari y diberikan X= x. Oleh karena itu, bahkan dalam kasus di mana Pr [ X= x ] = 0 atau kapan F( x ) tidak didefinisikan, fungsi dapat diberikan dalam bentuk tertentu dalam aplikasi nyata. 16.1.2 Conditional Expectation and Conditional Variance Sekarang kita mendefinisikan ekspektasi bersyarat dan varians berdasarkan distribusi bersyarat yang didefinisikan pada sub-bagian sebelumnya. 16.1.2.1 Discrete Case Untuk variabel acak diskrit Y, harapannya didefinisikan sebagai \\(\\mathrm{E}[Y]=\\sum_y y\\,p(y)\\) jika nilainya terbatas, dan variansnya didefinisikan sebagai \\[ \\mathrm{Var}[Y]=\\mathrm{E}\\{(Y-\\mathrm{E}[Y])^2\\}=\\sum_y y^2\\,p(y)-\\{\\mathrm{E}[Y]\\}^2 \\] Untuk variabel acak diskrit Y harapan bersyarat dari variabel acak Y diberikan acara tersebut X= x didefinisikan sebagai \\[ \\mathrm{E}[Y|X=x]=\\sum_y y\\,p(y|x), \\] Di mana X tidak harus menjadi variabel diskrit, sejauh fungsi probabilitas bersyarat hal ( y| x) diberikan. Perhatikan bahwa harapan bersyarat E [Y| X= x ] adalah angka tetap. Ketika kita mengganti X dengan X di sisi kanan persamaan di atas, kita dapat menentukan ekspektasi Y diberikan variabel acak X sebagai \\[ \\mathrm{E}[Y|X]=\\sum_y y\\,p(y|X), \\] yang masih merupakan variabel acak , dan keacakan itu berasal X. Dengan cara yang sama, kita dapat menentukan varian bersyarat dari variabel acak Y diberikan acara tersebut X= x sebagai $$ [Y|X=x]=[Y^2|X=x]-{[Y|X=x]}^2=_y y2,p(y|x)-{[Y|X=x]}2. $$ Varian dari Y diberikan X , V a r [Y| X] dapat ditentukan dengan mengganti X oleh X dalam persamaan di atas, dan V a r [Y| X] masih merupakan variabel acak dan keacakan berasal dari X. 16.1.2.2 Continuous Case Untuk variabel acak kontinu Y, harapannya didefinisikan sebagai \\(\\mathrm{E}[Y]=\\int_y y\\,f(y)dy\\) jika integralnya ada, dan variansnya didefinisikan sebagai \\[ \\mathrm{Var}[Y]=\\mathrm{E}\\{(X-\\mathrm{E}[Y])^2\\}=\\int_y y^2\\,f(y)dy-\\{\\mathrm{E}[Y]\\}^2 \\] jika nilainya terbatas. Untuk variabel acak kontinu bersama X Dan Y, harapan bersyarat dari variabel acak Y diberikan X= x didefinisikan sebagai \\[ \\mathrm{E}[Y|X=x]=\\int_y y\\,f(y|x)dy. \\] Di mana X tidak harus menjadi variabel kontinu, sejauh fungsi probabilitas bersyarat F( y| x) diberikan. Demikian pula, harapan bersyarat \\(\\mathrm{E}[Y|X=x]\\)adalah angka tetap. Ketika kita mengganti X dengan X di ruas kanan persamaan di atas, kita dapat menentukan ekspektasi dari Y diberikan variabel acak X sebagai \\[ \\mathrm{E}[Y|X]=\\int_y y\\,p(y|X)\\,dy, \\] yang masih merupakan variabel acak , dan keacakan itu berasal X. 16.2 Iterated Expectations and Total Varians Penjelasan pada subbab ini adalah: * Hukum Ekspektasi Iterasi untuk menghitung ekspektasi variabel acak berdasarkan distribusi kondisionalnya yang diberikan variabel acak lainnya * Law of Total Variance untuk menghitung varians suatu variabel acak berdasarkan distribusi kondisionalnya yang diberikan variabel acak lainnya * bagaimana menghitung ekspektasi dan varians berdasarkan contoh model dua tahap 16.2.1 Law of Iterated Expectations Pertimbangkan dua variabel acak X Dan Y, Dan h ( X, Y), variabel acak tergantung pada fungsinya H, X, dan Y. Dengan asumsi semua harapan ada dan terbatas, Hukum Harapan Iterasi menyatakan bahwa \\[ \\begin{equation} \\mathrm{E}[h(X,Y)]= \\mathrm{E} \\left\\{ \\mathrm{E} \\left[ h(X,Y) | X \\right] \\right \\}, \\end{equation} \\] di mana harapan (dalam) pertama diambil sehubungan dengan variabel acak Y dan ekspektasi kedua (di luar) diambil sehubungan dengan X. Untuk Hukum Ekspektasi Berulang, variabel acak mungkin diskrit, kontinu, atau kombinasi hibrid dari keduanya. Kami menggunakan contoh variabel diskrit dari X dan Y untuk mengilustrasikan perhitungan ekspektasi tak bersyarat menggunakan Hukum Harapan Iterasi. Untuk variabel acak kontinu, kita hanya perlu mengganti penjumlahan dengan integral, seperti yang diilustrasikan sebelumnya di lampiran. diberikan \\(p(y|x)\\) bersama dari X dan Y, harapan bersyarat dari \\(h(X,Y)\\) diberikan \\(X=x\\) didefinisikan sebagai, \\[ \\mathrm{E} \\left[ h(X,Y) | X=x \\right] = \\sum_y h(x,y) p(y|x), \\] dan harapan bersyarat dari h ( X, Y) diberikan X menjadi variabel acak dapat ditulis sebagai \\[ \\mathrm{E} \\left[ h(X,Y) | X \\right] = \\sum_y h(X,y) p(y|X). \\] Harapan tanpa syarat dari \\(h(X,Y)\\) ) kemudian dapat diperoleh dengan mengambil harapan \\(\\mathrm{E} \\left[ h(X,Y) | X \\right]\\) sehubungan dengan variabel acak X. Artinya, kita bisa mendapatkan \\(\\mathrm{E}[ h(X,Y)]\\) sebagai \\[ \\begin{aligned} \\mathrm{E} \\left\\{ \\mathrm{E} \\left[ h(X,Y) | X \\right] \\right \\} &amp;= \\sum_x \\left\\{\\sum_y h(x,y) p(y|x) \\right \\} p(x) \\\\ &amp;= \\sum_x \\sum_y h(x,y) p(y|x)p(x) \\\\ &amp;= \\sum_x \\sum_y h(x,y) p(x,y) = \\mathrm{E}[h(X,Y)] \\end{aligned}. \\] Hukum Ekspektasi Iterasi untuk kasus kontinyu dan hibrid dapat dibuktikan dengan cara yang sama, dengan mengganti penjumlahan yang sesuai dengan integral. 16.2.2 Law of Total Variance Dengan asumsi bahwa semua varians ada dan terbatas, Hukum Varians Total menyatakan bahwa \\[ \\begin{equation} \\mathrm{Var}[h(X,Y)]= \\mathrm{E} \\left\\{ \\mathrm{Var} \\left[h(X,Y) | X \\right] \\right \\} +\\mathrm{Var} \\left\\{ \\mathrm{E} \\left[ h(X,Y) | X \\right] \\right \\}, \\tag{16.2} \\end{equation} \\] di mana ekspektasi/varians (dalam) pertama diambil sehubungan dengan variabel acak Y dan ekspektasi/varians kedua (di luar) diambil sehubungan dengan X. Dengan demikian, varians tak bersyarat sama dengan ekspektasi varians bersyarat ditambah varians ekspektasi bersyarat. 16.2.3 Application Untuk menerapkan Hukum Ekspektasi Iterasi dan Hukum Varians Total, kami umumnya mengadopsi prosedur berikut. Identifikasi variabel acak yang dikondisikan, biasanya hasil tahap 1 (yang tidak diamati). Bersyarat pada hasil tahap 1, hitung ukuran ringkasan seperti rata-rata, varians, dan sejenisnya. Ada beberapa hasil dari langkah 2, satu untuk setiap hasil tahap 1. Kemudian, gabungkan hasil ini menggunakan ekspektasi iterasi atau aturan varian total. Campuran Populasi Hingga. Misalkan variabel acak \\(N_1\\) merupakan realisasi jumlah klaim dalam satu tahun polis dari populasi pengemudi yang baik dan \\(N_2\\) mewakili bahwa dari populasi pengemudi yang buruk. Untuk pengemudi tertentu, ada kemungkinan α bahwa (s) dia adalah pengemudi yang baik. Untuk pengundian tertentu N, kita punya \\[ N = \\begin{cases} N_1, &amp; \\text{jika (s)dia adakah pengemudi yang baik;}\\\\ N_2, &amp; \\text{otherwise}.\\\\ \\end{cases} \\] Membiarkan T menjadi indikator apakah dia pengemudi yang baik, dengan T= 1 yang menyatakan bahwa pengemudi adalah pengemudi yang baik \\(\\Pr[T=1]=\\alpha\\) dan \\(T=2\\) menyatakan bahwa pengemudi adalah pengemudi yang buruk \\(\\Pr[T=2]=1-\\alpha\\) kita dapat memperoleh jumlah klaim yang diharapkan sebagai \\[ \\mathrm{E}[N]= \\mathrm{E} \\left\\{ \\mathrm{E} \\left[ N | T \\right] \\right \\}= \\mathrm{E}[N_1] \\times \\alpha + \\mathrm{E}[N_2] \\times (1-\\alpha). \\] Untuk lebih konkret, misalkan itu \\(N_j\\) mengikuti distribusi Poisson dengan rata-rata \\(\\lambda_j\\), \\(j=1,2\\) maka \\[ \\mathrm{Var}[N|T=j]= \\mathrm{E}[N|T=j] = \\lambda_j, \\quad j = 1,2. \\] Dengan demikian, kita dapat memperoleh ekspektasi varian bersyarat sebagai \\[ \\mathrm{E} \\left\\{ \\mathrm{Var} \\left[ N | T \\right] \\right \\} = \\alpha \\lambda_1+ (1-\\alpha) \\lambda_2 \\] dan varian dari ekspektasi bersyarat sebagai \\[ \\mathrm{Var} \\left\\{ \\mathrm{E} \\left[ N | T \\right] \\right \\} = (\\lambda_1-\\lambda_2)^2 \\alpha (1-\\alpha). \\] Berdasarkan Hukum Varians Total, varians tak bersyarat dari N diberikan oleh \\[ \\mathrm{Var}[N]= \\alpha \\lambda_1+ (1-\\alpha) \\lambda_2 + (\\lambda_1-\\lambda_2)^2 \\alpha (1-\\alpha). \\] 16.3 Conjugate Distributions 16.3.1 Linear Exponential Family Distribusi keluarga eksponensial linier adalah \\[ f( x; \\gamma ,\\theta ) = \\exp \\left( \\frac{x\\gamma -b(\\gamma )}{\\theta} +S\\left( x,\\theta \\right) \\right). \\] Di Sini, X adalah variabel dependen dan γ adalah parameter minat. Kuantitas θ adalah parameter skala. Syarat b ( γ) hanya bergantung pada parameter γ, bukan variabel terikat. Statistik S( x , θ ) adalah fungsi dari variabel dependen dan parameter skala, bukan parameternya γ. Variabel dependen X dapat berupa diskrit, kontinyu atau kombinasi hybrid dari keduanya. Dengan demikian, F( ⋅ ) dapat ditafsirkan sebagai fungsi kerapatan atau massa, tergantung pada aplikasinya. Dibawah ini merupakan distribusi terpilih dari linear exponential family \\[ {\\small \\begin{matrix} \\begin{array}{l|ccccc} \\hline &amp; &amp; \\text{Density or} &amp; &amp; &amp; \\\\ \\text{Distribution} &amp; \\text{Parameters} &amp; \\text{Mass Function} &amp; \\text{Components} \\\\ \\hline \\text{General} &amp; \\gamma,~ \\theta &amp; \\exp \\left( \\frac{x\\gamma -b(\\gamma )}{\\theta} +S\\left( x,\\theta \\right) \\right) &amp; \\gamma,~ \\theta, b(\\gamma), S(x, \\theta)\\\\ \\text{Normal} &amp; \\mu, \\sigma^2 &amp; \\frac{1}{\\sigma \\sqrt{2\\pi }}\\exp \\left(-\\frac{(x-\\mu )^{2}}{2\\sigma ^{2}}\\right) &amp; \\mu, \\sigma^2, \\frac{\\gamma^2}{2}, - \\left(\\frac{x^2}{2\\theta} + \\frac{\\log(2 \\pi \\theta)}{2} \\right) \\\\ \\text{Binomal} &amp; \\pi &amp; {n \\choose x} \\pi ^x (1-\\pi)^{n-x} &amp; \\log \\left(\\frac{\\pi}{1-\\pi} \\right), 1, n \\log(1+e^{\\gamma} ), \\\\ &amp; &amp; &amp; \\log {n \\choose x} \\\\ \\text{Poisson} &amp; \\lambda &amp; \\frac{\\lambda^x}{x!} \\exp(-\\lambda) &amp; \\log \\lambda, 1, e^{\\gamma}, - \\log (x!) \\\\ \\text{Negative } &amp; r,p &amp; \\frac{\\Gamma(x+r)}{x!\\Gamma(r)} p^r ( 1-p)^x &amp; \\log(1-p), 1, -r \\log(1-e^{\\gamma}), \\\\ ~~~\\text{Binomial}^{\\ast} &amp; &amp; &amp; ~~~\\log \\left[ \\frac{\\Gamma(x+r)}{x! \\Gamma(r)} \\right] \\\\ \\text{Gamma} &amp; \\alpha, \\theta &amp; \\frac{1}{\\Gamma (\\alpha)\\theta ^ \\alpha} x^{\\alpha -1 }\\exp(-x/ \\theta) &amp; - \\frac{1}{\\alpha \\gamma}, \\frac{1}{\\alpha}, - \\log ( - \\gamma), -\\theta^{-1} \\log \\theta \\\\ &amp; &amp; &amp; - \\log \\left( \\Gamma(\\theta ^{-1}) \\right) + (\\theta^{-1} - 1) \\log x &amp; &amp; \\\\ \\hline \\end{array}\\\\ ^{\\ast} \\text{This assumes that the parameter r is fixed but need not be an integer.}\\\\ \\end{matrix} } \\] 16.3.2 Conjugate Distributions Sekarang asumsikan bahwa parameternya \\(\\gamma\\) acak dengan distribusi \\(\\pi(\\gamma, \\tau)\\), dimana \\(\\tau\\) adalah vektor parameter yang menggambarkan distribusi \\(\\gamma\\). Dalam model bayesian, distribusi \\(\\gamma\\) dikenal sebagai yang sebelumnya dan mencerminkan keyakinan atau informasi kami tentang \\(\\gamma\\). Kemungkinan \\(f(x|\\gamma)\\) adalah probabilitas bersyarat \\(\\gamma\\). distribusi dari \\(\\gamma\\) dengan pengetahuan tentang variabel acak \\(\\pi(\\gamma,\\tau| x)\\) disebut distribusi posterior. Untuk distribusi kemungkinan tertentu, prior dan posterior yang berasal dari keluarga parametrik yang sama dikenal sebagai keluarga distribusi konjugat. Untuk kemungkinan eksponensial linier, terdapat keluarga konjugasi alami. Secara khusus, pertimbangkan kemungkinan formulir \\(f(x|\\gamma) = \\exp \\left\\{(x\\gamma -b(\\gamma))/\\theta\\right\\} \\exp \\left\\{S\\left( x,\\theta \\right) \\right\\}\\) untuk kemungkinan ini, tentukan distribusi sebelumnya \\[ \\pi(\\gamma,\\tau) = C \\exp\\left\\{ \\gamma a_1(\\tau) - b(\\gamma)a_2(\\tau))\\right\\}, \\] dimana \\(C\\) adalah konstanta. Disini \\(a_1(\\tau)=a_1\\) dan \\(a_2(\\tau)=a_2\\) adalah fungsi dari parameter \\(\\gamma\\). b4abc2c5d190cd1ea18e0207c8734a0abd5b5641 "],["appendix-c-maximum-likelihood-theory.html", "Bab 17 Appendix C: Maximum Likelihood Theory 17.1 Likelihood Function 17.2 Maximum Likelihood Estimators 17.3 Statistical Inference Based on Maximum Likelihood Estimation", " Bab 17 Appendix C: Maximum Likelihood Theory Pratinjau Bab. Lampiran Bab 15 memperkenalkan teori kemungkinan maksimum terkait estimasi parameter dari keluarga parametrik. Lampiran ini memberikan contoh yang lebih spesifik dan mengembangkan beberapa konsep. Bagian 17.1 mengulas definisi fungsi kemungkinan dan memperkenalkan propertinya. Bagian 17.2 mengulas estimasi kemungkinan maksimum, dan memperluas properti sampel besar mereka untuk kasus di mana terdapat beberapa parameter dalam model. Bagian 17.3 mengulas inferensi statistik berdasarkan estimasi kemungkinan maksimum, dengan contoh-contoh khusus pada kasus dengan beberapa parameter. 17.1 Likelihood Function 17.1.1 Likelihood and Log-likelihood Functions Di sini, kami memberikan tinjauan singkat tentang fungsi kemungkinan dan fungsi log-kemungkinan dari Lampiran Bab 15. Biarkan \\(f(\\cdot|\\boldsymbol\\theta)\\) menjadi fungsi probabilitas dari \\(X\\) , fungsi massa probabilitas(probability mass function/PMF) jika \\(X\\) diskrit atau fungsi densitas probabilitas (probability density function/PDF) jika kontinu. Kemungkinan adalah fungsi dari parameter ( \\(\\theta\\) ) yang diberikan data ( \\(x\\) ). Oleh karena itu, itu adalah fungsi parameter dengan data yang tetap, bukan fungsi data dengan parameter yang tetap. Vektor data \\(x\\) biasanya merupakan realisasi dari sampel acak seperti yang didefinisikan dalam Lampiran Bab 15. \\[L(\\boldsymbol{\\theta}|\\mathbf{x})=f(\\mathbf{x}|\\boldsymbol{\\theta})=\\prod_{i=1}^nf(x_i|\\boldsymbol{\\theta}),\\] Diberikan suatu realisasi dari sampel acak \\(\\mathbf{x}=(x_1,x_2,\\cdots,x_n)\\) dengan ukuran n , fungsi kemungkinan didefinisikan sebagai berikut: \\[l(\\boldsymbol{\\theta}|\\mathbf{x})=\\log L(\\boldsymbol{\\theta}|\\mathbf{x})=\\sum_{i=1}^n\\log f(x_i|\\boldsymbol{\\theta}),\\] \\[l(\\boldsymbol{\\theta}|\\mathbf{x})=\\log L(\\boldsymbol{\\theta}|\\mathbf{x})=\\sum_{i=1}^n\\log f(x_i|\\boldsymbol{\\theta}),\\] Di Appendix Bab 15, kita telah menggunakan distribusi normal sebagai contoh untuk menggambarkan konsep fungsi kemungkinan (likelihood function) dan fungsi log-kemungkinan (log-likelihood function). Di sini, kita akan turunkan fungsi kemungkinan dan fungsi log-kemungkinan yang sesuai ketika distribusi populasi berasal dari keluarga distribusi Pareto. 17.1.2 Properties of Likelihood Functions Dalam statistik matematika, turunan pertama dari fungsi log-kemungkinan terhadap parameter, \\(u(\\boldsymbol\\theta)=\\partial l(\\boldsymbol \\theta|\\mathbf{x})/\\partial \\boldsymbol \\theta\\), disebut sebagai fungsi skor, atau vektor skor ketika terdapat beberapa parameter dalam \\(\\theta\\). Fungsi skor atau vektor skor dapat dituliskan sebagai berikut: \\[u(\\boldsymbol\\theta)=\\frac{ \\partial}{\\partial \\boldsymbol \\theta} l(\\boldsymbol \\theta|\\mathbf{x}) =\\frac{ \\partial}{\\partial \\boldsymbol \\theta} \\log \\prod_{i=1}^n f(x_i;\\boldsymbol \\theta ) =\\sum_{i=1}^n \\frac{ \\partial}{\\partial \\boldsymbol \\theta} \\log f(x_i;\\boldsymbol \\theta ),\\] \\[\\mathrm{E}[u(\\boldsymbol\\theta)]=\\mathrm{E} \\left[ \\frac{ \\partial}{\\partial \\boldsymbol \\theta} l(\\boldsymbol \\theta|\\mathbf{x}) \\right] = \\mathbf 0 .\\] di mana \\(u(\\boldsymbol\\theta)=(u_1(\\boldsymbol\\theta),u_2(\\boldsymbol\\theta),\\cdots,u_p(\\boldsymbol\\theta))\\) ketika \\(\\boldsymbol\\theta=(\\theta_1,\\cdots,\\theta_p)\\) berisi \\(p &gt; 2\\) parameter, dengan elemen uk(θ) = ∂l(θ|x)/∂θk merupakan turunan parsial terhadap \\(\\theta_k\\) \\(k=1,2,\\cdots,p\\) Fungsi kemungkinan memiliki sifat-sifat berikut: Salah satu sifat dasar dari fungsi kemungkinan adalah bahwa harapan dari fungsi skor terhadap \\(x\\) adalah 0. Yaitu, \\[\\begin{aligned} \\mathrm{E} \\left[ \\frac{ \\partial}{\\partial \\boldsymbol \\theta} l(\\boldsymbol \\theta|\\mathbf{x}) \\right] &amp;= \\mathrm{E} \\left[ \\frac{\\frac{\\partial}{\\partial \\boldsymbol \\theta}f(\\mathbf{x};\\boldsymbol \\theta)}{f(\\mathbf{x};\\boldsymbol \\theta )} \\right] = \\int\\frac{\\partial}{\\partial \\boldsymbol \\theta} f(\\mathbf{y};\\boldsymbol \\theta ) d \\mathbf y \\\\ &amp;= \\frac{\\partial}{\\partial \\boldsymbol \\theta} \\int f(\\mathbf{y};\\boldsymbol \\theta ) d \\mathbf y = \\frac{\\partial}{\\partial \\boldsymbol \\theta} 1 = \\mathbf 0.\\end{aligned} \\] nyatakan dengan \\({ \\partial^2 l(\\boldsymbol \\theta|\\mathbf{x}) }/{\\partial \\boldsymbol \\theta\\partial \\boldsymbol \\theta^{\\prime}}={ \\partial^2 l(\\boldsymbol \\theta|\\mathbf{x}) }/{\\partial \\boldsymbol \\theta^{2}}\\) turunan kedua dari fungsi log-kemungkinan saat θ adalah parameter tunggal, atau dengan \\({ \\partial^2 l(\\boldsymbol \\theta|\\mathbf{x}) }/{\\partial \\boldsymbol \\theta\\partial \\boldsymbol \\theta^{\\prime}}=(h_{jk})=({ \\partial^2 l(\\boldsymbol \\theta|\\mathbf{x}) }/\\partial x_j\\partial x_k)\\) matriks Hessiana dari fungsi log-kemungkinan saat mengandung beberapa parameter. Nyatakan \\([{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta}][{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta&#39;}]=u^2(\\boldsymbol \\theta)\\) saat \\(\\theta\\) adalah parameter tunggal, atau biarkan \\([{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta}][{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta&#39;}]=(uu_{jk})\\) menjadi matriks p×p saat θ mengandung total \\(p\\) parameter, dengan setiap elemen \\(uu_{jk}=u_j(\\boldsymbol \\theta)u_k(\\boldsymbol \\theta)\\) merupakan elemen ke-k dari vektor skor seperti yang didefinisikan sebelumnya. Sifat dasar lain dari fungsi kemungkinan adalah bahwa jumlah dari harapan matriks Hessiana dan harapan hasil kali Kronecker dari vektor skor dan transpose-nya adalah 0. Yaitu, \\[\\mathrm{E} \\left( \\frac{ \\partial^2 }{\\partial \\boldsymbol \\theta\\partial \\boldsymbol \\theta^{\\prime}} l(\\boldsymbol \\theta|\\mathbf{x}) \\right) + \\mathrm{E} \\left( \\frac{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta} \\frac{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta^{\\prime}}\\right) = \\mathbf 0.\\] \\[\\mathcal{I}(\\boldsymbol \\theta) = \\mathrm{E} \\left( \\frac{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial \\boldsymbol \\theta} \\frac{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial \\boldsymbol \\theta^{\\prime}} \\right) = -\\mathrm{E} \\left( \\frac{ \\partial^2}{\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta^{\\prime}} l(\\boldsymbol \\theta|\\mathbf{x}) \\right).\\] Saat ukuran sampel n mendekati tak hingga, fungsi skor (vektor) akan konvergen dalam distribusi ke distribusi normal (atau distribusi normal multivariat jika \\(\\theta\\) mengandung beberapa parameter) dengan rata-rata 0 dan varian (atau matriks kovarian dalam kasus multivariat) diberikan oleh \\(\\mathcal{I}(\\boldsymbol \\theta)\\). 17.2 Maximum Likelihood Estimators Dalam statistika, estimasi maksimum likelihood adalah nilai-nilai parameter θ yang paling mungkin dihasilkan oleh data. 17.2.1 Definition and Derivation of MLE Berdasarkan definisi yang diberikan dalam Lampiran Bab 15, nilai \\(\\theta\\), katakanlah \\(\\hat{\\boldsymbol \\theta}_{MLE}\\), yang memaksimalkan fungsi likelihood, disebut sebagai estimasi maksimum likelihood (MLE) dari \\(\\theta\\). Karena fungsi logaritma \\(\\log(\\cdot)\\) adalah fungsi satu-ke-satu, kita juga dapat menentukan \\(\\hat{\\boldsymbol \\theta}_{MLE}\\)dengan memaksimalkan fungsi log-likelihood, \\(l(\\boldsymbol \\theta|\\mathbf{x})\\). Dengan kata lain, MLE didefinisikan sebagai: \\[\\hat{\\boldsymbol \\theta}_{MLE} = {\\mbox{argmax}}_{\\boldsymbol{\\theta}\\in\\Theta}~l(\\boldsymbol{\\theta}|\\mathbf{x}).\\] Diberikan bentuk analitik dari fungsi likelihood, MLE dapat diperoleh dengan mengambil turunan pertama dari fungsi log-likelihood terhadap θ, dan mengatur nilai-nilai turunan parsial menjadi nol. Dengan kata lain, MLE adalah solusi dari persamaan-persamaan berikut: \\[\\frac{\\partial l(\\hat{\\boldsymbol{\\theta}}|\\mathbf{x})}{\\partial\\hat{\\boldsymbol{\\theta}}}=\\mathbf 0.\\] 17.2.2 Asymptotic Properties of MLE Dari Appendix Chapter 15, MLE memiliki beberapa sifat yang baik dalam sampel besar, di bawah kondisi reguler tertentu. Kami menyajikan hasil-hasil tersebut untuk kasus satu parameter di Appendix Chapter 15, tetapi hasil-hasil tersebut juga berlaku untuk kasus ketika \\(\\theta\\) mengandung beberapa parameter. Secara khusus, kami memiliki hasil-hasil berikut, dalam kasus umum ketika \\(\\boldsymbol{\\theta}=(\\theta_1,\\theta_2,\\cdots,\\theta_p)\\). MLE dari suatu parameter \\(\\theta\\), \\(\\hat{\\boldsymbol \\theta}_{MLE}\\), adalah estimator yang konsisten. Artinya, MLE \\(\\hat{\\boldsymbol \\theta}_{MLE}\\) konvergen dalam probabilitas menuju nilai sebenarnya \\(\\theta\\), saat ukuran sampel n menuju tak hingga. MLE memiliki sifat asymptotic normality, yang berarti bahwa estimator akan konvergen dalam distribusi menuju distribusi normal multivariat yang berpusat pada nilai sebenarnya, saat ukuran sampel menuju tak hingga. Secara khusus, \\[\\sqrt{n}(\\hat{\\boldsymbol{\\theta}}_{MLE}-\\boldsymbol{\\theta})\\rightarrow N\\left(\\mathbf 0,\\,\\boldsymbol{V}\\right),\\quad \\mbox{as}\\quad n\\rightarrow \\infty,\\] di mana V merupakan varian asimptotik (atau matriks kovarian) dari estimator. Oleh karena itu, MLE \\(\\hat{\\boldsymbol \\theta}_{MLE}\\) memiliki distribusi normal yang hampir dengan mean \\(\\theta\\) dan varian (matriks kovarian jika \\(\\boldsymbol{V}/n\\) saat ukuran sampel besar. MLE adalah estimator yang efisien, yang berarti memiliki varian asimptotik terkecil V, yang biasa disebut sebagai batas bawah Cramer-Rao. Secara khusus, batas bawah Cramer-Rao adalah invers dari informasi Fisher (matriks) \\(\\mathcal{I}(\\boldsymbol{\\theta})\\) yang didefinisikan sebelumnya dalam lampiran ini. Oleh karena itu, \\(\\mathrm{Var}(\\hat{\\boldsymbol{\\theta}}_{MLE})\\) dapat diestimasi berdasarkan informasi Fisher yang diamati. Berdasarkan hasil-hasil di atas, kita dapat melakukan inferensi statistik berdasarkan prosedur yang ditentukan dalam Appendix Chapter 15. 17.2.3 Use of Maximum Likelihood Estimation Metode estimasi maksimum likelihood memiliki banyak keunggulan dibandingkan metode alternatif seperti metode momen yang diperkenalkan dalam Appendix Chapter 15. Ini adalah alat umum yang berfungsi dalam banyak situasi. Misalnya, kita dapat menuliskan fungsi likelihood dalam bentuk tertutup untuk data yang tercensored dan tertruncated. Estimasi maksimum likelihood dapat digunakan untuk model regresi termasuk covariate, seperti regresi survival, generalized linear models, dan mixed models, yang mungkin mencakup covariate yang bergantung pada waktu. Dari efisiensi MLE, metode ini optimal, yang terbaik, dalam arti bahwa memiliki varian terkecil di antara kelas semua estimator yang tidak bias untuk ukuran sampel besar. Dari hasil mengenai asimptotik normalitas MLE, kita dapat memperoleh distribusi untuk estimator dalam sampel besar, memungkinkan pengguna untuk menilai variabilitas dalam estimasi dan melakukan inferensi statistik pada parameter. Pendekatan ini lebih sedikit menghabiskan komputasi dibandingkan metode resampling yang membutuhkan banyak fitting model. Meskipun memiliki banyak keunggulan, MLE memiliki kekurangan dalam kasus seperti generalized linear models ketika tidak memiliki bentuk analitik tertutup. Dalam kasus seperti itu, estimator maksimum likelihood dihitung secara iteratif menggunakan metode optimisasi numerik. Misalnya, kita dapat menggunakan algoritma iteratif Newton-Raphson atau variasinya untuk mendapatkan MLE. Algoritma iteratif membutuhkan nilai awal. Untuk beberapa masalah, pemilihan nilai awal yang dekat menjadi sangat penting, terutama dalam kasus di mana fungsi likelihood memiliki minimum atau maksimum lokal. Oleh karena itu, mungkin ada masalah konvergensi ketika nilai awal jauh dari nilai maksimum. Oleh karena itu, penting untuk memulai dari nilai yang berbeda di seluruh ruang parameter, dan membandingkan likelihood atau log-likelihood yang dimaksimumkan untuk memastikan bahwa algoritma telah konvergen ke maksimum global. 17.3 Statistical Inference Based on Maximum Likelihood Estimation Di Appendix Chapter 15, kami telah memperkenalkan metode berbasis maksimum likelihood untuk inferensi statistik ketika θ mengandung satu parameter. Di sini, kami akan memperluas hasil tersebut untuk kasus di mana terdapat beberapa parameter dalam θ. 17.3.1 Hypothesis Testing Di Appendix Chapter 15, kami mendefinisikan pengujian hipotesis terkait hipotesis nol, yaitu pernyataan mengenai parameter-parameter dari distribusi atau model. Salah satu jenis inferensi yang penting adalah untuk menilai apakah estimasi parameter secara signifikan secara statistik, artinya apakah nilai parameter tersebut nol atau tidak. Sebelumnya, kami telah belajar bahwa mle \\(\\mathrm{Var}(\\hat{\\boldsymbol{\\theta}}_{MLE})\\) memiliki distribusi normal untuk ukuran sampel yang besar dengan mean θ dan matriks kovarian varian \\(\\mathcal{I}^{-1}(\\boldsymbol \\theta)\\). Berdasarkan distribusi normal multivariat, elemen ke-j dari θ^MLE, katakanlah \\(\\mathrm{Var}(\\hat{\\boldsymbol{\\theta}}_{MLE})\\),\\(\\hat{\\theta}_{MLE,j}\\), memiliki distribusi normal univariat untuk ukuran sampel yang besar. Tentukan \\(se(\\hat{\\theta}_{MLE,j})\\), yaitu kesalahan standar (deviasi standar yang diestimasi), sebagai akar kuadrat elemen diagonal ke-j dari \\(\\mathcal{I}^{-1}(\\boldsymbol \\theta)_{MLE}\\). Untuk menilai hipotesis nol bahwa \\(\\theta_j=\\theta_0\\), kami mendefinisikan statistik t atau rasio t sebagai \\(t(\\hat{\\theta}_{MLE,j})=(\\hat{\\theta}_{MLE,j}-\\theta_0)/se(\\hat{\\theta}_{MLE,j})\\). Di bawah hipotesis nol, statistik t tersebut memiliki distribusi t-Student dengan derajat kebebasan sebesar \\(n−p\\), dengan \\(p\\) adalah dimensi dari \\(\\theta\\) . Untuk sebagian besar aplikasi aktuaria, kita memiliki ukuran sampel yang besar \\(n\\), sehingga distribusi \\(t\\) sangat dekat dengan distribusi normal (standar). Dalam kasus ketika n sangat besar atau ketika kesalahan standar diketahui, statistik \\(t\\) dapat disebut sebagai statistik \\(z\\) atau skor \\(z\\). Berdasarkan hasil dari Appendix Chapter 15, jika statistik \\(t\\) \\(t(\\hat{\\theta}_{MLE,j})\\) melebihi nilai batas (dalam nilai absolut), maka pengujian untuk parameter ke-j \\(\\theta_j\\) dianggap signifikan secara statistik. Jika \\(\\theta_j\\) adalah koefisien regresi dari variabel independen ke-j, maka kita mengatakan bahwa variabel ke-j tersebut signifikan secara statistik. Sebagai contoh, jika kita menggunakan tingkat signifikansi 5%, maka nilai batasnya adalah 1.96 dengan menggunakan pendekatan distribusi normal untuk kasus dengan ukuran sampel yang besar. Secara umum, dengan menggunakan tingkat signifikansi \\(100 \\alpha \\%\\), maka nilai batasnya adalah kuantil \\(100(1-\\alpha/2)\\%\\) dari distribusi t-Student dengan derajat kebebasan \\(n−p\\). Konsep lain yang berguna dalam pengujian hipotesis adalah p-value atau probability value. Berdasarkan definisi matematis dalam Appendix Chapter 15, p-value didefinisikan sebagai tingkat signifikansi terkecil di mana hipotesis nol akan ditolak. Oleh karena itu, p-value adalah statistik ringkasan yang berguna bagi analis data untuk dilaporkan karena memungkinkan pembaca memahami kekuatan bukti statistik tentang penyimpangan dari hipotesis nol. 17.3.2 MLE and Model Validation Selain pengujian hipotesis dan estimasi interval yang diperkenalkan di Appendix Chapter 15 dan subbagian sebelumnya, jenis inferensi penting lainnya adalah pemilihan model dari dua pilihan, di mana satu pilihan merupakan kasus khusus dari yang lain dengan beberapa parameter dibatasi. Untuk dua model tersebut, di mana satu model termasuk dalam model yang lain, kami telah memperkenalkan uji rasio kemungkinan (likelihood ratio test/LRT) di Appendix Chapter 15. Di sini, kami akan secara singkat mengulas proses melakukan LRT berdasarkan contoh khusus dari dua model alternatif. Misalkan kita memiliki sebuah model (besar) di mana kita memperoleh estimasi maximum likelihood, \\(\\hat{\\boldsymbol{\\theta}}_{MLE}v\\). Sekarang diasumsikan bahwa beberapa elemen \\(p\\) dalam \\(\\theta\\) adalah nol, dan kita menentukan estimasi maximum likelihood dari himpunan yang tersisa, dengan estimasi yang dihasilkan ditunjukkan sebagai \\(\\hat{\\boldsymbol{\\theta}}_{Reduced}\\). Berdasarkan definisi di Appendix Chapter 15, statistik \\(LRT= 2 \\left( l(\\hat{\\boldsymbol{\\theta}}_{MLE}) - l(\\hat{\\boldsymbol{\\theta}}_{Reduced}) \\right)\\), disebut sebagai statistik rasio kemungkinan. Di bawah hipotesis nol bahwa model yang dibatasi adalah benar, rasio kemungkinan memiliki distribusi chi-kuadrat dengan derajat kebebasan sebanyak d, yaitu jumlah variabel yang diatur menjadi nol. Uji seperti ini memungkinkan kita menentukan model mana dari dua model yang lebih mungkin benar, dengan mempertimbangkan data yang diamati. Jika statistik LRT besar relatif terhadap nilai kritis dari distribusi chi-kuadrat, maka kita menolak model yang dibatasi dan memilih model yang lebih besar. Detail mengenai nilai kritis dan metode alternatif berdasarkan kriteria informasi dijelaskan di Appendix Chapter 15. "],["appendix-d-summary-of-distributions.html", "Bab 18 Appendix D: Summary of Distributions 18.1 Distribusi Diskrit 18.2 Continuous Distribution 18.3 Limited Expected Values", " Bab 18 Appendix D: Summary of Distributions User Notes 18.1 Distribusi Diskrit Overview. Bagian ini merangkum distribusi probabilitas diskrit terpilih yang digunakan di seluruh Analisis Data Kerugian. Fungsi dan kode R yang relevan disediakan. 18.1.1 The (a,b,0) Class Poisson Geometric Binomial Negative Binomial Hide Poisson Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\lambda&gt;0 \\\\ \\hline ~~p_0 &amp; e^{-\\lambda} \\\\ \\hline \\small{\\text{Probability mass function}} &amp; \\frac{e^{-\\lambda}\\lambda^k}{k!} \\\\ ~~p_k &amp; \\\\ \\hline \\small{\\text{Expected value}} &amp; \\lambda \\\\ ~~\\mathrm{E}[N] &amp; \\\\ \\hline \\small{\\text{Variance}} &amp; \\lambda \\\\ \\hline \\small{\\text{Probability generating function}} &amp; e^{\\lambda(z-1)} \\\\ ~~P(z) &amp; \\\\ \\hline a \\small{\\text{ and }} b \\small{\\text{ for recursion}} &amp; a=0 \\\\ &amp; b=\\lambda \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Probability mass function}} &amp; \\text{dpois}(x=, lambda=\\lambda) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{ppois}(p=, lambda=\\lambda) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qpois}(q=, lambda=\\lambda) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rpois}(n=, lambda=\\lambda) \\\\ \\hline \\end{array} \\end{matrix} \\] Hide Geometric Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\beta&gt;0 \\\\ \\hline ~~p_0 &amp; \\frac{1}{1+\\beta} \\\\ \\hline \\small{\\text{Probability mass function}} &amp; \\frac{\\beta^k}{(1+\\beta)^{k+1}} \\\\ ~~p_k &amp; \\\\ \\hline \\small{\\text{Expected value}} &amp; \\beta \\\\ ~~\\mathrm{E}[N] &amp; \\\\ \\hline \\small{\\text{Variance}} &amp; \\beta(1+\\beta) \\\\ \\hline \\small{\\text{Probability generating function}} &amp; [1-\\beta(z-1)]^{-1} \\\\ ~~P(z) &amp; \\\\ \\hline a \\small{\\text{ and }} b \\small{\\text{ for recursion}} &amp; a=\\frac{\\beta}{1+\\beta} \\\\ &amp; b=0 \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Probability mass function}} &amp; \\text{dgeom}(x=, prob=\\frac{1}{1+\\beta}) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pgeom}(p=, prob=\\frac{1}{1+\\beta}) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qgeom}(q=, prob=\\frac{1}{1+\\beta}) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rgeom}(n=, prob=\\frac{1}{1+\\beta}) \\\\ \\hline \\end{array} \\end{matrix} \\] Hide Binomial Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; 0&lt;q&lt;1,~\\text{m is an integer} \\\\ &amp; 0 \\leq k \\leq m\\\\ \\hline ~~p_0 &amp;(1-q)^m \\\\ \\hline \\small{\\text{Probability mass function}} &amp; {m \\choose k}q^k(1-q)^{m-k} \\\\ ~~p_k &amp; \\\\ \\hline \\small{\\text{Expected value}} &amp; mq \\\\ ~~\\mathrm{E}[N] &amp; \\\\ \\hline \\small{\\text{Variance}} &amp; mq(1-q) \\\\ \\hline \\small{\\text{Probability generating function}} &amp; [1+q(z-1)]^m \\\\ ~~P(z) &amp; \\\\ \\hline a \\small{\\text{ and }} b \\small{\\text{ for recursion}} &amp; a=\\frac{-q}{1-q} \\\\ &amp; b=\\frac{(m+1)q}{1-q} \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Probability mass function}} &amp; \\text{dbinom}(x=, size=m, prob=q) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pbinom}(p=, size=m, prob=q) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qbinom}(q=, size=m, prob=q) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rbinom}(n=, size=m, prob=q) \\\\ \\hline \\end{array} \\end{matrix} \\] Hide Negative Binomial Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; r&gt;0, \\beta&gt;0 \\\\ \\hline ~~p_0 &amp; (1+\\beta)^{-r} \\\\ \\hline \\small{\\text{Probability mass function}} &amp; \\frac{r(r+1)\\cdots(r+k-1)\\beta^k}{k!(1+\\beta)^{r+k}} \\\\ ~~p_k &amp; \\\\ \\hline \\small{\\text{Expected value}} &amp; r\\beta \\\\ ~~\\mathrm{E}[N] &amp; \\\\ \\hline \\small{\\text{Variance}} &amp; r\\beta(1+\\beta) \\\\ \\hline \\small{\\text{Probability generating function}} &amp; [1-\\beta(z-1)]^{-r} \\\\ ~~P(z) &amp; \\\\ \\hline a \\small{\\text{ and }} b \\small{\\text{ for recursion}} &amp; a=\\frac{\\beta}{1+\\beta} \\\\ &amp; b=\\frac{(r-1)\\beta}{1+\\beta} \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Probability mass function}} &amp; \\text{dnbinom}(x=, size=r, prob=\\frac{1}{1+\\beta}) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pnbinom}(p=, size=r, prob=\\frac{1}{1+\\beta}) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qnbinom}(q=, size=r, prob=\\frac{1}{1+\\beta}) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rnbinom}(n=, size=r, prob=\\frac{1}{1+\\beta}) \\\\ \\hline \\end{array} \\end{matrix} \\] 18.1.2 The (a,b,1) Class Zero Truncated Poisson Zero Truncated Geometric Zero Truncated Binomial Zero Truncated Negative Binomial Logarithmic Hide Zero Truncated Poisson Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\lambda&gt;0 \\\\ \\hline ~~p^T_1 &amp; \\frac{\\lambda}{e^\\lambda-1} \\\\ \\hline \\small{\\text{Probability mass function}} &amp; \\frac{\\lambda^k}{k!(e^\\lambda-1)} \\\\ ~~p^T_k &amp; \\\\ \\hline \\small{\\text{Expected value}} &amp; \\frac{\\lambda}{1-e^{-\\lambda}} \\\\ ~~\\mathrm{E}[N] &amp; \\\\ \\hline \\small{\\text{Variance}} &amp; \\frac{\\lambda[1-(\\lambda+1)e^{-\\lambda}]}{(1-e^{-\\lambda})^2} \\\\ \\hline \\small{\\text{Probability generating function}} &amp; \\frac{e^{\\lambda z}-1}{e^\\lambda-1} \\\\ ~~P(z) &amp; \\\\ \\hline a \\small{\\text{ and }} b \\small{\\text{ for recursion}} &amp; a=0 \\\\ &amp; b=\\lambda \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Probability mass function}} &amp; \\text{dztpois}(x=, lambda=\\lambda) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pztpois}(p=, lambda=\\lambda) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qztpois}(q=, lambda=\\lambda) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rztpois}(n=, lambda=\\lambda) \\\\ \\hline \\end{array} \\end{matrix} \\] Hide Zero Truncated Geometric Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\beta&gt;0 \\\\ \\hline ~~p^T_1 &amp; \\frac{1}{1+\\beta} \\\\ \\hline \\small{\\text{Probability mass function}} &amp; \\frac{\\beta^{k-1}}{(1+\\beta)^k} \\\\ ~~p^T_k &amp; \\\\ \\hline \\small{\\text{Expected value}} &amp; 1+\\beta \\\\ ~~\\mathrm{E}[N] &amp; \\\\ \\hline \\small{\\text{Variance}} &amp; \\beta(1+\\beta) \\\\ \\hline \\small{\\text{Probability generating function}} &amp; \\frac{[1-\\beta(z-1)]^{-1}-(1+\\beta)^{-1}}{1-(1+\\beta)^{-1}} \\\\ ~~P(z) &amp; \\\\ \\hline a \\small{\\text{ and }} b \\small{\\text{ for recursion}} &amp; a=\\frac{\\beta}{1+\\beta} \\\\ &amp; b=0 \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Probability mass function}} &amp; \\text{dztgeom}(x=, prob=\\frac{1}{1+\\beta}) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pztgeom}(p=, prob=\\frac{1}{1+\\beta}) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qztgeom}(q=, prob=\\frac{1}{1+\\beta}) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rztgeom}(n=, prob=\\frac{1}{1+\\beta}) \\\\ \\hline \\end{array} \\end{matrix} \\] Hide Zero Truncated Binomial Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; 0&lt;q&lt;1,~\\text{m is an integer} \\\\ &amp; 0 \\leq k \\leq m\\\\ \\hline ~~p^T_1 &amp; \\frac{m(1-q)^{m-1}q}{1-(1-q)^m} \\\\ \\hline \\small{\\text{Probability mass function}} &amp; \\frac{{m \\choose k}q^k(1-q)^{m-k}}{1-(1-q)^m} \\\\ ~~p^T_k &amp; \\\\ \\hline \\small{\\text{Expected value}} &amp; \\frac{mq}{1-(1-q)^m} \\\\ ~~\\mathrm{E}[N] &amp; \\\\ \\hline \\small{\\text{Variance}} &amp; \\frac{mq[(1-q)-(1-q+mq)(1-q)^m]}{[1-(1-q)^m]^2} \\\\ \\hline \\small{\\text{Probability generating function}} &amp; \\frac{[1+q(z-1)^m]-(1-q)^m}{1-(1-q)^m} \\\\ ~~P(z) &amp; \\\\ \\hline a \\small{\\text{ and }} b \\small{\\text{ for recursion}} &amp; a=\\frac{-q}{1-q} \\\\ &amp; b=\\frac{(m+1)q}{1-q} \\\\ \\hline \\end{array} \\end{matrix} \\] R Commmands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Probability mass function}} &amp; \\text{dztbinom}(x=, size=m, prob=p) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pztbinom}(p=, size=m, prob=p) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qztbinom}(q=, size=m, prob=p) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rztbinom}(n=, size=m, prob=p) \\\\ \\hline \\end{array} \\end{matrix} \\] Hide Zero Truncated Negative Binomial Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; r&gt;-1, r\\neq0 \\\\ \\hline ~~p^T_1 &amp; \\frac{r\\beta}{(1+\\beta)^{r+1}-(1+\\beta)} \\\\ \\hline \\small{\\text{Probability mass function}} &amp; \\frac{r(r+1)\\cdots(r+k-1)}{k![(1+\\beta)^r-1]}(\\frac{\\beta}{1+\\beta})^k \\\\ ~~p^T_k &amp; \\\\ \\hline \\small{\\text{Expected value}} &amp; \\frac{r\\beta}{1-(1+\\beta)^{-r}} \\\\ ~~\\mathrm{E}[N] &amp; \\\\ \\hline \\small{\\text{Variance}} &amp; \\frac{r\\beta[(1+\\beta)-(1+\\beta+r\\beta)(1+\\beta)^{-r}]}{[1-(1+\\beta)^{-r}]^2} \\\\ \\hline \\small{\\text{Probability generating function}} &amp; \\frac{[1-\\beta(z-1)]^{-r}-(1+\\beta)^{-r}}{1-(1+\\beta)^{-r}} \\\\ ~~P(z) &amp; \\\\ \\hline a \\small{\\text{ and }} b \\small{\\text{ for recursion}} &amp; a=\\frac{\\beta}{1+\\beta} \\\\ &amp; b=\\frac{(r-1)\\beta}{1+\\beta} \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Probability mass function}} &amp; \\text{dztnbinom}(x=, size=r, prob=\\frac{1}{1+\\beta}) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pztnbinom}(p=, size=r, prob=\\frac{1}{1+\\beta}) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qztnbinom}(q=, size=r, prob=\\frac{1}{1+\\beta}) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rztnbinom}(n=, size=r, prob=\\frac{1}{1+\\beta}) \\\\ \\hline \\end{array} \\end{matrix} \\] Hide Logarithmic Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\beta&gt;0 \\\\ \\hline ~~p^T_1 &amp; \\frac{\\beta}{(1+\\beta)ln(1+\\beta)} \\\\ \\hline \\small{\\text{Probability mass function}} &amp; \\frac{\\beta^k}{k(1+\\beta)^k \\ln (1+\\beta)} \\\\ ~~p^T_k &amp; \\\\ \\hline \\small{\\text{Expected value}} &amp; \\frac{\\beta}{\\ln (1+\\beta)} \\\\ ~~\\mathrm{E}[N] &amp; \\\\ \\hline \\small{\\text{Variance}} &amp; \\frac{\\beta[1+\\beta-\\frac{\\beta}{ln(1+\\beta)}]}{\\ln (1+\\beta)} \\\\ \\hline \\small{\\text{Probability generating function}} &amp; 1-\\frac{ln[1-\\beta(z-1)]}{\\ln (1+\\beta)} \\\\ ~~P(z) &amp; \\\\ \\hline a \\small{\\text{ and }} b \\small{\\text{ for recursion}} &amp; a=\\frac{\\beta}{1+\\beta} \\\\ &amp; b=\\frac{-\\beta}{1+\\beta} \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Probability mass function}} &amp; \\text{dnbinom}(x=,prob=\\frac{\\beta}{1+\\beta}) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pnbinom}(p=,prob=\\frac{\\beta}{1+\\beta}) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qnbinom}(q=,prob=\\frac{\\beta}{1+\\beta}) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rnbinom}(n=,prob=\\frac{\\beta}{1+\\beta}) \\\\ \\hline \\end{array} \\end{matrix} \\] 18.2 Continuous Distribution 18.2.1 One Parameter Distributions Exponential Inv Exponential Single Parameter Pareto Hide Exponential Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\theta&gt;0 \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{1}{\\theta}e^{-x/\\theta} \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; 1-e^{-x/\\theta} \\\\ ~~F(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\theta^k\\Gamma(k+1) \\\\ ~~\\mathrm{E}[X^k] &amp; k&gt;-1 \\\\ \\hline VaR_p(x) &amp; -\\theta \\ln (1-p) \\\\ \\hline \\small{\\text{Limited Expected Value}} &amp; \\theta(1-e^{-x/\\theta}) \\\\ ~~\\mathrm{E}[X\\wedge x] &amp; \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dexp}(x=, rate=1/\\theta) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pexp}(p=, rate=1/\\theta) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qexp}(q=, rate=1/\\theta) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rexp}(n=, rate=1/\\theta) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph library(invgamma) ## ## Attaching package: &#39;invgamma&#39; ## The following objects are masked from &#39;package:actuar&#39;: ## ## dinvexp, dinvgamma, pinvexp, pinvgamma, qinvexp, qinvgamma, ## rinvexp, rinvgamma library(actuar) theta &lt;- 100 X &lt;- seq(from = 0, to = 1000, by = 1) plot(x=X,y=dexp(X,rate=1/theta),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Exponential Distribution&quot;) Hide Inverse Exponential Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\theta&gt;0 \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{\\theta e^{-\\theta/x}}{x^2} \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; e^{-\\theta/x} \\\\ ~~F(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\theta^k\\Gamma(1-k) \\\\ ~~\\mathrm{E}[X^k] &amp; k&lt;1 \\\\ \\hline \\mathrm{E}[(X\\wedge x)^k] &amp; \\theta^kG(1-k;\\theta/x)+x^k (1 - e^{-\\theta/x}) \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dinvexp}(x=, scale=\\theta) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pinvexp}(p=, scale=\\theta) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qinvexp}(q=, scale=\\theta) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rinvexp}(n=, scale=\\theta) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph theta &lt;- 0.01 X &lt;- seq(from = 0, to = 1000, by = 1) plot(x=X,y=dinvexp(X, rate = 1/theta),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Inverse Exponential Distribution&quot;) Hide Single Parameter Pareto Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\theta~\\text{is known},~x&gt;\\theta, \\alpha &gt; 0 \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{\\alpha\\theta^\\alpha}{x^{\\alpha+1}} \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; 1-(\\theta/x)^\\alpha \\\\ ~~F(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\frac{\\alpha\\theta^k}{\\alpha-k} \\\\ ~~\\mathrm{E}[X^k] &amp; k &lt; \\alpha \\\\ \\hline \\mathrm{E}[(X\\wedge x)^k] &amp; \\frac{\\alpha\\theta^k}{\\alpha-k}-\\frac{k\\theta^{\\alpha}}{(\\alpha-k)x^{\\alpha-k}} \\\\ &amp; x \\geq\\theta \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dpareto1}(x=, shape=\\alpha,min=\\theta) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{ppareto1}(p=, shape=\\alpha,min=\\theta) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qpareto1}(q=, shape=\\alpha,min=\\theta) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rpareto1}(n=, shape=\\alpha,min=\\theta) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph alpha &lt;- 3 theta &lt;- 100 X &lt;- seq(from = 0, to = 1000, by = 1) plot(x=X,y=dpareto1(X,shape=alpha,min=theta),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Single Parameter Pareto Distribution&quot;) 18.2.2 Two Parameter Distributions Pareto Inv Pareto Loglogistic Paralogistic Gamma Inv Gamma Weibull Inv Weibull Uniform Normal Hide Pareto Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\theta&gt;0, \\alpha&gt;0 \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{\\alpha\\theta^\\alpha}{(x+\\theta)^{\\alpha+1}} \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; 1-\\Big(\\frac{\\theta}{x+\\theta}\\Big)^\\alpha \\\\ ~~F(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\frac{\\theta^k\\Gamma(k+1)\\Gamma(\\alpha-k)}{\\Gamma(\\alpha)} \\\\ ~~\\mathrm{E}[X^k] &amp; -1&lt;k&lt;\\alpha \\\\ \\hline \\small{\\text{Limited Expected Value:}}~\\alpha\\neq1 &amp; \\frac{\\theta}{\\alpha-1}\\Big[1-\\Big(\\frac{\\theta}{x+\\theta}\\Big)^{\\alpha-1}\\Big] \\\\ ~~\\mathrm{E}[X\\wedge x] &amp; \\\\ \\hline \\small{\\text{Limited Expected Value:}}~\\alpha=1 &amp; -\\theta \\ln \\left(\\frac{\\theta}{x+\\theta}\\right) \\\\ ~~\\mathrm{E}[X\\wedge x] &amp; \\\\ \\hline \\mathrm{E}[(X\\wedge x)^k] &amp; \\frac{\\theta^k\\Gamma(k+1)\\Gamma(\\alpha-k)}{\\Gamma(\\alpha)}\\beta(k+1,\\alpha-k;\\frac{x}{x+\\theta})+x^k(\\frac{\\theta}{x+\\theta})^\\alpha \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dpareto}(x=, shape=\\alpha, scale=\\theta) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{ppareto}(p=, shape=\\alpha,scale=\\theta) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qpareto}(q=, shape=\\alpha,scale=\\theta) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rpareto}(n=, shape=\\alpha,scale=\\theta) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph alpha &lt;- 3 theta &lt;- 200 X &lt;- seq(from = 0, to = 1000, by = 1) plot(x=X,y=actuar::dpareto(X,shape=alpha,scale=theta),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Pareto Distribution&quot;) Hide Inverse Pareto Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\theta&gt;0, \\tau&gt;0 \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{\\tau\\theta x^{\\tau-1}}{(x+\\theta)^\\tau-1} \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\Big(\\frac{x}{x+\\theta}\\Big)^\\tau \\\\ ~~F(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\frac{\\theta^k\\Gamma(\\tau+k)\\Gamma(1-k)}{\\Gamma(\\tau)} \\\\ ~~\\mathrm{E}[X^k] &amp; -\\tau&lt;k&lt;1 \\\\ \\hline \\mathrm{E}[(X\\wedge x)^k] &amp; \\theta^k\\tau\\int^{x/(x+\\theta)}_0~y^{\\tau+k-1}(1-y)^{-k}dy+x^k[1-\\Big(\\frac{x}{x+\\theta}\\Big)^\\tau] \\\\ &amp; k&gt;-\\tau \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dinvpareto}(x=, shape=\\tau, scale=\\theta) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pinvpareto}(p=, shape=\\tau,scale=\\theta) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qinvpareto}(q=, shape=\\tau,scale=\\theta) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rinvpareto}(n=, shape=\\tau,scale=\\theta) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph tau &lt;- 5 theta &lt;- 100 X &lt;- seq(from=0,to=3000,by=1) plot(x=X,y=dinvpareto(X,shape=tau,scale=theta),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Inverse Pareto Distribution&quot;) Hide Loglogistic Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\theta&gt;0, \\gamma &gt; 0, u=\\frac{(x/\\theta)^\\gamma}{1+(x/\\theta)^\\gamma} \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{\\gamma(x/\\theta)^\\gamma}{x[1+(x/\\theta)^\\gamma]^2} \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; u \\\\ ~~F(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\theta^k\\Gamma(1+(k/\\gamma))\\Gamma(1-(k/\\gamma)) \\\\ ~~\\mathrm{E}[X^k] &amp; -\\gamma&lt;k&lt;\\gamma \\\\ \\hline \\mathrm{E}[(X\\wedge x)^k] &amp; \\theta^k\\Gamma(1+(k/\\gamma))\\Gamma(1-(k/\\gamma))\\beta(1+(k/\\gamma),1-(k/\\gamma);u)+x^k(1-u) \\\\ &amp; k&gt;-\\gamma \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph dloglogistic &lt;- function(x, gamma, theta){ p=gamma*(x/theta)^gamma/(x*(1+(x/theta)^gamma)^2) return(p) } gamma &lt;- 2 theta &lt;- 100 X &lt;- seq(from = 0, to = 1000, by = 1) plot(x=X,y=dloglogistic(X,gamma=gamma,theta=theta),type=&quot;l&quot;,col=&quot;red&quot;) Hide Paralogistic Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\theta&gt;0, \\alpha&gt;0, u=\\frac{1}{1+(x/\\theta)^\\alpha} \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{\\alpha^2(x/\\theta)^\\alpha}{x[1+(x/\\theta)^\\alpha]^{\\alpha+1}} \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; 1-u^\\alpha \\\\ ~~F(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\frac{\\theta^k\\Gamma(1+(k/\\alpha))\\Gamma(\\alpha-(k/\\alpha))}{\\Gamma(\\alpha)} \\\\ ~~\\mathrm{E}[X^k] &amp; -\\alpha&lt;k&lt;\\alpha^2 \\\\ \\hline \\mathrm{E}[(X\\wedge x)^k] &amp; \\frac{\\theta^k\\Gamma(1+(k/\\alpha))\\Gamma(\\alpha-(k/\\alpha))}{\\Gamma(\\alpha)}\\beta(1+(k/\\alpha),\\alpha-(k/\\alpha);1-u)+x^ku^\\alpha \\\\ &amp; k&gt;-\\alpha \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dparalogis}(x=, shape=\\alpha, scale=\\theta) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pparalogis}(p=, shape=\\alpha,scale=\\theta) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qparalogis}(q=, shape=\\alpha,scale=\\theta) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rparalogis}(n=, shape=\\alpha,scale=\\theta) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph alpha &lt;- 2 theta &lt;- 100 X &lt;- seq(from = 0, to = 1000, by = 1) plot(x=X,y=dparalogis(X,shape=alpha,scale=theta),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Paralogistic Distribution&quot;) Hide Gamma Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\theta&gt;0,~\\alpha&gt;0 \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{1}{\\theta^{\\alpha}\\Gamma(\\alpha)}x^{\\alpha-1}e^{-x/\\theta} \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\Gamma(\\alpha;\\frac{x}{\\theta}) \\\\ ~~F(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\theta^k\\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha)} \\\\ ~~\\mathrm{E}[X^k] &amp; k&gt;-\\alpha \\\\ \\hline &amp; \\frac{\\theta^k\\Gamma(k+\\alpha)}{\\Gamma(\\alpha)}\\Gamma(k+\\alpha; x/\\theta)+x^k[1-\\Gamma(\\alpha; x/\\theta)] \\\\ ~~\\mathrm{E}[X\\wedge x]^k &amp; k &gt; -\\alpha \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\small{\\text{Density function}} &amp; \\text{dgamma}(x=, shape=\\alpha, scale=\\theta) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pgamma}(p=, shape=\\alpha,scale=\\theta) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qgamma}(q=, shape=\\alpha,scale=\\theta) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rgamma}(n=, shape=\\alpha,scale=\\theta) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph alpha &lt;- 2 theta &lt;- 50 X &lt;- seq(from = 0, to = 1000, by = 1) plot(x=X,y=dgamma(X,shape=alpha,scale=theta),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Gamma Distribution&quot;) Hide Inverse Gamma Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{(\\theta/x)^\\alpha e^{-\\theta/x}}{x\\Gamma(\\alpha)} \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; 1-\\Gamma(\\alpha;\\theta/x) \\\\ ~~F(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\frac{\\theta^k\\Gamma(\\alpha-k)}{\\Gamma(\\alpha)} \\\\ ~~\\mathrm{E}[X^k] &amp; k&lt;\\alpha \\\\ \\hline \\mathrm{E}[(X\\wedge x)^k] &amp; \\frac{\\theta^k\\Gamma(\\alpha-k)}{\\Gamma(\\alpha)}[1-\\Gamma(\\alpha-k;\\theta/x)]+x^k\\Gamma(\\alpha;\\theta/x) \\\\ &amp; \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dinvgamma}(x=, shape=\\alpha, scale=\\theta) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pinvgamma}(p=, shape=\\alpha,scale=\\theta) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qinvgamma}(q=, shape=\\alpha,scale=\\theta) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rinvgamma}(n=, shape=\\alpha,scale=\\theta) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph alpha &lt;- 3 theta &lt;- 100 X &lt;- seq(from=0,to=400,by=1) plot(x=X,y=dinvgamma(X,shape=alpha,scale=theta),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Inverse Gamma Distribution&quot;) Hide Weibull Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\theta&gt;0,\\alpha&gt;0 \\\\ \\hline\\ \\small{\\text{Probability density}} &amp; \\frac{\\alpha \\Big(\\frac{x}{\\theta}\\Big)^\\alpha \\exp\\Big(-\\Big(\\frac{x}{\\theta}\\Big)^\\alpha\\Big)}{x} \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; 1-\\exp\\Big(-\\Big(\\frac{x}{\\theta}\\Big)^\\alpha\\Big) \\\\ ~~F(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\theta^k \\Gamma(1 + \\frac{k}{\\alpha}) \\\\ ~~\\mathrm{E}[X^k] &amp; k&gt;-\\alpha \\\\ \\hline \\mathrm{E}[(X\\wedge x)^k] &amp; \\theta^k\\Gamma(1+\\frac{k}{\\alpha})\\Gamma\\Big[1+\\frac{k}{\\alpha};\\Big(\\frac{x}{\\theta}\\Big)^\\alpha\\Big]+x^k\\exp\\Big(-\\Big(\\frac{x}{\\theta}\\Big)^\\alpha\\Big) \\\\ &amp; k&gt;-\\alpha \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dweibull}(x=, shape=\\alpha, scale=\\theta) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pweibull}(p=, shape=\\alpha,scale=\\theta) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qweibull}(q=, shape=\\alpha,scale=\\theta) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rweibull}(n=, shape=\\alpha,scale=\\theta) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph alpha &lt;- 2 theta &lt;- 100 X &lt;- seq(from = 0, to = 1000, by = 1) plot(x=X,y=dweibull(X,shape=alpha, scale=theta),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Weibull Distribution&quot;) Hide Inverse Weibull Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\theta&gt;0,\\tau&gt;0 \\\\ \\hline\\ \\small{\\text{Probability density}} &amp; \\frac{\\tau(\\theta/x)^\\tau \\exp\\Big(-\\Big(\\frac{\\theta}{x}\\Big)^\\tau\\Big)}{x} \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\exp\\Big(-\\Big(\\frac{\\theta}{x}\\Big)^\\tau\\Big) \\\\ ~~F(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\theta^k\\Gamma(1-(k/\\tau)) \\\\ ~~\\mathrm{E}[X^k] &amp; k&lt;\\tau \\\\ \\hline \\mathrm{E}[(X\\wedge x)^k] &amp; \\theta^k\\Gamma(1-(k/\\tau))[1-\\Gamma(1-(k/\\tau);(\\theta/x)^\\tau)]+x^k[1-e^{-(\\theta/x)^\\tau}] \\\\ &amp; \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dinvweibull}(x=, shape=\\tau, scale=\\theta) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pinvweibull}(p=, shape=\\tau,scale=\\theta) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qinvweibull}(q=, shape=\\tau,scale=\\theta) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rinvweibull}(n=, shape=\\tau,scale=\\theta) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph tau &lt;- 5 theta &lt;- 100 X &lt;- seq(from = 0, to = 1000, by = 1) plot(x=X,y=dinvweibull(X,shape=tau,scale=theta),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Inverse Weibull Distribution&quot;) Hide Uniform Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; -\\infty&lt;\\alpha&lt;\\beta&lt;\\infty \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{1}{\\beta-\\alpha} \\\\ \\text{f(x)} &amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\frac{x-\\alpha}{\\beta-\\alpha} \\\\ ~~F(x) &amp; \\\\ \\hline \\text{Mean} &amp; \\frac{\\beta+\\alpha}{2} \\\\ \\text{E[X]} &amp; \\\\ \\hline \\text{Variance} &amp; \\frac{(\\beta-\\alpha)^2}{12} \\\\ E[(X-\\mu)^2] &amp; \\\\ \\hline \\mathrm{E}[(X-\\mu)^k] &amp; \\mu_k=0~~~\\text{for odd }\\textit{k} \\\\ &amp; \\mu_k=\\frac{(\\beta-\\alpha)^k}{2^k (k+1)}~~~\\text{for even }\\textit{k} \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dunif}(x=, min=a, max=b) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{punif}(p=, min=a, max=b) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qunif}(q=, min=a, max=b) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{runif}(n=, min=a, max=b) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph alpha &lt;- 50 beta &lt;- 100 X &lt;- seq(alpha,beta,1) plot(x=X,y=dunif(X,alpha,beta),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Continuous Uniform Distribution&quot;) Hide Normal Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; -\\infty&lt;\\mu&lt;\\infty,~\\sigma&gt;0 \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) \\\\ \\text{f(x)} &amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right) \\\\ ~~F(x) &amp; \\\\ \\hline \\text{Mean} &amp; \\mu \\\\ \\text{E[X]} &amp; \\\\ \\hline \\text{Variance} &amp; \\sigma^2 \\\\ E[(X-\\mu)^2] &amp; \\\\ \\hline \\mathrm{E}[(x-\\mu)^k] &amp; \\mu_k=0~~~\\text{for even k} \\\\ &amp; \\mu_k=\\frac{k!\\sigma^2}{(\\frac{k}{2})! 2^{k/2}}~~~\\text{for odd k} \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dnorm}(x=, mean=\\mu, sd=\\sigma) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pnorm}(p=, mean=\\mu, sd=\\sigma) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qnorm}(q=, mean=\\mu, sd=\\sigma) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rnorm}(n=, mean=\\mu, sd=\\sigma) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph mu &lt;- 100 sigma &lt;- 10 X &lt;- seq(from=0,to=200,by=1) plot(x=X,y=dnorm(X,mean=mu,sd=sigma),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Normal Distribution&quot;) Hide Cauchy Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; -\\infty &lt;\\alpha &lt;\\infty, \\beta&gt;0 \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{1}{\\pi\\beta}[1+\\left( \\frac{x-\\alpha}{\\beta}\\right)^2]^{-1} \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dcauchy}(x=, location=\\alpha, scale=\\beta) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pcauchy}(p=, location=\\alpha, scale=\\beta) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qcauchy}(q=, location=\\alpha, scale=\\beta) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rcauchy}(n=, location=\\alpha, scale=\\beta) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph alpha &lt;- 50 beta &lt;- 100 X &lt;- seq(from = 0, to = 1000, by = 1) plot(x=X,y=dcauchy(X,location=alpha,scale=beta), type=&quot;l&quot;,ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Cauchy Distribution&quot;) 18.2.3 Three Parameter Distributions Generalized Pareto Burr Inv Burr Hide Generalized Pareto Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\theta&gt;0, \\alpha&gt;0, \\tau&gt;0, u=\\frac{x}{x+\\theta} \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{\\Gamma(\\alpha+\\tau)}{\\Gamma(\\alpha)\\Gamma(\\tau)}\\frac{\\theta^\\alpha x^{\\tau-1}}{(x+\\theta)^{\\alpha+\\tau}} \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\beta(\\tau,\\alpha;u) \\\\ ~~F(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\frac{\\theta^k\\Gamma(\\tau+1)\\Gamma(\\alpha-k)}{\\Gamma(\\alpha)\\Gamma(\\tau)} \\\\ ~~~~\\mathrm{E}[X^k] &amp; -\\tau&lt;k&lt;\\alpha \\\\ \\hline \\mathrm{E}[(X\\wedge x)^k] &amp; \\frac{\\theta^k\\Gamma(\\tau+k)\\Gamma(\\alpha-k)}{\\Gamma(\\alpha)\\Gamma(\\tau)}\\beta(\\tau+k,\\alpha-k;u)+x^k[1-\\beta(\\tau,\\alpha;u)] \\\\ &amp; k&gt;-\\tau \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dgenpareto}(x=, shape1=\\alpha, shape2=\\tau, scale=\\theta) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pgenpareto}(q=, shape1=\\alpha, shape2=\\tau, scale=\\theta) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qgenpareto}(p=, shape1=\\alpha, shape2=\\tau, scale=\\theta) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rgenpareto}(r=, shape1=\\alpha, shape2=\\tau, scale=\\theta) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph alpha &lt;- 3 tau &lt;- 5 theta &lt;- 100 X &lt;- seq(from = 0, to = 1000, by = 1) plot(x=X,y=dgenpareto(X,shape1=alpha,shape2=tau,scale=theta),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Generalized Pareto Distribution&quot;) Hide Burr Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\theta&gt;0, \\alpha&gt;0, \\gamma&gt;0, u=\\frac{1}{1+(x/\\theta)^\\gamma} \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{\\alpha\\gamma(x/\\theta)^\\gamma}{x[1+(x/\\theta)^\\gamma]^{\\alpha+1}} \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; 1-u^\\alpha \\\\ ~~F(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\frac{\\theta^k\\Gamma(1+(k/\\gamma))\\Gamma(\\alpha-(k/\\gamma))}{\\Gamma(\\alpha)} \\\\ ~~~~\\mathrm{E}[X^k] &amp; -\\gamma&lt;k&lt;\\alpha\\gamma \\\\ \\hline \\mathrm{E}[(X\\wedge x)^k] &amp; \\frac{\\theta^k\\Gamma(1+(k/\\gamma))\\Gamma(\\alpha-(k/\\gamma))}{\\Gamma(\\alpha)}\\beta(1+(k/\\gamma),\\alpha-(k/\\gamma);1-u)+x^ku^\\alpha \\\\ &amp; k&gt;-\\gamma \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dburr}(x=, shape1=\\alpha, shape2=\\gamma, scale=\\theta) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pburr}(p=, shape1=\\alpha, shape2=\\gamma, scale=\\theta) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qburr}(q=, shape1=\\alpha, shape2=\\gamma, scale=\\theta) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rburr}(n=, shape1=\\alpha, shape2=\\gamma, scale=\\theta) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph alpha &lt;- 2 gamma &lt;- 3 theta &lt;- 100 X &lt;- seq(from = 0, to = 1000, by = 1) plot(x=X,y=dburr(X,shape1=alpha,shape2=gamma,scale=theta),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Burr Distribution&quot;) Hide Inverse Burr Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\theta&gt;0, \\tau&gt;0, \\gamma&gt;0, u=\\frac{(x/\\theta)^\\gamma}{1+(x/\\theta)^\\gamma} \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{\\tau\\gamma(x/\\theta)^{\\tau \\gamma}}{x[1+(x/\\theta)^\\gamma]^{\\tau+1}} \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; u^\\tau \\\\ ~~F(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\frac{\\theta^k\\Gamma(\\tau+(k/\\gamma))\\Gamma(1-(k/\\gamma))}{\\Gamma(\\tau)} \\\\ ~~~~\\mathrm{E}[X^k] &amp; -\\tau\\gamma&lt;k&lt;\\gamma \\\\ \\hline \\mathrm{E}[(X\\wedge x)^k] &amp; \\frac{\\theta^k\\Gamma(\\tau+(k/\\gamma))\\Gamma(1-(k/\\gamma))}{\\Gamma(\\tau)}\\beta(\\tau+(k/\\gamma),1-(k/\\gamma);u)+x^k[1-u^\\tau] \\\\ &amp; k&gt;-\\tau\\gamma \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dinvburr}(x=, shape1=\\tau, shape2=\\gamma, scale=\\theta) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pinvburr}(p=, shape1=\\tau, shape2=\\gamma, scale=\\theta) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qinvburr}(q=, shape1=\\tau, shape2=\\gamma, scale=\\theta) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rinvburr}(n=, shape1=\\tau, shape2=\\gamma, scale=\\theta) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph tau &lt;- 2 gamma &lt;- 3 theta &lt;- 100 X &lt;- seq(from = 0, to = 1000, by = 1) plot(x=X,y=dinvburr(X,shape1=tau,shape2=gamma,scale=theta),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Inverse Burr Distribution&quot;) 18.2.4 Four Parameter Distribution GB2 Hide Generalized Beta of the Second Kind (GB2) Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\theta&gt;0, \\alpha_1&gt;0, \\alpha_2&gt;0, \\sigma&gt;0 \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{(x/\\theta)^{\\alpha_2/\\sigma}}{x \\sigma~\\mathrm{B}\\left( \\alpha_1,\\alpha_2\\right)\\left\\lbrack 1 + \\left( x/\\theta \\right)^{1/\\sigma} \\right\\rbrack^{\\alpha_1 + \\alpha_2}} \\\\ ~~ \\small{\\text{function }} f(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\frac{\\theta^{k}~\\mathrm{B}\\left( \\alpha_1 +k \\sigma,\\alpha_2 - k \\sigma \\right)}{\\mathrm{B}\\left( \\alpha_1,\\alpha_2 \\right)} \\\\ ~~~~\\mathrm{E}[X^k] &amp; \\textit{k}&gt;0 \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands Please see the R Codes for Loss Data Analytics site for information about this distribution. 18.2.5 Other Distributions Lognormal Inv Gaussian Hide Lognormal Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; -\\infty &lt;\\mu &lt;\\infty, \\sigma&gt;0 \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{1}{x\\sqrt{2\\pi}\\sigma} \\exp\\left( -\\frac{(\\ln x-\\mu)^2}{2\\sigma^2}\\right) \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\Phi\\left(\\frac{\\ln (x)-\\mu}{\\sigma}\\right) \\\\ ~~F(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\exp(k\\mu+\\frac{k^2\\sigma^2}{2}) \\\\ ~~\\mathrm{E}[X^k] &amp; \\\\ \\hline \\small{\\text{Limited Expected Value}} &amp; \\exp\\Big(k\\mu+\\frac{k^2\\sigma^2}{2}\\Big)\\Phi\\Big(\\frac{\\ln (x)-\\mu-k\\sigma^2}{\\sigma}\\Big)+x^k\\Big[1-\\Phi\\Big(\\frac{\\ln (x)-\\mu}{\\sigma}\\Big)\\Big] \\\\ ~~\\mathrm{E}[X\\wedge x] &amp; \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph dlognorm &lt;- function(x,mu,sigma){ p=(1/(x*sigma*sqrt(2*pi)))*exp(-((log(x)-mu)/sigma)^2) return(p) } mu &lt;- 20 sigma &lt;- 12 X &lt;- seq(from = 0, to = 1000, by = 1) plot(x=X,y=dlognorm(X,mu=mu,sigma=sigma),type=&quot;l&quot;,col=&quot;red&quot;) Hide Inverse Gaussian Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\theta&gt;0, \\mu&gt;0, z=\\frac{x-\\mu}{\\mu}~,~y=\\frac{x+\\mu}{\\mu} \\\\ \\hline \\small{\\text{Probability density}} &amp; \\Big(\\frac{\\theta}{2\\pi x^3}\\Big)^{1/2}\\exp\\Big(\\frac{-\\theta z^2}{2x}\\Big) \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\Phi\\Big[z\\Big(\\frac{\\theta}{x}\\Big)^{1/2}\\Big]+\\exp\\Big(\\frac{2\\theta}{\\mu}\\Big)\\Phi\\Big[-y\\Big(\\frac{\\theta}{x}\\Big)^{1/2}\\Big] \\\\ ~~F(x) &amp; \\\\ \\hline \\text{Mean} &amp; \\mu \\\\ \\mathrm{E}[X] &amp; \\\\ \\hline \\mathrm{Var[X]} &amp; \\frac{\\mu^3}{\\theta}\\\\ \\hline \\mathrm{E}[(X\\wedge x)^k] &amp; x-\\mu x\\Phi\\Big[z\\Big(\\frac{\\theta}{x}\\Big)^{1/2}\\Big]-(\\mu y)\\exp\\Big(\\frac{2\\theta}{\\mu}\\Big)\\Phi\\Big[-y\\Big(\\frac{\\theta}{x}\\Big)^{1/2}\\Big] \\\\ &amp; \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dinvgauss}(x=, mean=\\mu,dispersion=\\theta) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pinvgauss}(p=, mean=\\mu,dispersion=\\theta) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qinvgauss}(q=, mean=\\mu,dispersion=\\theta) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rinvgauss}(n=, mean=\\mu,dispersion=\\theta) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph mu &lt;- 100 theta &lt;- 1000 X &lt;- seq(from=0,to=100,by=1) plot(x=X,y=dinvgauss(X,mean=mu,dispersion=theta),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Inverse Gaussian Distribution&quot;) 18.2.6 Distributions with Finite Support Beta Generalized Beta Hide Beta Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\theta&gt;0, ~a&gt;0,~b&gt;0, u=\\frac{x}{\\theta},~0&lt;x&lt;\\theta \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} u^a(1-u)^{b-1}\\frac{1}{x} \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\beta(a,b;u) \\\\ ~~F(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\frac{\\theta^k \\Gamma(a+b)\\Gamma(a+k)}{\\Gamma(a)\\Gamma(a+b+k)} \\\\ ~~\\mathrm{E}[X^k] &amp; k&gt;-a \\\\ \\hline &amp; \\frac{\\theta^k a(a+1)\\cdots(a+k-1)}{(a+b)(a+b+1)\\cdots(a+b+k-1)}\\beta(a+k,b;u)+x^k[1-\\beta(a,b;u)] \\\\ ~~\\mathrm{E}[X\\wedge x]^k &amp; \\\\ \\hline \\end{array} \\end{matrix} \\] R Commands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dbeta}(x=, shape1=a,shape2=b,ncp=\\theta) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pbeta}(p=, shape1=a,shape2=b,ncp=\\theta) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qbeta}(q=, shape1=a,shape2=b,ncp=\\theta) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rbeta}(n=, shape1=a,shape2=b,ncp=\\theta) \\\\ \\hline \\end{array} \\end{matrix} \\] a &lt;- 2 b &lt;- 4 theta &lt;- 1 X &lt;- seq(from=0,to=1,by=.0001) plot(x=X,y=dbeta(X,shape1=a,shape2=b,ncp=theta),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Beta Distribution&quot;) Hide Generalized Beta Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Name} &amp; \\text{Function} \\\\ \\hline \\small{\\text{Parameter assumptions}} &amp; \\theta&gt;0, a&gt;0, b&gt;0, \\tau&gt;0, 0&lt;x&lt;\\theta~,~u=(x/\\theta)^\\tau \\\\ \\hline \\small{\\text{Probability density}} &amp; \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}u^\\alpha(1-u)^{b-1}\\frac{\\tau}{x} \\\\ ~~ \\small{\\text{function }} f(x)&amp; \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\beta(a,b;u) \\\\ ~~F(x) &amp; \\\\ \\hline \\textit{k}^{th}~\\small{\\text{raw moment}} &amp; \\frac{\\theta^k\\Gamma(a+b)\\Gamma(a+(k/\\tau))}{\\Gamma(a)\\Gamma(a+b+(k/\\tau))} \\\\ ~~\\mathrm{E}[X^k] &amp; k&gt;-\\alpha\\tau \\\\ \\hline \\mathrm{E}[(X\\wedge x)^k] &amp; \\frac{\\theta^k\\Gamma(a+b)\\Gamma(a+(k/\\tau))}{\\Gamma(a)\\Gamma(a+b+(k/\\tau))}\\beta(a+(k/\\tau),b;u)+x^k[1-\\beta(a,b;u)] \\\\ \\hline \\end{array} \\end{matrix} \\] R Commmands \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Function Name} &amp; \\text{R Command} \\\\ \\hline \\small{\\text{Density function}} &amp; \\text{dgenbeta}(x=, shape1=a,shape2=b,shape3=\\tau,scale=\\theta) \\\\ \\hline \\small{\\text{Distribution function}} &amp; \\text{pgenbeta}(p=, shape1=a,shape2=b,shape3=\\tau,scale=\\theta) \\\\ \\hline \\small{\\text{Quantile function}} &amp; \\text{qgenbeta}(q=, shape1=a,shape2=b,shape3=\\tau,scale=\\theta) \\\\ \\hline \\small{\\text{Random sampling function}} &amp; \\text{rgenbeta}(n=, shape1=a,shape2=b,shape3=\\tau,scale=\\theta) \\\\ \\hline \\end{array} \\end{matrix} \\] Illustrative Graph a &lt;- 3 b &lt;- 5 tau &lt;- 2 theta &lt;- 1000 X &lt;- seq(from = 0, to = 1000, by = 1) plot(x=X,y=dgenbeta(X,shape1=a,shape2=b,shape3=tau,scale=theta),type=&quot;l&quot;, ylab=&quot;Probability density&quot;,col=&quot;red&quot;,main=&quot;Generalized Beta Distribution&quot;) 18.3 Limited Expected Values Functions Graph Hide Functions 18.3.0.1 Functions Limited Expected Value Functions \\[ \\begin{matrix} \\begin{array}{l|c} \\hline \\text{Distribuion} &amp; \\text{Function} \\\\ \\hline \\text{GB2} &amp; \\frac{\\theta\\Gamma(\\tau+1)\\Gamma(\\alpha-1)}{\\Gamma(\\alpha)\\Gamma(\\tau)}\\beta(\\tau+1,\\alpha-1;\\frac{x}{x+\\beta})+x[1-\\beta(\\tau,\\alpha;\\frac{x}{x+\\beta})] \\\\ \\hline \\text{Burr} &amp; \\frac{\\theta\\Gamma(1+\\frac{1}{\\gamma})\\Gamma(\\alpha-\\frac{1}{\\gamma})}{\\Gamma(\\alpha)}\\beta(1+\\frac{1}{\\gamma},\\alpha-\\frac{1}{\\gamma};1-\\frac{1}{1+(x/\\theta)^\\gamma})+x\\Big(\\frac{1}{1+(x/\\theta)^\\gamma}\\Big)^\\alpha \\\\ \\hline \\text{Inverse Burr} &amp; \\frac{\\theta\\Gamma(\\tau+(1/\\gamma))\\Gamma(1-(1/\\gamma))}{\\Gamma(\\tau)}\\beta(\\tau+\\frac{1}{\\gamma},1-\\frac{1}{\\gamma};\\frac{(x/\\theta)^\\gamma}{1+(x/\\theta)^\\gamma})+x[1-\\Big(\\frac{(x/\\theta)^\\gamma}{1+(x/\\theta)^\\gamma}\\Big)^\\tau] \\\\ \\hline \\text{Pareto} &amp; \\\\ \\alpha=1 &amp; -\\theta \\ln \\Big(\\frac{\\theta}{x+\\theta}\\Big) \\\\ \\alpha\\neq1 &amp; \\frac{\\theta}{\\alpha-1}[1-\\Big(\\frac{\\theta}{x+\\theta}\\Big)^{\\alpha-1}] \\\\ \\hline \\text{Inverse Pareto} &amp; \\theta\\tau\\int^{x/(x+\\theta)}_0~y^\\tau(1-y)^{-1}dy+x[1-\\Big(\\frac{x}{x+\\theta}\\Big)^\\tau] \\\\ \\hline \\text{Loglogistic} &amp; \\theta\\Gamma(1+\\frac{1}{\\gamma})\\Gamma(1-\\frac{1}{\\gamma})\\beta(1+\\frac{1}{\\gamma},1-\\frac{1}{\\gamma};\\frac{(x/\\theta)^\\gamma}{1+(x/\\theta)^\\gamma})+x(1-\\frac{(x/\\theta)^\\gamma}{1+(x/\\theta)^\\gamma}) \\\\ \\hline \\text{Paralogistic} &amp; \\frac{\\theta\\Gamma(1+\\frac{1}{\\alpha})\\Gamma(\\alpha-\\frac{1}{\\alpha})}{\\Gamma(\\alpha)}\\beta(1+\\frac{1}{\\alpha},\\alpha-\\frac{1}{\\alpha};1-\\frac{1}{1+(x/\\theta)^\\alpha})+x\\Big(\\frac{1}{1+(x/\\theta)^\\alpha}\\Big)^\\alpha \\\\ \\hline \\text{Inverse Paralogistic} &amp; \\frac{\\theta\\Gamma(\\tau+\\frac{1}{\\tau})\\Gamma(1-\\frac{1}{\\tau})}{\\Gamma(\\tau)}\\beta(\\tau+\\frac{1}{\\tau},1-\\frac{1}{\\tau};\\frac{(x/\\theta)^\\tau}{1+(x/\\theta)^\\tau})+x[1-\\Big(\\frac{(x/\\theta)^\\tau}{1+(x/\\theta)^\\tau}\\Big)^\\tau] \\\\ \\hline \\text{Gamma} &amp; \\frac{\\theta\\Gamma(\\alpha+1)}{\\Gamma(\\alpha)}\\Gamma(\\alpha+1;\\frac{x}{\\theta})+x[1-\\Gamma(\\alpha;\\frac{x}{\\theta})] \\\\ \\hline \\text{Inverse Gamma} &amp; \\frac{\\theta\\Gamma(\\alpha-1)}{\\Gamma(\\alpha)}[1-\\Gamma(\\alpha-1;\\frac{\\theta}{x})]+x\\Gamma(\\alpha;\\frac{\\theta}{x}) \\\\ \\hline \\text{Weibull} &amp; \\theta\\Gamma(1+\\frac{1}{\\alpha})\\Gamma(1+\\frac{1}{\\alpha};\\Big(\\frac{x}{\\theta}\\Big)^\\alpha)+x*\\exp(-(x/\\theta)^\\alpha) \\\\ \\hline \\text{Inverse Weibull} &amp; \\theta\\Gamma(1-\\frac{1}{\\alpha})[1-\\Gamma(1-\\frac{1}{\\alpha};\\Big(\\frac{\\theta}{x}\\Big)^\\alpha)]+x[1-\\exp(-(\\theta/x)^\\alpha)] \\\\ \\hline \\text{Exponential} &amp; \\theta(1-\\exp(-(x/\\theta))) \\\\ \\hline \\text{Inverse Exponential} &amp; \\theta G(0;\\frac{\\theta}{x})+x(1-\\exp(-(\\theta/x))) \\\\ \\hline \\text{Lognormal} &amp; \\exp(\\mu+\\sigma^2/2)\\Phi\\Big(\\frac{\\ln (x)-\\mu-\\sigma^2}{\\sigma}\\Big)+x[1-\\Phi\\Big(\\frac{\\ln (x)-\\mu}{\\sigma}\\Big)] \\\\ \\hline \\text{Inverse Gaussian} &amp; x-\\mu\\Big(\\frac{x-\\mu}{\\mu}\\Big)\\Phi\\Big[\\Big(\\frac{x-\\mu}{\\mu}\\Big)\\Big(\\frac{\\theta}{x}\\Big)^{1/2}\\Big]-\\mu\\Big(\\frac{x+\\mu}{\\mu}\\Big)\\exp\\Big(\\frac{2\\theta}{\\mu}\\Big)\\Phi\\Big[-\\Big(\\frac{x+\\mu}{\\mu}\\Big)\\Big(\\frac{\\theta}{x}\\Big)^{1/2}\\Big] \\\\ \\hline \\text{Single-Parameter Pareto} &amp; \\frac{\\alpha\\theta}{\\alpha-1}-\\frac{\\theta^\\alpha}{(\\alpha-1)x^{\\alpha-1}} \\\\ \\hline \\text{Generalized Beta} &amp; \\frac{\\theta\\Gamma(a+b)\\Gamma(a+\\frac{1}{\\tau})}{\\Gamma(a)\\Gamma(a+b+\\frac{1}{\\tau})}\\beta(a+\\frac{1}{\\tau},b;\\Big(\\frac{x}{\\theta}\\Big)^\\tau)+x\\Big[1-\\beta(a,b;\\Big(\\frac{x}{\\theta}\\Big)^\\tau)\\Big] \\\\ \\hline \\text{Beta} &amp; \\frac{\\theta a}{(a+b)}\\beta(a+1,b;\\frac{x}{\\theta})+x[1-\\beta(a,b;\\frac{x}{\\theta})] \\\\ \\hline \\end{array} \\end{matrix} \\] Hide Illustrative Graph 18.3.0.2 Illustrative Graph Comparison of Limited Expected Values for Selected Distributions \\[ \\begin{matrix} \\begin{array}{l|c|c|c|c|c|c} \\hline \\text{Distribution} &amp; \\text{Parameters} &amp; \\mathrm{E}[X] &amp; E[X\\wedge100] &amp; E[X\\wedge250] &amp; E[X\\wedge500] &amp;E[X\\wedge1000] \\\\ \\hline \\text{Pareto} &amp; \\alpha = 3, \\theta = 200 &amp; 100 &amp; 55.55 &amp;80.25 &amp; 91.84 &amp; 97.22 \\\\ \\hline \\text{Exponential} &amp; \\theta = 100 &amp; 100 &amp; 63.21 &amp; 91.79 &amp; 99.33 &amp; 99.99 \\\\ \\hline \\text{Gamma} &amp; \\alpha = 2, \\theta = 50 &amp; 100 &amp; 72.93 &amp; 97.64 &amp; 99.97 &amp; 100 \\\\ \\hline \\text{Weibull} &amp; \\tau=2, \\theta=\\frac{200}{\\sqrt[]{\\pi}} &amp; 100 &amp; 78.99 &amp; 99.82 &amp; 100 &amp; 100 \\\\ \\hline \\text{GB2} &amp; \\alpha = 3,\\tau=2,\\theta = 100 &amp; 100 &amp; 62.50 &amp; 86.00 &amp; 94.91 &amp; 98.42 \\\\ \\hline \\end{array} \\end{matrix} \\] "],["appendix-e-conventions-for-notation.html", "Bab 19 Appendix E: Conventions for Notation", " Bab 19 Appendix E: Conventions for Notation "],["section.html", "", " "],["bibliography.html", "Bibliography", " Bibliography De Jong, P., &amp; Heller, G. Z. (2008). Generalized linear models for insurance data. Cambridge University Press, Cambridge. Denuit, M., Maréchal, X., Pitrebois, S., &amp; Walhin, J.-F. (2007). Actuarial modelling of claim counts: Risk classification, credibility and bonus-malus systems. John Wiley &amp; Sons, Chichester. Dionne, G., &amp; Vanasse, C. (1989). A generalization of automobile insurance rating models: The negative binomial distribution with a regression component. ASTIN Bulletin, 19(2), 199–212. Lemaire, J. (1998). Bonus-malus systems: The european and asian approach to merit rating. North American Actuarial Journal, 2(1), 26–38. McCullagh, P., &amp; Nelder, J. A. (1989). Generalized linear models, second edition. Chapman &amp; Hall, London. Norberg, R. (1976). A credibility theory for automobile bonus system. Scandinavian Actuarial Journal, 2, 92–107. Pitrebois, S., Denuit, M., &amp; Walhin, J.-F. (2003). Setting a bonus-malus scale in the presence of other rating factors: Taylor’s work revisited. ASTIN Bulletin, 33(2), 419–436. Tan, C. I. (2016). Optimal design of a bonus-malus system: Linear relativities revisited. Annals of Actuarial Science, 10(1), 52–64. Tan, C. I., Li, J., Li, J. S.-H., &amp; Balasooriya, U. (2015). Optimal relativities and transition rules of a bonus-malus system. Insurance: Mathematics and Economics, 61, 255–263. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
