<<<<<<< HEAD
[["simulation-and-resampling.html", "Bab 6 Simulation and Resampling 6.1 Dasar-Dasar Simulasi 6.2 Bootstrap dan Resampling 6.3 Cross Validation 6.4 Importance Sampling 6.5 6.5.1 Metropolis Hastings 6.6 6.5.2 Gibbs Sampler", " Bab 6 Simulation and Resampling Bagian 6.1 memperkenalkan simulasi, alat komputasi luar biasa yang sangat berguna dalam pengaturan multivariat yang kompleks. Bagian 6.2 memperkenalkan resampling dalam konteks bootstrap untuk menentukan ketepatan estimator. Resampling merupakan proses simulasi untuk menggambar dari distribusi empiris. 6.1 Dasar-Dasar Simulasi Menghasilkan sekitar realisasi independen yang terdistribusi secara merata Ubah realisasi yang terdistribusi secara seragam menjadi pengamatan dari distribusi probabilitas yang menarik Hitung jumlah bunga dan tentukan ketepatan jumlah yang dihitung 6.1.1 Menghasilkan Pengamatan Seragam Independen Generator Kongruensi Linier. Linear Congruential Generators (LCG) adalah sebuah metode yang membangkitkan bilangan acak yang banyak dipergunakan dalam program komputer. Pada metode ini, dilakukan perulangan pada periode waktu tertentu atau setelah sekian kali pembangkitan.Untuk menghasilkan urutan angka acak, mulailah dengan \\(B_0\\) , nilai awal yang dikenal sebagai ‘seed’ . Nilai ini diperbarui menggunakan hubungan rekursif \\[B_{n+1} = (a B_n + c) \\text{ modulo }m, ~~ n=0, 1, 2, \\ldots .\\] Algoritma ini disebut \\(a\\). Kasus \\(c = 0\\) disebut generator kongruensial perkalian Untuk nilai ilustrasi dari \\(a\\) Dan $m4 , menggunakan Microsoft Visual Basic \\(m=2^{24}\\) , \\(a = 1 , 140 , 671 , 485\\) , Dan \\(c = 12 , 820 , 163\\). Ini adalah mesin yang mendasari pembuatan angka acak dalam program Microsoft Excel. Urutan yang digunakan oleh analis didefinisikan sebagai \\(U_n=B_n/m.\\). Analis dapat menginterpretasikan urutan \\(U_{i}\\) menjadi (kira-kira) identik dan independen terdistribusi secara seragam pada interval (0,1). Untuk mengilustrasikan algoritme, maka pertimbangkan hal berikut. Contoh 6.1.2. Menghasilkan Nomor Acak Seragam di R. Kode berikut menunjukkan cara menghasilkan tiga angka seragam (0,1) dalam R menggunakan perintah runif. Fungsi set.seed() di R digunakan untuk membuat hasil yang dapat direproduksi saat menulis kode yang melibatkan pembuatan variabel yang mengambil nilai acak. set.seed(2017) U &lt;- runif(3) knitr::kable(U, digits=5, align = &quot;c&quot;, col.names = &quot;Uniform&quot;) Uniform 0.92424 0.53718 0.46920 6.1.2 Metode Transformasi Invers Metode transformasi invers digunakan untuk membangkitkan data acak dari distribusi peluang kontinu yang diketahui bentuk fungsinya. Dengan urutan bilangan acak seragam, kemudian diubah menjadi distribution of interest (\\(F\\)). \\[X_i=F^{-1}\\left( U_i \\right) .\\] \\[F^{-1}(y) = \\inf_x ~ \\{ F(x) \\ge y \\}\\] inf singkatan dari infimum atau batas bawah terbesar. Ini pada dasarnya adalah nilai \\(x\\) terkecil yang memenuhi pertidaksamaan \\(\\{F(x) \\ge y\\}\\). Hasilnya adalah urutan \\(X_{i}\\) kira-kira iid dengan fungsi distribusi \\(F\\) jika \\(U_{i}\\) adalah iid dengan fungsi distribusi seragam ( 0 , 1 ). Contoh 6.1.3. Menghasilkan Bilangan Acak Eksponensial. Misalkan ingin menghasilkan pengamatan dari distribusi eksponensial dengan parameter skala \\(θ\\) sehingga \\(F(x) = 1 - e^{-x/\\theta}\\). Untuk menghitung transformasi invers, maka dapat menggunakan langkah-langkah berikut: \\[\\begin{aligned} y = F(x) &amp;\\Leftrightarrow y = 1-e^{-x/\\theta} \\\\ &amp;\\Leftrightarrow -\\theta \\ln(1-y) = x = F^{-1}(y) . \\end{aligned}\\] Jadi, jika \\(U\\) memiliki distribusi seragam (0,1), maka \\(X = -\\theta \\ln(1-U)\\) memiliki distribusi eksponensial dengan parameter \\(θ\\). Seperti pada Contoh 6.1.2 kemudian mengubahnya menjadi variabel acak terdistribusi eksponensial independen dengan rata-rata \\(10\\). Sebagai alternatif, menggunakan fungsi rexp pada R digunakan untuk mensimulasikan sekumpulan bilangan acak yang diambil dari distribusi eksponensial. set.seed(2017) U &lt;- runif(3) X1 &lt;- -10*log(1-U) set.seed(2017) X2 &lt;- rexp(3, rate = 1/10) Contoh 6.1.4. Menghasilkan Angka Acak Pareto. Misalkan ingin menghasilkan pengamatan dari distribusi Pareto dengan parameter \\(α\\) dan \\(θ\\) sehingga \\(F(x) = 1 - \\left(\\frac{\\theta}{x+\\theta} \\right)^{\\alpha}\\). Untuk menghitung transformasi invers, maka dapat menggunakan langkah-langkah berikut: \\[\\begin{aligned} y = F(x) &amp;\\Leftrightarrow 1-y = \\left(\\frac{\\theta}{x+\\theta} \\right)^{\\alpha} \\\\ &amp;\\Leftrightarrow \\left(1-y\\right)^{-1/\\alpha} = \\frac{x+\\theta}{\\theta} = \\frac{x}{\\theta} +1 \\\\ &amp;\\Leftrightarrow \\theta \\left((1-y)^{-1/\\alpha} - 1\\right) = x = F^{-1}(y) .\\end{aligned}\\] Dengan demikian, \\(X = \\theta \\left((1-U)^{-1/\\alpha} - 1\\right)\\) memiliki distribusi Pareto dengan parameter \\(α\\) dan \\(θ\\) . Contoh 6.1.5. Menghasilkan Bilangan Acak Bernoulli. Misalkan ingin mensimulasikan variabel acak dari distribusi Bernoulli dengan parameter \\(Q= 0,85\\). Grafik fungsi distribusi kumulatif pada Gambar diatas menunjukkan bahwa fungsi kuantil dapat ditulis sebagai berikut. \\[\\begin{aligned} F^{-1}(y) = \\left\\{ \\begin{array}{cc} 0 &amp; 0&lt;y \\leq 0.85 \\\\ 1 &amp; 0.85 &lt; y \\leq 1.0 . \\end{array} \\right. \\end{aligned}\\] Jadi, dengan transformasi invers kita dapat mendefinisikan \\[\\begin{aligned} X = \\left\\{ \\begin{array}{cc} 0 &amp; 0&lt;U \\leq 0.85 \\\\ 1 &amp; 0.85 &lt; U \\leq 1.0 \\end{array} \\right. \\end{aligned}\\] Misalnya, ingin menghasilkan tiga angka acak untuk diperoleh set.seed(2017) U &lt;- runif(3) X &lt;- 1*(U &gt; 0.85) Contoh 6.1.6. Menghasilkan Angka Acak dari Distribusi Diskrit. Pertimbangkan waktu kegagalan mesin dalam lima tahun pertama. Distribusi waktu kegagalan diberikan sebagai: Dengan menggunakan grafik fungsi distribusi pada gambar diatas , dengan transformasi invers dapat definisikan \\[\\small{ \\begin{aligned} X = \\left\\{ \\begin{array}{cc} 1 &amp; 0&lt;U \\leq 0.1 \\\\ 2 &amp; 0.1 &lt; U \\leq 0.3\\\\ 3 &amp; 0.3 &lt; U \\leq 0.4\\\\ 4 &amp; 0.4 &lt; U \\leq 0.8 \\\\ 5 &amp; 0.8 &lt; U \\leq 1.0 . \\end{array} \\right. \\end{aligned} }\\] Untuk variabel acak diskrit umum mungkin tidak ada urutan hasil. Misalnya, seseorang dapat memiliki salah satu dari lima jenis produk asuransi jiwa dan dapat menggunakan algoritme berikut untuk menghasilkan hasil acak: \\[{\\small \\begin{aligned} X = \\left\\{ \\begin{array}{cc} \\textrm{whole life} &amp; 0&lt;U \\leq 0.1 \\\\ \\textrm{endowment} &amp; 0.1 &lt; U \\leq 0.3\\\\ \\textrm{term life} &amp; 0.3 &lt; U \\leq 0.4\\\\ \\textrm{universal life} &amp; 0.4 &lt; U \\leq 0.8 \\\\ \\textrm{variable life} &amp; 0.8 &lt; U \\leq 1.0 . \\end{array} \\right. \\end{aligned} }\\] Analis lain dapat menggunakan prosedur alternatif seperti: \\[{\\small \\begin{aligned} X = \\left\\{ \\begin{array}{cc} \\textrm{whole life} &amp; 0.9&lt;U&lt;1.0 \\\\ \\textrm{endowment} &amp; 0.7 \\leq U &lt; 0.9\\\\ \\textrm{term life} &amp; 0.6 \\leq U &lt; 0.7\\\\ \\textrm{universal life} &amp; 0.2 \\leq U &lt; 0.6 \\\\ \\textrm{variable life} &amp; 0 \\leq U &lt; 0.2 . \\end{array} \\right. \\end{aligned} }\\] Kedua algoritma menghasilkan (dalam jangka panjang) probabilitas yang sama, misalnya, \\(\\Pr(\\textrm{whole life})=0.1\\) , Dan seterusnya. Jadi, tidak ada yang salah ini menunjukkan bahwa ada lebih dari satu cara untuk mencapai suatu tujuan. Demikian pula, dapat menggunakan algoritme alternatif untuk hasil yang diurutkan (seperti waktu kegagalan 1, 2, 3, 4, atau 5, di atas). Contoh 6.1.7. Menghasilkan Angka Acak dari Distribusi Hybrid. Pertimbangkan variabel acak yaitu 0 dengan probabilitas 70% dan terdistribusi secara eksponensial dengan parameter \\(\\theta= 10,000\\) dengan probabilitas 30%. Dalam aplikasi asuransi, ini mungkin sesuai dengan peluang 70% tidak memiliki klaim asuransi dan peluang klaim 30% - jika klaim terjadi, maka itu didistribusikan secara eksponensial. Fungsi distribusi, digambarkan pada gambar dibawah ini , diberikan sebagai \\[\\begin{aligned} F(y) = \\left\\{ \\begin{array}{cc} 0 &amp; x&lt;0 \\\\ 1 - 0.3 \\exp(-x/10000) &amp; x \\ge 0 . \\end{array} \\right. \\end{aligned}\\] Dari Gambar diatas dapat dilihat bahwa transformasi invers untuk membangkitkan variabel acak dengan fungsi distribusi ini adalah \\[\\begin{aligned} X = F^{-1}(U) = \\left\\{ \\begin{array}{cc} 0 &amp; 0&lt; U \\leq 0.7 \\\\ -1000 \\ln (\\frac{1-U}{0.3}) &amp; 0.7 &lt; U &lt; 1 . \\end{array} \\right. \\end{aligned}\\] 6.1.3 (6.1.3) Presisi Simulasi Setelah mengetahui cara menghasilkan realisasi simulasi independen dari distribusi bunga, maka dapat menyusun distribusi empiris (distribusi empiris mengelompokkan data ke dalam suatu interval, di mana frekuensi data dalam setiap interval dapat digunakan untuk menentukan frekuensi relatifnya) dan memperkirakan distribusi yang diperlukan. Banyak dari aplikasi ini dapat direduksi menjadi masalah perkiraan \\(\\mathrm{E~}[h(X)]\\) , Di mana \\(h(\\cdot)\\) adalah beberapa fungsi yang diketahui. Berdasarkan simulasi R (replikasi), sehingga didapatkan \\(X_1,\\ldots,X_R\\). Dari sampel yang disimulasikan ini, dapat menghitung rata-rata sebagai berikut. \\[\\overline{h}_R=\\frac{1}{R}\\sum_{i=1}^{R} h(X_i)\\] sebagai perkiraan simulasi dari \\(\\mathrm{E~}[h(X)]\\). Untuk memperkirakan ketepatan perkiraan tersebut, maka menggunakan varians simulasi \\[s_{h,R}^2 = \\frac{1}{R-1} \\sum_{i=1}^{R}\\left( h(X_i) -\\overline{h}_R \\right) ^2.\\] Dari independensi, kesalahan standar estimasi adalah \\(s_{h,R}/\\sqrt{R}\\). Kesalahan standar estimasi dapat dibuat sekecil dengan meningkatkan jumlah replikasi \\(R\\). Contoh 6.1.8. Manajemen portofolio. Pada Bagian 3.4 telah mempelajari cara menghitung nilai ekspektasi polis dengan deductible. Sebagai contoh dari sesuatu yang tidak dapat dilakukan dengan ekspresi bentuk tertutup, kemudian akan mempertimbangkan dua risiko. (Ini adalah variasi dari contoh yang lebih kompleks yang akan dibahas sebagai Contoh 10.3.6). Dengan mempertimbangkan dua risiko properti dari perusahaan telekomunikasi: \\(X_1\\) - bangunan, dimodelkan menggunakan distribusi gamma dengan rata-rata 200 dan parameter skala 100. \\(X_2\\) - kendaraan bermotor, dimodelkan menggunakan distribusi gamma dengan mean 400 dan parameter skala 200. Nyatakan risiko total sebagai \\(X = X_1 + X_2\\). Untuk penyederhanaan, dapat diasumsikan bahwa risiko ini tidak bergantung. Untuk mengelola risiko maka diperlukan perlindungan atau penjamin asuransi dan bersedia mempertahankan jumlah bangunan dan kendaraan bermotor kecil secara internal, hingga \\(M\\). Jumlah acak lebih dari \\(M\\) akan memiliki pengaruh yang tidak terduga pada anggaran dan karenanya untuk jumlah ini dapat mencari perlindungan asuransi. Dinyatakan secara matematis, risiko yang dipertahankan adalah \\(Y_{retained}=\\min(X_1 + X_2,M)\\) dan bagian penanggung adalah \\(Y_{insurer} = X- Y_{retained}\\). Misalnya \\(M= 400\\) serta \\(R = 1000000\\). A. Dengan pengaturan tersebut, ingin menentukan perkiraan jumlah klaim dan standar deviasi terkait dari (i) yang ditahan, (ii) yang diterima oleh perusahaan asuransi, dan (iii) total jumlah keseluruhan. # Simulate the risks nSim &lt;- 1e6 #number of simulations set.seed(2017) #set seed to reproduce work X1 &lt;- rgamma(nSim ,alpha1,scale = theta1) X2 &lt;- rgamma(nSim ,alpha2,scale = theta2) # Portfolio Risks X &lt;- X1 + X2 Yretained &lt;- pmin(X, M) Yinsurer &lt;- X - Yretained Kemudian jumlah klaim yang diharapkan adalah # Expected Claim Amounts ExpVec &lt;- t(as.matrix(c(mean(Yretained),mean(Yinsurer),mean(X)))) sdVec &lt;- t(as.matrix(c(sd(Yretained),sd(Yinsurer),sd(X)))) outMat &lt;- rbind(ExpVec, sdVec) colnames(outMat) &lt;- c(&quot;Retained&quot;, &quot;Insurer&quot;,&quot;Total&quot;) row.names(outMat) &lt;- c(&quot;Mean&quot;,&quot;Standard Deviation&quot;) round(outMat,digits=2) B. Untuk klaim yang diasuransikan, kesalahan standar perkiraan simulasi adalah \\(s_{h,R}/\\sqrt{1000000} =/\\sqrt{1000000} =0.281\\). Untuk contoh ini, simulasi cepat dan nilai yang besar seperti 1000000 adalah pilihan yang mudah. Namun, untuk masalah yang lebih kompleks, ukuran simulasi mungkin menjadi masalah. Yinsurefct &lt;- function(numSim){ X1 &lt;- rgamma(numSim,alpha1,scale = theta1) X2 &lt;- rgamma(numSim,alpha2,scale = theta2) # Portfolio Risks X &lt;- X1 + X2 Yinsurer &lt;- X - pmin(X, M) return(Yinsurer) } R &lt;- 1e3 nPath &lt;- 20 set.seed(2017) simU &lt;- matrix(Yinsurefct(R*nPath),R,nPath) sumP2 &lt;- apply(simU, 2, cumsum)/(1:R) matplot(1:R,sumP2[,1:20],type=&quot;l&quot;,col=rgb(1,0,0,.2), ylim=c(100, 400), xlab=expression(paste(&quot;Number of Simulations (&quot;, italic(&#39;R&#39;), &quot;)&quot;)), ylab=&quot;Expected Insurer Claims&quot;) abline(h=mean(Yinsurer),lty=2) bonds &lt;- cbind(1.96*sd(Yinsurer)*sqrt(1/(1:R)),-1.96*sd(Yinsurer)*sqrt(1/(1:R))) matlines(1:R,bonds+mean(Yinsurer),col=&quot;red&quot;,lty=1) Dari grafik diatas dapat dilihat, semakin banyak jumlah simulasi R maka semakin sedikit jumlah klaim yang diharapkan. Penentuan Jumlah Simulasi Misalkan ingin berada dalam 1% dari rata-rata dengan kepastian 95%. Artinya, \\(\\Pr \\left( |\\overline{h}_R - \\mathrm{E~}[h(X)]| \\le 0.01 \\mathrm{E~}[h(X)] \\right) \\ge 0.95\\). Menurut teorema limit pusat, perkiraan harus terdistribusi secara normal dan mengharapkan R cukup besar untuk \\(0.01 \\mathrm{E~}[h(X)]/\\sqrt{\\mathrm{Var~}[h(X)]/R}) \\ge 1.96\\) . (Ingat bahwa 1,96 adalah persentil ke-97,5 dari distribusi normal standar.) Mengganti \\(\\mathrm{E~}[h(X)]\\) Dan \\(\\mathrm{Var~}[h(X)]\\) dengan estimasi,sehingga \\[\\frac{.01\\overline{h}_R}{s_{h,R}/\\sqrt{R}}\\geq 1.96\\] \\[\\begin{equation} R \\geq 38,416\\frac{s_{h,R}^2}{\\overline{h}_R^2}. \\tag{6.1} \\end{equation}\\] Contoh 6.1.9. Pilihan Perkiraan. Sebuah aplikasi penting dari simulasi adalah pendekatan dari \\(\\mathrm{E~}[h(X)]\\). Dalam contoh ini, kami menunjukkan bahwa pilihan dari \\(h(\\cdot)\\) fungsi dan distribusi \\(X\\) dapat berperan. Pertimbangkan pertanyaan berikut: apa itu \\(\\Pr[X&gt;2]\\). Kapan \\(X\\) mempunyai sebuah distribusi Cauchy (distribusi probabilitas kontinu), dengan fungsi kepadatan \\(f(x) =\\left(\\pi(1+x^2)\\right)^{-1}\\), pada garis sebenarnya? Nilai sebenarnya adalah \\[\\Pr\\left[X&gt;2\\right] = \\int_2^\\infty \\frac{dx}{\\pi(1+x^2)} .\\] true_value &lt;- integrate(function(x) 1/(pi*(1+x^2)),lower=2,upper=Inf)$value true_value ## [1] 0.1475836 Perkiraan 1. Sebagai alternatif, seseorang dapat menggunakan teknik simulasi untuk memperkirakan besaran tersebut. Dari kalkulus, dapat memeriksa bahwa fungsi kuantil dari distribusi Cauchy adalah \\(F^{-1}(y) = \\tan \\left( \\pi(y-0.5) \\right)\\) . Kemudian, dengan variasi seragam (0,1) yang disimulasikan, \\(U_1, \\ldots, U_R\\), sehingga dapat membangun estimator Q &lt;- function(u) tan(pi*(u-.5)) R &lt;- 1e6 set.seed(1) X &lt;- Q(runif(R)) p1 &lt;- mean(X&gt;2) se.p1 &lt;- sd(X&gt;2)/sqrt(R) p1 ## [1] 0.147439 se.p1 ## [1] 0.0003545432 Dengan satu juta simulasi, diperoleh estimasi sebesar 0,14744 dengan standard error 0,355 (dibagi 1000). Dapat dibuktikan bahwa varian dari \\(P_1\\) teratur \\(0.127/R\\). Perkiraan 2. Dengan pilihan lain dari \\(h(\\cdot)\\) Dan \\(f(\\cdot)\\) adalah mungkin untuk mengurangi ketidakpastian bahkan dengan menggunakan jumlah simulasi yang sama \\(R\\) . Untuk memulai, seseorang dapat menggunakan simetri distribusi Cauchy untuk menulis \\(\\Pr[X&gt;2]=0.5\\cdot\\Pr[|X|&gt;2]\\) . Dengan ini, dapat membuat estimator baru \\[p_2 = \\frac{1}{2R}\\sum_{i=1}^R \\mathrm{I}(|F^{-1}(U_i)|&gt;2) .\\] Dengan satu juta simulasi, diperoleh estimasi sebesar 0,14748 dengan standard error 0,228 (dibagi 1000). Dapat dibuktikan bahwa varian dari \\(P_2\\) teratur \\(0.052/R\\). Perkiraan 3. Integral tak wajar dapat ditulis dengan sifat simetri sederhana (karena fungsinya simetris dan integral pada garis real sama dengan 1 ). \\[\\int_2^\\infty \\frac{dx}{\\pi(1+x^2)}=\\frac{1}{2}-\\int_0^2\\frac{dx}{\\pi(1+x^2)} .\\] \\[p_3 = \\frac{1}{2}-\\frac{1}{R}\\sum_{i=1}^R h_3(2U_i), ~~~~~~\\text{where}~h_3(x)=\\frac{2}{\\pi(1+x^2)} .\\] Dengan satu juta simulasi, diperoleh estimasi sebesar 0,14756 dengan standard error 0,169 (dibagi 1000). Dapat dibuktikan bahwa varian dari \\(P_3\\) teratur \\(0,0285 / R\\). Perkiraan 4. Akhirnya, seseorang juga dapat mempertimbangkan beberapa perubahan variabel dalam integral. \\[\\int_2^\\infty \\frac{dx}{\\pi(1+x^2)}=\\int_0^{1/2}\\frac{y^{-2}dy}{\\pi(1-y^{-2})} .\\] \\[p_4 = \\frac{1}{R}\\sum_{i=1}^R h_4(U_i/2),~~~~~\\text{where}~h_4(x)=\\frac{1}{2\\pi(1+x^2)} .\\] Dengan satu juta simulasi, diperoleh estimasi sebesar 0,14759 dengan standard error 0,01 (dibagi 1000). Dapat dibuktikan bahwa varian dari \\(P_4\\) teratur \\(0,00009 / R\\) , yang jauh lebih kecil dari yang lainnya. Tabel berikut merupakan rangkuman dari empat pilihan \\(h(\\cdot)\\) Dan \\(f(\\cdot)\\)) untuk memperkirakan \\(\\Pr[X&gt;2] = 0,14758\\). Kesalahan standar bervariasi. Jadi, jika memiliki tingkat akurasi yang diinginkan, maka jumlah simulasi sangat bergantung pada bagaimana menulis integral yang akan diaproksimasi. 6.1.4 Simulasi dan Inferensi Statistik Simulasi tidak hanya membantu dalam memperkirakan nilai yang diharapkan tetapi juga berguna dalam menghitung aspek lain dari fungsi distribusi. Secara khusus, ini sangat berguna ketika distribusi statistik uji terlalu rumit untuk diturunkan. Dalam hal ini, seseorang dapat menggunakan simulasi untuk memperkirakan distribusi referensi. Contoh 6.1.10. Uji Distribusi Kolmogorov-Smirnov. Misalkan terdapata \\(n = 100\\) observasi \\(\\{x_1,\\cdots,x_n\\}\\) yang, tidak diketahui oleh analis, dihasilkan dari distribusi gamma dengan parameter \\(\\alpha = 6\\) Dan \\(\\theta=2\\) . Analis percaya bahwa data berasal dari distribusi lognormal dengan parameter 1 dan 0,4 dan ingin menguji asumsi ini. set.seed(1) n &lt;- 100 x &lt;- rgamma(n, 6, 2) u=seq(0,7,by=.01) vx = c(0,sort(x)) vy = (0:n)/n par(mfrow=c(1,2)) hist(x,probability = TRUE,main=&quot;Histogram&quot;, col=&quot;light blue&quot;, border=&quot;white&quot;,xlim=c(0,7),ylim=c(0,.4)) lines(u,dlnorm(u,1,.4),col=&quot;red&quot;,lty=2) plot(vx,vy,type=&quot;l&quot;,xlab=&quot;x&quot;,ylab=&quot;Cumulative Distribution&quot;,main=&quot;Empirical cdf&quot;) lines(u,plnorm(u,1,.4),col=&quot;red&quot;,lty=2) Dari grafik diatas dapat dilihat bahwa garis putus-putus merah tersebut sesuai dengan distribusi lognormal yang dihipotesiskan. Perlu digaris bawahi bahwa statistik Kolmogorov-Smirnov sama dengan perbedaan terbesar antara distribusi empiris dan hipotesis. Ini \\(\\max_x |F_n(x)-F_0(x)|\\), Di mana \\(F_0\\) adalah distribusi lognormal yang dihipotesiskan, sehingga # test statistic D &lt;- function(data, F0){ F &lt;- Vectorize(function(x) mean((data&lt;=x))) n &lt;- length(data) x &lt;- sort(data) d1=abs(F(x+1e-6)-F0(x+1e-6)) d2=abs(F(x-1e-6)-F0(x-1e-6)) return(max(c(d1,d2))) } D(x,function(x) plnorm(x,1,.4)) ## [1] 0.09703627 ks.test(x, plnorm, mean=1, sd=0.4) ## ## Asymptotic one-sample Kolmogorov-Smirnov test ## ## data: x ## D = 0.097037, p-value = 0.3031 ## alternative hypothesis: two-sided Secara khusus, untuk menghitung P-value, maka hasilkan ribuan sampel acak dari \\(LN(1,0.4)\\) distribusi (dengan ukuran yang sama), dan menghitung secara empiris distribusi statistik, ns &lt;- 1e4 d_KS &lt;- rep(NA,ns) # compute the test statistics for a large (ns) number of simulated samples for(s in 1:ns) d_KS[s] &lt;- D(rlnorm(n,1,.4),function(x) plnorm(x,1,.4)) mean(d_KS&gt;D(x,function(x) plnorm(x,1,.4))) ## [1] 0.2843 hist(d_KS,probability = TRUE,col=&quot;light blue&quot;,border=&quot;white&quot;,xlab=&quot;Test Statistic&quot;,main=&quot;&quot;) lines(density(d_KS),col=&quot;red&quot;) abline(v=D(x,function(x) plnorm(x,1,.4)),lty=2,col=&quot;red&quot;) Distribusi yang disimulasikan berdasarkan 10.000 sampel acak dirangkum grafik diatas. Di sini, statistik melebihi nilai empiris (0,09704) dalam 28,43%, sedangkan P-value adalah 0,3031. Baik untuk simulasi maupun teoretis P-value, kesimpulannya adalah data tidak memberikan bukti yang cukup untuk menolak hipotesis distribusi lognormal. Meskipun hanya perkiraan, pendekatan simulasi bekerja dalam berbagai distribusi dan uji statistik tanpa perlu mengembangkan nuansa teori yang mendasari untuk setiap situasi. Berikut ringkasan prosedur untuk mengembangkan distribusi simulasi dan p-value sebagai berikut: Gambarlah sampel berukuran n , katakanlah, \\(X_1, \\ldots, X_n\\), dari fungsi distribusi yang diketahui \\(F\\). Hitung statistik minat, dilambangkan sebagai \\(\\hat{\\theta}(X_1, \\ldots, X_n)\\). Panggil ini \\(\\hat{\\theta}^r\\) untuk replikasi ke -r . Ulangi ini \\(r=1, \\ldots, R\\) kali untuk mendapatkan sampel statistik, \\(\\hat{\\theta}^1, \\ldots,\\hat{\\theta}^R\\). Dari sampel statistik pada Langkah 2, \\(\\{\\hat{\\theta}^1, \\ldots,\\hat{\\theta}^R\\}\\), hitung ukuran ringkasan minat, seperti p-value. 6.2 Bootstrap dan Resampling Subbab ini akan mempelajari : Hasilkan distribusi bootstrap nonparametrik untuk statistik minat Gunakan distribusi bootstrap untuk menghasilkan estimasi presisi untuk statistik yang diminati, termasuk bias, standar deviasi, dan interval kepercayaan Lakukan analisis bootstrap untuk distribusi parametrik 6.2.1 Dasar-dasar Bootstrap Metode bootstrap adalah metode berbasis resampling data sampel dengan syarat pengembalian pada datanya dalam menyelesaikan statistik ukuran suatu sampel dengan harapan sampel tersebut mewakili data populai sebenarnya, biasanya ukuran resampling diambil secara ribuan kali agar dapat mewakili data populasinya. Algoritma resamplign umum dengan \\(\\{X_1, \\ldots, X_n\\}\\) untuk menunjukkan sampel asli dan \\(\\{X_1^*, \\ldots, X_n^*\\}\\) menunjukkan undian yang disimulasikan. Untuk setiap sampel, \\(n\\) merupakan undian simulasi, jumlah yang sama dengan ukuran sampel asli. Untuk membedakan prosedur ini dari simulasi, biasanya digunakan \\(B\\) (untuk bootstrap) sebagai jumlah sampel yang disimulasikan. Sehingga dapat dituliskan \\(\\{X_1^{(b)}, \\ldots, X_n^{(b)}\\}\\). Ada dua metode resampling dasar, model-free dan model-based , masing-masing sebagai nonparametrik dan parametrik . Pengundian yang disimulasikan berasal dari fungsi distribusi empiris \\(F_n(\\cdot)\\) , jadi setiap undian berasal \\(\\{X_1, \\ldots, X_n\\}\\) dengan probabilitas \\(1/n\\). Bootstrap Nonparametrik Gagasan bootstrap nonparametrik adalah menggunakan metode transformasi terbalik \\(F_N\\) , fungsi distribusi kumulatif empiris, digambarkan pada grafik dibawah ini. Karena \\(F_N\\) adalah step-function, \\(F_n^{-1}\\) subtitusi nilai-nilai \\(\\{x_1,\\cdots,x_n\\}\\) sehingga jika \\(y\\in(0,1/n)\\) (dengan probabilitas \\(1 / n\\) ) dengan menggambar nilai terkecil ( \\(\\min\\{x_i\\}\\) ) jika \\(y\\in(1/n,2/n)\\) (dengan probabilitas \\(1 / n\\) ) dengan menggambar nilai terkecil kedua, … jika \\(y\\in((n-1)/n,1)\\) (dengan probabilitas \\(1 / n\\) ) kami menggambar nilai terbesar ( \\(\\max\\{x_i\\}\\) ) Menggunakan metode transformasi terbalik dengan \\(F_N\\) berarti pengambilan sampel dari \\(\\{x_1,\\cdots,x_n\\}\\), dengan probabilitas \\(1 / n\\) . Menghasilkan sampel ukuran bootstrap \\(B\\) berarti pengambilan sampel dari \\(\\{x_1,\\cdots,x_n\\}\\) , dengan probabilitas \\(1 / n\\) , dengan penggantian. set.seed(1) n &lt;- 10 x &lt;- rexp(n, 1/6) m &lt;- 8 bootvalues &lt;- sample(x, size=m, replace=TRUE) 6.2.2 Presisi Bootstrap: Bias, Standar Deviasi, dan Mean Square Error Berikut adalah rangkuman prosedur bootstrap nonparametrik sebagai berikut: Dari sampel \\(\\{X_1, \\ldots, X_n\\}\\), gambar sampel berukuran n (dengan penggantian), katakanlah, \\(X_1^*, \\ldots, X_n^*\\) . Dari undian yang disimulasikan, hitung statistik minat, dilambangkan sebagai \\(\\hat{\\theta}(X_1^*, \\ldots, X_n^*)\\) . Panggil ini \\(\\hat{\\theta}_b^*\\) untuk ulangan ke-b . Ulangi ini \\(b=1, \\ldots, B\\) kali untuk mendapatkan sampel statistik \\(\\hat{\\theta}_1^*, \\ldots,\\hat{\\theta}_B^*\\). Dari sampel statistik pada Langkah 2,\\(\\{\\hat{\\theta}_1^*, \\ldots, \\hat{\\theta}_B^*\\}\\) hitung ukuran ringkasan minat. Pada bagian ini, ada tiga langkah ringkasan yaitu bias, standar deviasi, dan mean square error ( MSE ). Tabel dibawah ini merangkum ketiga ukuran. Di Sini, \\(\\overline{\\hat{\\theta^*}}\\) adalah rata-rata dari \\(\\{\\hat{\\theta}_1^*, \\ldots,\\hat{\\theta}_B^*\\}\\). # Example from Derrig et al BIData &lt;- read.csv(&quot;Data/DerrigResampling.csv&quot;, header =T) BIData$Censored &lt;- 1*(BIData$AmountPaid &gt;= BIData$PolicyLimit) BIDataUncensored &lt;- subset(BIData, Censored == 0) LER.boot &lt;- function(ded, data, indices){ resample.data &lt;- data[indices,] sumClaims &lt;- sum(resample.data$AmountPaid) sumClaims_d &lt;- sum(pmin(resample.data$AmountPaid,ded)) LER &lt;- sumClaims_d/sumClaims return(LER) } ##Derrig et al set.seed(2019) dVec2 &lt;- c(4000, 5000, 10500, 11500, 14000, 18500) OutBoot &lt;- matrix(0,length(dVec2),6) for (i in 1:length(dVec2)) { OutBoot[i,1] &lt;- dVec2[i] results &lt;- boot(data=BIDataUncensored, statistic=LER.boot, R=1000, ded=dVec2[i]) OutBoot[i,2] &lt;- results$t0 biasboot &lt;- mean(results$t)-results$t0 -&gt; OutBoot[i,3] sdboot &lt;- sd(results$t) -&gt; OutBoot[i,4] temp &lt;- boot.ci(results) OutBoot[i,5] &lt;- temp$normal[2] OutBoot[i,6] &lt;- temp$normal[3] } Berdasarkan tabel diatas hasil estimasi bootstrap. Misalnya, di D= 14000 , estimasi nonparametrik LER adalah 0,97678. Ini memiliki perkiraan bias 0,00018 dengan standar deviasi 0,00701. Untuk beberapa aplikasi, mungkin ingin menerapkan estimasi bias ke estimasi asli untuk memberikan estimator yang dikoreksi bias. Untuk ilustrasi ini, biasnya kecil sehingga koreksi semacam itu tidak relevan. Standar deviasi bootstrap memberikan ukuran presisi. Untuk satu penerapan standar deviasi dapat menggunakan pendekatan normal untuk membuat selang kepercayaan. Misalnya, pada R fungsi boot.ci menghasilkan interval kepercayaan normal sebesar 95%. Ini dihasilkan dengan membuat interval dua kali panjang standar deviasi bootstrap 1,95994, berpusat di sekitar estimator yang dikoreksi bias (1,95994 adalah kuantil ke-97,5 dari distribusi normal). Misalnya, CI 95% normal yang lebih rendah di \\(D= 14000\\) adalah \\((0.97678-0.00018)- 1.95994*0.00701\\). Contoh 6.2.2. Memperkirakan \\(\\exp(\\mu)\\) . Bootstrap dapat digunakan untuk mengukur bias estimator, misalnya. Pertimbangkan di sini sampel \\(\\mathbf{x}=\\{x_1,\\cdots,x_n\\}\\) adalah rata-rata μ . sample_x &lt;- c(2.46,2.80,3.28,3.86,2.85,3.67,3.37,3.40,5.22,2.55, 2.79,4.50,3.37,2.88,1.44,2.56,2.00,2.07,2.19,1.77) Misalkan kuantitas bunga adalah \\(\\theta=\\exp(\\mu)\\). Penaksir alami akan menjadi \\(\\widehat{\\theta}_1=\\exp(\\overline{x})\\). Estimator ini bias (karena ketidaksetaraan Jensen) tetapi tidak bias secara asimtotik. Untuk sampel, perkiraannya adalah sebagai berikuT (theta_1 &lt;- exp(mean(sample_x))) ## [1] 19.13463 Seseorang dapat menggunakan teorema limit pusat untuk mendapatkan koreksi menggunakan \\[\\overline{X}\\approx\\mathcal{N}\\left(\\mu,\\frac{\\sigma^2}{n}\\right)\\text{ where }\\sigma^2=\\text{Var}[X_i] ,\\] sehingga dengan fungsi pembangkit momen normal didapatkan \\[\\mathrm{E}~\\left[\\exp(\\overline{X})\\right] \\approx \\exp\\left(\\mu+\\frac{\\sigma^2}{2n}\\right) .\\] Oleh karena itu, seseorang dapat mempertimbangkan secara alami \\[\\widehat{\\theta}_2=\\exp\\left(\\overline{x}-\\frac{\\widehat{\\sigma}^2}{2n}\\right) .\\] n &lt;- length(sample_x) (theta_2 &lt;- exp(mean(sample_x)-var(sample_x)/(2*n))) ## [1] 18.73334 Sebagai strategi lain, seseorang juga dapat menggunakan pendekatan Taylor untuk mendapatkan penaksir yang lebih akurat (seperti dalam metode delta) \\[g(\\overline{x})=g(\\mu)+(\\overline{x}-\\mu)g&#39;(\\mu)+(\\overline{x}-\\mu)^2\\frac{g&#39;&#39;(\\mu)}{2}+\\cdots\\] Alternatif selanjutnya adalah menggunakan strategi bootstrap dengan sampel bootstrap \\(\\mathbf{x}^{\\ast}_{b}\\) sehingga \\(\\overline{x}^{\\ast}_{b}\\). \\[\\widehat{\\theta}_3=\\frac{1}{B}\\sum_{b=1}^B\\exp(\\overline{x}^{\\ast}_{b}) .\\] library(boot) results &lt;- boot(data=sample_x, statistic=function(y,indices) exp(mean(y[indices])), R=1000) theta_3 &lt;- mean(results$t) Ini menghasilkan tiga estimator, estimator mentah \\(\\widehat{\\theta}_1=19.135\\), koreksi urutan kedua \\(\\widehat{\\theta}_2= 18.733\\), dan estimator bootstrap \\(\\widehat{\\theta}_3= 19.388\\). Bagaimana cara kerjanya dengan ukuran sampel yang berbeda? Diasumsikan bahwa \\(X_i\\) dihasilkan dari distribusi lognormal \\(LN(0,1)\\) , sehingga \\(\\mu = \\exp(0 + 1/2) = 1.648721\\) Dan \\(\\theta = \\exp(1.648721)= 5,200326\\). Dengan menggunakan simulasi untuk menggambar ukuran sampel. param &lt;- function(x){ n &lt;- length(x) theta_1 &lt;- exp(mean(x)) theta_2 &lt;- exp(mean(x)-var(x)/(2*n)) results &lt;- boot(data=x, statistic=function(y,indices) exp(mean(y[indices])), R=999) theta_3 &lt;- mean(results$t) return(c(theta_1,theta_2,theta_3)) } set.seed(2074) ns&lt;- 200 est &lt;- function(n){ call_param &lt;- function(i) param(rlnorm(n,0,1)) V &lt;- Vectorize(call_param)(1:ns) apply(V,1,median) } VN=seq(15,100,by=5) Est &lt;- Vectorize(est)(VN) matplot(VN,t(Est),type=&quot;l&quot;, col=2:4, lty=2:4, ylim=exp(exp(1/2))+c(-1,1), xlab=&quot;sample size (n)&quot;, ylab=&quot;estimator&quot;) abline(h=exp(exp(1/2)),lty=1, col=1) legend(&quot;topleft&quot;, c(&quot;raw estimator&quot;, &quot;second order correction&quot;, &quot;bootstrap&quot;), col=2:4,lty=2:4, bty=&quot;n&quot;) Hasil perbandingan dirangkum dalam gambar diatas menunjukkan bahwa estimator bootstrap mendekati nilai parameter sebenarnya untuk hampir semua ukuran sampel. Bias dari ketiga estimator berkurang dengan meningkatnya ukuran sampel. 6.2.3 Interval Keyakinan Prosedur bootstrap menghasilkan \\(B\\) bentuk ulang dari \\(\\hat{\\theta}_1^*, \\ldots,\\hat{\\theta}_B^*\\) dari penaksir \\(\\hat{\\theta}\\) . Dalam Contoh 6.2.1, dapat dilihat bagaimana menggunakan pendekatan normal standar untuk membuat interval kepercayaan untuk parameter yang diinginkan. Namun, mengingat poin utamanya adalah menggunakan bootstrapping untuk menghindari ketergantungan pada asumsi perkiraan normalitas, tidak mengherankan jika tersedia interval kepercayaan alternatif. Untuk estimator \\(\\hat{\\theta}\\) , interval kepercayaan bootstrap dasar adalah \\[\\begin{equation} \\left(2 \\hat{\\theta} - q_U, 2 \\hat{\\theta} - q_L \\right) , \\tag{6.2} \\end{equation}\\] Di mana \\(q_L\\) Dan \\(q_U\\) adalah kuantil 2,5% bawah dan atas dari sampel bootstrap \\(\\hat{\\theta}_1^*, \\ldots,\\hat{\\theta}_B^*\\) Untuk melihat dari mana asalnya, mula-mula \\((q_L, q_U)\\) menyediakan interval 95% untuk \\(\\hat{\\theta}_1^*, \\ldots,\\hat{\\theta}_B^*\\) . Jadi, untuk acak \\(\\hat{\\theta}_b^*\\), ada kemungkinan 95% itu \\(q_L \\le \\hat{\\theta}_b^* \\le q_U\\). Membalikkan pertidaksamaan dan menjumlahkan \\(\\hat{\\theta}\\) ke setiap sisi memberikan interval 95% \\[\\hat{\\theta} -q_U \\le \\hat{\\theta} - \\hat{\\theta}_b^* \\le \\hat{\\theta} -q_L .\\] Jadi, \\(\\left( \\hat{\\theta}-q_U, \\hat{\\theta} -q_L\\right)\\) adalah interval 95% untuk \\(\\hat{\\theta} - \\hat{\\theta}_b^*\\). Ide perkiraan bootstrap mengatakan bahwa ini juga merupakan interval 95% untuk \\(\\theta - \\hat{\\theta}\\). Dengan menambahkan \\(\\hat{\\theta}\\) ke setiap sisi memberikan interval 95% dalam persamaan diatas. Banyak alternatif interval bootstrap yang tersedia. Yang paling mudah dijelaskan adalah interval bootstrap persentil yang didefinisikan sebagai \\((q_L,q_U)\\). Contoh 6.2.3. Klaim Cidera Tubuh dan Tindakan Risiko. Untuk melihat bagaimana interval kepercayaan bootstrap bekerja, dengan kembali ke klaim otomatis cedera tubuh yang dipertimbangkan dalam Contoh 6.2.1 . Alih-alih rasio eliminasi kerugian, misalkan ingin memperkirakan persentil ke-95 \\(F^{-1}(0.95)\\) dan ukuran didefinisikan sebagai \\[TVaR_{0.95}[X] = \\mathrm{E}[X | X &gt; F^{-1}(0.95)] .\\] Pengukuran ini disebut dengan ekor nilai berisiko; itu adalah nilai yang diharapkan dari X bersyarat X melebihi persentil ke-95. Bagian 10.2 menjelaskan bagaimana quantiles dan tail value-at-risk adalah dua contoh paling penting dari apa yang disebut sebagai ukuran risiko . Untuk saat ini, hanya akan menganggap ini sebagai ukuran yang ingin diperkirakan. Untuk persentil, dengan menggunakan estimator nonparametrik \\(F^{-1}_n(0.95)\\) didefinisikan dalam Bagian 4.1.1.3 . Untuk tail value-at-risk, menggunakan prinsip plug-in untuk menentukan estimator nonparametrik \\[TVaR_{n,0.95}[X] = \\frac{\\sum_{i=1}^n X_i I(X_i &gt; F^{-1}_n(0.95))}{\\sum_{i=1}^n I(X_i &gt; F^{-1}_n(0.95))} ~.\\] Dalam ungkapan ini, penyebut menghitung jumlah pengamatan yang melebihi persentil ke-95 \\(F^{-1}_n(0.95)\\) . Pembilang menjumlahkan kerugian untuk pengamatan yang melebihi \\(F^{-1}_n(0.95)\\) . Tabel dibawah ini merangkum penaksir untuk pecahan terpilih. # Example from Derrig et al #BIData &lt;- read.csv(&quot;Data/DerrigResampling.csv&quot;, header =T) BIData$Censored &lt;- 1*(BIData$AmountPaid &gt;= BIData$PolicyLimit) BIDataUncensored &lt;- subset(BIData, Censored == 0) set.seed(2017) PercentVec &lt;- c(0.50, 0.80, 0.90, 0.95, 0.98) OutBoot1 &lt;- matrix(0,5,10) for (i in 1:length(PercentVec)) { OutBoot1[i,1] &lt;- PercentVec[i] results &lt;- boot(data=BIDataUncensored$AmountPaid, statistic=function(X,indices) quantile(X[indices],PercentVec[i]), R=1000) if (i==1){bootreal &lt;- results$t} OutBoot1[i,2] &lt;- results$t0 OutBoot1[i,3] &lt;- mean(results$t)-results$t0 OutBoot1[i,4] &lt;- sd(results$t) temp &lt;- boot.ci(results, type = c(&quot;norm&quot;, &quot;basic&quot;, &quot;perc&quot;)) OutBoot1[i,5] &lt;- temp$normal[2] OutBoot1[i,6] &lt;- temp$normal[3] OutBoot1[i,7] &lt;- temp$basic[4] OutBoot1[i,8] &lt;- temp$basic[5] OutBoot1[i,9] &lt;- temp$percent[4] OutBoot1[i,10] &lt;- temp$percent[5] } Misalnya, ketika pecahannya adalah 0,50, dapat melihat bahwa kuantil 2,5 bawah dan atas dari simulasi bootstrap adalah \\(q_L= 6000\\) dan \\(q_U= 6700\\). Ini membentuk interval kepercayaan bootstrap persentil. Dengan estimator nonparametrik \\(6500\\), ini menghasilkan batas bawah dan atas interval kepercayaan dasar masing-masing \\(6300\\) dan \\(7000\\). CTE.boot &lt;- function(data, indices, RiskLevel){ resample.data &lt;- data[indices,] X &lt;- resample.data$AmountPaid cutoff &lt;- quantile(X, RiskLevel) CTE &lt;- sum(X*(X &gt; cutoff))/sum(X &gt; cutoff) return(CTE) } set.seed(2017) PercentVec &lt;- c(0.50, 0.80, 0.90, 0.95, 0.98) OutBoot1 &lt;- matrix(0,5,10) for (i in 1:length(PercentVec)) { OutBoot1[i,1] &lt;- PercentVec[i] results &lt;- boot(data=BIDataUncensored, statistic=CTE.boot, R=1000, RiskLevel=PercentVec[i]) OutBoot1[i,2] &lt;- results$t0 OutBoot1[i,3] &lt;- mean(results$t)-results$t0 OutBoot1[i,4] &lt;- sd(results$t) temp &lt;- boot.ci(results, type = c(&quot;norm&quot;, &quot;basic&quot;, &quot;perc&quot;)) OutBoot1[i,5] &lt;- temp$normal[2] OutBoot1[i,6] &lt;- temp$normal[3] OutBoot1[i,7] &lt;- temp$basic[4] OutBoot1[i,8] &lt;- temp$basic[5] OutBoot1[i,9] &lt;- temp$percent[4] OutBoot1[i,10] &lt;- temp$percent[5] } Tabel di atas menunjukkan kalkulasi serupa untuk tail value-at-risk. Dalam setiap kasus, dapat melihat bahwa deviasi standar bootstrap meningkat seiring dengan peningkatan fraksi. Hal ini karena ada lebih sedikit pengamatan untuk memperkirakan kuantil seiring meningkatnya fraksi, yang menyebabkan ketidaktepatan yang lebih besar. Interval kepercayaan juga menjadi lebih lebar. Menariknya, tampaknya tidak ada pola yang sama dalam estimasi bias tersebut. 6.2.4 Bootstrap Parametrik Gagasan dari bootstrap nonparametrik adalah untuk mengambil sampel ulang dengan menggambar variabel independen dari fungsi distribusi kumulatif empiris \\(F_n\\). Sebaliknya, dengan bootstrap parametrik, kami menarik variabel independen dari \\(F_{\\widehat{\\theta}}\\) di mana distribusi yang mendasarinya diasumsikan dalam keluarga parametrik \\(\\mathcal{F}=\\{F_{\\theta},\\theta\\in\\Theta\\}\\) . Biasanya, parameter dari distribusi ini diperkirakan berdasarkan sampel dan dinotasikan sebagai \\(\\hat{\\theta}\\). contoh 6.2.4. distribusi lognormal. Pertimbangkan lagi kumpulan datanya sample_x &lt;- c(2.46,2.80,3.28,3.86,2.85,3.67,3.37,3.40, 5.22,2.55,2.79,4.50,3.37,2.88,1.44,2.56,2.00,2.07,2.19,1.77) Bootstrap klasik (nonparametrik) didasarkan pada contoh berikut. x &lt;- sample(sample_x,replace=TRUE) Sebagai gantinya, untuk bootstrap parametrik harus mengasumsikan bahwa distribusi dari \\(x_i\\) adalah dari kelompok tertentu. Sebagai contoh, kode berikut menggunakan distribusi lognormal. library(MASS) fit &lt;- fitdistr(sample_x, dlnorm, list(meanlog = 1, sdlog = 1)) fit x &lt;- rlnorm(length(sample_x), meanlog=fit$estimate[1], sdlog=fit$estimate[2]) set.seed(2074) CV &lt;- matrix(NA,1e5,2) for(s in 1:nrow(CV)){ x1 &lt;- sample(sample_x,replace=TRUE) x2 &lt;- rlnorm(length(sample_x), meanlog=fit$estimate[1], sdlog=fit$estimate[2]) CV[s,] &lt;- c(sd(x1)/mean(x1),sd(x2)/mean(x2)) } plot(density(CV[,1]),col=&quot;red&quot;,main=&quot;&quot;,xlab=&quot;Coefficient of Variation&quot;, lty=1) lines(density(CV[,2]),col=&quot;blue&quot;,lty=2) abline(v=sd(sample_x)/mean(sample_x),lty=3) legend(&quot;topright&quot;,c(&quot;nonparametric&quot;,&quot;parametric(LN)&quot;), col=c(&quot;red&quot;,&quot;blue&quot;),lty=1:2,bty=&quot;n&quot; Grafik di atas membandingkan distribusi bootstrap untuk koefisien variasi, yang satu berdasarkan pendekatan nonparametrik dan yang lainnya berdasarkan pendekatan parametrik, dengan asumsi distribusi lognormal. Contoh 6.2.5. Pengamatan yang Disensor Bootstrap. Bootstrap parametrik menarik realisasi simulasi dari perkiraan parametrik dari fungsi distribusi. Dengan cara yang sama, sehingga dapat menggambar realisasi simulasi dari estimasi fungsi distribusi. Sebagai salah satu contoh, dengan mengambil dari estimasi yang dihaluskan dari fungsi distribusi yang diperkenalkan di Bagian 4.1.1.4 . Kasus khusus lainnya, yang dipertimbangkan di sini adalah menggambar estimasi dari estimator Kaplan-Meier yang dibahas di Bagian 4.3.2.2. Dengan cara ini, dapat ditangani pengamatan yang disensor. Secara khusus, kembali ke data cedera tubuh pada Contoh 6.2.1 dan 6.2.3 tetapi sekarang menyertakan 17 klaim yang disensor oleh batasan kebijakan. Dalam Contoh 4.3.6 menggunakan kumpulan data lengkap ini untuk mengestimasi estimator Kaplan-Meier dari fungsi survival yang diperkenalkan di Bagian 4.3.2.2 . Tabel 6.6 menyajikan estimasi bootstrap kuantil dari estimator fungsi survival Kaplan-Meier. Ini termasuk perkiraan presisi bootstrap, bias dan standar deviasi, serta interval kepercayaan dasar 95%. # Example from Derrig et al library(survival) # for Surv(), survfit() BIData$UnCensored &lt;- 1*(BIData$AmountPaid &lt; BIData$PolicyLimit) ## KM estimate KM0 &lt;- survfit(Surv(AmountPaid, UnCensored) ~ 1, type=&quot;kaplan-meier&quot;, data=BIData) set.seed(2019) PercentVec &lt;- c(0.50, 0.80, 0.90, 0.95, 0.98) OutBoot1 &lt;- matrix(NA,5,6) KM.survobj &lt;- Surv(BIData$AmountPaid, BIData$UnCensored) for (i in 1:length(PercentVec)) { OutBoot1[i,1] &lt;- PercentVec[i] results &lt;- bootkm(KM.survobj, q=1-PercentVec[i], B=1000, pr = FALSE) if (i==1){bootreal &lt;- results} OutBoot1[i,2] &lt;- quantile(KM0, PercentVec[i])$quantile OutBoot1[i,3] &lt;- mean(results)-OutBoot1[i,2] OutBoot1[i,4] &lt;- sd(results) # temp &lt;- boot.ci(results, type = c(&quot;norm&quot;, &quot;basic&quot;,&quot;perc&quot;)) OutBoot1[i,5] &lt;- 2*OutBoot1[i,2]-quantile(results,.975, type=6) OutBoot1[i,6] &lt;- 2*OutBoot1[i,2]-quantile(results,.025, type=6) } Hasil pada tabel di atas konsisten dengan hasil untuk subsampel tanpa sensor pada Tabel 6.4 . Pada tabel di atas tercatat kesulitan dalam memperkirakan kuantil pada pecahan besar karena penyensoran. Namun, untuk fraksi berukuran sedang (0,50, 0,80, dan 0,90), estimasi nonparametrik Kaplan-Meier (KM NP) dari kuantil konsisten dengan Tabel 6.4 . Standar Deviasi bootstrap lebih kecil pada 0,50 (sesuai dengan median) tetapi lebih besar pada level 0,80 dan 0,90. Analisis data tersensor yang dirangkum dalam tabel di atas menggunakan lebih banyak data daripada analisis subsampel tanpa sensor pada Tabel 6.4 , tetapi juga mengalami kesulitan dalam mengekstraksi informasi untuk kuantil besar. 6.3 Cross Validation Dalam bagian ini, kita akan mempelajari caranya: Membandingkan dan membedakan validasi silang dengan teknik simulasi dan metode bootstrap. Menggunakan teknik validasi silang untuk pemilihan model Menjelaskan metode jackknife sebagai kasus khusus validasi silang dan menghitung estimasi bias dan kesalahan standar jackknife Validasi silang, yang diperkenalkan secara singkat pada Bagian 4.2.4, adalah teknik yang didasarkan pada hasil simulasi. Sekarang kita akan membandingkan dan membedakan validasi silang dengan teknik simulasi lain yang telah diperkenalkan dalam bab ini.” Simulasi, atau Monte-Carlo, yang diperkenalkan pada Bagian 6.1, memungkinkan kita untuk menghitung nilai ekspektasi dan rangkuman distribusi statistik lainnya, seperti nilai-p, dengan mudah. Bootstrap, dan metode resampling lainnya yang diperkenalkan pada Bagian 6.2, menyediakan estimator presisi, atau variabilitas, statistik. Validasi silang penting ketika menilai seberapa akurat model prediktif akan bekerja dalam praktiknya. Tumpang tindih memang ada, namun tetap saja akan sangat membantu untuk memikirkan tujuan luas yang terkait dengan setiap metode statistik. Untuk membahas validasi silang, mari kita ingat kembali dari Bagian 4.2 beberapa ide kunci dari validasi model. Ketika menilai, atau memvalidasi, sebuah model, kita melihat kinerja yang diukur pada data baru, atau setidaknya bukan data yang digunakan untuk mencocokkan model. Pendekatan klasik, yang dijelaskan di Bagian 4.2.3, adalah membagi sampel menjadi dua: satu bagian (dataset pelatihan) digunakan untuk menyesuaikan model dan bagian lainnya (dataset pengujian) digunakan untuk memvalidasi. Namun, keterbatasan dari pendekatan ini adalah bahwa hasilnya bergantung pada pembagian; meskipun keseluruhan sampel tetap, pembagian antara sub-sampel pelatihan dan pengujian bervariasi secara acak. Sampel pelatihan yang berbeda berarti parameter estimasi model akan berbeda. Parameter model yang berbeda dan sampel uji yang berbeda berarti statistik validasi akan berbeda. Dua orang analis dapat menggunakan data yang sama dan model yang sama, namun mencapai kesimpulan yang berbeda tentang kelayakan suatu model (berdasarkan pembagian acak yang berbeda), sebuah situasi yang membuat frustasi. 6.3.1 k-Fold Cross-Validation Untuk mengurangi kesulitan ini, biasanya digunakan pendekatan validasi silang seperti yang diperkenalkan di Bagian 4.2.4. Ide utamanya adalah meniru pendekatan pengujian/pelatihan dasar untuk validasi model dengan mengulanginya berkali-kali melalui rata-rata dari beberapa bagian data yang berbeda. Keuntungan utamanya adalah bahwa statistik validasi tidak terikat pada model parametrik (atau nonparametrik) tertentu - seseorang dapat menggunakan statistik nonparametrik atau statistik yang memiliki interpretasi ekonomi - sehingga dapat digunakan untuk membandingkan model yang tidak bersarang (tidak seperti prosedur rasio kemungkinan). Contoh 6.3.1. Dana Properti Wisconsin. Untuk data dana properti 2010 yang diperkenalkan pada Bagian 1.3, kami mencocokkan distribusi gamma dan Pareto dengan 1.377 data klaim. Untuk rincian kecocokan terkait, lihat Lampiran Bagian 15.4.4. Sekarang kita mempertimbangkan statistik Kolmogorov-Smirnov yang diperkenalkan di Bagian 4.1.2.2. Ketika seluruh dataset telah sesuai, statistik kecocokan Kolmogorov-Smirnov untuk distribusi gamma adalah 0,2639 dan untuk distribusi Pareto adalah 0,0478. Nilai yang lebih rendah untuk distribusi Pareto menunjukkan bahwa distribusi ini lebih cocok daripada gamma. Untuk melihat bagaimana validasi silang k-lipatan bekerja, kami membagi data secara acak menjadi \\(k=8\\) kelompok, atau lipatan, yang masing-masing memiliki sekitar \\(1377/8≈172\\) pengamatan. Kemudian, kami mencocokkan model gamma dan Pareto pada set data dengan tujuh lipatan pertama (sekitar $172⋅7 = 120$4 pengamatan), menentukan estimasi parameter, dan kemudian menggunakan model-model yang cocok dengan data yang ditahan untuk menentukan statistik Kolmogorov-Smirnov. library(VGAM) ## Loading required package: stats4 ## Loading required package: splines ## ## Attaching package: &#39;VGAM&#39; ## The following objects are masked from &#39;package:boot&#39;: ## ## logit, simplex library(MASS) claim_lev &lt;- read.csv(&quot;data/CLAIMLEVEL.csv&quot;, header = TRUE) claim_data &lt;- subset(claim_lev, Year == 2010); # Randomly re-order the data - &quot;shuffle it&quot; n &lt;- nrow(claim_data) set.seed(12347) cvdata &lt;- claim_data[sample(n), ] # Number of folds k &lt;- 8 cvalvec &lt;- matrix(0,2,k) for (i in 1:k) { indices &lt;- (((i-1) * round((1/k)*nrow(cvdata))) + 1):((i*round((1/k) * nrow(cvdata)))) # Pareto fit.pareto &lt;- vglm(Claim ~ 1, paretoII, loc = 0, data = cvdata[-indices,]) ksResultPareto &lt;- ks.test(cvdata[indices,]$Claim, &quot;pparetoII&quot;, loc = 0, shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1])) cvalvec[1,i] &lt;- ksResultPareto$statistic # Gamma fit.gamma &lt;- glm(Claim ~ 1, data = cvdata[-indices,], family = Gamma(link = log)) gamma_theta &lt;- exp(coef(fit.gamma)) * gamma.dispersion(fit.gamma) alpha &lt;- 1 / gamma.dispersion(fit.gamma) ksResultGamma &lt;- ks.test(cvdata[indices,]$Claim, &quot;pgamma&quot;, shape = alpha, scale = gamma_theta) cvalvec[2,i] &lt;- ksResultGamma$statistic } KScv &lt;- rowSums(cvalvec)/k Hasilnya tampak pada Gambar 6.12 di mana sumbu horizontal adalah Fold=1. Proses ini diulangi untuk tujuh lipatan lainnya. Hasil yang dirangkum dalam Gambar 6.12 menunjukkan bahwa Pareto secara konsisten memberikan distribusi prediktif yang lebih dapat diandalkan daripada gamma. # Plot the statistics matplot(1:k,t(cvalvec),type=&quot;b&quot;, col=c(1,3), lty=1:2, ylim=c(0,0.4), pch = 0, xlab=&quot;Fold&quot;, ylab=&quot;KS Statistic&quot;) legend(&quot;left&quot;, c(&quot;Pareto&quot;, &quot;Gamma&quot;), col=c(1,3),lty=1:2, bty=&quot;n&quot;) “Figure 6.2:” Statistik Kolmogorov-Smirnov (KS) yang telah divalidasi silang untuk Data Klaim Dana Asuransi. Garis hitam solid untuk distribusi Pareto, garis putus-putus hijau untuk distribusi gamma. Statistik KS mengukur deviasi terbesar antara distribusi yang sesuai dengan distribusi empiris untuk masing-masing dari 8 kelompok, atau lipatan, data yang dipilih secara acak. 6.3.2 6.3.2 Leave-One-Out Cross-Validation Kasus khusus di mana \\(k=n\\) dikenal sebagai validasi silang tinggalkan-satu-keluar. Kasus ini secara historis sangat menonjol dan terkait erat dengan jackknifestatistik yang merupakan pendahulu dari teknik bootstrap. Meskipun kita menyajikannya sebagai kasus khusus validasi silang, akan sangat membantu jika kami memberikan definisi eksplisit. Pertimbangkan sebuah statistik umum \\(θˆ = t(x)\\) yang merupakan penaksir untuk sebuah parameter yang diminati \\(θ\\). Ide dari jackknife adalah menghitung n nilai \\(θˆ_{-i} = t(x-i)\\), di mana \\(x-i\\) adalah subsampel dari \\(x\\) dengan nilai \\(ke-i\\) dihilangkan. Rata-rata dari nilai-nilai ini dilambangkan sebagai \\[\\overline{\\widehat{\\theta}}_{(\\cdot)}=\\frac{1}{n}\\sum_{i=1}^n \\widehat{\\theta}_{-i} .\\] Nilai-nilai ini dapat digunakan untuk membuat estimasi bias dari statistik \\(\\hatθ\\) \\[\\begin{equation} Bias_{jack} = (n-1) \\left(\\overline{\\widehat{\\theta}}_{(\\cdot)} - \\widehat{\\theta}\\right) \\tag{6.3} \\end{equation}\\] serta estimasi standar deviasi \\[\\begin{equation} s_{jack} =\\sqrt{\\frac{n-1}{n}\\sum_{i=1}^n \\left(\\widehat{\\theta}_{-i} -\\overline{\\widehat{\\theta}}_{(\\cdot)}\\right)^2} ~. \\tag{6.4} \\end{equation}\\] Contoh 6.3.2. Koefisien Variasi. Sebagai ilustrasi, pertimbangkan sebuah sampel fiktif kecil \\(x = {x_1,...,x_n}\\) dengan realisasi sample_x &lt;- c(2.46,2.80,3.28,3.86,2.85,3.67,3.37,3.40, 5.22,2.55,2.79,4.50,3.37,2.88,1.44,2.56,2.00,2.07,2.19,1.77) Misalkan kita tertarik dengan \\(\\theta = CV = \\sqrt{\\mathrm{Var~}[X]}/\\mathrm{E~}[X]\\) Dengan dataset ini, estimator koefisien variasi menjadi 0,31196. Namun, seberapa handalkah estimasi tersebut? Untuk menjawab pertanyaan ini, kita dapat menghitung estimator pisau lipat dari bias dan deviasi standarnya. Kode berikut ini menunjukkan bahwa penaksir jackknife untuk bias adalah \\(Bias_{jack} = -0,00627\\) dan standar deviasi jackknife adalah \\(s_{jack} = 0,01293\\). CVar &lt;- function(x) sqrt(var(x))/mean(x) JackCVar &lt;- function(i) sqrt(var(sample_x[-i]))/mean(sample_x[-i]) JackTheta &lt;- Vectorize(JackCVar)(1:length(sample_x)) BiasJack &lt;- (length(sample_x)-1)*(mean(JackTheta) - CVar(sample_x)) sd(JackTheta) ## [1] 0.01293001 Contoh 6.3.3. Klaim Cidera Badan dan Rasio Eliminasi Kerugian. Pada Contoh 6.2.1, kita telah menunjukkan bagaimana menghitung estimasi bootstrap dari bias dan deviasi standar untuk rasio eliminasi kerugian dengan menggunakan data klaim cedera badan pada Contoh 4.1.11. Sekarang kita menindaklanjuti dengan memberikan jumlah yang sebanding dengan menggunakan statistik jackknife. Tabel 6.7 merangkum hasil estimasi jackknife. Tabel ini menunjukkan bahwa estimasi jackknife terhadap bias dan deviasi standar dari rasio eliminasi kerugian \\(E [min (X, d)]/E [X]\\) sebagian besar konsisten dengan metodologi bootstrap. Selain itu, kita dapat menggunakan standar deviasi untuk membangun interval kepercayaan berbasis normal, yang berpusat di sekitar penaksir yang dikoreksi bias. Sebagai contoh, pada \\(d = 14000\\), kita melihat pada Contoh 4.1.11 bahwa estimasi nonparametrik dari \\(LER\\) adalah 0.97678. Estimasi ini memiliki bias sebesar 0,00010, sehingga menghasilkan estimator terkoreksi-bias sebesar 0,97688. Interval kepercayaan 95% dihasilkan dengan membuat interval dua kali panjang 1,96 deviasi standar jackknife, yang berpusat pada estimator terkoreksi bias (1,96 adalah perkiraan kuantil ke-97,5 dari distribusi normal standar). library(boot) # Example from Derrig et al BIData &lt;- read.csv(&quot;data/DerrigResampling.csv&quot;, header =T) BIData$Censored &lt;- 1*(BIData$AmountPaid &gt;= BIData$PolicyLimit) BIDataUncensored &lt;- subset(BIData, Censored == 0) LER.boot &lt;- function(ded, data, indices){ resample.data &lt;- data[indices,] sumClaims &lt;- sum(resample.data$AmountPaid) sumClaims_d &lt;- sum(pmin(resample.data$AmountPaid,ded)) LER &lt;- sumClaims_d/sumClaims return(LER) } x &lt;- BIDataUncensored$AmountPaid LER.jack&lt;- function(ded,i){ LER &lt;- sum(pmin(x[-i],ded))/sum(x[-i]) return(LER) } LER &lt;- function(ded) sum(pmin(x,ded))/sum(x) ##Derrig et al set.seed(2019) dVec2 &lt;- c(4000, 5000, 10500, 11500, 14000, 18500) OutJack &lt;- matrix(0,length(dVec2),8) for (j in 1:length(dVec2)) { OutJack[j,1] &lt;- dVec2[j] results &lt;- boot(data=BIDataUncensored, statistic=LER.boot, R=1000, ded=dVec2[j]) OutJack[j,2] &lt;- results$t0 biasboot &lt;- mean(results$t)-results$t0 -&gt; OutJack[j,3] sdboot &lt;- sd(results$t) -&gt; OutJack[j,4] temp &lt;- boot.ci(results) LER.jack.ded&lt;- function(i) LER.jack(ded=dVec2[j],i) JackTheta.ded &lt;- Vectorize(LER.jack.ded)(1:length(x)) OutJack[j,5] &lt;- BiasJack.ded &lt;- (length(x)-1)*(mean(JackTheta.ded) - LER(ded=dVec2[j])) OutJack[j,6] &lt;- sd(JackTheta.ded) OutJack[j,7:8] &lt;- mean(JackTheta.ded)+qt(c(0.025,0.975),length(x)-1)*OutJack[j,6] } Table 6.7. Estimasi Jackknife dari LER pada Deductible yang Dipilih d NP Estimate Bootstrap Bias Bootstrap SD Jackknife Bias Jackknife SD Lower Jackknife 95% CI Upper Jackknife 95% CI 4000 0.54113 0.00011 0.01237 0.00031 0.00061 0.53993 0.54233 5000 0.64960 0.00027 0.01412 0.00033 0.00068 0.64825 0.65094 10500 0.93563 0.00004 0.01017 0.00019 0.00053 0.93460 0.93667 11500 0.95281 -0.00003 0.00941 0.00016 0.00047 0.95189 0.95373 14000 0.97678 0.00016 0.00687 0.00010 0.00034 0.97612 0.97745 18500 0.99382 0.00014 0.00331 0.00003 0.00017 0.99350 0.99415 Diskusi. Salah satu dari banyak hal menarik tentang kasus khusus leave-one-out adalah kemampuan untuk mereplikasi estimasi dengan tepat. Artinya, ketika ukuran lipatan hanya satu, maka tidak ada ketidakpastian tambahan yang disebabkan oleh validasi silang. Ini berarti bahwa para analis dapat mereplikasi pekerjaan satu sama lain dengan tepat, sebuah pertimbangan yang penting. Statistik Jackknife dikembangkan untuk memahami ketepatan estimator, menghasilkan estimator bias dan deviasi standar pada persamaan (6.3) dan (6.4). Hal ini sesuai dengan tujuan yang telah kita kaitkan dengan teknik bootstrap, bukan metode validasi silang. Hal ini menunjukkan bagaimana teknik statistik dapat digunakan untuk mencapai tujuan yang berbeda. 6.3.3 Cross-Validation and Bootstrap Bootstrap berguna untuk memberikan estimator presisi, atau variabilitas, dari statistik. Hal ini juga berguna untuk validasi model. Pendekatan bootstrap untuk validasi model mirip dengan prosedur validasi leave-one-out dan k-fold: Buat sampel bootstrap dengan mengambil sampel ulang (dengan penggantian) \\(n\\) indeks dalam \\({1, ⋯, n}\\). Ini akan menjadi sampel pelatihan kita. Perkirakan model yang sedang dipertimbangkan berdasarkan sampel ini. Uji, atau sampel validasi, terdiri dari pengamatan yang tidak dipilih untuk pelatihan. Mengevaluasi model yang cocok (berdasarkan data pelatihan) dengan menggunakan data uji. Ulangi proses ini beberapa kali (katakanlah \\(B\\)). Ambil rata-rata dari hasil-hasilnya dan pilih model berdasarkan statistik evaluasi rata-rata. Contoh 6.3.4. Dana Properti Wisconsin. Kembali ke Contoh 6.3.1 di mana kita menyelidiki kecocokan distribusi gamma dan Pareto pada data dana properti. Kita kembali membandingkan kinerja prediksi menggunakan statistik Kolmogorov-Smirnov (KS), namun kali ini menggunakan prosedur bootstrap untuk membagi data antara sampel pelatihan dan pengujian. Berikut ini adalah kode ilustrasinya. library(goftest) n &lt;- nrow(claim_data) set.seed(12347) indices &lt;- 1:n # Number of Bootstrap Samples B &lt;- 100 cvalvec &lt;- matrix(0,2,B) for (i in 1:B) { bootindex &lt;- unique(sample(indices, size=n, replace= TRUE)) traindata &lt;- claim_data[bootindex,] testdata &lt;- claim_data[-bootindex,] # Pareto fit.pareto &lt;- vglm(Claim ~ 1, paretoII, loc = 0, data = traindata) ksResultPareto &lt;- ks.test(testdata$Claim, &quot;pparetoII&quot;, loc = 0, shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1])) cvalvec[1,i] &lt;- ksResultPareto$statistic # Gamma fit.gamma &lt;- glm(Claim ~ 1, data = traindata, family = Gamma(link = log)) gamma_theta &lt;- exp(coef(fit.gamma)) * gamma.dispersion(fit.gamma) alpha &lt;- 1 / gamma.dispersion(fit.gamma) ksResultGamma &lt;- ks.test(testdata$Claim, &quot;pgamma&quot;, shape = alpha, scale = gamma_theta) cvalvec[2,i] &lt;- ksResultGamma$statistic } KSBoot &lt;- rowSums(cvalvec)/B Kami melakukan pengambilan sampel dengan menggunakan B= 100 ulangan. Statistik KS rata-rata untuk distribusi Pareto adalah 0,058 dibandingkan dengan rata-rata untuk distribusi gamma, 0,262. Hal ini konsisten dengan hasil sebelumnya dan memberikan bukti lain bahwa Pareto adalah model yang lebih baik untuk data ini dibandingkan dengan gamma. 6.4 Importance Sampling Bagian 6.1 memperkenalkan teknik Monte Carlo dengan menggunakan teknik inversi: untuk membangkitkan sebuah variabel acak \\(X\\) dengan distribusi \\(F\\), terapkan \\(F^{-1}\\) pada pemanggilan sebuah generator acak (seragam pada interval satuan). Bagaimana jika kita ingin menggambar sesuai dengan \\(X\\), dengan syarat \\(X∈[a,b]\\)? Seseorang dapat menggunakan mekanisme terima-tolak: menarik \\(x\\) dari distribusi \\(F\\) jika \\(x\\in[a,b]\\): simpan (“terima”) jika \\(x\\notin[a,b]\\): gambar yang lain (“tolak”) Amati bahwa dari n nilai yang awalnya dihasilkan, kita simpan di sini hanya \\([F(b)-F(a)] ⋅ n\\) hasil imbang, rata-rata. Contoh 6.4.1. Penarikan dari Distribusi Normal. Misalkan kita menggambar dari distribusi normal dengan rata-rata 2,5 dan varians 1, \\(N(2,5,1)\\), tetapi hanya tertarik pada gambar yang lebih besar dari \\(a≥2\\) dan kurang dari \\(b≤4\\). Artinya, kita hanya dapat menggunakan \\(F(4)-F(2)=Φ(4-2.5)-Φ(2-2.5) = 0.9332 - 0.3085 = 0.6247\\) proporsi undian. Gambar 6.13 menunjukkan bahwa beberapa hasil undian berada di dalam interval \\((2,4)\\) dan beberapa di luarnya. mu = 2.5 sigma = 1 a = 2 b = 4 Fa = pnorm(a,mu,sigma) Fb = pnorm(b,mu,sigma) pic_ani = function(){ u=seq(0,5,by=.01) plot(u,pnorm(u,mu,sigma),col=&quot;white&quot;,ylab=&quot;&quot;,xlab=&quot;&quot;) rect(-1,-1,6,2,col=rgb(1,0,0,.2),border=NA) rect(a,Fa,b,Fb,col=&quot;white&quot;,border=NA) lines(u,pnorm(u,mu,sigma),lwd=2) abline(v=c(a,b),lty=2,col=&quot;red&quot;) ru &lt;- runif(1) clr &lt;- &quot;red&quot; if((qnorm(ru,mu,sigma)&gt;=a)&amp;(qnorm(ru,mu,sigma)&lt;=b)) clr &lt;- &quot;blue&quot; segments(-1,ru,qnorm(ru,mu,sigma),ru,col=clr,lwd=2) arrows(qnorm(ru,mu,sigma),ru,qnorm(ru,mu,sigma),0,col=clr,lwd=2,length = .1) } for (i in 1:numAnimation) {pic_ani()} Sebagai gantinya, seseorang dapat menggambar menurut distribusi bersyarat \\(F^⋆\\) yang didefinisikan sebagai \\[F^{\\star}(x) = \\Pr(X \\le x | a &lt; X \\le b) =\\frac{F(x)-F(a)}{F(b)-F(a)}, \\ \\ \\ \\text{for } a &lt; x \\le b .\\] Dengan menggunakan metode inverse transform pada Bagian 6.1.2, kita mendapatkan hasil imbang \\[X^\\star=F^{\\star-1}\\left( U \\right) = F^{-1}\\left(F(a)+U\\cdot[F(b)-F(a)]\\right)\\] memiliki distribusi \\(F⋆^\\). Dinyatakan dengan cara lain, definisikan \\[\\tilde{U} = (1-U)\\cdot F(a)+U\\cdot F(b)\\] dan kemudian gunakan \\(F^{-1}(\\tilde{U})\\). Dengan pendekatan ini, setiap undian dihitung. Hal ini dapat dikaitkan dengan mekanisme pengambilan sampel kepentingan: kita menarik lebih sering di wilayah yang kita harapkan memiliki kuantitas yang memiliki kepentingan. Transformasi ini dapat dianggap sebagai “perubahan ukuran.” pic_ani = function(){ u=seq(0,5,by=.01) plot(u,pnorm(u,mu,sigma),col=&quot;white&quot;,ylab=&quot;&quot;,xlab=&quot;&quot;) rect(-1,-1,6,2,col=rgb(1,0,0,.2),border=NA) rect(a,Fa,b,Fb,col=&quot;white&quot;,border=NA) lines(u,pnorm(u,mu,sigma),lwd=2) abline(h=pnorm(c(a,b),mu,sigma),lty=2,col=&quot;red&quot;) ru &lt;- runif(1) rutilde &lt;- (1-ru)*Fa+ru*Fb segments(-1,rutilde,qnorm(rutilde,mu,sigma),rutilde,col=&quot;blue&quot;,lwd=2) arrows(qnorm(rutilde,mu,sigma),rutilde,qnorm(rutilde,mu,sigma),0,col=&quot;blue&quot;,lwd=2,length = .1) } for (i in 1:numAnimation) {pic_ani()} Pada Contoh 6.4.1., kebalikan dari distribusi normal sudah tersedia (dalam R, fungsinya adalah qnorm). Namun, untuk aplikasi lain, hal ini tidak terjadi. Kemudian, kita cukup menggunakan metode numerik untuk menentukan \\(X^⋆\\) sebagai solusi dari persamaan \\(F(X^\\star) =\\tilde{U}\\) di mana \\(\\tilde{U}=(1-U)\\cdot F(a)+U\\cdot F(b)\\)). Lihat kode ilustrasi berikut ini. pic_ani = function(){ u=seq(0,5,by=.01) plot(u,pnorm(u,mu,sigma),col=&quot;white&quot;,ylab=&quot;&quot;,xlab=&quot;&quot;) rect(-1,-1,6,2,col=rgb(1,0,0,.2),border=NA) rect(2,-1,4,2,col=&quot;white&quot;,border=NA) lines(u,pnorm(u,mu,sigma),lty=2) pnormstar &lt;- Vectorize(function(x){ y=(pnorm(x,mu,sigma)-Fa)/(Fb-Fa) if(x&lt;=a) y &lt;- 0 if(x&gt;=b) y &lt;- 1 return(y) }) qnormstar &lt;- function(u) as.numeric(uniroot((function (x) pnormstar(x) - u), lower = 2, upper = 4)[1]) lines(u,pnormstar(u),lwd=2) abline(v=c(2,4),lty=2,col=&quot;red&quot;) ru &lt;- runif(1) segments(-1,ru,qnormstar(ru),ru,col=&quot;blue&quot;,lwd=2) arrows(qnormstar(ru),ru,qnormstar(ru),0,col=&quot;blue&quot;,lwd=2,length = .1) } for (i in 1:numAnimation) {pic_ani()} Sebenarnya materi yang dari web untuk 6.5 itu masih sedang dalam penulisan dan belum selesai dalam pengeditan. Jadi apa yang ditulis disini hanya memberikan gambaran besarnya saja. Ide dari teknik Monte Carlo bergantung pada hukum bilangan besar (yang menjamin konvergensi rata-rata terhadap integral) dan teorema limit pusat (yang digunakan untuk mengukur ketidakpastian dalam perhitungan). Perlu diingat kembali jika (\\(X_i\\)) adalah urutan ke-i dari variabel acak dengan distribusi F, maka \\[ \\frac{1}{\\sqrt{n}}\\left(\\sum_{i=1}^n h(X_i)-\\int h(x)dF(x)\\right)\\overset{\\mathcal{L}}{\\rightarrow }\\mathcal{N}(0,\\sigma^2),\\text{ as }n\\rightarrow\\infty , \\] atau beberapa varian \\(σ^2&gt;0\\) . Namun sebenarnya, teorema ergodik dapat digunakan untuk melemahkan hasil sebelumnya, karena independensi variabel tidak diperlukan. Lebih tepatnya, jika (\\(X_i\\)) adalah Proses Markov dengan ukuran invarian \\(μ\\) , di bawah beberapa asumsi teknis tambahan, maka dapat diperoleh \\[ \\frac{1}{\\sqrt{n}}\\left(\\sum_{i=1}^n h(X_i)-\\int h(x)d\\mu(x)\\right)\\overset{\\mathcal{L}}{\\rightarrow }\\mathcal{N}(0,\\sigma_\\star^2),\\text{ as }n\\rightarrow\\infty. \\] untuk beberapa varian \\(σ^2_⋆&gt;0\\) . Oleh karena itu, dari sifat ini, dapat melihat bahwa tidak selalu mungkin untuk menghasilkan nilai-nilai independen dari F , tetapi untuk menghasilkan proses Markov dengan ukuran invarian F , dan untuk mempertimbangkan rata-rata dari proses (tidak harus independen). Dengan mempertimbangkan kasus vektor Gaussian terkendala: kami ingin menghasilkan pasangan acak dari vektor acak \\(X\\) , tetapi kami hanya tertarik pada kasus di mana jumlah komposisinya cukup besar, yang dapat ditulis \\(X^T1&gt;m\\) untuk nilai nyata \\(m\\) . Tentu saja, dimungkinkan untuk menggunakan algoritme terima-tolak, tetapi kami telah melihat bahwa ini mungkin sangat tidak efisien. Satu dapat menggunakan Metropolis Hastingsand Gibbs sampler untuk menghasilkan proses Markov dengan ukuran invarian tersebut. 6.5 6.5.1 Metropolis Hastings Algoritma agak sederhana untuk dihasilkan dari \\(f\\) : dapat dimulai dengan nilai layak \\(x_1\\) . Kemudian, pada langkah \\(t\\) , kita perlu menentukan kernel transisi : diberikan \\(x_t\\) , kita memerlukan distribusi bersyarat untuk \\(X_{t+1}\\) diberikan \\(x_t\\) . Algoritme akan bekerja dengan baik jika distribusi bersyarat itu dapat dengan mudah disimulasikan. dengan \\(π(⋅|xt)\\) menunjukkan probabilitas itu. Gambarkan nilai potensial \\(x^⋆_{t+1}\\) , dan \\(u\\) , dari distribusi seragam. Selanjutnya Menghitung \\(R= \\frac{f(x_{t+1}^\\star)}{f(x_t)}\\) jika \\(u&lt;r\\) , lalu atur \\(x_{t+1}=x^⋆_t\\) jika \\(u≤r\\) , maka atur \\(x_{t+1}=x_t\\) Di sini r disebut rasio penerimaan selanjutnya menerima nilai baru dengan probabilitas r (atau sebenarnya yang terkecil antara 1 dan r karena r dapat melebihi 1 ). Misalnya, asumsikan bahwa \\(f(⋅|xt)\\) seragam pada \\([x_t−ε,x_t+ε]\\) untuk beberapa \\(ε&gt;0\\) , dan di mana$ $f (distribusi target kita) adalah \\(N(0,1)\\) . Kami tidak akan pernah menarik dari \\(f\\) , tetapi kami akan menggunakannya untuk menghitung rasio penerimaan kami di setiap langkah. metrop1 &lt;- function(n=1000,eps=0.5){ vec &lt;- matrix(NA, n, 3) x=0 vec[1] &lt;- x for (i in 2:n) { innov &lt;- runif(1,-eps,eps) mov &lt;- x+innov R &lt;- min(1,dnorm(mov)/dnorm(x)) u &lt;- runif(1) if (u &lt; R) x &lt;- mov vec[i,] &lt;- c(x,mov,R) } return(vec)} #install.packages(&#39;gifski&#39;) #if (packageVersion(&#39;knitr&#39;) &lt; &#39;1.20.14&#39;) { # remotes::install_github(&#39;yihui/knitr&#39;) #} vec &lt;- metrop1(25) u=seq(-3,3,by=.01) pic_ani = function(k){ plot(1:k,vec[1:k,1],pch=19,xlim=c(0,25),ylim=c(-2,2),xlab=&quot;&quot;,ylab=&quot;&quot;) if(vec[k+1,1]==vec[k+1,2]) points(k+1,vec[k+1,1],col=&quot;blue&quot;,pch=19) if(vec[k+1,1]!=vec[k+1,2]) points(k+1,vec[k+1,1],col=&quot;red&quot;,pch=19) points(k+1,vec[k+1,2],cex=1.5) arrows(k+1,vec[k,1]-.5,k+1,vec[k,1]+.5,col=&quot;green&quot;,angle=90,code = 3,length=.1) polygon(c(k+dnorm(u)*10,rep(k,length(u))),c(u,rev(u)),col=rgb(0,1,0,.3), border=NA) segments(k,vec[k,1],k+dnorm(vec[k,1])*10,vec[k,1]) segments(k,vec[k+1,2],k+dnorm(vec[k+1,2])*10,vec[k+1,2]) text(k,2,round(vec[k+1,3],digits=3)) } for (k in 2:23) {pic_ani(k)} Selanjutnya dapat menggunakan simulasi, maka didapat vec &lt;- metrop1(10000) simx &lt;- vec[1000:10000,1] par(mfrow=c(1,4)) plot(simx,type=&quot;l&quot;) hist(simx,probability = TRUE,col=&quot;light blue&quot;,border=&quot;white&quot;) lines(u,dnorm(u),col=&quot;red&quot;) qqnorm(simx) acf(simx,lag=100,lwd=2,col=&quot;light blue&quot;) 6.6 6.5.2 Gibbs Sampler Dapat mempertimbangkan beberapa vektor \\(X=(X_1,⋯,X_d)\\) dengan komponen independen, \\(X_i∼E(λ_i)\\) . Selanjutnya mengambil sampel untuk sampel dari \\(X\\) yang diberikan \\(X^T1&gt;s\\) untuk beberapa ambang batas \\(s&gt;0\\) . beberapa titik awal x0 , Mengambil secara acak \\(i∈{1,⋯,d}\\) \\(X_i\\) mengingat \\(X_i&gt;s−x^T_{(−i)}1\\) berdistribusi Eksponensial \\(E(λ_i)\\) Menggambar \\(Y∼E(λ_i)\\) dan atur \\(x_i=y+(s−x^T_{(−i)}1)_+\\) hingga \\(x^T_{(−i)}1+x_i&gt;s\\) sim &lt;- NULL lambda &lt;- c(1,2) X &lt;- c(3,3) s &lt;- 5 for(k in 1:1000){ i &lt;- sample(1:2,1) X[i] &lt;- rexp(1,lambda[i])+max(0,s-sum(X[-i])) while(sum(X)&lt;s){ X[i] &lt;- rexp(1,lambda[i])+max(0,s-sum(X[-i])) } sim &lt;- rbind(sim,X) } plot(sim,xlim=c(1,11),ylim=c(0,4.3)) polygon(c(-1,-1,6),c(-1,6,-1),col=&quot;red&quot;,density=15,border=NA) abline(5,-1,col=&quot;red&quot;) Konstruksi urutan (algoritma MCMC bersifat iteratif) dapat divisualisasikan di bawah ini lambda &lt;- c(1,2) X &lt;- c(3,3) sim &lt;- X s &lt;- 5 for(k in 1:100){ set.seed(k) i &lt;- sample(1:2,1) X[i] &lt;- rexp(1,lambda[i])+max(0,s-sum(X[-i])) while(sum(X)&lt;s){ X[i] &lt;- rexp(1,lambda[i])+max(0,s-sum(X[-i])) } sim &lt;- rbind(sim,X) } pic_ani = function(n){ plot(sim[1:n,],xlim=c(1,11),ylim=c(0,5),xlab=&quot;&quot;,ylab=&quot;&quot;) i=which(apply(sim[(n-1):n,],2,diff)==0) if(i==1) abline(v=sim[n,1],col=&quot;grey&quot;) if(i==2) abline(h=sim[n,2],col=&quot;grey&quot;) if(n&gt;=1) points(sim[n,1],sim[n,2],pch=19,col=&quot;blue&quot;,cex=1.4) if(n&gt;=2) points(sim[n-1,1],sim[n-1,2],pch=19,col=&quot;red&quot;,cex=1.4) polygon(c(-1,-1,6),c(-1,6,-1),col=&quot;red&quot;,density=15,border=NA) abline(5,-1,col=&quot;red&quot;) } for (i in 2:100) {pic_ani(i)} "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
=======
<<<<<<< HEAD
[["model-selection-and-estimation.html", "Bab 4 Model Selection and Estimation 4.1 Nonparametric Inference 4.2 Model Selection 4.3 Estimasi Menggunakan Data Modifikasi 4.4 Bayesian Inference", " Bab 4 Model Selection and Estimation 4.1 Nonparametric Inference Di bagian ini, Anda mempelajari cara: Perkirakan momen, kuantil, dan distribusi tanpa mengacu pada distribusi parametrik Ringkas data secara grafis tanpa mengacu pada distribusi parametrik Tentukan ukuran yang meringkas penyimpangan parametrik dari kecocokan nonparametrik Gunakan estimator nonparametrik untuk memperkirakan parameter yang dapat digunakan untuk memulai prosedur estimasi parametrik 4.1.1 Estimasi Nonparametrik Pada bagian pembahasan sebelumnya telah mempelajari cara meringkas distribusi dengan cara menghitung, varians, kuantil/persentil, dan sebagainya. Untuk memperkirakan langkah-langkah ringkasan menggunakan kumpulan data, salah satu strateginya adalah: menganggap bentuk parametrik untuk distribusi, seperti binomial negatif untuk frekuensi atau distribusi gamma untuk tingkat keparahan, memperkirakan parameter distribusi itu, gunakan distribusi dengan estimasi parameter untuk menghitung ukuran ringkasan yang diinginkan. Ini adalah pendekatan parametrik . Strategi lain adalah memperkirakan ukuran ringkasan yang diinginkan langsung dari pengamatan tanpa mengacu pada model parametrik. Tidak mengherankan, ini dikenal sebagai pendekatan nonparametrik mempertimbangkan jenis skema pengambilan sampel yang paling dasar dan mengasumsikan bahwa observasi adalah realisasi dari serangkaian variabel acak \\(X_1, \\ldots, X_n\\) yang iid menarik dari distribusi populasi yang tidak diketahui \\(F(  )\\). Cara yang setara untuk mengatakan ini adalah itu \\(X_1, \\ldots, X_n\\), adalah sampel acak (dengan penggantian) dari F( ) .Kemudian menjelaskan estimator nonparametrik dari banyak ukuran penting yang meringkas sebuah distribusi. 4.1.1.1 Estimator Momen Pada bagian 2.2.2. telah mendefinisikan momen untuk frekuensi dan pada bagian 3.1.1 untuk keparahan. Secara khusus, k -momen ke-, \\(\\mathrm{E~}[X^k] = \\mu^{\\prime}_k\\) , merangkum banyak aspek distribusi untuk berbagai pilihan k . Di Sini, k kadang-kadang disebut k th momen populasi untuk membedakannya dari k momen sampel, \\[\\frac{1}{n} \\sum_{i=1}^n X_i^k ,\\] yang merupakan estimator nonparametrik yang sesuai. Dalam aplikasi tipikal, k adalah bilangan bulat positif, meskipun tidak perlu dalam teori. Kasus khusus yang penting adalah momen pertama di mana \\(k = 1\\) . Dalam hal ini, simbol prima ( \\(\\prime\\) ) dan 1 subskrip biasanya dijatuhkan dan satu digunakan \\(\\mu=\\mu^{\\prime}_1\\) untuk menunjukkan mean populasi, atau hanya mean . Estimator sampel yang sesuai untuk \\(\\) disebut rata-rata sampel , dilambangkan dengan bilah di atas variabel acak: \\[\\overline{X} =\\frac{1}{n} \\sum_{i=1}^n X_i .\\] Jenis ringkasan ukuran minat lainnya adalah k -momen pusat ke- , \\(\\mathrm{E~} [(X-\\mu)^k] = \\mu_k\\) . (Kadang-kadang, \\(\\mu^{\\prime}_k\\) disebut k -th momen mentah untuk membedakannya dari momen sentral k .). Estimator nonparametrik, atau sampel, dari \\(\\mu_k\\) adalah \\[\\frac{1}{n} \\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^k .\\] Momen pusat kedua ( \\(k = 2\\) ) adalah kasus penting yang biasanya akan diberikan simbol baru, \\(\\sigma^2 = \\mathrm{E~} [(X-\\mu)^2]\\) , dikenal sebagai varians . Sifat penduga momen sampel dari varians seperti \\(n^{-1}\\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^2\\) telah dipelajari secara ekstensif tetapi bukan satu-satunya estimator yang mungkin. Versi yang paling banyak digunakan adalah versi di mana ukuran sampel efektif dikurangi satu, jadi kami mendefinisikannya \\[s^2 = \\frac{1}{n-1} \\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^2.\\] Membagi dengan \\(n  1\\) alih-alih N masalah kecil ketika Anda memiliki ukuran sampel yang besar \\(N\\) seperti yang umum dalam aplikasi asuransi. Estimator varians sampel \\(s^2\\) tidak memihak dalam arti bahwa \\(\\mathrm{E~} [s^2] = \\sigma^2\\) , properti yang diinginkan terutama saat menginterpretasikan hasil analisis. 4.1.1.2 Fungsi Distribusi Empiris Kita telah melihat bagaimana menghitung estimator nonparametrik dari k saat ini \\(\\mathrm{E~} [X^k]\\) . Dengan cara yang sama, untuk fungsi apa pun yang diketahui g () , kita dapat memperkirakan \\(\\mathrm{E~} [\\mathrm{g}(X)]\\) menggunakan\\(n^{-1}\\sum_{i=1}^n \\mathrm{g}(X_i)\\) Sekarang perhatikan fungsinya \\(\\mathrm{g}(X) = I(X \\le x)\\) untuk tetap \\(X\\) . Di sini, notasi $I(  \\() adalah fungsi indikator ; itu mengembalikan 1 jika acara (  ) benar dan 0 sebaliknya. Perhatikan bahwa sekarang variabel acak\\) g (X$) memiliki distribusi Bernoulli (distribusi binomial dengan \\(n = 1\\) ). Kita dapat menggunakan distribusi ini untuk dengan mudah menghitung jumlah seperti rata-rata dan varians. Misalnya, untuk pilihan ini \\(g ()\\) , nilai harapannya adalah \\(\\mathrm{E~} [I(X \\le x)] = \\Pr(X \\le x) = F(x)\\) , fungsi distribusi dievaluasi pada \\(X\\) . Menggunakan prinsip analog , kami mendefinisikan estimator nonparametrik dari fungsi distribusi \\[ \\begin{aligned} F_n(x) &amp;= \\frac{1}{n} \\sum_{i=1}^n I\\left(X_i \\le x\\right) \\\\ &amp;= \\frac{\\text{number of observations less than or equal to }x}{n} . \\end{aligned} \\] Sebagai $F_N(  $) didasarkan hanya pada pengamatan dan tidak mengasumsikan keluarga parametrik untuk distribusi, itu nonparametrik dan juga dikenal sebagai fungsi distribusi empiris . Ia juga dikenal sebagai fungsi distribusi kumulatif empiris dan, dalam R, seseorang dapat menggunakan ecdf(.) fungsi tersebut untuk menghitungnya. Contoh 4.1.1. Kumpulan Data Mainan . Sebagai ilustrasi, pertimbangkan kumpulan data fiktif, atau mainan. \\(n = 10\\) observasi. Tentukan fungsi distribusi empiris. \\[ {\\small \\begin{array}{c|cccccccccc} \\hline i &amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;10 \\\\ X_i&amp; 10 &amp;15 &amp;15 &amp;15 &amp;20 &amp;23 &amp;23 &amp;23 &amp;23 &amp;30\\\\ \\hline \\end{array} }\\] Kemudian memeriksa bahwa rata-rata sampel adalah \\(\\overline{X} = 19.7\\) dan bahwa varians sampel adalah \\(S^2= 34,45556\\) . Fungsi distribusi empiris yang sesuai adalah \\[ \\begin{aligned} F_n(x) &amp;= \\left\\{ \\begin{array}{ll} 0 &amp; \\text{ for }\\ x&lt;10 \\\\ 0.1 &amp; \\text{ for }\\ 10 \\leq x&lt;15 \\\\ 0.4 &amp; \\text{ for }\\ 15 \\leq x&lt;20 \\\\ 0.5 &amp; \\text{ for }\\ 20 \\leq x&lt;23 \\\\ 0.9 &amp; \\text{ for }\\ 23 \\leq x&lt;30 \\\\ 1 &amp; \\text{ for }\\ x \\geq 30, \\end{array} \\right.\\end{aligned}\\] (xExample &lt;- c(10,rep(15,3),20,rep(23,4),30)) PercentilesxExample &lt;- ecdf(xExample) plot(PercentilesxExample, main=&quot;&quot;,xlab=&quot;x&quot;) 4.1.1.3 Quartiles, Percentiles and Quantiles Pada bagian 3.1.1 median , yaitu angka yang kira-kira setengah dari kumpulan data berada di bawah (atau di atasnya) . Kuartil pertama adalah angka yang kira-kira 25% datanya berada di bawahnya dan kuartil ketiga adalah angka yang kira-kira 75% datanya berada di bawahnya. 100 hal persentil adalah angka sehingga \\(100×p\\) persen dari data di bawahnya. Untuk menggeneralisasi konsep ini, pertimbangkan fungsi distribusi \\(F(\\)) , yang mungkin kontinu atau tidak, dan biarkan Q menjadi pecahan sehingga \\(0 &lt; q&lt; 1\\) . Kami ingin mendefinisikan quantile , katakanlah \\(q_F\\) , menjadi bilangan sedemikian sehingga \\(F(q_F) \\approx q\\) . Perhatikan bahwa ketika \\(q=0.5\\) , \\(q_F\\) adalah median; Kapan \\(q=0.25\\) , \\(q_F\\) adalah kuartil pertama, dan seterusnya. Dengan cara yang sama, ketika \\(q = 0, 0.01, 0.02, \\ldots, 0.99, 1.00\\) , yang dihasilkan QF adalah persentil. Jadi, kuantil menggeneralisasikan konsep median, kuartil, dan persentil. Lebih tepatnya, untuk diberikan \\(0 &lt; q&lt; 1\\) , tentukan q kuantil \\(q_F\\) untuk menjadi nomor yang memenuhi: \\[ \\begin{equation} F(q_F-) \\le q \\le F(q_F) \\tag{4.1} \\end{equation}\\] Untuk mendapatkan pemahaman yang lebih baik tentang definisi ini, mari kita lihat beberapa kasus khusus. Pertama, pertimbangkan kasus di mana X adalah variabel acak kontinu sehingga fungsi distribusi \\(F()\\) tidak memiliki titik lompatan, seperti yang diilustrasikan pada Gambar 4.2 . Pada gambar ini, beberapa pecahan, Q1 , Q2 , Dan Q3 ditunjukkan dengan kuantil yang sesuai \\(q_{F,1} , q_{F,2} , dan q_{F,3}\\) . Dalam setiap kasus, dapat dilihat bahwa \\(F(q_F-)= F(q_F)\\) sehingga ada kuantil unik. Karena kita dapat menemukan invers unik dari fungsi distribusi di mana saja \\(0 &lt; q&lt; 1\\) , kita bisa menulis \\(q_F= F^{-1}(q)\\) Gambar 4.3 menunjukkan tiga kasus untuk fungsi distribusi. Panel kiri sesuai dengan kasus kontinu yang baru saja dibahas. Panel tengah menampilkan titik lompatan yang serupa dengan yang telah kita lihat dalam fungsi distribusi empiris Gambar 4.1 . Untuk nilai \\(q\\) ditampilkan di panel ini, kami masih memiliki nilai kuantil yang unik \\(q_F\\) . Meskipun ada banyak nilai Q seperti yang \\(F(q_F-) \\le q \\le F(q_F)\\) , untuk nilai tertentu dari \\(q\\) , hanya ada satu solusi untuk persamaan (4.1) . Panel kanan menggambarkan situasi di mana kuantil tidak dapat ditentukan secara unik untuk \\(q\\) ditampilkan karena ada berbagai \\(q_F\\) persamaan yang memuaskan (4.1) . Contoh 4.1.2. Kumpulan Data Mainan: Lanjutan. Tentukan kuantil yang sesuai dengan persentil ke-20, ke-50, dan ke-95. Solusi . Perhatikan Gambar 4.1 . Kasus \\(q=0.20\\) sesuai dengan panel tengah Gambar Gambar 4.3 , jadi persentil ke-20 adalah 15. Kasus \\(q=0.50\\) sesuai dengan panel kanan, jadi mediannya adalah angka antara 20 dan 23 inklusif. Banyak paket perangkat lunak menggunakan rata-rata 21,5 (misalnya R, seperti yang terlihat di bawah). Untuk persentil ke-95, solusinya adalah 30. Kita dapat melihat dari Gambar 4.1 bahwa 30 juga sesuai dengan persentil ke-99 dan ke-99,99. quantile(xExample, probs=c(0.2, 0.5, 0.95), type=6) Dengan mengambil rata-rata tertimbang antara pengamatan data, kuantil empiris yang dihaluskan dapat menangani kasus seperti panel kanan pada Gambar 4.3 . Itu Q kuantil empiris yang dihaluskan didefinisikan sebagai \\[\\hat{\\pi}_q = (1-h) X_{(j)} + h X_{(j+1)}\\] Di mana \\(j=\\lfloor(n+1)q\\rfloor\\) , Dan\\(X_{(1)}, \\ldots, X_{(n)}\\) adalah nilai yang diurutkan (dikenal sebagai statistik urutan ) yang sesuai dengan \\(X_1, \\ldots, X_n\\). (Ingat bahwa tanda kurung    adalah fungsi lantai yang menunjukkan nilai bilangan bulat terbesar.) Perhatikan bah wa \\(\\hat{\\pi}_q\\)$ hanyalah sebuah interpolasi linear antara \\(X_{( j )}\\) dan \\(X_{(j+1)}\\). Contoh 4.1.3. Kumpulan Data Mainan: Lanjutan. Tentukan persentil yang dihaluskan ke-50 dan ke-20. Solusi Ambil \\(n = 10\\) Dan \\(q= 0,5\\). Kemudian, \\(j=\\lfloor(11)(0.5) \\rfloor= \\lfloor 5.5 \\rfloor=5\\), . Maka kuantil empiris yang dihaluskan ke-0,5 adalah \\[\\hat{\\pi}_{0.5} = (1-0.5) X_{(5)} + (0.5) X_{(6)} = 0.5 (20) + (0.5)(23) = 21.5.\\] Sekarang ambil \\(n = 10\\) Dan \\(q= 0,2\\) . Pada kasus ini, \\(j=\\lfloor(11)(0.2)\\rfloor=\\lfloor 2.2 \\rfloor=2\\) . Maka kuantil empiris yang dihaluskan ke-0,2 adalah \\[\\hat{\\pi}_{0.2} = (1-0.2) X_{(2)} + (0.2) X_{(3)} = 0.8 (15) + (0.2)(15) = 15.\\] 4.1.1.4 Penduga Kepadatan Variabel Diskrit. Ketika variabel acak adalah diskrit, memperkirakan fungsi massa probabilitas \\(f(x) = \\Pr(X=x)\\) mudah. Kami hanya menggunakan rata-rata sampel, yang didefinisikan sebagai \\[f_n(x) = \\frac{1}{n} \\sum_{i=1}^n I(X_i = x),\\] yang merupakan proporsi sampel sama dengan X Variabel Berkelanjutan dalam Grup. Untuk variabel acak kontinu, pertimbangkan formulasi diskrit di mana domain dari F(  ) dipartisi oleh konstanta \\(\\{c_0 &lt; c_1 &lt; \\cdots &lt; c_k\\}\\) ke dalam interval bentuk \\([c_{j-1}, c_j)\\) , untuk \\(j=1, \\ldots, k\\) . Pengamatan data dengan demikian dikelompokkan berdasarkan interval di mana mereka jatuh. Kemudian, kita dapat menggunakan definisi dasar dari fungsi massa empiris, atau variasi seperti \\[f_n(x) = \\frac{n_j}{n \\times (c_j - c_{j-1})} \\ \\ \\ \\ \\ \\ c_{j-1} \\le x &lt; c_j,\\] Di mana \\(N_J\\) adalah jumlah pengamatan ( \\(X_i\\) ) yang termasuk dalam interval \\([c_{j-1}, c_j)\\). Variabel Berkelanjutan (tidak dikelompokkan). Memperluas gagasan ini ke contoh di mana kami mengamati data individual, perhatikan bahwa kami selalu dapat membuat pengelompokan arbitrer dan menggunakan rumus ini. Lebih formal, biarkan \\(b &gt; 0\\) menjadi konstanta positif kecil, yang dikenal sebagai bandwidth , dan menentukan penaksir kepadatan menjadi \\[\\begin{equation} f_n(x) = \\frac{1}{2nb} \\sum_{i=1}^n I(x-b &lt; X_i \\le x + b) \\tag{4.2} \\end{equation}\\] Secara lebih umum, tentukan penaksir kerapatan kernel dari pdf di X sebagai \\[\\begin{equation} f_n(x) = \\frac{1}{nb} \\sum_{i=1}^n w\\left(\\frac{x-X_i}{b}\\right) , \\tag{4.3} \\end{equation}\\] Di mana w adalah fungsi kerapatan probabilitas yang berpusat di sekitar 0. Perhatikan bahwa persamaan (4.2) adalah kasus khusus penduga kerapatan kernel di mana \\(w(x) = \\frac{1}{2}I(-1 &lt; x \\le 1)\\) , juga dikenal sebagai kernel seragam . Pilihan populer lainnya ditunjukkan pada Tabel 4.1 . \\[{\\small \\begin{matrix} \\begin{array}{l|cc} \\hline \\text{Kernel} &amp; w(x) \\\\ \\hline \\text{Uniform } &amp; \\frac{1}{2}I(-1 &lt; x \\le 1) \\\\ \\text{Triangle} &amp; (1-|x|)\\times I(|x| \\le 1) \\\\ \\text{Epanechnikov} &amp; \\frac{3}{4}(1-x^2) \\times I(|x| \\le 1) \\\\ \\text{Gaussian} &amp; \\phi(x) \\\\ \\hline \\end{array}\\end{matrix} }\\] Di Sini, \\(\\phi(\\cdot)\\) adalah fungsi kepadatan normal standar. Seperti yang akan kita lihat pada contoh berikut, pilihan bandwidth \\(B\\) hadir dengan tradeoff bias-varians antara mencocokkan fitur distribusi lokal dan mengurangi volatilitas. Contoh 4.1.4. Dana Properti. Gambar 4.4 menunjukkan histogram (dengan persegi panjang abu-abu yang diarsir) dari klaim properti logaritmik dari tahun 2010. Kurva tebal (biru) mewakili kerapatan kernel Gaussian di mana bandwidth dipilih secara otomatis menggunakan aturan ad hoc berdasarkan ukuran sampel dan volatilitas data ini . Untuk dataset ini, bandwidth ternyata b = 0,3255 . Sebagai perbandingan, kurva putus-putus (merah) menunjukkan penaksir densitas dengan lebar pita sama dengan 0,1 dan kurva halus berwarna hijau menggunakan lebar pita 1. Sebagaimana diantisipasi, lebar pita yang lebih kecil (0,1) menunjukkan mengambil rata-rata lokal dengan data yang lebih sedikit sehingga kita mendapatkan ide yang lebih baik dari rata-rata lokal, tetapi dengan harga volatilitas yang lebih tinggi. Sebaliknya, bandwidth yang lebih besar (1) memperhalus fluktuasi lokal, menghasilkan kurva yang lebih halus yang mungkin melewatkan gangguan pada rata-rata lokal. Untuk aplikasi aktuaria, kami terutama menggunakan estimator densitas kernel untuk mendapatkan kesan visual cepat dari data. Dari perspektif ini, Anda cukup menggunakan aturan ad hoc default untuk pemilihan bandwidth, mengetahui bahwa Anda memiliki kemampuan untuk mengubahnya tergantung pada situasi yang dihadapi. ClaimLev &lt;- read.csv(&quot;Data/CLAIMLEVEL.csv&quot;, header=TRUE); #nrow(ClaimLev); # 6258 ClaimData&lt;-subset(ClaimLev,Year==2010); #2010 subset #Density Comparison hist(log(ClaimData$Claim), main=&quot;&quot;, ylim=c(0,.35),xlab=&quot;Log Expenditures&quot;, freq=FALSE, col=&quot;lightgray&quot;) lines(density(log(ClaimData$Claim)), col=&quot;blue&quot;,lwd=2.5) lines(density(log(ClaimData$Claim), bw=1), col=&quot;green&quot;) lines(density(log(ClaimData$Claim), bw=.1), col=&quot;red&quot;, lty=3) legend(&quot;topright&quot;, c(&quot;b=0.3255 (default)&quot;, &quot;b=0.1&quot;, &quot;b=1.0&quot;), lty=c(1,3,1), lwd=c(2.5,1,1), col=c(&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;), cex=1) #density(log(ClaimData$Claim))$bw ##default bandwidth Estimator densitas nonparametrik, seperti estimator kernel, sering digunakan dalam praktik. Konsep ini juga dapat diperluas untuk memberikan versi halus dari fungsi distribusi empiris. Mengingat definisi penaksir densitas kernel, penaksir kernel dari fungsi distribusi dapat ditemukan sebagai \\[\\begin{aligned} \\tilde{F}_n(x) = \\frac{1}{n} \\sum_{i=1}^n W\\left(\\frac{x-X_i}{b}\\right).\\end{aligned}\\] Di mana \\(W\\) adalah fungsi distribusi yang terkait dengan densitas kernel \\(w\\) . Sebagai ilustrasi, untuk kernel yang seragam, kita punya \\(w(y) = \\frac{1}{2}I(-1 &lt; y \\le 1)\\) , Jadi \\[\\begin{aligned} W(y) = \\begin{cases} 0 &amp; y&lt;-1\\\\ \\frac{y+1}{2}&amp; -1 \\le y &lt; 1 \\\\ 1 &amp; y \\ge 1 \\\\ \\end{cases}\\end{aligned} .\\] Contoh 4.1.5. Soal Ujian Aktuaria. Anda mempelajari lima nyawa untuk memperkirakan waktu dari timbulnya penyakit hingga kematian. Waktu kematian adalah: \\[\\begin{array}{ccccc} 2 &amp; 3 &amp; 3 &amp; 3 &amp; 7 \\\\ \\end{array}\\] Menggunakan kernel segitiga dengan bandwidth 2 , hitung taksiran fungsi densitas pada 2,5. Solusi. Untuk perkiraan kepadatan kernel, kami punya \\[f_n(x) = \\frac{1}{nb} \\sum_{i=1}^n w\\left(\\frac{x-X_i}{b}\\right),\\] Di mana \\(n = 5\\) , \\(b = 2\\) , Dan \\(x = 2,5\\) . Untuk inti segitiga, \\(w(x) = (1-|x|)\\times I(|x| \\le 1)\\) . Dengan demikian, \\[\\begin{array}{c|c|c} \\hline X_i &amp; \\frac{x-X_i}{b} &amp; w\\left(\\frac{x-X_i}{b} \\right) \\\\ \\hline 2 &amp; \\frac{2.5-2}{2}=\\frac{1}{4} &amp; (1-\\frac{1}{4})(1) = \\frac{3}{4} \\\\ \\hline 3 &amp; &amp; \\\\ 3 &amp; \\frac{2.5-3}{2}=\\frac{-1}{4} &amp; \\left(1-\\left| \\frac{-1}{4} \\right| \\right)(1) = \\frac{3}{4} \\\\ 3 &amp; &amp; \\\\ \\hline 7 &amp; \\frac{2.5-7}{2}=-2.25 &amp; (1-|-2.25|)(0) = 0\\\\ \\hline \\end{array}\\] Kemudian perkiraan densitas kernel di \\(x = 2,5\\) adalah \\[f_n(2.5) = \\frac{1}{5(2)}\\left( \\frac{3}{4} + (3) \\frac{3}{4} + 0 \\right) = \\frac{3}{10}\\] 4.1.1.5 Plug-in Principle Salah satu cara untuk membuat penaksir nonparametrik dari beberapa kuantitas adalah dengan menggunakan prinsip analog atau plug-in di mana seseorang menggantikan cdf yang tidak diketahui \\(F\\) dengan estimasi yang diketahui seperti cdf empiris \\(F_N\\) . Jadi, jika kita mencoba memperkirakan \\(\\mathrm{E}~[\\mathrm{g}(X)]=\\mathrm{E}_F~[\\mathrm{g}(X)]\\) untuk fungsi generik g , maka kami mendefinisikan estimator nonparametrik menjadi \\(\\mathrm{E}_{F_n}~[\\mathrm{g}(X)]=n^{-1}\\sum_{i=1}^n \\mathrm{g}(X_i)\\). Untuk melihat cara kerjanya, sebagai kasus khusus dari g , kami menganggap kerugian per variabel acak pembayaran \\(Y = (X-d)_+\\) dan rasio eliminasi kerugian yang diperkenalkan di Bagian 3.4.1. Kita dapat mengungkapkan ini sebagai \\[LER(d) = \\frac{\\mathrm{E~}[X - (X-d)_+]}{\\mathrm{E~}[X]} =\\frac{\\mathrm{E~}[\\min(X,d)]}{\\mathrm{E~}[X]} ,\\] Contoh. 4.1.6. Klaim Cidera Tubuh dan Rasio Penghapusan Kerugian Kami menggunakan sampel 432 klaim mobil tertutup dari Boston dari Derrig, Ostaszewski, dan Rempala ( 2001 ) . Kerugian dicatat untuk pembayaran karena cedera tubuh dalam kecelakaan mobil. Kerugian tidak dapat dikurangkan tetapi dibatasi oleh berbagai jumlah pertanggungan maksimum yang juga tersedia dalam data. Ternyata hanya 17 dari 432 (  4%) tunduk pada batasan kebijakan ini sehingga kami mengabaikan data ini untuk ilustrasi ini. Kerugian rata-rata yang dibayarkan adalah 6906 dalam dolar AS. Gambar 4.5 menunjukkan aspek lain dari distribusi. Secara khusus, panel sebelah kiri menunjukkan fungsi distribusi empiris, panel sebelah kanan memberikan plot kepadatan nonparametrik. Dampak kerugian cedera tubuh dapat dikurangi dengan pengenaan limit atau pembelian polis reasuransi (lihat Bagian 10.3). Untuk mengukur dampak dari alat mitigasi risiko ini, biasanya menghitung rasio eliminasi kerugian (LER) seperti yang diperkenalkan di Bagian 3.4.1. Fungsi distribusi tidak tersedia sehingga harus diestimasi dengan cara tertentu. Menggunakan prinsip plug-in, estimator nonparametrik dapat didefinisikan sebagai \\[LER_n(d) = \\frac{n^{-1} \\sum_{i=1}^n \\min(X_i,d)}{n^{-1} \\sum_{i=1}^n X_i} = \\frac{\\sum_{i=1}^n \\min(X_i,d)}{\\sum_{i=1}^n X_i} .\\] Gambar 4.6 menunjukkan estimator \\(LER_n(d)\\) untuk berbagai pilihan \\(d\\) . Misalnya, di \\(d= 1.000\\) dan punya \\(LER_n( 1000 )  0,1442\\). Dengan demikian, memberlakukan batas 1.000 berarti ekspektasi klaim yang ditahan 14,42 persen lebih rendah bila dibandingkan dengan ekspektasi klaim dengan deductible nol. 4.1.2 Tools for Model Selection and Diagnostics Bagian sebelumnya memperkenalkan estimator nonparametrik di mana tidak ada bentuk parametrik yang diasumsikan tentang distribusi yang mendasarinya. Namun, dalam banyak aplikasi aktuaria, analis berusaha menggunakan kecocokan parametrik dari distribusi untuk kemudahan penjelasan dan kemampuan untuk memperluasnya ke situasi yang lebih kompleks seperti memasukkan variabel penjelas dalam pengaturan regresi. Saat memasang distribusi parametrik, seorang analis mungkin mencoba menggunakan distribusi gamma untuk mewakili sekumpulan data kerugian. Namun, analis lain mungkin lebih suka menggunakan distribusi Pareto. Bagaimana cara menentukan model mana yang akan dipilih? Alat nonparametrik dapat digunakan untuk menguatkan pemilihan model parametrik. Pada dasarnya, pendekatannya adalah untuk menghitung langkah-langkah ringkasan yang dipilih di bawah model parametrik yang dipasang dan membandingkannya dengan kuantitas yang sesuai di bawah model nonparametrik. Karena model nonparametrik tidak mengasumsikan distribusi tertentu dan hanya merupakan fungsi dari data, model ini digunakan sebagai tolok ukur untuk menilai seberapa baik distribusi/model parametrik mewakili data. Juga, ketika ukuran sampel meningkat, distribusi empiris hampir pasti menyatu dengan distribusi populasi yang mendasarinya (berdasarkan hukum jumlah besar yang kuat). Dengan demikian distribusi empiris adalah proksi yang baik untuk populasi. Perbandingan estimator parametrik dengan nonparametrik dapat mengingatkan analis akan kekurangan dalam model parametrik dan terkadang menunjukkan cara untuk meningkatkan spesifikasi parametrik. Prosedur diarahkan menilai validitas model yang dikenal sebagaidiagnostik model . 4.1.2.1 Perbandingan Grafik Distribusi Kita telah melihat teknik overlay grafik untuk tujuan perbandingan. Untuk memperkuat penerapan teknik ini, Gambar 4.7membandingkan distribusi empiris dengan dua distribusi pas parametrik. Panel kiri menunjukkan fungsi distribusi distribusi klaim. Titik-titik yang membentuk kurva berbentuk S mewakili fungsi distribusi empiris pada setiap pengamatan. Kurva biru tebal memberikan nilai yang sesuai untuk distribusi gamma yang pas dan ungu muda untuk distribusi Pareto yang pas. Karena Pareto lebih dekat dengan fungsi distribusi empiris daripada gamma, ini memberikan bukti bahwa Pareto adalah model yang lebih baik untuk kumpulan data ini. Panel kanan memberikan informasi serupa untuk fungsi kerapatan dan memberikan pesan yang konsisten. Berdasarkan (hanya) angka-angka ini, distribusi Pareto adalah pilihan yang jelas bagi analis. Untuk cara lain untuk membandingkan kesesuaian dua model yang cocok, pertimbangkan plot probabilitas-probabilitas (\\(pp\\)) . A \\[pp\\] plot membandingkan probabilitas kumulatif di bawah dua model. Untuk tujuan kami, kedua model ini adalah fungsi distribusi empiris nonparametrik dan model pas parametrik. Gambar 4.8 menunjukkan \\(pp\\) plot untuk data Dana Properti yang diperkenalkan di Bagian 1.3 . Gamma yang dipasang di sebelah kiri dan Pareto yang dipasang di sebelah kanan, dibandingkan dengan fungsi distribusi data empiris yang sama. Garis lurus mewakili kesetaraan antara dua distribusi yang dibandingkan, sehingga titik yang dekat dengan garis diinginkan. Seperti yang terlihat pada demonstrasi sebelumnya, Pareto jauh lebih dekat dengan distribusi empiris daripada gamma, memberikan bukti tambahan bahwa Pareto adalah model yang lebih baik. Itu QQ plot membandingkan dua model yang dipasang melalui kuantilnya. Seperti hal hal plot, kami membandingkan nonparametrik dengan model pas parametrik. Kuantil dapat dievaluasi pada setiap titik kumpulan data, atau pada kisi (misalnya, di 0 , 0,001 , 0,002 ,  , 0,999 , 1,000 ), tergantung aplikasinya. Pada Gambar 4.9 , untuk setiap titik pada kisi tersebut, sumbu horizontal menampilkan kuantil empiris dan sumbu vertikal menampilkan kuantil parametrik yang sesuai (gamma untuk dua panel atas, Pareto untuk dua panel bawah). Kuantil diplot pada skala asli di panel kiri dan pada skala log di panel kanan untuk memungkinkan kita melihat di mana kekurangan distribusi yang pas. Garis lurus mewakili kesetaraan antara distribusi empiris dan distribusi pas. Dari plot ini, kita sekali lagi melihat bahwa Pareto secara keseluruhan lebih cocok daripada gamma. Selain itu, panel kanan bawah menunjukkan bahwa distribusi Pareto bekerja dengan baik dengan klaim besar, tetapi memberikan kecocokan yang lebih buruk untuk klaim kecil. Contoh 4.1.7. Soal Ujian Aktuaria. Grafik di bawah ini menunjukkan \\(pp\\) plot distribusi pas dibandingkan dengan sampel. Solusi. Ekor dari distribusi yang pas terlalu tebal di sebelah kiri, terlalu tipis di sebelah kanan, dan distribusi yang pas memiliki probabilitas yang lebih kecil di sekitar median daripada sampel. Untuk melihat ini, ingat bahwa hal hal plot grafik distribusi kumulatif dari dua distribusi pada sumbunya (empiris pada sumbu x dan dipasang pada sumbu y dalam kasus ini). Untuk nilai kecil dari X , model yang dipasang memberikan probabilitas yang lebih besar untuk berada di bawah nilai itu daripada yang terjadi dalam sampel (mis F( x ) &gt;FN( x ) ). Ini menunjukkan bahwa model memiliki ekor kiri yang lebih berat daripada datanya. Untuk nilai besar dari X , model kembali memberikan probabilitas yang lebih besar untuk berada di bawah nilai itu dan dengan demikian lebih kecil kemungkinannya untuk berada di atas nilai itu (mis S( x ) &lt;SN( x ) ). Hal ini menunjukkan bahwa model memiliki ekor kanan yang lebih ringan dari pada data. Selain itu, saat kita mulai dari 0,4 hingga 0,6 pada sumbu horizontal (dengan demikian melihat 20% tengah data), hal hal plot meningkat dari sekitar 0,3 menjadi 0,4. Ini menunjukkan bahwa model hanya menempatkan sekitar 10% dari probabilitas dalam kisaran ini. 4.1.2.2 Graphical Comparison of Distributions Saat memilih model, akan sangat membantu untuk menampilkan tampilan grafis. Namun, untuk melaporkan hasil, melengkapi tampilan grafis dengan statistik terpilih yang meringkas kebaikan kesesuaian model dapat efektif. Tabel 4.2 menyediakan tiga statistik kebaikan yang umum digunakan . Dalam tabel ini, \\(F_N\\) adalah distribusi empiris, \\(F\\) adalah distribusi pas atau hipotesis, dan \\(F_i^* = F(x_i)\\) . \\[{\\small \\begin{matrix} \\begin{array}{l|cc} \\hline \\text{Statistic} &amp; \\text{Definition} &amp; \\text{Computational Expression} \\\\ \\hline \\text{Kolmogorov-} &amp; \\max_x |F_n(x) - F(x)| &amp; \\max(D^+, D^-) \\text{ where } \\\\ ~~~\\text{Smirnov} &amp;&amp; D^+ = \\max_{i=1, \\ldots, n} \\left|\\frac{i}{n} - F_i^*\\right| \\\\ &amp;&amp; D^- = \\max_{i=1, \\ldots, n} \\left| F_i^* - \\frac{i-1}{n} \\right| \\\\ \\text{Cramer-von Mises} &amp; n \\int (F_n(x) - F(x))^2 f(x) dx &amp; \\frac{1}{12n} + \\sum_{i=1}^n \\left(F_i^* - (2i-1)/n\\right)^2 \\\\ \\text{Anderson-Darling} &amp; n \\int \\frac{(F_n(x) - F(x))^2}{F(x)(1-F(x))} f(x) dx &amp; -n-\\frac{1}{n} \\sum_{i=1}^n (2i-1) \\log\\left(F_i^*(1-F_{n+1-i})\\right)^2 \\\\ \\hline \\end{array} \\\\ \\end{matrix} }\\] Statistik Kolmogorov-Smirnov adalah perbedaan absolut maksimum antara fungsi distribusi yang dipasang dan fungsi distribusi empiris. Alih-alih membandingkan perbedaan antara titik tunggal, statistik Cramer-von Mises mengintegrasikan perbedaan antara fungsi distribusi empiris dan pas pada seluruh rentang nilai. Statistik Anderson-Darling juga mengintegrasikan perbedaan ini pada rentang nilai, meskipun diboboti oleh kebalikan dari varian. Oleh karena itu lebih menekankan pada ekor distribusi (yaitu kapan \\(F( x )\\) atau \\(1-F(x)=S(x)\\) kecil). Contoh 4.1.8. Soal Ujian Aktuaria (dimodifikasi). Contoh pembayaran klaim adalah: \\[\\begin{array}{ccccc} 29 &amp; 64 &amp; 90 &amp; 135 &amp; 182 \\\\ \\end{array}\\] Bandingkan distribusi klaim empiris dengan distribusi eksponensial dengan rata-rata 100 dengan menghitung nilai statistik uji Kolmogorov-Smirnov. Solusi. Untuk distribusi eksponensial dengan rata-rata 100 , fungsi distribusi kumulatif adalah \\(F(x)=1-e^{-x/100}\\) . Dengan demikian, \\[\\begin{array}{ccccc} \\hline x &amp; F(x) &amp; F_n(x) &amp; F_n(x-) &amp; \\max(|F(x)-F_n(x)|,|F(x)-F_n(x-)|) \\\\ \\hline 29 &amp; 0.2517 &amp; 0.2 &amp; 0 &amp; \\max(0.0517, 0.2517) = 0.2517 \\\\ 64 &amp; 0.4727 &amp; 0.4 &amp; 0.2 &amp; \\max(0.0727, 0.2727) = 0.2727 \\\\ 90 &amp; 0.5934 &amp; 0.6 &amp; 0.4 &amp; \\max(0.0066, 0.1934) = 0.1934 \\\\ 135 &amp; 0.7408 &amp; 0.8 &amp; 0.6 &amp; \\max(0.0592, 0.1408) = 0.1408 \\\\ 182 &amp; 0.8380 &amp; 1 &amp; 0.8 &amp; \\max(0.1620, 0.0380) = 0.1620 \\\\ \\hline \\end{array}\\] Oleh karena itu, statistik uji Kolmogorov-Smirnov adalah \\[KS = \\max(0.2517, 0.2727, 0.1934, 0.1408, 0.1620) = 0.2727 .\\] 4.1.3 Starting Values Metode pencocokan momen dan persentil merupakan metode estimasi nonparametrik yang memberikan alternatif kemungkinan maksimum. Umumnya, kemungkinan maksimum adalah teknik yang lebih disukai karena menggunakan data secara lebih efisien. (Lihat Lampiran Bab 17 untuk definisi efisiensi yang tepat.) Namun, metode pencocokan momen dan persentil berguna karena lebih mudah diinterpretasikan dan karena itu memungkinkan aktuaris atau analis untuk menjelaskan prosedur kepada orang lain. Selain itu, prosedur estimasi numerik (misalnya jika dilakukan di R) untuk kemungkinan maksimum adalah iteratif dan membutuhkan nilai awal untuk memulai proses rekursif. Meskipun banyak masalah yang kuat untuk pemilihan nilai awal, untuk beberapa situasi kompleks, penting untuk memiliki nilai awal yang mendekati nilai optimal (tidak diketahui). Metode momen dan pencocokan persentil adalah teknik yang dapat menghasilkan perkiraan yang diinginkan tanpa investasi komputasi yang serius dan dengan demikian dapat digunakan sebagai nilai awal untuk menghitung kemungkinan maksimum. 4.1.3.1 Method of Moments Metode ini merupakan estimasi parameter populasi dengan pendekatan momen parametrik menggunakan momen sampel empiris. pada momen ini, momen distribusi parametrik menggunakan momen empiris atau nonparametrik kemudian dapat dipecahkan secara aljabar untuk estimasi parameter. Contoh 4.1.9. Dana Properti. Untuk dana properti 2010, ada \\(n = 1 , 377\\) klaim individu (dalam ribuan dolar) dengan \\[m_1 = \\frac{1}{n} \\sum_{i=1}^n X_i = 26.62259 \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ m_2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 = 136154.6 .\\] Sesuaikan parameter distribusi gamma dan Pareto menggunakan metode momen. Solusi. Agar sesuai dengan distribusi gamma, kami memiliki \\(\\mu_1 = \\alpha \\theta\\) Dan \\(\\mu_2^{\\prime} = \\alpha(\\alpha+1) \\theta^2\\) . Menyamakan keduanya menghasilkan metode penaksir momen, aljabar mudah menunjukkannya \\[\\alpha = \\frac{\\mu_1^2}{\\mu_2^{\\prime}-\\mu_1^2} \\ \\ \\ \\text{and} \\ \\ \\ \\theta = \\frac{\\mu_2^{\\prime}-\\mu_1^2}{\\mu_1}.\\] Jadi, metode penduga momen adalah \\[\\begin{aligned} \\hat{\\alpha} &amp;= \\frac{26.62259^2}{136154.6-26.62259^2} = 0.005232809 \\\\ \\hat{\\theta} &amp;= \\frac{136154.6-26.62259^2}{26.62259} = 5,087.629. \\end{aligned}\\] Sebagai perbandingan, nilai kemungkinan maksimum berubah menjadi \\(\\hat{\\alpha}_{MLE} = 0.2905959\\) Dan \\(\\hat{\\theta}_{MLE} = 91.61378\\) , jadi ada perbedaan besar antara dua prosedur estimasi. Ini adalah salah satu indikasi, seperti yang telah kita lihat sebelumnya, bahwa model gamma kurang cocok. Sebaliknya, sekarang asumsikan distribusi Pareto sehingga \\(\\mu_1 = \\theta/(\\alpha -1)\\) Dan \\(\\mu_2^{\\prime} = 2\\theta^2/((\\alpha-1)(\\alpha-2) )\\) . Perhatikan bahwa ungkapan ini untuk 2 hanya berlaku untuk  &gt; 2 . Pertunjukan aljabar yang mudah \\[\\alpha = 1+ \\frac{\\mu_2^{\\prime}}{\\mu_2^{\\prime}-\\mu_1^2} \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ \\ \\theta = (\\alpha-1)\\mu_1.\\] Jadi, metode penduga momen adalah \\[ \\begin{aligned} \\hat{\\alpha} &amp;= 1+ \\frac{136154.6}{136154.6-26,62259^2} = 2.005233 \\\\ \\hat{\\theta} &amp;= (2.005233-1) \\cdot 26.62259 = 26.7619 \\end{aligned}\\] Nilai kemungkinan maksimum berubah menjadi \\(\\hat{\\alpha}_{MLE} = 0.9990936\\) Dan \\(\\hat{\\theta}_{MLE} = 2.2821147\\) . Sangat menarik bahwa \\(\\hat{\\alpha}_{MLE}&lt;1\\) ; untuk distribusi Pareto, ingat itu \\( &lt; 1\\) berarti rata-ratanya tak terhingga. Ini adalah indikasi lain bahwa kumpulan data klaim properti adalah distribusi ekor panjang. Seperti contoh di atas, ada fleksibilitas dengan metode momen. Misalnya, kita dapat mencocokkan momen kedua dan ketiga alih-alih yang pertama dan kedua, menghasilkan estimator yang berbeda. Selain itu, tidak ada jaminan bahwa solusi akan ada untuk setiap masalah. Untuk data yang disensor atau terpotong, momen pencocokan dimungkinkan untuk beberapa masalah, tetapi secara umum, ini adalah skenario yang lebih sulit. Terakhir, untuk distribusi di mana momen tidak ada atau tidak terbatas, metode momen tidak tersedia. Sebagai alternatif, seseorang dapat menggunakan teknik pencocokan persentil. 4.1.3.2 Percentile Matching Di bawah pencocokan persentil , kami memperkirakan kuantil atau persentil dari distribusi parametrik menggunakan kuantil atau persentil empiris (nonparametrik) yang dijelaskan di Bagian 4.1.1.3 . Contoh 4.1.10. Dana Properti. Untuk dana properti 2010, kami mengilustrasikan pencocokan pada kuantil. Secara khusus, distribusi Pareto secara intuitif menyenangkan karena solusi bentuk tertutup untuk kuantil. Ingatlah bahwa fungsi distribusi untuk distribusi Pareto adalah \\[F(x) = 1 - \\left(\\frac{\\theta}{x+\\theta}\\right)^{\\alpha}.\\] Aljabar mudah menunjukkan bahwa kita dapat menyatakan kuantil sebagai \\[F^{-1}(q) = \\theta \\left( (1-q)^{-1/\\alpha} -1 \\right).\\] untuk sebagian kecil q , \\(0 &lt; q&lt; 1\\). Tentukan estimasi parameter distribusi Pareto menggunakan kuantil empiris ke-25 dan ke-95. Solusi. Persentil ke-25 (kuartil pertama) ternyata adalah 0,78853 dan persentil ke-95 adalah 50.98293 (keduanya dalam ribuan dolar). Dengan dua persamaan \\[0.78853 = \\theta \\left( 1- (1-.25)^{-1/\\alpha} \\right) \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ 50.98293 = \\theta \\left( 1- (1-.75)^{-1/\\alpha} \\right)\\] dan dua yang tidak diketahui, solusinya adalah \\[\\hat{\\alpha} = 0.9412076 \\ \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ \\hat{\\theta} = 2.205617 .\\] Sehingga kesimpulannya adalah rutin numerik diperlukan untuk solusi ini karena tidak ada solusi analitik yang tersedia. Selanjutnya, ingatlah perkiraan kemungkinan maksimumadalah ^ML E= 0,9990936 Dan ^ML E= 2,2821147 , sehingga pencocokan persentil memberikan perkiraan yang lebih baik untuk distribusi Pareto daripada metode momen. Contoh 4.1.11. Soal Ujian Aktuaria. Anda diberikan: Kerugian mengikuti distribusi loglogistik dengan fungsi distribusi kumulatif: \\[F(x) = \\frac{\\left(x/\\theta\\right)^{\\gamma}}{1+\\left(x/\\theta\\right)^{\\gamma}}\\] Contoh kerugiannya adalah: \\[\\begin{array}{ccccccccccc} 10 &amp;35 &amp;80 &amp;86 &amp;90 &amp;120 &amp;158 &amp;180 &amp;200 &amp;210 &amp;1500 \\\\ \\end{array}\\] Hitung estimasi dari \\(\\) dengan pencocokan persentil, menggunakan perkiraan persentil ke-40 dan ke-80 yang dihaluskan secara empiris. Solusi. Dengan 11 pengamatan, kami memiliki \\(j=\\lfloor(n+1)q\\rfloor = \\lfloor 12(0.4) \\rfloor = \\lfloor 4.8\\rfloor=4\\). Dengan interpolasi, perkiraan persentil ke-40 yang dihaluskan secara empiris adalah \\(\\hat{\\pi}_{0.4} = (1-h) X_{(j)} + h X_{(j+1)} = 0.2(86)+0.8(90)=89.2\\). Demikian pula, untuk perkiraan persentil yang dihaluskan secara empiris ke-80, kami memiliki \\(12 ( 0,8 ) = 9,6\\) jadi perkiraannya \\(\\hat{\\pi}_{0.8} = 0.4(200)+0.6(210)=206\\). Dengan menggunakan distribusi kumulatif loglogistik, kita perlu menyelesaikan dua persamaan berikut untuk parameter \\({\\hat{\\theta}}\\) Dan \\({\\hat{\\gamma}}\\) : \\[0.4=\\frac{(89.2/{\\hat{\\theta}})^{\\hat{\\gamma}}}{1+(89.2/{\\hat{\\theta}})^{\\hat{\\gamma}}} \\ \\ \\ \\text{and} \\ \\ \\ \\ 0.8=\\frac{(206/{\\hat{\\theta}})^{\\hat{\\gamma}}}{1+(206/{\\hat{\\theta}})^{\\hat{\\gamma}}} .\\] Pemecahan untuk setiap ekspresi kurung memberi \\(\\frac{2}{3}=(89.2/\\theta)^{\\hat{\\gamma}}\\) Dan \\(4=(206/{\\hat{\\theta}})^{\\hat{\\gamma}}\\) . Mengambil rasio persamaan kedua dengan yang pertama memberi \\(6=(206/89.2)^{\\hat{\\gamma}}\\Rightarrow {\\hat{\\gamma}}=\\frac{\\log(6)}{\\log(206/89.2)} = 2.1407\\). Kemudian \\(4^{1/2.1407}=206/{\\hat{\\theta}} \\Rightarrow {\\hat{\\theta}}=107.8\\). Seperti metode momen, pencocokan persentil hampir terlalu fleksibel dalam arti bahwa estimator dapat bervariasi tergantung pada persentil berbeda yang dipilih. Misalnya, seorang aktuaris dapat menggunakan estimasi pada persentil ke-25 dan ke-95 sedangkan yang lain menggunakan persentil ke-20 dan ke-80. Secara umum estimasi parameter akan berbeda dan tidak ada alasan kuat untuk memilih salah satu dari yang lain. Seperti halnya metode momen, pencocokan persentil menarik karena memberikan teknik yang dapat diterapkan dengan mudah dalam situasi tertentu dan memiliki dasar intuitif. Meskipun sebagian besar aplikasi aktuaria menggunakan estimator kemungkinan maksimum, akan lebih mudah untuk memiliki pendekatan alternatif seperti metode momen dan pencocokan persentil yang tersedia. 4.2 Model Selection Menjelaskan proses pemilihan model berdasarkan: dataset dalam sampel atau pelatihan, dataset out -of-sampel atau uji, dan metode yang menggabungkan pendekatan ini dikenal sebagai cross-validation . 4.2.1 Pemilihan Model Iteratif Dalam memeriksa data secara grafis, membuat hipotesis struktur model, dan membandingkan data dengan model kandidat untuk merumuskan model yang lebih baik. Box ( 1980 ) menggambarkan ini sebagai proses berulang yang ditunjukkan pada Gambar dibawah ini src=https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.2.1-1png?raw=true width=300 height=300 style=display: block; margin-left: auto; margin-right: auto; margin-top: 10px;&gt; Proses berulang ini memberikan resep yang berguna untuk menyusun tugas menentukan model untuk mewakili satu set data. Langkah pertama, tahap perumusan model, dilakukan dengan memeriksa data secara grafis dan menggunakan pengetahuan hubungan sebelumnya, seperti dari teori ekonomi atau praktik industri. Langkah kedua dalam iterasi adalah fitting berdasarkan asumsi model yang ditentukan. Asumsi ini harus konsisten dengan data untuk menggunakan model secara valid. Langkah ketiga adalah pemeriksaan diagnostik ; data dan model harus konsisten satu sama lain sebelum kesimpulan tambahan dapat dibuat. Pengecekan diagnostik adalah bagian penting dari formulasi model; itu dapat mengungkapkan kesalahan yang dilakukan pada langkah sebelumnya dan memberikan cara untuk memperbaiki kesalahan ini. 4.2.2 Model Selection Based on a Training Dataset Biasanya merujuk ke kumpulan data yang digunakan untuk analisis sebagai kumpulan data dalam sampel atau pelatihan . Teknik yang tersedia untuk memilih model tergantung pada apakah hasilnya X diskrit, kontinu, atau campuran dari keduanya, meskipun prinsipnya sama. Grafik dan Tindakan Ringkasan Dasar lainnya. Mulailah dengan meringkas data secara grafis dan dengan statistik yang tidak bergantung pada bentuk parametrik tertentu. Tes Rasio Kemungkinan. Untuk membandingkan kecocokan model, jika satu model merupakan bagian dari model lainnya, maka uji rasio kemungkinan dapat digunakan; pendekatan umum untuk pengujian rasio kemungkinan Kebaikan Statistik Fit. Secara umum, model bukan himpunan bagian yang tepat satu sama lain sehingga statistik kecocokan secara keseluruhan sangat membantu untuk membandingkan model. Kriteria informasi adalah salah satu jenis kebaikan statistik. Untuk memilih distribusi yang sesuai, statistik yang membandingkan kecocokan parametrik dengan alternatif nonparametrik. 4.2.3 Model Selection Based on a Test Dataset Validasi model adalah proses konfirmasi bahwa model yang diusulkan sesuai, terutama mengingat tujuan penyelidikan. Keterbatasan penting dari proses pemilihan model hanya berdasarkan data dalam sampel adalah bahwa hal itu dapat rentan terhadap data-snooping , yaitu menyesuaikan sejumlah besar model ke satu set data. Memilih model hanya berdasarkan data dalam sampel juga tidak mendukung tujuan inferensi prediktif . 4.2.4 Model Selection Based on Cross-Validation Meskipun validasi out-of-sample adalah standar emas dalam pemodelan prediktif, tidak selalu praktis untuk melakukannya. Alasan utamanya adalah kita memiliki ukuran sampel yang terbatas dan kriteria pemilihan model di luar sampel dalam persamaan (4.4) bergantung pada pemisahan data secara acak . Ini berarti bahwa analis yang berbeda, bahkan ketika mengerjakan kumpulan data yang sama dan pendekatan pemodelan yang sama, dapat memilih model yang berbeda. Prosedur Validasi Silang. Sebagai alternatif, seseorang dapat menggunakan cross-validation , sebagai berikut. Prosedur dimulai dengan menggunakan mekanisme acak untuk membagi data menjadi K himpunan bagian dengan ukuran yang kira-kira sama yang dikenal sebagai lipatan , di mana analis biasanya menggunakan 5 hingga 10. Selanjutnya, yang satu menggunakan yang pertama K-1 subsampel untuk memperkirakan parameter model. Kemudian, prediksi hasil untuk K th subsampel dan gunakan ukuran seperti pada persamaan (4.4) untuk meringkas kecocokan. Sekarang, ulangi ini dengan menahan masing-masing K subsampel, meringkas dengan statistik out-of-sample. Jadi, rangkumlah ini K statistik, biasanya dengan rata-rata, untuk memberikan satu statistik keseluruhan untuk tujuan perbandingan. Ulangi langkah-langkah ini untuk beberapa model kandidat dan pilih model dengan statistik validasi silang terendah secara keseluruhan. 4.3 Estimasi Menggunakan Data Modifikasi Penjelasan pada subbab ini: Mendeskripsikan data yang dikelompokkan, disensor, dan terpotong Perkirakan distribusi parametrik berdasarkan data yang dikelompokkan, disensor, dan terpotong Perkirakan distribusi secara nonparametrik berdasarkan data yang dikelompokkan, disensor, dan terpotong 4.3.1 Estimasi Parametrik menggunakan Data Modifikasi Seperti yang kita ketahui bahwa Estimasi parametrik bersifat kuantitatif dan menggunakan statistik untuk menghitung perkiraan jumlah sumber daya yang dibutuhkan untuk menyelesaikan proyek Anda, baik itu biaya atau waktu, atau bahkan sumber daya manusia. Bagian 3.5 memperkenalkan konsep observasi yang  dimodifikasi  karena dua jenis batasan umum: penyensoran dan pemotongan. Misalnya, adalah umum untuk berpikir tentang asuransi yang dapat dikurangkan sebagai menghasilkan data yang terpotong (dari kiri) atau batasan polis sebagai menghasilkan data yang disensor (dari kanan). Sudut pandang ini dari perusahaan asuransi utama (penjual asuransi). Secara khusus, bagian ini akan membahas metode estimasi parametrik untuk tiga alternatif data individual, lengkap, dan tidak dimodifikasi: data dengan sensor interval hanya tersedia dalam kelompok, data yang terbatas ataudisensor , dan data yang tidak dapat diamati karena pemotongan . 4.3.1.1 Estimasi Parametrik menggunakan Data yang Dikelompokkan Pertimbangkan sampel ukuran N diamati dari distribusinya \\(F(  )\\), tetapi dalam kelompok sehingga kita hanya mengetahui kelompok tempat setiap pengamatan jatuh, bukan nilai pastinya. Ini disebut sebagai data yang dikelompokkan atau disensor interval . Memformalkan ide ini, misalkan ada k kelompok atau interval yang dibatasi oleh batas \\(C_0&lt;C_1&lt;  &lt;C_k.\\) Untuk setiap pengamatan, kami hanya mengamati interval jatuhnya \\(((C_{j  1},C_J))\\), bukan nilai yang tepat. Dengan demikian, kita hanya mengetahui jumlah observasi pada setiap interval. Konstanta \\({C_0&lt;C_1&lt;  &lt;C_k}\\) membentuk beberapa partisi dari domain \\(F(  )\\). Kemudian probabilitas pengamatan \\(X_i\\) jatuh di \\(J\\)th interval ke- adalah \\[ Pr(X_i \\in (c_{j-1},c_j])=F(c_j)-F(c_{j-1}) \\] Fungsi massa probabilitas yang sesuai untuk pengamatan adalah src=https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.3.1-1.png?raw=true width=300 height=300 style=display: block; margin-left: auto; margin-right: auto; margin-top: 10px;&gt; Sekarang, tentukan $N_J$ menjadi jumlah pengamatan yang termasuk dalam $J$th interval, $(C_{j  1},C_J]$. Jadi, fungsi kemungkinan (sehubungan dengan parameter) $$) adalah src=&quot;https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.3.1-2.png?raw=true&quot; width=&quot;300&quot; height=&quot;300&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; margin-top: 10px;&quot;&gt; Dan fungsi log-kemungkinan adalah src=&quot;https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.3.1-3png?raw=true&quot; width=&quot;300&quot; height=&quot;300&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; margin-top: 10px;&quot;&gt; Diberikan data : 1. Kerugian mengikuti distribusi eksponensial dengan rata-rata $$. 2. Sebuah sampel acak dari 20 kerugian didistribusikan sebagai berikut: src=&quot;https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/example4.3-1.png?raw=true&quot; width=&quot;300&quot; height=&quot;300&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; margin-top: 10px;&quot;&gt; Hitung estimasi kemungkinan maksimum dari $$ $$ \\begin{aligned} L(\\theta) &amp;= F(1000)^7[F(2000)-F(1000)]^6[1-F(2000)]^7 \\\\ &amp;= (1-e^{-1000/\\theta})^7(e^{-1000/\\theta} - e^{-2000/\\theta})^6(e^{-2000/\\theta})^7 \\\\ &amp;= (1-p)^7(p-p^2)^6(p^2)^7 \\\\ &amp;= p^{20}(1-p)^{13} \\end{aligned} $$ di mana $p = e^{-1000/}$. Memaksimalkan ekspresi ini sehubungan dengan $p$ setara dengan memaksimalkan kemungkinan terhadap $$. Maksimum terjadi pada $p=\\frac{20}{33}$. sehingga $\\hat{\\theta}=\\frac{-1000}{\\log(20/33)}= 1996.90$ #### Cencored Data Penyensoran terjadi ketika kita hanya mencatat nilai yang terbatas dari sebuah observasi. Bentuk yang paling umum adalah penyensoran kanan, di mana kita mencatat nilai yang lebih kecil dari variabel dependen &quot;benar&quot; dan nilai penyensoran. Dengan menggunakan notasi, dengan `X` mewakili hasil yang diminati, seperti kerugian akibat kejadian yang diasuransikan atau waktu hingga kejadian. Dengan $C_U$ menyatakan jumlah penyensoran. Dengan pengamatan tersensor kanan, mencatat $X_U^* = min(X, C_U) = XC_U$. Lalu juga mencatat apakah penyensoran telah terjadi atau tidak. $_U = I(XC_U)$ adalah variabel biner yang bernilai 0 jika penyensoran terjadi dan 1 jika tidak, yaitu, $_U$ menunjukkan apakah X tidak disensor atau tidak. Sebagai contoh $C_U$ dapat merepresentasikan batas atas pertanggungan sebuah polis asuransi. Kerugian dapat melebihi jumlah $C_U$ tetapi perusahaan asuransi hanya memiliki $C_U$ dalam catatannya sebagai jumlah yang dibayarkan dan tidak memiliki jumlah kerugian aktual $X$ dalam catatannya. Sama halnya dengan penyensoran kiri, dapat mencatat yang lebih besar dari variabel yang diminati dan variabel yang disensor. Jika $C_L$ digunakan untuk merepresentasikan jumlah penyensoran, maka mencatat $X_L^*=max(X,C_L)$ bersama dengan indikator penyensoran $_L=I(X&gt;C_L)$. Sebagai contoh, reasuradur akan menanggung kerugian penanggung yang lebih besar dari $C_L$ ini berarti reasuradur bertanggung jawab atas kelebihan $X_L^*$ pada $C_L$. Dengan menggunakan notasi, kerugian reasuradur adalah $Y = X_L^*L-C_L$ Untuk melihat hal ini, pertama-tama pertimbangkan kasus di mana pemegang polis mengalami kerugian $X &lt; C_L$. Kemudian, penanggung akan membayar seluruh klaim dan $Y=C_L-C_L=0$ tidak ada kerugian bagi reasuradur. Sebaliknya, jika kerugian $XC_L$ maka $Y = X-C_L$ merupakan klaim yang ditahan oleh reasuradur. Dengan kata lain, jika terjadi kerugian, reasuradur mencatat jumlah sebenarnya jika melebihi batas $C_L$ dan jika tidak, hanya mencatat akan mengalami kerugian sebesar 0. #### Truncated data Pengamatan yang disensor dicatat untuk studi, meskipun dalam bentuk yang terbatas. Sebaliknya, hasil yang terpotong adalah jenis data yang hilang. Sebuah hasil berpotensi terpotong ketika ketersediaan pengamatan bergantung pada hasil. Dalam asuransi, biasanya pengamatan terpotong kiri pada $C_L$ ketika jumlahnya adalah $$ \\begin{aligned} Y &amp;= \\left\\{ \\begin{array}{cl} \\text{we do not observe }X &amp; X \\le C_L \\\\ X &amp; X &gt; C_L \\end{array} \\right.\\end{aligned} $$ Dengan kata lain, jika X kurang dari ambang batas $C_L$ maka ia tidak teramati. $C_L$ dapat merepresentasikan deductible dari sebuah polis asuransi. Jika kerugian yang diasuransikan kurang dari deductible, maka perusahaan asuransi mungkin tidak mengamati atau mencatat kerugian sama sekali. Jika kerugian melebihi deductible, maka kelebihan $X-C_L$ adalah klaim yang ditanggung oleh penanggung. Dimana dapat didefinisikan kerugian per pembayaran sebagai $$ \\begin{aligned} Y^{P} = \\left\\{ \\begin{matrix} \\text{Undefined} &amp; X \\le d \\\\ X - d &amp; X &gt; d \\end{matrix} \\right. \\end{aligned} $$ sehingga jika kerugian melebihi deductible, kami mencatat jumlah kelebihan $X-d$. Hal ini sangat penting ketika mempertimbangkan jumlah yang akan dibayarkan oleh perusahaan asuransi. Namun, untuk tujuan estimasi pada bagian ini, tidak terlalu penting jika kita mengurangkan konstanta yang diketahui seperti $C_L = d$. Sehingga, untuk variabel terpotong $Y$ kita menggunakan konvensi yang lebih sederhana dan tidak mengurangkan $d$. Demikian pula untuk data terpotong kanan, jika X melebihi ambang batas $C_U$ maka data tersebut tidak diobservasi. Dalam hal ini, jumlahnya adalah $$ \\begin{aligned} Y &amp;= \\left\\{ \\begin{array}{cl} X &amp; X \\le C_U \\\\ \\text{we do not observe }X &amp; X &gt; C_U. \\end{array} \\right.\\end{aligned} $$ Contoh klasik dari pemotongan dari kanan termasuk X sebagai ukuran jarak ke bintang. Ketika jaraknya melebihi tingkat tertentu $C_U$ maka bintang tersebut tidak lagi dapat diamati. Gambar dibawah ini membandingkan pengamatan yang terpotong dan tersensor. Nilai-nilai X yang lebih besar dari batas penyensoran &quot;atas&quot; $C_U$ tidak teramati sama sekali (tersensor kanan), sedangkan nilai X yang lebih kecil dari batas pemotongan &quot;bawah&quot; $C_L$ tetap diamati, tetapi diamati sebagai $C_L$ daripada nilai X yang sebenarnya (tersensor kiri). #### Parametric Estimation using Cencored and Truncated data Untuk mempermudah, dapat diasumsikan jumlah penyensoran tidak acak dan hasil yang kontinu X . Sebagai permulaan, pertimbangkan kasus data tersensor kanan di mana merekam $X_U^* = min(X, C_U) = XC_U$) dan indikator penyensoran $ = I(XC_U)$ . Jika penyensoran terjadi sehingga $=0$ maka $X&gt;C_U$ dan peluangnya adalah $Pr(X&gt;C_U)=1-F(C_U)$. Jika penyensoran tidak terjadi sehingga $ = 1$ maka $XC_U$ dan likelihoodnya adalah $f(x)$ . Ringkasnya, didapatkan likelihood dari sebuah pengamatan tunggal sebagai $$ \\begin{aligned} \\left\\{ \\begin{array}{ll} 1-F(C_U) &amp; \\text{if }\\delta=0 \\\\ f(x) &amp; \\text{if } \\delta = 1 \\end{array} \\right. = \\left\\{ f(x)\\right\\}^{\\delta} \\left\\{1-F(C_U)\\right\\}^{1-\\delta} . \\end{aligned} $$ Ekspresi ruas kanan memungkinkan dalam menyajikan peluang dengan lebih ringkas. Sekarang, untuk sampel ke-i dengan ukuran n , peluangnya adalah $$ \\begin{aligned} L(\\theta) = \\prod_{i=1}^n \\left\\{ f(x_i)\\right\\}^{\\delta_i} \\left\\{1-F(C_{Ui})\\right\\}^{1-\\delta_i} = \\prod_{\\delta_i=1} f(x_i) \\prod_{\\delta_i=0} \\{1-F(C_{Ui})\\} \\end{aligned} $$ dengan waktu penyensoran potensial ${(C_{U1},...,C_{Un})}$ . Di sini, notasi &quot;${i} = 1$&quot; berarti mengambil hasil kali dari pengamatan yang tidak disensor, dan demikian pula untuk &quot;${i} = 0$ &quot; Di sisi lain, data terpotong ditangani dalam inferensi kemungkinan melalui probabilitas bersyarat. Secara khusus, kontribusi likelihood dapat disesuaikan dengan membaginya dengan probabilitas bahwa variabel tersebut diamati. Sebagai rangkuman, kami memiliki kontribusi berikut pada fungsi likelihood untuk enam jenis hasil: src=&quot;https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/table4-1.png?raw=true&quot; width=&quot;300&quot; height=&quot;300&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; margin-top: 10px;&quot;&gt; Untuk hasil yang diketahui dan data yang disensor, kemungkinannya adalah \\[ \\begin{aligned} L(\\theta) = \\prod_{E} f(x_i) \\prod_{R} \\{1-F(C_{Ui})\\} \\prod_{L} F(C_{Li}) \\prod_{I} (F(C_{Ui})-F(C_{Li})), \\end{aligned} \\] di mana \\(&quot;_E&quot;\\) adalah hasil kali pengamatan dengan nilai Exact, dan demikian pula untuk Right-,Left- and Interval-censoring. Untuk data yang disensor kanan dan terpotong kiri, kemungkinannya adalah \\[ \\begin{aligned} L(\\theta) = \\prod_{E} \\frac{f(x_i)}{1-F(C_{Li})} \\prod_{R} \\frac{1-F(C_{Ui})}{1-F(C_{Li})}, \\end{aligned} \\] dan juga untuk kombinasi lainnya. Example 4.3.2. Actuarial Exam Question Diberikan data : Sebuah contoh kerugian adalah: 600 700 900 Tidak ada informasi yang tersedia mengenai kerugian sebesar 500 atau kurang. Kerugian diasumsikan mengikuti distribusi eksponensial dengan rata-rata \\(\\). Hitung estimasi kemungkinan maksimum dari \\(\\) Pengamatan ini terpotong pada angka 500. Kontribusi dari setiap pengamatan terhadap fungsi likelihood adalah \\(\\frac{f(x)}{1-F(500)} = \\frac{\\theta^{-1}e^{-x/\\theta}}{e^{-500/\\theta}}\\) Lalu Fungsi Likelihoodnya adalah \\(L(\\theta)= \\frac{\\theta^{-1} e^{-600/\\theta} \\theta^{-1} e^{-700/\\theta} \\theta^{-1} e^{-900/\\theta}}{(e^{-500/\\theta})^3} = \\theta^{-3}e^{-700/\\theta}\\) Log-Likehoodnya adalah \\(l(\\theta) = \\log L(\\theta) = -3 \\log \\theta - 700 \\theta^{-1}\\) Memaksimalkan ekspresi ini dengan menetapkan turunan terhadap  sama dengan 0, Maka memiliki \\(L&#39;(\\theta) = -3 \\theta^{-1} + 700 \\theta^{-2} = 0 \\ \\Rightarrow \\ \\hat{\\theta} = \\frac{700}{3} = 233.33 .\\) 4.3.2 Nonparametric Estimation using Modified Data Estimator nonparametrik memberikan tolok ukur yang berguna, sehingga akan sangat membantu untuk memahami prosedur estimasi untuk data yang dikelompokkan, disensor, dan dipotong 4.3.2.1 Grouped Data Pengamatan dapat dikelompokkan (juga disebut sebagai interval tersensor) dalam arti bahwa pengamatan sebagai bagian dari salah satu dari k interval dalam bentuk \\((c_{j-1},c_j)\\) , untuk \\(j = 1,...,k\\) . Pada batas-batasnya, fungsi distribusi empiris didefinisikan dengan cara yang biasa: \\[ \\begin{aligned} F_n(c_j) = \\frac{\\text{number of observations } \\le c_j}{n} \\end{aligned} \\] Ogive Estimator Untuk nilai lain dari \\(x(c_{j-1},c_j)\\) dapat mengestimasi fungsi distribusi dengan ogive estimator yang menginterpolasi secara linear antara \\(F_n(c_{j-1})\\) dan \\(Fn_(c_j)\\) yaitu nilai dari batas-batas \\(F_n(c_{j-1})\\) dan \\(Fn_(c_j)\\) dihubungkan dengan sebuah garis lurus. Hal ini secara formal dapat dinyatakan sebagai \\[ \\begin{aligned} F_n(x) = \\frac{c_j-x}{c_j-c_{j-1}} F_n(c_{j-1}) + \\frac{x-c_{j-1}}{c_j-c_{j-1}} F_n(c_j) \\ \\ \\ \\text{for } c_{j-1} \\le x &lt; c_j \\end{aligned} \\] Sehinga Densitas yang sesuai adalah \\[ \\begin{aligned} f_n(x) = F^{\\prime}n(x) = \\frac{F_n(c_j)-F_n(c{j-1})}{c_j - c_{j-1}} \\ \\ \\ \\text{for } c_{j-1} &lt; x &lt; c_j . \\end{aligned} \\] Example 4.3.4. Actuarial Exam Question Diberikan informasi berikut ini mengenai jumlah klaim untuk 100 klaim: src=https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/example4.3.4-1.png?raw=true width=300 height=300 style=display: block; margin-left: auto; margin-right: auto; margin-top: 10px;&gt; Dengan menggunakan ogive, hitunglah estimasi probabilitas bahwa klaim yang dipilih secara acak adalah antara 2000 dan 6000. Pada batas-batasnya, fungsi distribusi empiris didefinisikan dengan cara yang biasa, sehingga memiliki \\(F_{100}(1000) = 0.16, \\ F_{100}(3000)=0.38, \\ F_{100}(5000)=0.63, \\ F_{100}(10000)=0.81\\) Untuk ukuran klaim lainnya, penaksir ogive melakukan interpolasi linier di antara nilai-nilai ini: \\[ \\begin{array}{ll} F_{100}(2000) &amp;= 0.5F_{100}(1000) + 0.5F_{100}(3000) = 0.5(0.16)+0.5(0.38)=0.27 \\\\ F_{100}(6000) &amp;=0.8F_{100}(5000)+0.2F_{100}(10000) = 0.8(0.63)+0.2(0.81)=0.666 \\end{array} \\] Dengan demikian, probabilitas klaim antara 2000 dan 6000 adalah \\(F_{100}(6000) - F_{100}(2000) = 0.666-0.27 = 0.396\\) 4.3.2.2 Right-Censored Empirical Distribution Function Akan sangat berguna untuk mengkalibrasi penaksir parametrik dengan metode nonparametrik yang tidak bergantung pada bentuk parametrik distribusi. Penaksir batas produk menurut (Kaplan dan Meier 1958) merupakan penaksir yang terkenal untuk fungsi distribusi dengan adanya penyensoran. Motivasi untuk Penaksir Batas Produk Kaplan-Meier Untuk menjelaskan mengapa product-limit bekerja dengan sangat baik dengan observasi tersensor, pertama-tama dapat melihat ke kasus tanpa penyensoran. Di sini, fungsi distribusi empiris \\(F_n(x)\\) adalah penaksir tak bias dari fungsi distribusi \\(F(x)\\) . Hal ini karena \\(F_n(x)\\) adalah rata-rata dari variabel indikator yang masing-masing tidak bias, yaitu, \\(E [I(X_ix)]=Pr(X_ix)=F(x)\\) Sekarang misalkan hasil acak disensor di sebelah kanan dengan jumlah yang membatasi, katakanlah, CU sehingga dapat mencatat yang lebih kecil dari keduanya, \\(X^* = min(X, C_U)\\) . Untuk nilai-nilai \\(x\\) yang lebih kecil dari \\(C_U\\), variabel indikator masih memberikan penaksir yang tidak bias terhadap fungsi distribusi sebelum kita mencapai batas penyensoran. Artinya, \\(E [I(X^x)]=F(x)\\) karena \\(I(X^x)=I(Xx)\\) untuk \\(x&lt;C_U\\) . Dengan cara yang sama, \\(E[I(X^&gt;x)]=1-F(x)=S(x)\\) . Tetapi, untuk \\(x&gt;C_U\\) , \\(I(X^x)\\) secara umum bukan merupakan penaksir tak bias dari F(x). Sebagai alternatif, pertimbangkan dua peubah acak yang memiliki batas penyensoran yang berbeda. Sebagai ilustrasi, misalkan kita mengamati \\(X^1=min(X_1,5)\\) dan \\(X^2 = min(X_2,10)\\) di mana \\(X_1\\) dan \\(X_2\\) adalah undian independen dari distribusi yang sama. Untuk \\(x5\\) fungsi distribusi empiris \\(F_2(x)\\) adalah penaksir tak bias dari \\(F(x)\\). Akan tetapi, untuk \\(5&lt;x10\\) pengamatan pertama tidak dapat digunakan untuk fungsi distribusi karena adanya batasan penyensoran. Sebagai gantinya, strategi yang dikembangkan oleh (Kaplan dan Meier 1958) adalah dengan menggunakan \\(S_2(5)\\) sebagai penaksir dari \\(S(5)\\) dan kemudian menggunakan observasi kedua untuk mengestimasi fungsi survival bersyarat pada kelangsungan hidup hingga waktu ke-5, \\(Pr(X&gt;x|X&gt;5)=\\frac{S(x)}{S(5)}\\) . Secara khusus, untuk \\(5&lt;x10\\) penaksir dari fungsi survival adalah \\[ \\begin{aligned} \\hat{S}(x) = S_2(5) \\times I(X_2^* &gt; x ) \\end{aligned} \\] Kaplan-Meier Product Limit Estimator Dengen memperluas ide dalam setiap observasi i,dengan ui menjadi batas atas penyensoran \\((=) jikatidakadapenyensoran\\). Dengan demikian, nilai yang tercatat adalah xi dalam kasus tidak ada penyensoran dan ui jika ada penyensoran. Dengan \\(t_1&lt;&lt;t_k\\)menjadi k titik berbeda di mana kerugian yang tidak disensor terjadi, dan biarkan \\(s_j\\) adalah jumlah kerugian yang tidak tersensor \\(x_i\\) yang tidak tersensor pada \\(t_j\\). Himpunan risiko yang sesuai adalah jumlah observasi yang aktif (tidak tersensor) pada nilai yang kurang dari \\(t_j\\) yang dinotasikan sebagai \\(R_j = \\sum_{i=1}^n I(x_i \\geq t_{j}) + \\sum_{i=1}^n I(u_i \\geq t_{j})\\) Dengan notasi ini, penaksir product-limit dari fungsi distribusi \\[ \\begin{equation} \\hat{F}(x) = \\left\\{ \\begin{array}{ll} 0 &amp; x&lt;t_{1} \\\\ 1-\\prod_{j:t_{j} \\leq x}\\left( 1-\\frac{s_j}{R_{j}}\\right) &amp; x \\geq t_{1} \\end{array} \\right. . \\tag{4.6} \\end{equation} \\] Sebagai contohnya, jika x lebih kecil dari kerugian terkecil yang tidak tersensor, maka \\(x&lt;t1\\) dan \\(F^(x)=0\\) . Sebagai contoh lain, jika \\(x\\) berada di antara kerugian tersensor terkecil kedua dan ketiga, maka \\(x(t_2,t_3]\\) dan \\(\\hat{F}(x) = 1 - \\left(1- \\frac{s_1}{R_{1}}\\right)\\left(1- \\frac{s_2}{R_{2}}\\right)\\) .Taksiran yang sesuai dari fungsi survival adalah \\(\\hat{S}(x) = 1 - \\hat{F}(x)\\) 4.3.3 Example 4.3.5. Actuarial Exam Question. Berikut ini adalah contoh dari 10 pembayaran: \\[ 4 \\space \\space 4 \\space \\space 5+\\space \\space 5+ \\space\\space 5+ \\space\\space 8 \\space\\space 10+ \\space\\space 10+ \\space\\space 12 \\space\\space 15 \\] dimana + menunjukkan bahwa kerugian telah melebihi batas polis. Dengan menggunakan estimator batas produk Kaplan-Meier, hitunglah probabilitas bahwa kerugian pada suatu polis melebihi 11, \\(\\hat{S}(11)\\) Terdapat empat waktu kejadian (pengamatan yang tidak disensor). Untuk setiap waktu tj kita dapat menghitung jumlah kejadian sj dan himpunan risiko \\(R_j\\) sebagai berikut: src=https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/example4.3.6-1.png?raw=true width=300 height=300 style=display: block; margin-left: auto; margin-right: auto; margin-top: 10px;&gt; Dengan demikian, estimasi Kaplan-Meier dari S(11) adalah \\[ \\begin{aligned} \\hat{S}(11) &amp;= \\prod_{j:t_j\\leq 11} \\left( 1- \\frac{s_j}{R_j} \\right) = \\prod_{j=1}^{2} \\left( 1- \\frac{s_j}{R_j} \\right)\\\\ &amp;= \\left(1-\\frac{2}{10} \\right) \\left(1-\\frac{1}{5} \\right) = (0.8)(0.8)= 0.64. \\\\ \\end{aligned} \\] Right-Censored, Left-Truncated Empirical Distribution Function Selain penyensoran kanan, selanjutnya adalah memperluas kerangka kerja untuk memungkinkan data terpotong ke kiri. Seperti sebelumnya, untuk setiap observasi i , dengan \\(u_i\\) menjadi batas penyensoran atas ( \\(=\\) jika tidak ada penyensoran). Selanjutnya, \\(d_i\\) merupakan batas pemotongan bawah (0 jika tidak ada pemotongan). Dengan demikian, nilai yang tercatat (jika lebih besar dari \\(d_i\\) ) adalah \\(x_i\\) dalam kasus tidak ada penyensoran dan \\(u_i\\) jika ada penyensoran. Lalu untuk $t_1&lt;&lt;t_k $menjadi \\(k\\) titik-titik yang berbeda di mana sebuah kejadian yang menarik terjadi, dan biarkan \\(s_j\\) adalah jumlah kejadian yang terekam \\(x_i\\) pada titik waktu \\(t_j\\). Himpunan risiko yang sesuai adalah \\(R_j = \\sum_{i=1}^n I(x_i \\geq t_{j}) + \\sum_{i=1}^n I(u_i \\geq t_{j}) - \\sum_{i=1}^n I(d_i \\geq t_{j}).\\) Dengan definisi baru dari himpunan risiko ini, penaksir batas hasil kali dari fungsi distribusi adalah seperti pada persamaan product limit estimator. Rumus Greenwood (Greenwood 1926) menurunkan rumus untuk estimasi varians dari penaksir batas-produk menjadi \\(\\widehat{Var}(\\hat{F}(x)) = (1-\\hat{F}(x))^{2} \\sum {j:t{j} \\leq x} \\dfrac{s_j}{R_{j}(R_{j}-s_j)}.\\) Seperti biasa, dapat mengacu pada akar kuadrat dari estimasi varians sebagai kesalahan standar, sebuah kuantitas yang secara rutin digunakan dalam interval kepercayaan dan untuk pengujian hipotesis. Untuk menghitungnya, metode survfit R mengambil sebuah objek data survival dan membuat sebuah objek baru yang berisi estimasi Kaplan-Meier dari fungsi survival bersama dengan interval kepercayaan. Metode Kaplan-Meier (type='kaplan-meier') digunakan secara default untuk membuat estimasi kurva survival. Fungsi survival diskrit yang dihasilkan memiliki massa titik pada waktu kejadian yang diamati (tanggal pelepasan) \\(t_j\\) dimana probabilitas suatu kejadian yang diberi ketahanan hidup pada durasi tersebut diestimasi sebagai jumlah kejadian yang diamati pada durasi sj dibagi dengan jumlah subjek yang terpapar atau berisiko sesaat sebelum durasi kejadian \\(R_j\\). Penaksir Alternatif Dua jenis estimasi alternatif juga tersedia untuk metode survfit. Alternatif pertama (type='fh2') menangani hubungan, pada dasarnya, dengan mengasumsikan bahwa beberapa kejadian pada durasi yang sama terjadi dalam urutan yang berubah-ubah. Alternatif lain (type='fleming-harrington') menggunakan estimasi Nelson-Aalen (Aalen 1978) dari fungsi hazard kumulatif untuk mendapatkan estimasi fungsi survival. Estimasi bahaya kumulatif \\(H^(x)\\) dimulai dari nol dan bertambah pada setiap durasi kejadian yang diamati \\(t_j\\) dengan jumlah kejadian \\(s_j\\) dibagi dengan jumlah yang berisiko \\(R_j\\). Dengan notasi yang sama seperti di atas, penaksir Nelson-Äalen dari fungsi distribusi adalah \\[ \\begin{aligned} \\hat{F}_{NA}(x) &amp;= \\left\\{ \\begin{array}{ll} 0 &amp; x&lt;t_{1} \\\\ 1- \\exp \\left(-\\sum_{j:t_{j} \\leq x}\\frac{s_j}{R_j} \\right) &amp; x \\geq t_{1} \\end{array} \\right. .\\end{aligned} \\] Itu merupakan hasil dari estimator Nelson-Äalen dari fungsi hazard kumulatif \\(\\hat{H}(x)=\\sum_{j:t_j\\leq x} \\frac{s_j}{R_j}\\) dan hubungan antara fungsi survival dan fungsi hazard kumulatif, \\(\\hat{S}_{NA}(x)=e^{-\\hat{H}(x)}\\) 4.4 Bayesian Inference Penjelasan pada subbab ini: Jelaskan model Bayesian sebagai alternatif dari pendekatan frequentist dan rangkum lima komponen dari pendekatan pemodelan ini. Ringkas distribusi parameter posterior dan gunakan distribusi posterior ini untuk memprediksi hasil baru. Gunakan distribusi konjugat untuk menentukan distribusi parameter posterior. 4.4.1 Introduction to Bayesian Inference Sampai saat ini, metode inferensial kami berfokus pada pengaturan frequentist , di mana sampel diambil berulang kali dari suatu populasi. Vektor parameter  adalah tetap belum diketahui, sedangkan hasil X adalah realisasi variabel acak. Sebaliknya, di bawah kerangka Bayesian , kami melihat parameter model dan data sebagai variabel acak. Kami tidak yakin tentang parameternya  dan gunakan alat probabilitas untuk mencerminkan ketidakpastian ini. Dibawah ini merupakan rumus aturan bayes: src=https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.4.1-1.png?raw=true width=300 height=300 style=display: block; margin-left: auto; margin-right: auto; margin-top: 10px;&gt; Di mana, Pr(parameters): adalah distribusi parameter, yang dikenal sebagai distribusi sebelumnya . Pr(data|parameters): adalah distribusi sampling. Dalam konteks frequentist, ini digunakan untuk membuat kesimpulan tentang parameter dan dikenal sebagai kemungkinan . Pr(parameters|data):adalah distribusi parameter setelah mengamati data, yang dikenal sebagai distribusi posterior . Pr(data): adalah distribusi marjinal dari data. Ini umumnya diperoleh dengan mengintegrasikan (atau menjumlahkan) distribusi gabungan data dan parameter di atas nilai parameter. 4.4.2 Bayesian Model Distribusi Sebelumnya. Secara khusus, pikirkan tentang parameter  sebagai vektor acak dan biarkan (  ) menunjukkan fungsi massa atau kepadatan yang sesuai. Ini adalah pengetahuan yang kita miliki sebelum hasil diamati dan disebut distribusi sebelumnya . Biasanya, distribusi sebelumnya adalah distribusi reguler dan terintegrasi atau dijumlahkan menjadi satu, tergantung pada apakah  kontinu atau diskrit. Namun, kami mungkin sangat tidak yakin (atau tidak tahu) tentang distribusinya  ; mesin Bayesian memungkinkan situasi berikut \\[ \\int \\pi(\\theta) ~d\\theta = \\infty, \\] dalam hal ini, \\(\\pi(\\cdot)\\) disebut priot yang tidak tepat Distribusi Bersama. Distribusi hasil dan parameter model adalah distribusi gabungan dari dua besaran acak. Fungsi kerapatan persendiannya dilambangkan sebagai \\[ f(x , \\boldsymbol \\theta) = f(x|\\boldsymbol \\theta )\\pi(\\boldsymbol \\theta) \\] Distribusi Hasil Marginal. Distribusi hasil dapat dinyatakan sebagai \\[ f(x) = \\int f(x | \\boldsymbol \\theta)\\pi(\\boldsymbol \\theta) ~d \\boldsymbol \\theta. \\] Ini analog dengan distribusi campuran frequentist. Dalam distribusi campuran, kami menggabungkan (atau mencampur) subpopulasi yang berbeda. Dalam konteks Bayesian, distribusi marjinal adalah kombinasi dari realisasi parameter yang berbeda (dalam beberapa literatur, Anda dapat menganggap ini sebagai kombinasi keadaan alam yang berbeda). Distribusi Parameter Posterior. Setelah hasil diamati (karenanya terminologi posterior), seseorang dapat menggunakan teorema Bayes untuk menulis fungsi kerapatan sebagai \\[ \\pi(\\boldsymbol \\theta | x) =\\frac{f(x , \\boldsymbol \\theta)}{f(x)} =\\frac{f(x|\\boldsymbol \\theta )\\pi(\\boldsymbol \\theta)}{f(x)} . \\] 4.4.3 Bayesian Inference 4.4.3.1 Summarizing the Posterior Distributiob of Paremeters Salah satu cara untuk meringkas distribusi adalah dengan menggunakan pernyataan tipe interval kepercayaan . Untuk meringkas distribusi parameter posterior , interval \\([a,b]\\) dikatakan sebagai \\(100(1-\\alpha)\\%\\) interval kredibilitas untuk \\(\\theta\\) jika \\[ \\Pr (a \\le \\theta \\le b | \\mathbf{x}) \\ge 1- \\alpha. \\] Estimasi Bayes adalah nilai yang meminimalkan kerugian yang diharapkan \\[ \\mathrm{E~}[ l(\\hat{\\theta}, \\theta)] \\] 4.4.3.2 Bayesian Predictive Distribution Untuk jenis inferensi statistik lainnya, seringkali menarik untuk memprediksi nilai hasil acak yang belum diamati. Khususnya untuk data baru y, distribusi prediktifnya adalah \\[ f(y|x) = \\int f(y|\\theta) \\pi(\\theta|x) d\\theta . \\] Ini juga kadang-kadang disebut distribusi prediktif posterior karena distribusi data baru tergantung pada kumpulan data dasar. Menggunakan kerugian kesalahan kuadrat untuk fungsi kerugian, prediksi Bayesian dari Y adalah \\[ \\begin{aligned} \\mathrm{E}(Y|X) &amp;= \\int ~y f(y|X) dy = \\int y \\left(\\int f(y|\\theta) \\pi(\\theta|X) d\\theta \\right) dy \\\\ &amp;= \\int \\left(\\int y f(y|\\theta) ~dy \\right) \\pi(\\theta|X) ~d\\theta \\\\ &amp;= \\int \\mathrm{E}(Y|\\theta) \\pi(\\theta|X) ~d\\theta . \\end{aligned} \\] Seperti disebutkan sebelumnya, untuk beberapa situasi distribusi parameter adalah diskrit, tidak kontinu. Memiliki serangkaian kemungkinan parameter yang terpisah memungkinkan kita untuk menganggapnya sebagai keadaan alam alternatif, sebuah interpretasi yang membantu. 4.4.4 Conjugate Distributions Untuk menghubungkan distribusi parameter sebelum dan sesudah, kami memiliki hubungan \\[ \\begin{array}{ccc} \\pi(\\boldsymbol \\theta | x) &amp; = &amp; \\frac{f(x|\\boldsymbol \\theta )\\pi(\\boldsymbol \\theta)}{f(x)} \\\\ &amp; \\propto &amp; f(x|\\boldsymbol \\theta ) \\pi(\\boldsymbol \\theta) \\\\ \\text{Posterior} &amp; \\text{is proportional to} &amp; \\text{likelihood} \\times \\text{prior} . \\end{array} \\] Untuk distribusi konjugasi, posterior dan sebelumnya milik keluarga distribusi yang sama. Ilustrasi berikut melihat kasus khusus gamma-Poisson, yang paling terkenal dalam aplikasi aktuaria. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
=======
<<<<<<< HEAD
[["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
=======
<<<<<<< HEAD
[["aggregate-loss-models.html", "Bab 5 Aggregate Loss Models 5.1 Introduction 5.2 Moments and Distribution 5.3 5.4 Menghitung Distribusi Klaim Agregat", " Bab 5 Aggregate Loss Models 5.1 Introduction Sub bab ini membahas mengenai pembangunan model probabilitas untuk menggambarkan klaim agregat oleh sistem asuransi yang terjadi dalam periode waktu tertentu. Sistem asuransi dapat berupa polis tunggal, kontrak asuransi kelompok, lini bisnis , atau seluruh buku bisnis perusahaan asuransi. Dalam bab ini, klaim agregat mengacu pada jumlah klaim dari portofolio kontrak asuransi. Pertimbangkan portofolio asuransi dari \\(N\\) kontrak individu, dan \\(S\\) menunjukkan kerugian agregat portofolio dalam jangka waktu tertentu. Ada dua pendekatan untuk memodelkan kerugian agregat \\(S\\) , model risiko individu dan model risiko kolektif. Model risiko individu menekankan kerugian dari masing-masing kontrak individu dan mewakili kerugian agregat sebagai: \\[S_n=X_1 +X_2 +\\cdots+X_n,\\] Di mana \\(X_i~(i=1,\\ldots,n)\\) diinterpretasikan sebagai jumlah kerugian dari \\(X_i\\) kontrak. \\(N\\) menunjukkan jumlah kontrak dalam portofolio dan dengan demikian merupakan angka tetap daripada variabel acak. Untuk model risiko individu, biasanya diasumsikan \\(X_i\\) ini independen. Karena fitur kontrak yang berbeda seperti cakupan dan paparan , \\(X_i\\) belum tentu terdistribusi secara identik. Fitur penting dari distribusi masing-masing \\(X_i\\) adalah massa probabilitas pada nol yang sesuai dengan peristiwa tidak adanya klai Model risiko kolektif mewakili kerugian agregat dalam hal distribusi frekuensi dan distribusi keparahan: \\[S_N=X_1 +X_2 + \\cdots + X_N .\\] Sejumlah klaim acak \\(N\\) yang dapat mewakili baik jumlah kerugian atau jumlah pembayaran. Sebaliknya, dalam model risiko individual biasanya menggunakan sejumlah kontrak tetap \\(N\\).\\(X_1, X_2, \\ldots, X_N\\) sebagai representasi dari jumlah masing-masing kerugian. Setiap kerugian mungkin atau mungkin tidak sesuai dengan kontrak unik. Misalnya, mungkin ada banyak klaim yang timbul dari satu kontrak. Itu wajar untuk dipikirkan \\(X_i&gt;0\\) karena jika \\(X_i=0\\) maka tidak ada klaim yang terjadi. Biasanya kita menganggap bahwa kondisional pada \\(X_{1},X_{2},\\ldots ,X_{n}\\) adalah iid variabel acak. Distribusi dari N dikenal sebagai distribusi frekuensi , dan distribusi umum dari \\(X\\) dikenal sebagai distribusi keparahan . Dengan berasumsi \\(N\\) Dan \\(X\\) sendiri. Dengan model risiko kolektif, sehingga dapat menguraikan kerugian agregat menjadi frekuensi \\(( N )\\) proses dan tingkat keparahan \\(( X )\\) model. Fleksibilitas ini memungkinkan analis untuk mengomentari dua komponen terpisah ini. Misalnya, pertumbuhan penjualan karena standar penjaminan emisi yang lebih rendah dapat menyebabkan frekuensi kerugian yang lebih tinggi tetapi mungkin tidak memengaruhi keparahan. Demikian pula, inflasi atau kekuatan ekonomi lainnya dapat berdampak pada keparahan tetapi tidak pada frekuensi. 5.2 Moments and Distribution Jadi model risiko keleksif \\(SN=X_1+...+X_N\\) dan tidak bergantung pada N Misalkan \\(μ = E(X_i)\\) dan \\(σ^2=Var(X_i)\\) untuk semua \\(i\\) Dengan demikian, bersyarat pada N kita memiliki ekspektasi jumlah adalah jumlah ekspektasi dan varians. \\[ \\begin{aligned} {\\rm E}(S|N) &amp;= {\\rm E}(X_1 + \\cdots + X_N|N) = \\mu N \\\\ {\\rm Var}(S|N) &amp;= {\\rm Var}(X_1 + \\cdots + X_N|N) = \\sigma^2 N. \\end{aligned} \\] Dengan menggunakan hukum ekspektasi berulang,rata-rata kerugian agregat adalah \\({\\rm E}(S_N)={\\rm E}_N[{\\rm E}_S(S|N)] = {\\rm E}_N(N\\mu) = \\mu ~{\\rm E}(N).\\) Dengan menggunakan hukum varians total, varians dari kerugian agregat adalah \\[ \\begin{aligned} {\\rm Var}(S_N) &amp;= {\\rm E}_N[{\\rm Var}(S_N|N)] + {\\rm Var}_N[{\\rm E}(S_N|N)] \\\\ &amp;= \\mathrm{E}_N \\left[ \\sigma^2 N \\right] + \\mathrm{Var}_N\\left[ \\mu N \\right] \\\\ &amp;=\\sigma^2~{\\rm E}[N] + \\mu^2~ {\\rm Var}[N] . \\end{aligned} \\] Kasus Khusus: Frekuensi Berdistribusi Poisson. Jika \\(N∼Poi(λ)\\) maka \\[ \\begin{aligned} \\mathrm{E}(N) &amp;= \\mathrm{Var}(N) = \\lambda\\\\ \\mathrm{E}(S_N) &amp;= \\lambda ~\\mathrm{E}(X)\\\\ \\mathrm{Var}(S_N) &amp;= \\lambda (\\sigma^2 + \\mu^2) = \\lambda ~\\mathrm{E} (X^2). \\end{aligned} \\] 5.2.0.1 5.3.1 Actuarial Exam Question Jumlah kecelakaan mengikuti distribusi Poisson dengan rata-rata 12. Setiap kecelakaan menghasilkan 1, 2, atau 3 penuntut dengan probabilitas masing-masing 1/2, 1/3, dan 1/6. Hitunglah varians dalam jumlah total penuntut. JAWABAN \\[ \\begin{aligned} &amp; \\mathrm{E}(X^2) = 1^2 \\left( \\frac{1}{2}\\right) + 2^2\\left(\\frac{1}{3} \\right) + 3^2\\left(\\frac{1}{6}\\right) = \\frac{10}{3} \\\\ \\Rightarrow &amp;\\mathrm{Var}(S_N) = \\lambda \\ \\mathrm{E}(X^2) = 12\\left(\\frac{10}{3}\\right) = 40 . \\end{aligned} \\] Sebagai alternatif, Dapat menggunakan pendekatan umum, \\(\\mathrm{Var}(S_N) = \\sigma^2 \\mathrm{E}(N) + \\mu^2 \\mathrm{Var}(N)\\), Dimana \\[ \\begin{aligned} \\mathrm{E}(N) &amp;= \\mathrm{Var}(N) = 12 \\\\ \\mu &amp;= \\mathrm{E}(X) = 1\\left(\\frac{1}{2}\\right) + 2\\left(\\frac{1}{3}\\right) + 3\\left(\\frac{1}{6}\\right) = \\frac{5}{3} \\\\ \\sigma^2 &amp;= \\mathrm{E}(X^2) - [\\mathrm{E}(X)]^2 = \\frac{10}{3} - \\frac{25}{9} = \\frac{5}{9} \\\\ \\Rightarrow \\ \\mathrm{Var}(S_N) &amp;= \\left(\\frac{5}{9}\\right)\\left(12\\right) + \\left(\\frac{5}{3}\\right)^2\\left(12\\right) = 40 . \\end{aligned} \\] Secara umum, momen-momen SN dapat diturunkan dari fungsi pembangkit momen (mgf). Karena \\(X_i\\) adalah iid, dapat dinyatakan mgf dari X sebagai \\(M_{X(t)}= E(e^{tX})\\) . Dengan menggunakan hukum ekspektasi yang diiterasi, mgf dari \\(S_N\\) adalah \\[ \\begin{aligned} M_{S_N}(t) &amp;= \\mathrm{E}(e^{t S_N})=\\mathrm{E}_N[\\mathrm{E}(e^{tS_N}|N)]\\\\ &amp;= \\mathrm{E}_N \\left[ ~\\mathrm{E}\\left( e^{t(X_1+\\cdots+X_N)}\\right) ~\\right] = \\mathrm{E}_N \\left[ \\mathrm{E}(e^{tX_1})\\cdots\\mathrm{E}(e^{tX_N}) \\right] ~~ \\text{since } X_i \\text{&#39;s are independent} \\\\ &amp;= \\mathrm{E}N[~(M{X}(t))^N~] . \\end{aligned} \\] Lalu dapat melihat fungsi pembangkit probabilitas(pgf) dari N adalah \\(P_N(z)= E(Z^N)\\). Dengan menyatakan \\(M_X(t)=z\\), lalu mengganti ke dalam ekspresi untuk mgf dari SN di atas, maka diperoleh \\[ \\begin{aligned} M_{S_N}(t) = \\mathrm{E~}(z^N) = P_{N}(z) = P_{N}[M_{X}(t)]. \\end{aligned} \\] Demikian pula, jika \\(S_N\\) merupakan diskrit, dapat menunjukkan juga pgf dari \\(S_N\\) adalah \\[ \\begin{aligned} P_{S_N}(z) = P_{N}[P_{X}(z)] . \\end{aligned} \\] Untuk mendapatkan \\(E(S_N) = M′S_N(0)\\) dapat menggunakan aturan rantai: \\(M_{S_N}&#39;(t) = \\frac{\\partial}{\\partial t} P_{N}(M_{X}(t)) = P_{N}&#39;(M_{X}(t)) M_{X}&#39;(t)\\\\\\) Lalu Memanggil \\(M_{X}(0) = 1, M_{X}&#39;(0) = \\mathrm{E}(X) = \\mu, P_{N}&#39;(1) = \\mathrm{E}(N)\\) Jadi, \\(\\mathrm{E}(S_N) = M_{S_N}&#39;(0) = P_{N}&#39;(M_{X}(0)) M_{X}&#39;(0) = \\mu {\\rm E}(N) .\\) Demikian pula, dapat menggunakan relasi \\(E(S^2_N) = M′′_{S_N}(0)\\) untuk mendapatkan \\(\\mathrm{Var}(S_N) = \\sigma^2 \\mathrm{E}(N) + \\mu^2 \\mathrm{Var}(N).\\) Special Case. Poisson Frequency. Misalkan \\(N∼Poi(λ)\\) dengan demikian, pgf dari \\(N\\) adalah \\(P_N(z) = e^{λ(z-1)}\\) dan mgf dari \\(S_N\\) adalah \\[ \\begin{aligned} M_{S_N}(t) &amp;= P_N[M_X(t)] = e^{\\lambda(M_{X}(t) - 1)}. \\end{aligned} \\] Mengambil hasil turunan \\[ \\begin{aligned} M_{S_N}&#39;(t) &amp;= e^{\\lambda(M_{X}(t) - 1)}~ \\lambda~ M_{X}&#39;(t) = M_{S_N}(t) ~\\lambda ~M_{X}&#39;(t)\\\\ M_{S_N}&#39;&#39;(t) &amp;= M_{S_N}(t) ~\\lambda~ M_{X}&#39;&#39;(t) + [~M_{S_N}(t)~\\lambda~ M_{X}&#39;(t)~] ~\\lambda~ M_{X}&#39;(t) . \\end{aligned} \\] Mengevaluasi hal ini pada t = 0 menghasilkan \\[ \\begin{aligned} \\mathrm{E}(S_N) &amp;= M_{S_N}&#39;(0) = \\lambda \\mathrm{E}(X) = \\lambda \\mu \\end{aligned} \\] Lalu \\[ \\begin{aligned} M_{S_N}&#39;&#39;(0) &amp;= \\lambda \\mathrm{E}(X^2) + \\lambda^2 \\mu^2\\\\ \\Rightarrow \\mathrm{Var}(S_N) &amp;= \\lambda \\mathrm{E}(X^2) + \\lambda^2 \\mu^2 - (\\lambda \\mu)^2 = \\lambda~ \\mathrm{E}(X^2). \\end{aligned} \\] 5.2.0.2 Example 5.3.2. Actuarial Exam Question Dimisalkan produser acara kuis televisi yang memberikan hadiah uang tunai. Jumlah hadiah(N) dan jumlah hadiah(X) memiliki distribusi sebagai berikut: \\[{\\small \\begin{matrix} \\begin{array}{ccccc}\\hline n &amp; \\Pr(N=n) &amp; &amp; x &amp; \\Pr(X=x)\\\\ \\hline 1 &amp; 0.8 &amp; &amp; 0 &amp; 0.2 \\\\ 2 &amp; 0.2 &amp; &amp; 100 &amp; 0.7 \\\\ &amp; &amp; &amp; 1000 &amp; 0.1\\\\\\hline \\end{array} \\end{matrix} }\\] Sehingga Anggaran untuk hadiah sama dengan jumlah hadiah uang tunai yang diharapkan ditambah dengan deviasi standar dari jumlah hadiah uang tunai. Hitung anggaran! JAWABAN Diperlukan untuk menghitung rata-rata dan standar deviasi dari agregat (jumlah) hadiah uang tunai. Momen-momen dari distribusi frekuensi N adalah \\[ \\begin{aligned} \\mathrm{E}(N) &amp;= 1 (0.8) + 2 (0.2) =1.2\\\\ \\mathrm{E}(N^2) &amp;= 1^2 (0.8) + 2^2 (0.2) =1.6\\\\ \\mathrm{Var}(N) &amp;= \\mathrm{E}(N^2) - \\left[ \\mathrm{E}(N) \\right]^2= 0.16 . \\end{aligned} \\] Momen-momen dari distribusi tingkat keparahan X adalah \\[ \\begin{aligned} \\mathrm{E}(X) &amp;= 0 (0.2) + 100 (0.7) + 1000 (0.1) = 170 = \\mu\\\\ \\mathrm{E}(X^2) &amp;= 0^2 (0.2) + 100^2 (0.7) + 1000^2 (0.1) = 107,000\\\\ \\mathrm{Var}(X) &amp;= \\mathrm{E}(X^2) - \\left[ \\mathrm{E}(X) \\right]^2=78,100 = \\sigma^2 . \\end{aligned} \\] Dengan demikian, rata-rata dan varians dari keseluruhan hadiah uang tunai adalah \\[ \\begin{aligned} \\mathrm{E}(S_N) &amp;= \\mu \\mathrm{E}(N) = 170 (1.2) = 204 \\\\ \\mathrm{Var}(S_N) &amp;= \\sigma^2 \\mathrm{E}(N) + \\mu^2 \\mathrm{Var}(N)\\\\ &amp;= 78,100 (1.2) + 170^2 (0.16) = 98,344 . \\end{aligned} \\] Sehingga anggaran yang dibutuhkan sebagai berikut \\[ \\begin{aligned} Budget &amp;= \\mathrm{E}(S_N) + \\sqrt{\\mathrm{Var}(S_N)} \\\\ &amp;= 204 + \\sqrt{98,344} = 517.60 . \\end{aligned} \\] Distribusi \\(S_N\\) disebut distribusi majemuk, dan dapat diturunkan berdasarkan konvolusi \\(F_X\\) sebagai berikut: \\[ \\begin{aligned} F_{S_N}(s) &amp;= \\Pr \\left(X_1 + \\cdots + X_N \\le s \\right) \\\\ &amp;= \\mathrm{E} \\left[ \\Pr \\left(X_1 + \\cdots + X_N \\le s|N=n \\right) \\right]\\\\ &amp;= \\mathrm{E} \\left[ F_{X}^{\\ast N}(s) \\right] \\\\ &amp;= p_0 + \\sum_{n=1}^{\\infty }p_n F_{X}^{\\ast n}(s) . \\end{aligned} \\] Ketika \\(E(N)\\) dan \\(Var(N)\\) diketahui, kita juga dapat menggunakan suatu jenis teorema limit pusat untuk mengestimasi distribusi \\(S_N\\) seperti pada model risiko individu. Yaitu, \\(\\frac{S_N - \\mathrm{E}(S_N)}{\\sqrt{\\mathrm{Var}(S_N)}}\\) kira-kira mengikuti distribusi normal standar \\(N(0,1)\\) . Dari jenis teorema limit pusat ini, aproksimasi bekerja dengan baik jika \\(E[N]\\) cukup besar. 5.2.1 5.3.2 Stop-loss Insurance Modifikasi pertanggungan pada tingkat polis perorangan Pertanggungan atas kerugian agregat \\(S_N\\) , yang dikenakan sebuah deductible \\(d\\) disebut dengan . Nilai yang diharapkan dari jumlah kerugian agregat yang melebihi deductible, \\[ \\begin{eqnarray*} \\mathrm{E}[(S-d)_+] \\end{eqnarray*} \\] dikenal sebagai premi stop-loss bersih. Untuk menghitung premi stop-loss neto, kita memiliki \\[ \\begin{eqnarray*} \\mathrm{E}(S_N-d)_+ &amp;=&amp; \\left\\{\\begin{array}{ll} \\int_{d}^{\\infty}(s-d) f_{S_N}(s) ds&amp; \\text{for continuous } S_N\\\\ \\sum_{s&gt;d}(s-d) f_{S_N}(s) &amp; \\text{for discrete } S_N\\\\ \\end{array}\\right.\\\\ &amp;=&amp; \\mathrm{E}(S_N) - \\mathrm{E}(S_N\\wedge d)\\\\ \\end{eqnarray*} \\] ### * 5.3.6. Actuarial Exam Question* Dalam satu minggu tertentu, jumlah proyek yang mengharuskan Anda bekerja lembur memiliki distribusi geometris dengan \\(β = 2\\) . Untuk setiap proyek, distribusi jumlah jam lembur dalam seminggu, X adalah sebagai berikut \\[ {\\small \\begin{matrix} \\begin{array}{ccc} \\hline x &amp; &amp; f(x)\\\\ \\hline 5 &amp; &amp; 0.2 \\\\ 10 &amp; &amp; 0.3 \\\\ 20 &amp; &amp; 0.5\\\\ \\hline \\end{array} \\end{matrix} } \\] Jumlah proyek dan jumlah jam lembur tidak bergantung. Anda akan dibayar untuk jam lembur yang melebihi 15 jam dalam seminggu. Hitunglah jumlah jam lembur yang akan diterima dalam seminggu. JAWABAN Jumlah proyek dalam seminggu yang membutuhkan kerja lembur memiliki distribusi \\(N∼Geo(β=2)\\) sedangkan jumlah jam kerja lembur per proyek memiliki distribusi \\(X\\) seperti yang telah dijelaskan di atas. Jumlah keseluruhan jam lembur dalam seminggu adalah SN dan oleh karena itu kita mencari \\(\\mathrm{E}(S_N-15)_+ = \\mathrm{E}(S_N) - \\mathrm{E}(S_N \\wedge 15).\\) Untuk mencari \\(\\mathrm{E}(S_N) = \\mathrm{E}(X) ~\\mathrm{E}(N)\\), maka akan didapat \\[ \\begin{aligned} &amp;\\mathrm{E}(X) = 5(0.2) + 10(0.3)+ 20(0.5)= 14 \\\\ &amp;\\mathrm{E}(N) = 2 \\\\ \\Rightarrow \\ &amp;\\mathrm{E}(S) = \\mathrm{E}(X) ~ \\mathrm{E}(N) = 14(2) = 28 . \\end{aligned} \\] Untuk mencari \\(\\mathrm{E} (S_N \\wedge 15) = 0 \\Pr (S_N=0) + 5 \\Pr(S_N=5) + 10 \\Pr(S_N=10) + 15 \\Pr(S_N \\geq 15))\\), maka akan didapat \\[ \\begin{aligned} \\Pr(S_N=0) &amp;= \\Pr(N=0) = \\frac{1}{1+\\beta} = \\frac{1}{3} \\\\ \\Pr(S_N=5) &amp;= \\Pr(X=5, \\ N=1) = 0.2 \\left(\\frac{2}{9} \\right)= \\frac{0.4}{9}\\\\ \\Pr(S_N=10) &amp;= \\Pr(X=10, \\ N=1) + \\Pr(X_1=X_2=5, N=2) \\\\ &amp;= 0.3 \\left(\\frac{2}{9} \\right) + (0.2)(0.2) \\left( \\frac{4}{27} \\right)= 0.0726 \\\\ \\Pr(S_N \\geq 15) &amp;= 1 - \\left(\\frac{1}{3} + \\frac{0.4}{9} + 0.0726 \\right) = 0.5496\\\\ \\Rightarrow \\mathrm{E}(S_N \\wedge 15) &amp;= 0 \\Pr (S_N=0) + 5 \\Pr(S_N=5) + 10 \\Pr(S_N=10) + 15 \\Pr(S_N \\geq 15) \\\\ &amp;= 0 \\left( \\frac{1}{3} \\right) + 5 \\left( \\frac{0.4}{9} \\right) + 10 (0.0726) + 15 (0.5496) = 9.193 .\\\\ \\end{aligned} \\] Oleh Karena itu: \\[ \\begin{aligned} \\mathrm{E}(S_N-15)_+ &amp;= \\mathrm{E}(S_N) - \\mathrm{E}(S_N \\wedge 15) \\\\ &amp;= 28 - 9.193 = 18.807 . \\end{aligned} \\] Recursive Net Stop-Loss Premium Calculation Untuk kasus diskrit, ini dapat dihitung secara rekursif sebagai \\[ \\begin{aligned} \\mathrm{E}\\left[ \\left( S_N-(j+1)h \\right) _{+} \\right]=\\mathrm{E}\\left[ ( S_N-jh )_{+} \\right] -h \\left( 1-F_{S_N}(jh) \\right) . \\end{aligned} \\] Ini mengasumsikan bahwa dukungan \\(S_N\\) tersebar secara merata di atas unit-unit h. Untuk menetapkan ini, kita mengasumsikan bahwa \\(h = 1\\) Kita memiliki \\[ \\begin{aligned} \\mathrm{E}\\left[ \\left( S_N-(j+1) \\right) _{+} \\right] &amp;=\\mathrm{E}(S_N) - \\mathrm{E}[S_N\\wedge (j+1)] \\ ,\\ \\text{ and } \\\\ \\mathrm{E}\\left[ \\left( S_N - j \\right)_+ \\right] &amp;=\\mathrm{E}(S_N) - \\mathrm{E}[S_N\\wedge j] \\end{aligned} \\] Jadi, \\[ \\begin{aligned} \\mathrm{E}\\left[ \\left(S_N-(j+1) \\right) _{+}\\right] - \\mathrm{E}\\left[ ( S_N-j )_{+} \\right] &amp;= \\left\\{\\mathrm{E}(S_N) - \\mathrm{E}(S_N\\wedge (j+1)) \\right\\} - \\left\\{\\mathrm{E}(S_N) - \\mathrm{E}(S_N\\wedge j) \\right\\} \\\\ &amp;= \\mathrm{E}\\left(S_N \\wedge j \\right) - \\mathrm{E}\\left[ S \\wedge (j+1) \\right] \\end{aligned} \\] Maka kita dapat menulis \\[ \\begin{aligned} \\mathrm{E}\\left[S_N\\wedge (j+1)\\right] &amp;= \\sum_{x=0}^{j}xf_{S_N}(x) + (j+1)~\\Pr(S_N \\ge j+1) \\\\ &amp;= \\sum_{x=0}^{j-1}x f_{S_N}(x) + j~\\Pr(S_N=j) + (j+1)~\\Pr(S_N \\ge j+1) \\end{aligned} \\] Demikian pula, \\[ \\begin{aligned} \\mathrm{E}(S_N\\wedge j) = \\sum_{x=0}^{j-1}xf_{S_N}(x) + j~\\Pr(S_N\\ge j) \\end{aligned} \\] Dengan ekspresi ini, kami memiliki \\[ \\begin{aligned} &amp; \\mathrm{E}\\left[ \\left( S_N-(j+1) \\right) _{+} \\right] - \\mathrm{E~}\\left[ ( S_N-j )_{+} \\right] \\\\ &amp;= \\mathrm{E}\\left(S_N \\wedge j \\right) - \\mathrm{E}\\left[ S \\wedge (j+1) \\right] \\\\ &amp;= \\left\\{ \\sum_{x=0}^{j-1}xf_{S_N}(x) + j~\\Pr(S_N\\ge j) \\right\\} - \\left\\{ \\sum_{x=0}^{j-1}x f_{S_N}(x) + j~\\Pr(S_N=j) + (j+1)~\\Pr(S_N \\ge j+1) \\right\\} \\\\ &amp;= j~\\left[\\Pr(S_N \\geq j) - \\Pr(S_N=j) \\right]- (j+1)~\\Pr(S_N \\ge j+1) \\\\ &amp;= j~\\Pr( S_N &gt; j) - (j+1)~\\Pr(S_N \\ge j+1) ~~~~ \\text{ (note } \\Pr(S_N &gt; j) = \\Pr(S_N \\geq j+1) \\text{)} \\\\ &amp;= -\\Pr(S_N\\ge j+1) = -\\left[1 - F_{S_N}(j)\\right], \\end{aligned} \\] sesuai kebutuhan. 5.2.2 5.3.7. Actuarial Exam Question - Continued Ingatlah bahwa tujuan dari pertanyaan ini adalah untuk menghitung \\(E(S_N-15)_+\\) . Perhatikan bahwa dukungan dari \\(S_N\\) berjarak sama dengan satuan 5, sehingga pertanyaan ini juga dapat dikerjakan secara rekursif, menggunakan ekspresi di atas dengan langkah-langkah \\(h=5\\) : Step 1: \\[ \\begin{aligned} \\mathrm{E~}(S_N-5)_+ &amp;= \\mathrm{E}(S_N) - 5 [1-\\Pr(S_N \\leq 0) ]\\\\ %\\Pr (S_N\\geq 5) \\\\ &amp;= 28 - 5 \\left(1 - \\frac{1}{3}\\right) = \\frac{74}{3}=24.6667 . \\end{aligned} \\] Step 2: \\[ \\begin{aligned} \\mathrm{E~}(S_N-10)_+ &amp;= \\mathrm{E~}(S_N-5)_+ - 5 [1-\\Pr(S_N \\leq 5)]\\\\ %\\Pr (S_N\\ge 10) \\\\ &amp;= \\frac{74}{3} - 5\\left( 1 - \\frac{1}{3} - \\frac{0.4}{9}\\right) = 21.555 . \\end{aligned} \\] Step 3: \\[ \\begin{aligned} \\mathrm{E~}(S_N-15)_+ &amp;= \\mathrm{E~}(S_N-10)_+ - 5 [1-\\Pr(S_N \\leq 10)] \\\\ %\\Pr (S_N\\ge 15) \\\\ &amp;= \\mathrm{E~}(S_N-10)_+ - 5\\Pr (S_N\\ge 15) \\\\ &amp;= 21.555 - 5 (0.5496) = 18.807 . \\end{aligned} \\] 5.2.3 5.3.4 Analytic Results Terdapat beberapa kombinasi distribusi frekuensi klaim dan tingkat keparahan yang menghasilkan distribusi yang mudah dihitung untuk kerugian agregat. Bagian ini memberikan beberapa contoh sederhana. Meskipun contoh-contoh ini mudah dihitung, namun pada umumnya terlalu sederhana untuk digunakan dalam praktik. 5.2.3.1 5.3.8 Example Salah satunya adalah ekspresi bentuk tertutup untuk distribusi kerugian agregat dengan mengasumsikan distribusi frekuensi geometris dan distribusi tingkat keparahan eksponensial. Asumsikan bahwa jumlah klaim \\(N\\) adalah geometrik dengan rata-rata \\(E(N)=β\\) dan jumlah klaim \\(X\\) adalah eksponensial dengan \\(E(X)=θ\\) .Dapat diingat bahwa pgf dari N dan pgf dari X adalah: \\[ \\begin{aligned} P_N (z) &amp;=\\frac{1}{1- \\beta (z-1)}\\\\ M_{X}(t) &amp;=\\frac{1}{1-\\theta t} . \\end{aligned} \\] Dengan demikian, mgf dari kerugian agregat \\(S_N\\) dapat dinyatakan dengan dua cara \\[ \\begin{eqnarray} M_{S_N}(t) &amp;=&amp; P_N [M_{X}(t)] = \\frac{1}{1 - \\beta \\left( \\frac{1}{1-\\theta t} - 1\\right)} \\nonumber\\\\ &amp;=&amp; 1+ \\frac{\\beta}{1+\\beta} \\left([1-\\theta(1+\\beta)t]^{-1}-1 \\right)\\\\ &amp;=&amp; \\frac{1}{1+\\beta}(1) +\\frac{\\beta}{1+\\beta} \\left( \\frac{1}{1-\\theta (1+\\beta)t}\\right) . \\end{eqnarray} \\] Sehingga, \\(S_N\\) ekuivalen dengan distribusi majemuk \\(S_N=X^∗_1+⋯+X^∗_N∗\\) dengan \\(N^∗\\) adalah Bernoulli dengan rata-rata \\(β/(1+β)\\) dan \\(X^∗\\) adalah eksponensial dengan mean \\(θ(1+β)\\). Untuk melihat hal ini, kita periksa mgf dari S : \\[ \\begin{aligned} M_{S_N}(t) = P_N [M_{X}(t)] = P_{N^{*}} [M_{X^{*}}(t)], \\end{aligned} \\] Dimana, \\[ \\begin{aligned} P_{N^*} (z) &amp;=1+ \\frac{\\beta}{1+ \\beta} (z-1),\\\\ M_{X^*} (t) &amp;=\\frac{1}{1- {{\\theta(1+\\beta)}} t}. \\end{aligned} \\] \\(S_N\\) juga ekuivalen dengan campuran dua titik dari 0 dan \\(X^∗\\). Secara khusus, \\[ \\begin{array}{cl} S_N &amp;= \\left\\{ \\begin{array}{cl} 0 &amp; {\\rm with~ probability ~Pr}(N^*=0) = 1/(1+\\beta) \\\\ Y^{*} &amp; {\\rm with~ probability ~Pr}(N^*=1) = \\beta/(1+\\beta) . \\end{array} \\right. \\end{array} \\] Fungsi distribusi \\(S_N\\) dalah \\[ \\begin{eqnarray*} \\Pr(S_N=0) &amp;=&amp; \\frac{1}{1+\\beta}\\\\ \\Pr(S_N&gt;s) &amp;=&amp; \\Pr(X^*&gt;s) =\\frac{\\beta}{1+\\beta} \\exp\\left( -\\frac{s}{ \\theta (1+\\beta)}\\right) \\end{eqnarray*} \\] dengan pdf untuk \\(s&gt;0\\) \\[ \\begin{eqnarray*} f_{S_N}(s) = \\frac{\\beta}{\\theta (1+\\beta)^2}\\exp\\left( -\\frac{s}{ \\theta (1+\\beta)}\\right) . \\end{eqnarray*} \\] 5.2.4 5.3.4 Tweedie Distribution Pada bagian ini, kita akan membahas distribusi gabungan tertentu di mana jumlah klaim memiliki distribusi Poisson dan jumlah klaim memiliki distribusi gamma. Spesifikasi ini menghasilkan apa yang dikenal sebagai distribusi Tweedie. Distribusi Tweedie memiliki probabilitas massa pada nol dan komponen kontinu untuk nilai positif. Karena fitur ini, distribusi ini banyak digunakan dalam pemodelan klaim asuransi, di mana massa nol ditafsirkan sebagai tidak ada klaim dan komponen positif sebagai jumlah klaim. Secara khusus, pertimbangkan model risiko kolektif \\(S_N = X_1 + ⋯ + X_N\\). Dengan menganggap bahwa N memiliki distribusi Poisson dengan mean \\(λ\\) dan masing-masing \\(X_i\\) memiliki distribusi gamma dengan parameter bentuk \\(α\\) dan parameter skala \\(γ\\) . Distribusi Tweedie diturunkan sebagai jumlah Poisson dari variabel gamma. Untuk memahami distribusi \\(S_N\\) pertama-tama kita akan melihat probabilitas massa pada nilai nol. Kerugian agregat adalah nol ketika tidak ada klaim yang terjadi, yaitu \\({\\rm Pr}(S_N=0)= {\\rm Pr}(N=0)=e^{-\\lambda}.\\) Selain itu, perhatikan bahwa \\(S_N\\) bersyarat pada N = n yang dinotasikan dengan \\(S_n = X_1 + ⋯ + X_n\\) mengikuti distribusi gamma dengan bentuk \\(nα\\) dan skala \\(γ\\) . Dengan demikian, untuk \\(s&gt;0\\) densitas dari distribusi Tweedie dapat dihitung sebagai \\[ \\begin{aligned} f_{S_N}(s)&amp;=\\sum_{n=1}^{\\infty} p_n f_{S_n}(s)\\\\ &amp;=\\sum_{n=1}^{\\infty}e^{-\\lambda}\\frac{(\\lambda)^n}{n!}\\frac{\\gamma^{na}}{\\Gamma(n\\alpha)}s^{n\\alpha-1}e^{-s\\gamma} . \\end{aligned} \\] Dengan demikian, distribusi Tweedie dapat dianggap sebagai campuran antara distribusi nol dan distribusi bernilai positif, yang membuatnya menjadi alat yang mudah digunakan untuk memodelkan klaim asuransi dan untuk menghitung premi murni. Rata-rata dan varians dari model Poisson gabungan Tweedie adalah: \\({\\rm E} (S_N)=\\lambda\\frac{\\alpha}{\\gamma}~~~~{\\rm and}~~~~{\\rm Var} (S_N)=\\lambda\\frac{\\alpha(1+\\alpha)}{\\gamma^2}.\\) Sebagai fitur penting lainnya, distribusi Tweedie adalah kasus khusus dari model dispersi eksponensial, sebuah kelas model yang digunakan untuk menggambarkan komponen acak dalam model linier umum. Untuk melihat hal ini, kami mempertimbangkan reparameterisasi berikut: \\[ \\begin{equation*} \\lambda=\\frac{\\mu^{2-p}}{\\phi(2-p)},~~~~\\alpha=\\frac{2-p}{p-1},~~~~\\frac{1}{\\gamma}=\\phi(p-1)\\mu^{p-1} . \\end{equation*} \\] Dengan hubungan di atas, kita dapat menunjukkan bahwa distribusi \\(S_N\\) adalah \\(f_{S_N}(s)=\\exp\\left[\\frac{1}{\\phi}\\left(\\frac{-s}{(p-1)\\mu^{p-1}}-\\frac{\\mu^{2-p}}{2-p}\\right)+C(s;\\phi)\\right]\\) Dimana \\[ C(s;\\phi)=\\left\\{\\begin{array}{ll} \\displaystyle 0 &amp; {\\rm if}~ s=0 \\\\ \\displaystyle \\log \\sum\\limits_{n \\ge 1} \\left\\{\\frac{(1/\\phi)^{1/(p-1)}s^{(2-p)/(p-1)}}{(2-p)(p-1)^{(2-p)/(p-1)}}\\right\\}^{n}\\frac{1}{n!~\\Gamma[n(2-p)/(p-1)]s} &amp; {\\rm if}~ s&gt;0 . \\end{array}\\right. \\] Oleh karena itu, distribusi \\(S_N\\) termasuk ke dalam keluarga eksponensial dengan parameter \\(μ\\) , \\(ϕ\\) , dan \\(1&lt;p&lt;2\\) , dan kita memiliki \\({\\rm E} (S_N)=\\mu~~~~{\\rm and}~~~~{\\rm Var} (S_N)=\\phi\\mu^{p}.\\) Hal ini memungkinkan kita untuk menggunakan distribusi Tweedie dengan model linear umum untuk memodelkan klaim. Perlu juga disebutkan dua kasus pembatas dari model Tweedie: \\(p→1\\) menghasilkan distribusi Poisson dan \\(p → 2\\) menghasilkan distribusi gamma. Dengan demikian, model Tweedie mengakomodasi situasi di antara distribusi gamma dan Poisson, yang secara intuitif masuk akal karena merupakan jumlah Poisson dari variabel acak gamma. ======= 5.3 5.4 Menghitung Distribusi Klaim Agregat Bagian ini membahas dua pendekatan praktis untuk menghitung distribusi kerugian agregat, yaitu metode rekursif dan simulasi. 5.3.1 metode rekursif penggunaan metode rekursif untuk membangun model majemuk dengan komponen frekuensi \\(N\\) yang termasuk dalam kelas \\((a,b,0)\\) atau \\((a,b,1)\\), dan komponen tingkat keparahan \\(X\\) yang memiliki distribusi diskrit. Namun, jika distribusi tingkat keparahan \\(X\\) kontinu, praktik yang umum dilakukan adalah mendiskritkan distribusinya terlebih dahulu agar metode rekursif dapat diterapkan. Dalam hal ini, diasumsikan bahwa N termasuk dalam kelas \\((a,b,1)\\), sehingga nilai probabilitas \\(N\\) pada waktu \\(k\\) dinyatakan sebagai \\(pk = (a + bk) pk-1\\). Selanjutnya, diasumsikan bahwa support (nilai yang mungkin) dari \\(X\\) terbatas pada \\({0,1,...,m}\\), dan distribusinya diskrit. Oleh karena itu, fungsi probabilitas dari \\(S_N\\) dapat dinyatakan dalam \\[\\begin{aligned} f_{S_N}(s)&amp;=\\Pr (S_N=s) \\\\ &amp;=\\frac{1}{1-af_{X}(0)}\\left\\{ \\left[ p_1 -(a+b)p_{0}\\right] f_X (s)+\\sum_{x=1}^{s\\wedge m}\\left( a+\\frac{bx}{s} \\right) f_X (x)f_{S_N}(s-x)\\right\\}. \\end{aligned}\\] Jika \\(N\\) berada dalam kelas \\((a,b,0)\\) maka \\(p1 = (a + b)p0\\) dan seterusnya \\[\\begin{align*} f_{S_N}(s)=\\frac{1}{1-af_X (0)}\\left\\{ \\sum_{x=1}^{s\\wedge m}\\left( a+\\frac{bx }{s}\\right) f_X (x)f_{S_N}(s-x)\\right\\}. \\end{align*}\\] karena model ARIMA yang digunakan berbeda. Persamaan tersebut hanya memperhitungkan faktor skala \\(a\\) dan \\(b\\) dan mengakumulasi probabilitas dari setiap nilai \\(x\\) dari \\(X\\) hingga mencapai nilai \\(s\\) yang diinginkan 5.3.1.1 contoh Jumlah klaim dalam suatu periode \\(N\\) memiliki distribusi geometrik dengan mean 4. Besarnya setiap klaim \\(X\\) mengikuti \\(Pr(X=x)=0.25\\), untuk \\(x=1,2,3,4\\). Jumlah klaim dan jumlah klaim bersifat independen. \\(S_N\\) adalah jumlah klaim keseluruhan pada periode tersebut. Hitunglah \\(F_{S_N}(3)\\). Solusi Distribusi tingkat keparahan \\(X\\) adalah sebagai berikut \\(f_X(x)=\\frac14\\), \\(x=1,2,3,4\\). Distribusi frekuensi \\(N\\) adalah geometris dengan rata-rata 4, yang merupakan anggota dari kelas \\((a,b,0)\\) dengan \\(b=0\\), \\(a=\\frac\\beta{1+\\beta}=\\frac45\\), dan \\(p0=\\frac1{1+\\beta}=\\frac15\\). nilai dari komponen tingkat keparahan \\(X\\) adalah \\({1,…,m=4}\\), yang bersifat diskrit dan terbatas. Dengan demikian, kita dapat menggunakan metode rekursif \\[\\begin{aligned} f_{S_N} (x) &amp;= 1 \\sum_{y=1}^{x\\wedge m} (a+0) f_X (y) f_{S_N} (x-y) \\\\ &amp;= \\frac{4}{5} \\sum_{y=1}^{x\\wedge m} f_X (y) f_{S_N} (x-y) . \\end{aligned}\\] Solusi ditemukan dengan menggunakan metode rekursif, di mana fungsi probabilitas \\(f_{S_N}(x)\\) untuk setiap nilai \\(x\\) dihitung menggunakan rumus \\(f_{S_N}(x) = \\sum_{y=1}^{x\\wedge m} (a+0) f_X(y) f_{S_N}(x-y)\\), di mana \\(m=4\\) adalah nilai maksimum dari distribusi nilai klaim \\(X\\), dan \\(a=\\frac{\\beta}{1+\\beta}=\\frac{4}{5}\\) dan \\(p_0=\\frac{1}{1+\\beta}=\\frac{1}{5}\\) adalah parameter dari distribusi frekuensi geometri yang diberikan. khususnya kita memiliki \\[\\begin{aligned} f_{S_N} (0) &amp;= \\Pr(N=0) = p_0=\\frac{1}{5}\\\\ f_{S_N} (1) &amp;= \\frac{4}{5}\\sum_{y=1}^{1} f_X (y) f_{S_N} (1-y) = \\frac{4}{5} f_X(1) f_{S_N}(0)\\\\ &amp;= \\frac{4}{5}\\left( \\frac{1}{4}\\right)\\left(\\frac{1}{5} \\right) = \\frac{1}{25}\\\\ f_{S_N} (2) &amp;= \\frac{4}{5}\\sum_{y=1}^{2} f_X (y) f_{S_N} (2-y) = \\frac{4}{5} \\left[ f_X(1)f_{S_N}(1) + f_X(2) f_{S_N}(0) \\right] \\\\ &amp;= \\frac{4}{5}\\left[ \\frac{1}{4} \\left( \\frac{1}{25} + \\frac{1}{5}\\right) \\right] = \\frac{4}{5}\\left( \\frac{6}{100}\\right) = \\frac{6}{125}\\\\ f_{S_N} (3) &amp;= \\frac{4}{5} \\left[ f_X(1) f_{S_N}(2) + f_X(2)f_{S_N}(1) + f_X(3) f_{S_N}(0) \\right]\\\\ &amp;= \\frac{4}{5}\\left[ \\frac{1}{4} \\left( \\frac{1}{25} + \\frac{1}{5} + \\frac{6}{125}\\right) \\right] = \\frac{1}{5}\\left( \\frac{5+25+6}{125}\\right) = 0.0576\\\\ \\Rightarrow \\ F_{S_N} (3) &amp;= f_{S_N} (0)+f_{S_N} (1)+f_{S_N} (2) +f_{S_N} (3) = 0.3456 . \\end{aligned}\\] Setelah menghitung nilai \\(f_{S_N}(0)\\), \\(f_{S_N}(1)\\), \\(f_{S_N}(2)\\), dan \\(f_{S_N}(3)\\), fungsi distribusi kumulatif \\(F_{S_N}(3)\\) diperoleh dengan menjumlahkan nilai-nilai tersebut. Hasil akhirnya adalah \\(F_{S_N}(3) = 0.3456\\). 5.3.2 simulasi Distribusi kerugian agregat dapat dievaluasi dengan menggunakan simulasi Monte Carlo. Untuk kerugian agregat, Simulasi Monte Carlo digunakan untuk menghasilkan sampel acak dari kerugian keseluruhan berdasarkan distribusi probabilitas yang dianggap sesuai untuk distribusi frekuensi dan tingkat keparahan klaim. gunanya adalah seseorang dapat menghitung distribusi empiris dari \\(S_N\\) dengan menggunakan sampel acak. Nilai ekspektasi dan varians dari kerugian agregat juga dapat diperkirakan dengan menggunakan rata-rata sampel dan varians sampel dari nilai simulasi. Sekarang kita rangkum prosedur simulasi untuk model kerugian agregat. Misalkan \\(m\\) adalah ukuran sampel acak yang dihasilkan dari kerugian agregat. Individual Risk Model: $S_n = X_1 + ⋯ + X_n $ misalkan \\(j=1,...,m\\) menjadi penghitung, dimulai dari \\(j = 1\\) Hitung setiap realisasi kerugian individu \\(x_{ij}\\) untuk \\(i=1,...,n\\) . Sebagai contoh, hal ini dapat dilakukan dengan menggunakan metode transformasi invers Hitung kerugian keseluruhan \\(s_j = x_{1j} + ⋯ + x_{nj}\\). terkahir Ulangi dua langkah di atas untuk \\(j=2,...,m\\) untuk mendapatkan sampel berukuran \\(m\\) dari \\(S_n\\), dengan kata lain \\({s_1,...,s_m}\\). Collective Risk Model : \\(S_n = X_1 + ... + X_n\\) misalkan \\(j=1,...,m\\) menjadi penghitung, dimulai dari \\(j = 1\\) Hitung jumlah klaim \\(n_j\\) dari distribusi frekuensi \\(N\\). Diberikan \\(n_j\\), hasilkan jumlah setiap klaim secara independen dari distribusi tingkat keparahan \\(X\\), dilambangkan dengan \\(x_{1j},...,x_{{n_j}j}\\). Hitung kerugian keseluruhan \\(s_j = x_{1j} + ⋯ + x_{{n_j}j}\\). Ulangi tiga langkah di atas untuk \\(j=2,...,m\\) untuk mendapatkan sampel berukuran \\(m\\) dari \\(S_N\\), dengan kata lain \\({s_1,...,s_m}\\) Dengan sampel acak \\(S\\), distribusi empiris dapat dihitung sebagai \\[\\begin{aligned} \\hat{F}_S(s)=\\frac{1}{m}\\sum_{i=1}^{m}I(s_i\\leq s), \\end{aligned}\\] Untuk individual risk model, kerugian keseluruhan dihitung sebagai jumlah kerugian individu yang acak, sedangkan untuk collective Risk Model, kerugian keseluruhan dihitung sebagai jumlah kerugian dari sejumlah klaim. Dalam kedua kasus, sampel acak dihasilkan dari distribusi probabilitas yang dianggap sesuai, dan kemudian distribusi empiris dari sampel tersebut dihitung untuk memperkirakan distribusi probabilitas dari kerugian agregat. dimana \\(I(\\cdot)\\) adalah fungsi indikator. distribusi empiris \\(\\hat{F}_S(s)\\) akan dikonvergen ke \\({F}_S(s)\\), dikarenakan ukuran sampel \\(m\\rightarrow \\infty\\) Dalam perhitungannya, asumsi-asumsi awal dibuat tentang distribusi probabilitas dan parameter-parameternya, kemudian model-model ini diestimasi menggunakan data yang tersedia dan kualitas kecocokan model dievaluasi menggunakan alat validasi model. Proses ini memberikan cara yang berguna untuk memperkirakan risiko yang terkait dengan kerugian agregat, dan dapat membantu perusahaan atau organisasi dalam merencanakan dan mengelola risiko mereka. Prosedur di atas mengasumsikan bahwa distribusi probabilitas, termasuk nilai parameter, dari distribusi frekuensi dan tingkat keparahan telah diketahui. Dalam praktiknya, kita perlu mengasumsikan distribusi-distribusi ini terlebih dahulu, mengestimasi parameter-parameternya dari data, dan kemudian menilai kualitas kecocokan model dengan menggunakan berbagai alat validasi model. Sebagai contoh, asumsi-asumsi dalam model risiko kolektif menyarankan estimasi dua tahap di mana satu model dikembangkan untuk jumlah klaim \\(N\\) dari data jumlah klaim, dan model lainnya dikembangkan untuk tingkat keparahan klaim \\(X\\) dari data jumlah klaim. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
=======
[["model-selection-and-estimation.html", "Bab 4 Model Selection and Estimation 4.1 Nonparametric Inference 4.2 Model Selection 4.3 Estimasi Menggunakan Data Modifikasi 4.4 Bayesian Inference", " Bab 4 Model Selection and Estimation 4.1 Nonparametric Inference Di bagian ini, Anda mempelajari cara: Perkirakan momen, kuantil, dan distribusi tanpa mengacu pada distribusi parametrik Ringkas data secara grafis tanpa mengacu pada distribusi parametrik Tentukan ukuran yang meringkas penyimpangan parametrik dari kecocokan nonparametrik Gunakan estimator nonparametrik untuk memperkirakan parameter yang dapat digunakan untuk memulai prosedur estimasi parametrik 4.1.1 Estimasi Nonparametrik Pada bagian pembahasan sebelumnya telah mempelajari cara meringkas distribusi dengan cara menghitung, varians, kuantil/persentil, dan sebagainya. Untuk memperkirakan langkah-langkah ringkasan menggunakan kumpulan data, salah satu strateginya adalah: menganggap bentuk parametrik untuk distribusi, seperti binomial negatif untuk frekuensi atau distribusi gamma untuk tingkat keparahan, memperkirakan parameter distribusi itu, gunakan distribusi dengan estimasi parameter untuk menghitung ukuran ringkasan yang diinginkan. Ini adalah pendekatan parametrik . Strategi lain adalah memperkirakan ukuran ringkasan yang diinginkan langsung dari pengamatan tanpa mengacu pada model parametrik. Tidak mengherankan, ini dikenal sebagai pendekatan nonparametrik mempertimbangkan jenis skema pengambilan sampel yang paling dasar dan mengasumsikan bahwa observasi adalah realisasi dari serangkaian variabel acak \\(X_1, \\ldots, X_n\\) yang iid menarik dari distribusi populasi yang tidak diketahui \\(F( ⋅ )\\). Cara yang setara untuk mengatakan ini adalah itu \\(X_1, \\ldots, X_n\\), adalah sampel acak (dengan penggantian) dari F( ⋅) .Kemudian menjelaskan estimator nonparametrik dari banyak ukuran penting yang meringkas sebuah distribusi. 4.1.1.1 Estimator Momen Pada bagian 2.2.2. telah mendefinisikan momen untuk frekuensi dan pada bagian 3.1.1 untuk keparahan. Secara khusus, k -momen ke-, \\(\\mathrm{E~}[X^k] = \\mu^{\\prime}_k\\) , merangkum banyak aspek distribusi untuk berbagai pilihan k . Di Sini, μ′k kadang-kadang disebut k th momen populasi untuk membedakannya dari k momen sampel, \\[\\frac{1}{n} \\sum_{i=1}^n X_i^k ,\\] yang merupakan estimator nonparametrik yang sesuai. Dalam aplikasi tipikal, k adalah bilangan bulat positif, meskipun tidak perlu dalam teori. Kasus khusus yang penting adalah momen pertama di mana \\(k = 1\\) . Dalam hal ini, simbol prima ( \\(\\prime\\) ) dan 1 subskrip biasanya dijatuhkan dan satu digunakan \\(\\mu=\\mu^{\\prime}_1\\) untuk menunjukkan mean populasi, atau hanya mean . Estimator sampel yang sesuai untuk \\(μ\\) disebut rata-rata sampel , dilambangkan dengan bilah di atas variabel acak: \\[\\overline{X} =\\frac{1}{n} \\sum_{i=1}^n X_i .\\] Jenis ringkasan ukuran minat lainnya adalah k -momen pusat ke- , \\(\\mathrm{E~} [(X-\\mu)^k] = \\mu_k\\) . (Kadang-kadang, \\(\\mu^{\\prime}_k\\) disebut k -th momen mentah untuk membedakannya dari momen sentral μk .). Estimator nonparametrik, atau sampel, dari \\(\\mu_k\\) adalah \\[\\frac{1}{n} \\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^k .\\] Momen pusat kedua ( \\(k = 2\\) ) adalah kasus penting yang biasanya akan diberikan simbol baru, \\(\\sigma^2 = \\mathrm{E~} [(X-\\mu)^2]\\) , dikenal sebagai varians . Sifat penduga momen sampel dari varians seperti \\(n^{-1}\\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^2\\) telah dipelajari secara ekstensif tetapi bukan satu-satunya estimator yang mungkin. Versi yang paling banyak digunakan adalah versi di mana ukuran sampel efektif dikurangi satu, jadi kami mendefinisikannya \\[s^2 = \\frac{1}{n-1} \\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^2.\\] Membagi dengan \\(n − 1\\) alih-alih N masalah kecil ketika Anda memiliki ukuran sampel yang besar \\(N\\) seperti yang umum dalam aplikasi asuransi. Estimator varians sampel \\(s^2\\) tidak memihak dalam arti bahwa \\(\\mathrm{E~} [s^2] = \\sigma^2\\) , properti yang diinginkan terutama saat menginterpretasikan hasil analisis. 4.1.1.2 Fungsi Distribusi Empiris Kita telah melihat bagaimana menghitung estimator nonparametrik dari k saat ini \\(\\mathrm{E~} [X^k]\\) . Dengan cara yang sama, untuk fungsi apa pun yang diketahui g (⋅) , kita dapat memperkirakan \\(\\mathrm{E~} [\\mathrm{g}(X)]\\) menggunakan\\(n^{-1}\\sum_{i=1}^n \\mathrm{g}(X_i)\\) Sekarang perhatikan fungsinya \\(\\mathrm{g}(X) = I(X \\le x)\\) untuk tetap \\(X\\) . Di sini, notasi $I( ⋅ \\() adalah fungsi indikator ; itu mengembalikan 1 jika acara ( ⋅ ) benar dan 0 sebaliknya. Perhatikan bahwa sekarang variabel acak\\) g (X$) memiliki distribusi Bernoulli (distribusi binomial dengan \\(n = 1\\) ). Kita dapat menggunakan distribusi ini untuk dengan mudah menghitung jumlah seperti rata-rata dan varians. Misalnya, untuk pilihan ini \\(g (⋅)\\) , nilai harapannya adalah \\(\\mathrm{E~} [I(X \\le x)] = \\Pr(X \\le x) = F(x)\\) , fungsi distribusi dievaluasi pada \\(X\\) . Menggunakan prinsip analog , kami mendefinisikan estimator nonparametrik dari fungsi distribusi \\[ \\begin{aligned} F_n(x) &amp;= \\frac{1}{n} \\sum_{i=1}^n I\\left(X_i \\le x\\right) \\\\ &amp;= \\frac{\\text{number of observations less than or equal to }x}{n} . \\end{aligned} \\] Sebagai $F_N( ⋅ $) didasarkan hanya pada pengamatan dan tidak mengasumsikan keluarga parametrik untuk distribusi, itu nonparametrik dan juga dikenal sebagai fungsi distribusi empiris . Ia juga dikenal sebagai fungsi distribusi kumulatif empiris dan, dalam R, seseorang dapat menggunakan ecdf(.) fungsi tersebut untuk menghitungnya. Contoh 4.1.1. Kumpulan Data Mainan . Sebagai ilustrasi, pertimbangkan kumpulan data fiktif, atau “mainan”. \\(n = 10\\) observasi. Tentukan fungsi distribusi empiris. \\[ {\\small \\begin{array}{c|cccccccccc} \\hline i &amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;10 \\\\ X_i&amp; 10 &amp;15 &amp;15 &amp;15 &amp;20 &amp;23 &amp;23 &amp;23 &amp;23 &amp;30\\\\ \\hline \\end{array} }\\] Kemudian memeriksa bahwa rata-rata sampel adalah \\(\\overline{X} = 19.7\\) dan bahwa varians sampel adalah \\(S^2= 34,45556\\) . Fungsi distribusi empiris yang sesuai adalah \\[ \\begin{aligned} F_n(x) &amp;= \\left\\{ \\begin{array}{ll} 0 &amp; \\text{ for }\\ x&lt;10 \\\\ 0.1 &amp; \\text{ for }\\ 10 \\leq x&lt;15 \\\\ 0.4 &amp; \\text{ for }\\ 15 \\leq x&lt;20 \\\\ 0.5 &amp; \\text{ for }\\ 20 \\leq x&lt;23 \\\\ 0.9 &amp; \\text{ for }\\ 23 \\leq x&lt;30 \\\\ 1 &amp; \\text{ for }\\ x \\geq 30, \\end{array} \\right.\\end{aligned}\\] (xExample &lt;- c(10,rep(15,3),20,rep(23,4),30)) PercentilesxExample &lt;- ecdf(xExample) plot(PercentilesxExample, main=&quot;&quot;,xlab=&quot;x&quot;) 4.1.1.3 Quartiles, Percentiles and Quantiles Pada bagian 3.1.1 median , yaitu angka yang kira-kira setengah dari kumpulan data berada di bawah (atau di atasnya) . Kuartil pertama adalah angka yang kira-kira 25% datanya berada di bawahnya dan kuartil ketiga adalah angka yang kira-kira 75% datanya berada di bawahnya. 100 hal persentil adalah angka sehingga \\(100×p\\) persen dari data di bawahnya. Untuk menggeneralisasi konsep ini, pertimbangkan fungsi distribusi \\(F(⋅\\)) , yang mungkin kontinu atau tidak, dan biarkan Q menjadi pecahan sehingga \\(0 &lt; q&lt; 1\\) . Kami ingin mendefinisikan quantile , katakanlah \\(q_F\\) , menjadi bilangan sedemikian sehingga \\(F(q_F) \\approx q\\) . Perhatikan bahwa ketika \\(q=0.5\\) , \\(q_F\\) adalah median; Kapan \\(q=0.25\\) , \\(q_F\\) adalah kuartil pertama, dan seterusnya. Dengan cara yang sama, ketika \\(q = 0, 0.01, 0.02, \\ldots, 0.99, 1.00\\) , yang dihasilkan QF adalah persentil. Jadi, kuantil menggeneralisasikan konsep median, kuartil, dan persentil. Lebih tepatnya, untuk diberikan \\(0 &lt; q&lt; 1\\) , tentukan q kuantil \\(q_F\\) untuk menjadi nomor yang memenuhi: \\[ \\begin{equation} F(q_F-) \\le q \\le F(q_F) \\tag{4.1} \\end{equation}\\] Untuk mendapatkan pemahaman yang lebih baik tentang definisi ini, mari kita lihat beberapa kasus khusus. Pertama, pertimbangkan kasus di mana X adalah variabel acak kontinu sehingga fungsi distribusi \\(F(⋅)\\) tidak memiliki titik lompatan, seperti yang diilustrasikan pada Gambar 4.2 . Pada gambar ini, beberapa pecahan, Q1 , Q2 , Dan Q3 ditunjukkan dengan kuantil yang sesuai \\(q_{F,1} , q_{F,2} , dan q_{F,3}\\) . Dalam setiap kasus, dapat dilihat bahwa \\(F(q_F-)= F(q_F)\\) sehingga ada kuantil unik. Karena kita dapat menemukan invers unik dari fungsi distribusi di mana saja \\(0 &lt; q&lt; 1\\) , kita bisa menulis \\(q_F= F^{-1}(q)\\) Gambar 4.3 menunjukkan tiga kasus untuk fungsi distribusi. Panel kiri sesuai dengan kasus kontinu yang baru saja dibahas. Panel tengah menampilkan titik lompatan yang serupa dengan yang telah kita lihat dalam fungsi distribusi empiris Gambar 4.1 . Untuk nilai \\(q\\) ditampilkan di panel ini, kami masih memiliki nilai kuantil yang unik \\(q_F\\) . Meskipun ada banyak nilai Q seperti yang \\(F(q_F-) \\le q \\le F(q_F)\\) , untuk nilai tertentu dari \\(q\\) , hanya ada satu solusi untuk persamaan (4.1) . Panel kanan menggambarkan situasi di mana kuantil tidak dapat ditentukan secara unik untuk \\(q\\) ditampilkan karena ada berbagai \\(q_F\\) persamaan yang memuaskan (4.1) . Contoh 4.1.2. Kumpulan Data Mainan: Lanjutan. Tentukan kuantil yang sesuai dengan persentil ke-20, ke-50, dan ke-95. Solusi . Perhatikan Gambar 4.1 . Kasus \\(q=0.20\\) sesuai dengan panel tengah Gambar Gambar 4.3 , jadi persentil ke-20 adalah 15. Kasus \\(q=0.50\\) sesuai dengan panel kanan, jadi mediannya adalah angka antara 20 dan 23 inklusif. Banyak paket perangkat lunak menggunakan rata-rata 21,5 (misalnya R, seperti yang terlihat di bawah). Untuk persentil ke-95, solusinya adalah 30. Kita dapat melihat dari Gambar 4.1 bahwa 30 juga sesuai dengan persentil ke-99 dan ke-99,99. quantile(xExample, probs=c(0.2, 0.5, 0.95), type=6) Dengan mengambil rata-rata tertimbang antara pengamatan data, kuantil empiris yang dihaluskan dapat menangani kasus seperti panel kanan pada Gambar 4.3 . Itu Q kuantil empiris yang dihaluskan didefinisikan sebagai \\[\\hat{\\pi}_q = (1-h) X_{(j)} + h X_{(j+1)}\\] Di mana \\(j=\\lfloor(n+1)q\\rfloor\\) , Dan\\(X_{(1)}, \\ldots, X_{(n)}\\) adalah nilai yang diurutkan (dikenal sebagai statistik urutan ) yang sesuai dengan \\(X_1, \\ldots, X_n\\). (Ingat bahwa tanda kurung ⌊ ⋅ ⌋ adalah fungsi lantai yang menunjukkan nilai bilangan bulat terbesar.) Perhatikan bah wa \\(\\hat{\\pi}_q\\)$ hanyalah sebuah interpolasi linear antara \\(X_{( j )}\\) dan \\(X_{(j+1)}\\). Contoh 4.1.3. Kumpulan Data Mainan: Lanjutan. Tentukan persentil yang dihaluskan ke-50 dan ke-20. Solusi Ambil \\(n = 10\\) Dan \\(q= 0,5\\). Kemudian, \\(j=\\lfloor(11)(0.5) \\rfloor= \\lfloor 5.5 \\rfloor=5\\), . Maka kuantil empiris yang dihaluskan ke-0,5 adalah \\[\\hat{\\pi}_{0.5} = (1-0.5) X_{(5)} + (0.5) X_{(6)} = 0.5 (20) + (0.5)(23) = 21.5.\\] Sekarang ambil \\(n = 10\\) Dan \\(q= 0,2\\) . Pada kasus ini, \\(j=\\lfloor(11)(0.2)\\rfloor=\\lfloor 2.2 \\rfloor=2\\) . Maka kuantil empiris yang dihaluskan ke-0,2 adalah \\[\\hat{\\pi}_{0.2} = (1-0.2) X_{(2)} + (0.2) X_{(3)} = 0.8 (15) + (0.2)(15) = 15.\\] 4.1.1.4 Penduga Kepadatan Variabel Diskrit. Ketika variabel acak adalah diskrit, memperkirakan fungsi massa probabilitas \\(f(x) = \\Pr(X=x)\\) mudah. Kami hanya menggunakan rata-rata sampel, yang didefinisikan sebagai \\[f_n(x) = \\frac{1}{n} \\sum_{i=1}^n I(X_i = x),\\] yang merupakan proporsi sampel sama dengan X Variabel Berkelanjutan dalam Grup. Untuk variabel acak kontinu, pertimbangkan formulasi diskrit di mana domain dari F( ⋅ ) dipartisi oleh konstanta \\(\\{c_0 &lt; c_1 &lt; \\cdots &lt; c_k\\}\\) ke dalam interval bentuk \\([c_{j-1}, c_j)\\) , untuk \\(j=1, \\ldots, k\\) . Pengamatan data dengan demikian “dikelompokkan” berdasarkan interval di mana mereka jatuh. Kemudian, kita dapat menggunakan definisi dasar dari fungsi massa empiris, atau variasi seperti \\[f_n(x) = \\frac{n_j}{n \\times (c_j - c_{j-1})} \\ \\ \\ \\ \\ \\ c_{j-1} \\le x &lt; c_j,\\] Di mana \\(N_J\\) adalah jumlah pengamatan ( \\(X_i\\) ) yang termasuk dalam interval \\([c_{j-1}, c_j)\\). Variabel Berkelanjutan (tidak dikelompokkan). Memperluas gagasan ini ke contoh di mana kami mengamati data individual, perhatikan bahwa kami selalu dapat membuat pengelompokan arbitrer dan menggunakan rumus ini. Lebih formal, biarkan \\(b &gt; 0\\) menjadi konstanta positif kecil, yang dikenal sebagai bandwidth , dan menentukan penaksir kepadatan menjadi \\[\\begin{equation} f_n(x) = \\frac{1}{2nb} \\sum_{i=1}^n I(x-b &lt; X_i \\le x + b) \\tag{4.2} \\end{equation}\\] Secara lebih umum, tentukan penaksir kerapatan kernel dari pdf di X sebagai \\[\\begin{equation} f_n(x) = \\frac{1}{nb} \\sum_{i=1}^n w\\left(\\frac{x-X_i}{b}\\right) , \\tag{4.3} \\end{equation}\\] Di mana w adalah fungsi kerapatan probabilitas yang berpusat di sekitar 0. Perhatikan bahwa persamaan (4.2) adalah kasus khusus penduga kerapatan kernel di mana \\(w(x) = \\frac{1}{2}I(-1 &lt; x \\le 1)\\) , juga dikenal sebagai kernel seragam . Pilihan populer lainnya ditunjukkan pada Tabel 4.1 . \\[{\\small \\begin{matrix} \\begin{array}{l|cc} \\hline \\text{Kernel} &amp; w(x) \\\\ \\hline \\text{Uniform } &amp; \\frac{1}{2}I(-1 &lt; x \\le 1) \\\\ \\text{Triangle} &amp; (1-|x|)\\times I(|x| \\le 1) \\\\ \\text{Epanechnikov} &amp; \\frac{3}{4}(1-x^2) \\times I(|x| \\le 1) \\\\ \\text{Gaussian} &amp; \\phi(x) \\\\ \\hline \\end{array}\\end{matrix} }\\] Di Sini, \\(\\phi(\\cdot)\\) adalah fungsi kepadatan normal standar. Seperti yang akan kita lihat pada contoh berikut, pilihan bandwidth \\(B\\) hadir dengan tradeoff bias-varians antara mencocokkan fitur distribusi lokal dan mengurangi volatilitas. Contoh 4.1.4. Dana Properti. Gambar 4.4 menunjukkan histogram (dengan persegi panjang abu-abu yang diarsir) dari klaim properti logaritmik dari tahun 2010. Kurva tebal (biru) mewakili kerapatan kernel Gaussian di mana bandwidth dipilih secara otomatis menggunakan aturan ad hoc berdasarkan ukuran sampel dan volatilitas data ini . Untuk dataset ini, bandwidth ternyata b = 0,3255 . Sebagai perbandingan, kurva putus-putus (merah) menunjukkan penaksir densitas dengan lebar pita sama dengan 0,1 dan kurva halus berwarna hijau menggunakan lebar pita 1. Sebagaimana diantisipasi, lebar pita yang lebih kecil (0,1) menunjukkan mengambil rata-rata lokal dengan data yang lebih sedikit sehingga kita mendapatkan ide yang lebih baik dari rata-rata lokal, tetapi dengan harga volatilitas yang lebih tinggi. Sebaliknya, bandwidth yang lebih besar (1) memperhalus fluktuasi lokal, menghasilkan kurva yang lebih halus yang mungkin melewatkan gangguan pada rata-rata lokal. Untuk aplikasi aktuaria, kami terutama menggunakan estimator densitas kernel untuk mendapatkan kesan visual cepat dari data. Dari perspektif ini, Anda cukup menggunakan aturan ad hoc default untuk pemilihan bandwidth, mengetahui bahwa Anda memiliki kemampuan untuk mengubahnya tergantung pada situasi yang dihadapi. ClaimLev &lt;- read.csv(&quot;Data/CLAIMLEVEL.csv&quot;, header=TRUE); #nrow(ClaimLev); # 6258 ClaimData&lt;-subset(ClaimLev,Year==2010); #2010 subset #Density Comparison hist(log(ClaimData$Claim), main=&quot;&quot;, ylim=c(0,.35),xlab=&quot;Log Expenditures&quot;, freq=FALSE, col=&quot;lightgray&quot;) lines(density(log(ClaimData$Claim)), col=&quot;blue&quot;,lwd=2.5) lines(density(log(ClaimData$Claim), bw=1), col=&quot;green&quot;) lines(density(log(ClaimData$Claim), bw=.1), col=&quot;red&quot;, lty=3) legend(&quot;topright&quot;, c(&quot;b=0.3255 (default)&quot;, &quot;b=0.1&quot;, &quot;b=1.0&quot;), lty=c(1,3,1), lwd=c(2.5,1,1), col=c(&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;), cex=1) #density(log(ClaimData$Claim))$bw ##default bandwidth Estimator densitas nonparametrik, seperti estimator kernel, sering digunakan dalam praktik. Konsep ini juga dapat diperluas untuk memberikan versi halus dari fungsi distribusi empiris. Mengingat definisi penaksir densitas kernel, penaksir kernel dari fungsi distribusi dapat ditemukan sebagai \\[\\begin{aligned} \\tilde{F}_n(x) = \\frac{1}{n} \\sum_{i=1}^n W\\left(\\frac{x-X_i}{b}\\right).\\end{aligned}\\] Di mana \\(W\\) adalah fungsi distribusi yang terkait dengan densitas kernel \\(w\\) . Sebagai ilustrasi, untuk kernel yang seragam, kita punya \\(w(y) = \\frac{1}{2}I(-1 &lt; y \\le 1)\\) , Jadi \\[\\begin{aligned} W(y) = \\begin{cases} 0 &amp; y&lt;-1\\\\ \\frac{y+1}{2}&amp; -1 \\le y &lt; 1 \\\\ 1 &amp; y \\ge 1 \\\\ \\end{cases}\\end{aligned} .\\] Contoh 4.1.5. Soal Ujian Aktuaria. Anda mempelajari lima nyawa untuk memperkirakan waktu dari timbulnya penyakit hingga kematian. Waktu kematian adalah: \\[\\begin{array}{ccccc} 2 &amp; 3 &amp; 3 &amp; 3 &amp; 7 \\\\ \\end{array}\\] Menggunakan kernel segitiga dengan bandwidth 2 , hitung taksiran fungsi densitas pada 2,5. Solusi. Untuk perkiraan kepadatan kernel, kami punya \\[f_n(x) = \\frac{1}{nb} \\sum_{i=1}^n w\\left(\\frac{x-X_i}{b}\\right),\\] Di mana \\(n = 5\\) , \\(b = 2\\) , Dan \\(x = 2,5\\) . Untuk inti segitiga, \\(w(x) = (1-|x|)\\times I(|x| \\le 1)\\) . Dengan demikian, \\[\\begin{array}{c|c|c} \\hline X_i &amp; \\frac{x-X_i}{b} &amp; w\\left(\\frac{x-X_i}{b} \\right) \\\\ \\hline 2 &amp; \\frac{2.5-2}{2}=\\frac{1}{4} &amp; (1-\\frac{1}{4})(1) = \\frac{3}{4} \\\\ \\hline 3 &amp; &amp; \\\\ 3 &amp; \\frac{2.5-3}{2}=\\frac{-1}{4} &amp; \\left(1-\\left| \\frac{-1}{4} \\right| \\right)(1) = \\frac{3}{4} \\\\ 3 &amp; &amp; \\\\ \\hline 7 &amp; \\frac{2.5-7}{2}=-2.25 &amp; (1-|-2.25|)(0) = 0\\\\ \\hline \\end{array}\\] Kemudian perkiraan densitas kernel di \\(x = 2,5\\) adalah \\[f_n(2.5) = \\frac{1}{5(2)}\\left( \\frac{3}{4} + (3) \\frac{3}{4} + 0 \\right) = \\frac{3}{10}\\] 4.1.1.5 Plug-in Principle Salah satu cara untuk membuat penaksir nonparametrik dari beberapa kuantitas adalah dengan menggunakan prinsip analog atau plug-in di mana seseorang menggantikan cdf yang tidak diketahui \\(F\\) dengan estimasi yang diketahui seperti cdf empiris \\(F_N\\) . Jadi, jika kita mencoba memperkirakan \\(\\mathrm{E}~[\\mathrm{g}(X)]=\\mathrm{E}_F~[\\mathrm{g}(X)]\\) untuk fungsi generik g , maka kami mendefinisikan estimator nonparametrik menjadi \\(\\mathrm{E}_{F_n}~[\\mathrm{g}(X)]=n^{-1}\\sum_{i=1}^n \\mathrm{g}(X_i)\\). Untuk melihat cara kerjanya, sebagai kasus khusus dari g , kami menganggap kerugian per variabel acak pembayaran \\(Y = (X-d)_+\\) dan rasio eliminasi kerugian yang diperkenalkan di Bagian 3.4.1. Kita dapat mengungkapkan ini sebagai \\[LER(d) = \\frac{\\mathrm{E~}[X - (X-d)_+]}{\\mathrm{E~}[X]} =\\frac{\\mathrm{E~}[\\min(X,d)]}{\\mathrm{E~}[X]} ,\\] Contoh. 4.1.6. Klaim Cidera Tubuh dan Rasio Penghapusan Kerugian Kami menggunakan sampel 432 klaim mobil tertutup dari Boston dari Derrig, Ostaszewski, dan Rempala ( 2001 ) . Kerugian dicatat untuk pembayaran karena cedera tubuh dalam kecelakaan mobil. Kerugian tidak dapat dikurangkan tetapi dibatasi oleh berbagai jumlah pertanggungan maksimum yang juga tersedia dalam data. Ternyata hanya 17 dari 432 ( ≈ 4%) tunduk pada batasan kebijakan ini sehingga kami mengabaikan data ini untuk ilustrasi ini. Kerugian rata-rata yang dibayarkan adalah 6906 dalam dolar AS. Gambar 4.5 menunjukkan aspek lain dari distribusi. Secara khusus, panel sebelah kiri menunjukkan fungsi distribusi empiris, panel sebelah kanan memberikan plot kepadatan nonparametrik. Dampak kerugian cedera tubuh dapat dikurangi dengan pengenaan limit atau pembelian polis reasuransi (lihat Bagian 10.3). Untuk mengukur dampak dari alat mitigasi risiko ini, biasanya menghitung rasio eliminasi kerugian (LER) seperti yang diperkenalkan di Bagian 3.4.1. Fungsi distribusi tidak tersedia sehingga harus diestimasi dengan cara tertentu. Menggunakan prinsip plug-in, estimator nonparametrik dapat didefinisikan sebagai \\[LER_n(d) = \\frac{n^{-1} \\sum_{i=1}^n \\min(X_i,d)}{n^{-1} \\sum_{i=1}^n X_i} = \\frac{\\sum_{i=1}^n \\min(X_i,d)}{\\sum_{i=1}^n X_i} .\\] Gambar 4.6 menunjukkan estimator \\(LER_n(d)\\) untuk berbagai pilihan \\(d\\) . Misalnya, di \\(d= 1.000\\) dan punya \\(LER_n( 1000 ) ≈ 0,1442\\). Dengan demikian, memberlakukan batas 1.000 berarti ekspektasi klaim yang ditahan 14,42 persen lebih rendah bila dibandingkan dengan ekspektasi klaim dengan deductible nol. 4.1.2 Tools for Model Selection and Diagnostics Bagian sebelumnya memperkenalkan estimator nonparametrik di mana tidak ada bentuk parametrik yang diasumsikan tentang distribusi yang mendasarinya. Namun, dalam banyak aplikasi aktuaria, analis berusaha menggunakan kecocokan parametrik dari distribusi untuk kemudahan penjelasan dan kemampuan untuk memperluasnya ke situasi yang lebih kompleks seperti memasukkan variabel penjelas dalam pengaturan regresi. Saat memasang distribusi parametrik, seorang analis mungkin mencoba menggunakan distribusi gamma untuk mewakili sekumpulan data kerugian. Namun, analis lain mungkin lebih suka menggunakan distribusi Pareto. Bagaimana cara menentukan model mana yang akan dipilih? Alat nonparametrik dapat digunakan untuk menguatkan pemilihan model parametrik. Pada dasarnya, pendekatannya adalah untuk menghitung langkah-langkah ringkasan yang dipilih di bawah model parametrik yang dipasang dan membandingkannya dengan kuantitas yang sesuai di bawah model nonparametrik. Karena model nonparametrik tidak mengasumsikan distribusi tertentu dan hanya merupakan fungsi dari data, model ini digunakan sebagai tolok ukur untuk menilai seberapa baik distribusi/model parametrik mewakili data. Juga, ketika ukuran sampel meningkat, distribusi empiris hampir pasti menyatu dengan distribusi populasi yang mendasarinya (berdasarkan hukum jumlah besar yang kuat). Dengan demikian distribusi empiris adalah proksi yang baik untuk populasi. Perbandingan estimator parametrik dengan nonparametrik dapat mengingatkan analis akan kekurangan dalam model parametrik dan terkadang menunjukkan cara untuk meningkatkan spesifikasi parametrik. Prosedur diarahkan menilai validitas model yang dikenal sebagaidiagnostik model . 4.1.2.1 Perbandingan Grafik Distribusi Kita telah melihat teknik overlay grafik untuk tujuan perbandingan. Untuk memperkuat penerapan teknik ini, Gambar 4.7membandingkan distribusi empiris dengan dua distribusi pas parametrik. Panel kiri menunjukkan fungsi distribusi distribusi klaim. Titik-titik yang membentuk kurva “berbentuk S” mewakili fungsi distribusi empiris pada setiap pengamatan. Kurva biru tebal memberikan nilai yang sesuai untuk distribusi gamma yang pas dan ungu muda untuk distribusi Pareto yang pas. Karena Pareto lebih dekat dengan fungsi distribusi empiris daripada gamma, ini memberikan bukti bahwa Pareto adalah model yang lebih baik untuk kumpulan data ini. Panel kanan memberikan informasi serupa untuk fungsi kerapatan dan memberikan pesan yang konsisten. Berdasarkan (hanya) angka-angka ini, distribusi Pareto adalah pilihan yang jelas bagi analis. Untuk cara lain untuk membandingkan kesesuaian dua model yang cocok, pertimbangkan plot probabilitas-probabilitas (\\(pp\\)) . A \\[pp\\] plot membandingkan probabilitas kumulatif di bawah dua model. Untuk tujuan kami, kedua model ini adalah fungsi distribusi empiris nonparametrik dan model pas parametrik. Gambar 4.8 menunjukkan \\(pp\\) plot untuk data Dana Properti yang diperkenalkan di Bagian 1.3 . Gamma yang dipasang di sebelah kiri dan Pareto yang dipasang di sebelah kanan, dibandingkan dengan fungsi distribusi data empiris yang sama. Garis lurus mewakili kesetaraan antara dua distribusi yang dibandingkan, sehingga titik yang dekat dengan garis diinginkan. Seperti yang terlihat pada demonstrasi sebelumnya, Pareto jauh lebih dekat dengan distribusi empiris daripada gamma, memberikan bukti tambahan bahwa Pareto adalah model yang lebih baik. Itu QQ plot membandingkan dua model yang dipasang melalui kuantilnya. Seperti hal hal plot, kami membandingkan nonparametrik dengan model pas parametrik. Kuantil dapat dievaluasi pada setiap titik kumpulan data, atau pada kisi (misalnya, di 0 , 0,001 , 0,002 , … , 0,999 , 1,000 ), tergantung aplikasinya. Pada Gambar 4.9 , untuk setiap titik pada kisi tersebut, sumbu horizontal menampilkan kuantil empiris dan sumbu vertikal menampilkan kuantil parametrik yang sesuai (gamma untuk dua panel atas, Pareto untuk dua panel bawah). Kuantil diplot pada skala asli di panel kiri dan pada skala log di panel kanan untuk memungkinkan kita melihat di mana kekurangan distribusi yang pas. Garis lurus mewakili kesetaraan antara distribusi empiris dan distribusi pas. Dari plot ini, kita sekali lagi melihat bahwa Pareto secara keseluruhan lebih cocok daripada gamma. Selain itu, panel kanan bawah menunjukkan bahwa distribusi Pareto bekerja dengan baik dengan klaim besar, tetapi memberikan kecocokan yang lebih buruk untuk klaim kecil. Contoh 4.1.7. Soal Ujian Aktuaria. Grafik di bawah ini menunjukkan \\(pp\\) plot distribusi pas dibandingkan dengan sampel. Solusi. Ekor dari distribusi yang pas terlalu tebal di sebelah kiri, terlalu tipis di sebelah kanan, dan distribusi yang pas memiliki probabilitas yang lebih kecil di sekitar median daripada sampel. Untuk melihat ini, ingat bahwa hal hal plot grafik distribusi kumulatif dari dua distribusi pada sumbunya (empiris pada sumbu x dan dipasang pada sumbu y dalam kasus ini). Untuk nilai kecil dari X , model yang dipasang memberikan probabilitas yang lebih besar untuk berada di bawah nilai itu daripada yang terjadi dalam sampel (mis F( x ) &gt;FN( x ) ). Ini menunjukkan bahwa model memiliki ekor kiri yang lebih berat daripada datanya. Untuk nilai besar dari X , model kembali memberikan probabilitas yang lebih besar untuk berada di bawah nilai itu dan dengan demikian lebih kecil kemungkinannya untuk berada di atas nilai itu (mis S( x ) &lt;SN( x ) ). Hal ini menunjukkan bahwa model memiliki ekor kanan yang lebih ringan dari pada data. Selain itu, saat kita mulai dari 0,4 hingga 0,6 pada sumbu horizontal (dengan demikian melihat 20% tengah data), hal hal plot meningkat dari sekitar 0,3 menjadi 0,4. Ini menunjukkan bahwa model hanya menempatkan sekitar 10% dari probabilitas dalam kisaran ini. 4.1.2.2 Graphical Comparison of Distributions Saat memilih model, akan sangat membantu untuk menampilkan tampilan grafis. Namun, untuk melaporkan hasil, melengkapi tampilan grafis dengan statistik terpilih yang meringkas kebaikan kesesuaian model dapat efektif. Tabel 4.2 menyediakan tiga statistik kebaikan yang umum digunakan . Dalam tabel ini, \\(F_N\\) adalah distribusi empiris, \\(F\\) adalah distribusi pas atau hipotesis, dan \\(F_i^* = F(x_i)\\) . \\[{\\small \\begin{matrix} \\begin{array}{l|cc} \\hline \\text{Statistic} &amp; \\text{Definition} &amp; \\text{Computational Expression} \\\\ \\hline \\text{Kolmogorov-} &amp; \\max_x |F_n(x) - F(x)| &amp; \\max(D^+, D^-) \\text{ where } \\\\ ~~~\\text{Smirnov} &amp;&amp; D^+ = \\max_{i=1, \\ldots, n} \\left|\\frac{i}{n} - F_i^*\\right| \\\\ &amp;&amp; D^- = \\max_{i=1, \\ldots, n} \\left| F_i^* - \\frac{i-1}{n} \\right| \\\\ \\text{Cramer-von Mises} &amp; n \\int (F_n(x) - F(x))^2 f(x) dx &amp; \\frac{1}{12n} + \\sum_{i=1}^n \\left(F_i^* - (2i-1)/n\\right)^2 \\\\ \\text{Anderson-Darling} &amp; n \\int \\frac{(F_n(x) - F(x))^2}{F(x)(1-F(x))} f(x) dx &amp; -n-\\frac{1}{n} \\sum_{i=1}^n (2i-1) \\log\\left(F_i^*(1-F_{n+1-i})\\right)^2 \\\\ \\hline \\end{array} \\\\ \\end{matrix} }\\] Statistik Kolmogorov-Smirnov adalah perbedaan absolut maksimum antara fungsi distribusi yang dipasang dan fungsi distribusi empiris. Alih-alih membandingkan perbedaan antara titik tunggal, statistik Cramer-von Mises mengintegrasikan perbedaan antara fungsi distribusi empiris dan pas pada seluruh rentang nilai. Statistik Anderson-Darling juga mengintegrasikan perbedaan ini pada rentang nilai, meskipun diboboti oleh kebalikan dari varian. Oleh karena itu lebih menekankan pada ekor distribusi (yaitu kapan \\(F( x )\\) atau \\(1-F(x)=S(x)\\) kecil). Contoh 4.1.8. Soal Ujian Aktuaria (dimodifikasi). Contoh pembayaran klaim adalah: \\[\\begin{array}{ccccc} 29 &amp; 64 &amp; 90 &amp; 135 &amp; 182 \\\\ \\end{array}\\] Bandingkan distribusi klaim empiris dengan distribusi eksponensial dengan rata-rata 100 dengan menghitung nilai statistik uji Kolmogorov-Smirnov. Solusi. Untuk distribusi eksponensial dengan rata-rata 100 , fungsi distribusi kumulatif adalah \\(F(x)=1-e^{-x/100}\\) . Dengan demikian, \\[\\begin{array}{ccccc} \\hline x &amp; F(x) &amp; F_n(x) &amp; F_n(x-) &amp; \\max(|F(x)-F_n(x)|,|F(x)-F_n(x-)|) \\\\ \\hline 29 &amp; 0.2517 &amp; 0.2 &amp; 0 &amp; \\max(0.0517, 0.2517) = 0.2517 \\\\ 64 &amp; 0.4727 &amp; 0.4 &amp; 0.2 &amp; \\max(0.0727, 0.2727) = 0.2727 \\\\ 90 &amp; 0.5934 &amp; 0.6 &amp; 0.4 &amp; \\max(0.0066, 0.1934) = 0.1934 \\\\ 135 &amp; 0.7408 &amp; 0.8 &amp; 0.6 &amp; \\max(0.0592, 0.1408) = 0.1408 \\\\ 182 &amp; 0.8380 &amp; 1 &amp; 0.8 &amp; \\max(0.1620, 0.0380) = 0.1620 \\\\ \\hline \\end{array}\\] Oleh karena itu, statistik uji Kolmogorov-Smirnov adalah \\[KS = \\max(0.2517, 0.2727, 0.1934, 0.1408, 0.1620) = 0.2727 .\\] 4.1.3 Starting Values Metode pencocokan momen dan persentil merupakan metode estimasi nonparametrik yang memberikan alternatif kemungkinan maksimum. Umumnya, kemungkinan maksimum adalah teknik yang lebih disukai karena menggunakan data secara lebih efisien. (Lihat Lampiran Bab 17 untuk definisi efisiensi yang tepat.) Namun, metode pencocokan momen dan persentil berguna karena lebih mudah diinterpretasikan dan karena itu memungkinkan aktuaris atau analis untuk menjelaskan prosedur kepada orang lain. Selain itu, prosedur estimasi numerik (misalnya jika dilakukan di R) untuk kemungkinan maksimum adalah iteratif dan membutuhkan nilai awal untuk memulai proses rekursif. Meskipun banyak masalah yang kuat untuk pemilihan nilai awal, untuk beberapa situasi kompleks, penting untuk memiliki nilai awal yang mendekati nilai optimal (tidak diketahui). Metode momen dan pencocokan persentil adalah teknik yang dapat menghasilkan perkiraan yang diinginkan tanpa investasi komputasi yang serius dan dengan demikian dapat digunakan sebagai nilai awal untuk menghitung kemungkinan maksimum. 4.1.3.1 Method of Moments Metode ini merupakan estimasi parameter populasi dengan pendekatan momen parametrik menggunakan momen sampel empiris. pada momen ini, momen distribusi parametrik menggunakan momen empiris atau nonparametrik kemudian dapat dipecahkan secara aljabar untuk estimasi parameter. Contoh 4.1.9. Dana Properti. Untuk dana properti 2010, ada \\(n = 1 , 377\\) klaim individu (dalam ribuan dolar) dengan \\[m_1 = \\frac{1}{n} \\sum_{i=1}^n X_i = 26.62259 \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ m_2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 = 136154.6 .\\] Sesuaikan parameter distribusi gamma dan Pareto menggunakan metode momen. Solusi. Agar sesuai dengan distribusi gamma, kami memiliki \\(\\mu_1 = \\alpha \\theta\\) Dan \\(\\mu_2^{\\prime} = \\alpha(\\alpha+1) \\theta^2\\) . Menyamakan keduanya menghasilkan metode penaksir momen, aljabar mudah menunjukkannya \\[\\alpha = \\frac{\\mu_1^2}{\\mu_2^{\\prime}-\\mu_1^2} \\ \\ \\ \\text{and} \\ \\ \\ \\theta = \\frac{\\mu_2^{\\prime}-\\mu_1^2}{\\mu_1}.\\] Jadi, metode penduga momen adalah \\[\\begin{aligned} \\hat{\\alpha} &amp;= \\frac{26.62259^2}{136154.6-26.62259^2} = 0.005232809 \\\\ \\hat{\\theta} &amp;= \\frac{136154.6-26.62259^2}{26.62259} = 5,087.629. \\end{aligned}\\] Sebagai perbandingan, nilai kemungkinan maksimum berubah menjadi \\(\\hat{\\alpha}_{MLE} = 0.2905959\\) Dan \\(\\hat{\\theta}_{MLE} = 91.61378\\) , jadi ada perbedaan besar antara dua prosedur estimasi. Ini adalah salah satu indikasi, seperti yang telah kita lihat sebelumnya, bahwa model gamma kurang cocok. Sebaliknya, sekarang asumsikan distribusi Pareto sehingga \\(\\mu_1 = \\theta/(\\alpha -1)\\) Dan \\(\\mu_2^{\\prime} = 2\\theta^2/((\\alpha-1)(\\alpha-2) )\\) . Perhatikan bahwa ungkapan ini untuk μ′2 hanya berlaku untuk α &gt; 2 . Pertunjukan aljabar yang mudah \\[\\alpha = 1+ \\frac{\\mu_2^{\\prime}}{\\mu_2^{\\prime}-\\mu_1^2} \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ \\ \\theta = (\\alpha-1)\\mu_1.\\] Jadi, metode penduga momen adalah \\[ \\begin{aligned} \\hat{\\alpha} &amp;= 1+ \\frac{136154.6}{136154.6-26,62259^2} = 2.005233 \\\\ \\hat{\\theta} &amp;= (2.005233-1) \\cdot 26.62259 = 26.7619 \\end{aligned}\\] Nilai kemungkinan maksimum berubah menjadi \\(\\hat{\\alpha}_{MLE} = 0.9990936\\) Dan \\(\\hat{\\theta}_{MLE} = 2.2821147\\) . Sangat menarik bahwa \\(\\hat{\\alpha}_{MLE}&lt;1\\) ; untuk distribusi Pareto, ingat itu \\(α &lt; 1\\) berarti rata-ratanya tak terhingga. Ini adalah indikasi lain bahwa kumpulan data klaim properti adalah distribusi ekor panjang. Seperti contoh di atas, ada fleksibilitas dengan metode momen. Misalnya, kita dapat mencocokkan momen kedua dan ketiga alih-alih yang pertama dan kedua, menghasilkan estimator yang berbeda. Selain itu, tidak ada jaminan bahwa solusi akan ada untuk setiap masalah. Untuk data yang disensor atau terpotong, momen pencocokan dimungkinkan untuk beberapa masalah, tetapi secara umum, ini adalah skenario yang lebih sulit. Terakhir, untuk distribusi di mana momen tidak ada atau tidak terbatas, metode momen tidak tersedia. Sebagai alternatif, seseorang dapat menggunakan teknik pencocokan persentil. 4.1.3.2 Percentile Matching Di bawah pencocokan persentil , kami memperkirakan kuantil atau persentil dari distribusi parametrik menggunakan kuantil atau persentil empiris (nonparametrik) yang dijelaskan di Bagian 4.1.1.3 . Contoh 4.1.10. Dana Properti. Untuk dana properti 2010, kami mengilustrasikan pencocokan pada kuantil. Secara khusus, distribusi Pareto secara intuitif menyenangkan karena solusi bentuk tertutup untuk kuantil. Ingatlah bahwa fungsi distribusi untuk distribusi Pareto adalah \\[F(x) = 1 - \\left(\\frac{\\theta}{x+\\theta}\\right)^{\\alpha}.\\] Aljabar mudah menunjukkan bahwa kita dapat menyatakan kuantil sebagai \\[F^{-1}(q) = \\theta \\left( (1-q)^{-1/\\alpha} -1 \\right).\\] untuk sebagian kecil q , \\(0 &lt; q&lt; 1\\). Tentukan estimasi parameter distribusi Pareto menggunakan kuantil empiris ke-25 dan ke-95. Solusi. Persentil ke-25 (kuartil pertama) ternyata adalah 0,78853 dan persentil ke-95 adalah 50.98293 (keduanya dalam ribuan dolar). Dengan dua persamaan \\[0.78853 = \\theta \\left( 1- (1-.25)^{-1/\\alpha} \\right) \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ 50.98293 = \\theta \\left( 1- (1-.75)^{-1/\\alpha} \\right)\\] dan dua yang tidak diketahui, solusinya adalah \\[\\hat{\\alpha} = 0.9412076 \\ \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ \\hat{\\theta} = 2.205617 .\\] Sehingga kesimpulannya adalah rutin numerik diperlukan untuk solusi ini karena tidak ada solusi analitik yang tersedia. Selanjutnya, ingatlah perkiraan kemungkinan maksimumadalah α^ML E= 0,9990936 Dan θ^ML E= 2,2821147 , sehingga pencocokan persentil memberikan perkiraan yang lebih baik untuk distribusi Pareto daripada metode momen. Contoh 4.1.11. Soal Ujian Aktuaria. Anda diberikan: Kerugian mengikuti distribusi loglogistik dengan fungsi distribusi kumulatif: \\[F(x) = \\frac{\\left(x/\\theta\\right)^{\\gamma}}{1+\\left(x/\\theta\\right)^{\\gamma}}\\] Contoh kerugiannya adalah: \\[\\begin{array}{ccccccccccc} 10 &amp;35 &amp;80 &amp;86 &amp;90 &amp;120 &amp;158 &amp;180 &amp;200 &amp;210 &amp;1500 \\\\ \\end{array}\\] Hitung estimasi dari \\(θ\\) dengan pencocokan persentil, menggunakan perkiraan persentil ke-40 dan ke-80 yang dihaluskan secara empiris. Solusi. Dengan 11 pengamatan, kami memiliki \\(j=\\lfloor(n+1)q\\rfloor = \\lfloor 12(0.4) \\rfloor = \\lfloor 4.8\\rfloor=4\\). Dengan interpolasi, perkiraan persentil ke-40 yang dihaluskan secara empiris adalah \\(\\hat{\\pi}_{0.4} = (1-h) X_{(j)} + h X_{(j+1)} = 0.2(86)+0.8(90)=89.2\\). Demikian pula, untuk perkiraan persentil yang dihaluskan secara empiris ke-80, kami memiliki \\(12 ( 0,8 ) = 9,6\\) jadi perkiraannya \\(\\hat{\\pi}_{0.8} = 0.4(200)+0.6(210)=206\\). Dengan menggunakan distribusi kumulatif loglogistik, kita perlu menyelesaikan dua persamaan berikut untuk parameter \\({\\hat{\\theta}}\\) Dan \\({\\hat{\\gamma}}\\) : \\[0.4=\\frac{(89.2/{\\hat{\\theta}})^{\\hat{\\gamma}}}{1+(89.2/{\\hat{\\theta}})^{\\hat{\\gamma}}} \\ \\ \\ \\text{and} \\ \\ \\ \\ 0.8=\\frac{(206/{\\hat{\\theta}})^{\\hat{\\gamma}}}{1+(206/{\\hat{\\theta}})^{\\hat{\\gamma}}} .\\] Pemecahan untuk setiap ekspresi kurung memberi \\(\\frac{2}{3}=(89.2/\\theta)^{\\hat{\\gamma}}\\) Dan \\(4=(206/{\\hat{\\theta}})^{\\hat{\\gamma}}\\) . Mengambil rasio persamaan kedua dengan yang pertama memberi \\(6=(206/89.2)^{\\hat{\\gamma}}\\Rightarrow {\\hat{\\gamma}}=\\frac{\\log(6)}{\\log(206/89.2)} = 2.1407\\). Kemudian \\(4^{1/2.1407}=206/{\\hat{\\theta}} \\Rightarrow {\\hat{\\theta}}=107.8\\). Seperti metode momen, pencocokan persentil hampir terlalu fleksibel dalam arti bahwa estimator dapat bervariasi tergantung pada persentil berbeda yang dipilih. Misalnya, seorang aktuaris dapat menggunakan estimasi pada persentil ke-25 dan ke-95 sedangkan yang lain menggunakan persentil ke-20 dan ke-80. Secara umum estimasi parameter akan berbeda dan tidak ada alasan kuat untuk memilih salah satu dari yang lain. Seperti halnya metode momen, pencocokan persentil menarik karena memberikan teknik yang dapat diterapkan dengan mudah dalam situasi tertentu dan memiliki dasar intuitif. Meskipun sebagian besar aplikasi aktuaria menggunakan estimator kemungkinan maksimum, akan lebih mudah untuk memiliki pendekatan alternatif seperti metode momen dan pencocokan persentil yang tersedia. 4.2 Model Selection Menjelaskan proses pemilihan model berdasarkan: dataset dalam sampel atau pelatihan, dataset out -of-sampel atau uji, dan metode yang menggabungkan pendekatan ini dikenal sebagai cross-validation . 4.2.1 Pemilihan Model Iteratif Dalam memeriksa data secara grafis, membuat hipotesis struktur model, dan membandingkan data dengan model kandidat untuk merumuskan model yang lebih baik. Box ( 1980 ) menggambarkan ini sebagai proses berulang yang ditunjukkan pada Gambar dibawah ini src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.2.1-1png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Proses berulang ini memberikan resep yang berguna untuk menyusun tugas menentukan model untuk mewakili satu set data. Langkah pertama, tahap perumusan model, dilakukan dengan memeriksa data secara grafis dan menggunakan pengetahuan hubungan sebelumnya, seperti dari teori ekonomi atau praktik industri. Langkah kedua dalam iterasi adalah fitting berdasarkan asumsi model yang ditentukan. Asumsi ini harus konsisten dengan data untuk menggunakan model secara valid. Langkah ketiga adalah pemeriksaan diagnostik ; data dan model harus konsisten satu sama lain sebelum kesimpulan tambahan dapat dibuat. Pengecekan diagnostik adalah bagian penting dari formulasi model; itu dapat mengungkapkan kesalahan yang dilakukan pada langkah sebelumnya dan memberikan cara untuk memperbaiki kesalahan ini. 4.2.2 Model Selection Based on a Training Dataset Biasanya merujuk ke kumpulan data yang digunakan untuk analisis sebagai kumpulan data dalam sampel atau pelatihan . Teknik yang tersedia untuk memilih model tergantung pada apakah hasilnya X diskrit, kontinu, atau campuran dari keduanya, meskipun prinsipnya sama. Grafik dan Tindakan Ringkasan Dasar lainnya. Mulailah dengan meringkas data secara grafis dan dengan statistik yang tidak bergantung pada bentuk parametrik tertentu. Tes Rasio Kemungkinan. Untuk membandingkan kecocokan model, jika satu model merupakan bagian dari model lainnya, maka uji rasio kemungkinan dapat digunakan; pendekatan umum untuk pengujian rasio kemungkinan Kebaikan Statistik Fit. Secara umum, model bukan himpunan bagian yang tepat satu sama lain sehingga statistik kecocokan secara keseluruhan sangat membantu untuk membandingkan model. Kriteria informasi adalah salah satu jenis kebaikan statistik. Untuk memilih distribusi yang sesuai, statistik yang membandingkan kecocokan parametrik dengan alternatif nonparametrik. 4.2.3 Model Selection Based on a Test Dataset Validasi model adalah proses konfirmasi bahwa model yang diusulkan sesuai, terutama mengingat tujuan penyelidikan. Keterbatasan penting dari proses pemilihan model hanya berdasarkan data dalam sampel adalah bahwa hal itu dapat rentan terhadap data-snooping , yaitu menyesuaikan sejumlah besar model ke satu set data. Memilih model hanya berdasarkan data dalam sampel juga tidak mendukung tujuan inferensi prediktif . 4.2.4 Model Selection Based on Cross-Validation Meskipun validasi out-of-sample adalah standar emas dalam pemodelan prediktif, tidak selalu praktis untuk melakukannya. Alasan utamanya adalah kita memiliki ukuran sampel yang terbatas dan kriteria pemilihan model di luar sampel dalam persamaan (4.4) bergantung pada pemisahan data secara acak . Ini berarti bahwa analis yang berbeda, bahkan ketika mengerjakan kumpulan data yang sama dan pendekatan pemodelan yang sama, dapat memilih model yang berbeda. Prosedur Validasi Silang. Sebagai alternatif, seseorang dapat menggunakan cross-validation , sebagai berikut. Prosedur dimulai dengan menggunakan mekanisme acak untuk membagi data menjadi K himpunan bagian dengan ukuran yang kira-kira sama yang dikenal sebagai lipatan , di mana analis biasanya menggunakan 5 hingga 10. Selanjutnya, yang satu menggunakan yang pertama K-1 subsampel untuk memperkirakan parameter model. Kemudian, “prediksi” hasil untuk K th subsampel dan gunakan ukuran seperti pada persamaan (4.4) untuk meringkas kecocokan. Sekarang, ulangi ini dengan menahan masing-masing K subsampel, meringkas dengan statistik out-of-sample. Jadi, rangkumlah ini K statistik, biasanya dengan rata-rata, untuk memberikan satu statistik keseluruhan untuk tujuan perbandingan. Ulangi langkah-langkah ini untuk beberapa model kandidat dan pilih model dengan statistik validasi silang terendah secara keseluruhan. 4.3 Estimasi Menggunakan Data Modifikasi Penjelasan pada subbab ini: Mendeskripsikan data yang dikelompokkan, disensor, dan terpotong Perkirakan distribusi parametrik berdasarkan data yang dikelompokkan, disensor, dan terpotong Perkirakan distribusi secara nonparametrik berdasarkan data yang dikelompokkan, disensor, dan terpotong 4.3.1 Estimasi Parametrik menggunakan Data Modifikasi Seperti yang kita ketahui bahwa Estimasi parametrik bersifat kuantitatif dan menggunakan statistik untuk menghitung perkiraan jumlah sumber daya yang dibutuhkan untuk menyelesaikan proyek Anda, baik itu biaya atau waktu, atau bahkan sumber daya manusia. Bagian 3.5 memperkenalkan konsep observasi yang “ dimodifikasi ” karena dua jenis batasan umum: penyensoran dan pemotongan. Misalnya, adalah umum untuk berpikir tentang asuransi yang dapat dikurangkan sebagai menghasilkan data yang terpotong (dari kiri) atau batasan polis sebagai menghasilkan data yang disensor (dari kanan). Sudut pandang ini dari perusahaan asuransi utama (penjual asuransi). Secara khusus, bagian ini akan membahas metode estimasi parametrik untuk tiga alternatif data individual, lengkap, dan tidak dimodifikasi: data dengan sensor interval hanya tersedia dalam kelompok, data yang terbatas ataudisensor , dan data yang tidak dapat diamati karena pemotongan . 4.3.1.1 Estimasi Parametrik menggunakan Data yang Dikelompokkan Pertimbangkan sampel ukuran N diamati dari distribusinya \\(F( ⋅ )\\), tetapi dalam kelompok sehingga kita hanya mengetahui kelompok tempat setiap pengamatan jatuh, bukan nilai pastinya. Ini disebut sebagai data yang dikelompokkan atau disensor interval . Memformalkan ide ini, misalkan ada k kelompok atau interval yang dibatasi oleh batas \\(C_0&lt;C_1&lt; ⋯ &lt;C_k.\\) Untuk setiap pengamatan, kami hanya mengamati interval jatuhnya \\(((C_{j − 1},C_J))\\), bukan nilai yang tepat. Dengan demikian, kita hanya mengetahui jumlah observasi pada setiap interval. Konstanta \\({C_0&lt;C_1&lt; ⋯ &lt;C_k}\\) membentuk beberapa partisi dari domain \\(F( ⋅ )\\). Kemudian probabilitas pengamatan \\(X_i\\) jatuh di \\(J\\)th interval ke- adalah \\[ Pr(X_i \\in (c_{j-1},c_j])=F(c_j)-F(c_{j-1}) \\] Fungsi massa probabilitas yang sesuai untuk pengamatan adalah src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.3.1-1.png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Sekarang, tentukan $N_J$ menjadi jumlah pengamatan yang termasuk dalam $J$th interval, $(C_{j − 1},C_J]$. Jadi, fungsi kemungkinan (sehubungan dengan parameter) $θ$) adalah src=&quot;https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.3.1-2.png?raw=true&quot; width=&quot;300&quot; height=&quot;300&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; margin-top: 10px;&quot;&gt; Dan fungsi log-kemungkinan adalah src=&quot;https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.3.1-3png?raw=true&quot; width=&quot;300&quot; height=&quot;300&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; margin-top: 10px;&quot;&gt; Diberikan data : 1. Kerugian mengikuti distribusi eksponensial dengan rata-rata $θ$. 2. Sebuah sampel acak dari 20 kerugian didistribusikan sebagai berikut: src=&quot;https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/example4.3-1.png?raw=true&quot; width=&quot;300&quot; height=&quot;300&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; margin-top: 10px;&quot;&gt; Hitung estimasi kemungkinan maksimum dari $θ$ $$ \\begin{aligned} L(\\theta) &amp;= F(1000)^7[F(2000)-F(1000)]^6[1-F(2000)]^7 \\\\ &amp;= (1-e^{-1000/\\theta})^7(e^{-1000/\\theta} - e^{-2000/\\theta})^6(e^{-2000/\\theta})^7 \\\\ &amp;= (1-p)^7(p-p^2)^6(p^2)^7 \\\\ &amp;= p^{20}(1-p)^{13} \\end{aligned} $$ di mana $p = e^{-1000/θ}$. Memaksimalkan ekspresi ini sehubungan dengan $p$ setara dengan memaksimalkan kemungkinan terhadap $θ$. Maksimum terjadi pada $p=\\frac{20}{33}$. sehingga $\\hat{\\theta}=\\frac{-1000}{\\log(20/33)}= 1996.90$ #### Cencored Data Penyensoran terjadi ketika kita hanya mencatat nilai yang terbatas dari sebuah observasi. Bentuk yang paling umum adalah penyensoran kanan, di mana kita mencatat nilai yang lebih kecil dari variabel dependen &quot;benar&quot; dan nilai penyensoran. Dengan menggunakan notasi, dengan `X` mewakili hasil yang diminati, seperti kerugian akibat kejadian yang diasuransikan atau waktu hingga kejadian. Dengan $C_U$ menyatakan jumlah penyensoran. Dengan pengamatan tersensor kanan, mencatat $X_U^* = min(X, C_U) = X∧C_U$. Lalu juga mencatat apakah penyensoran telah terjadi atau tidak. $δ_U = I(X≤C_U)$ adalah variabel biner yang bernilai 0 jika penyensoran terjadi dan 1 jika tidak, yaitu, $δ_U$ menunjukkan apakah X tidak disensor atau tidak. Sebagai contoh $C_U$ dapat merepresentasikan batas atas pertanggungan sebuah polis asuransi. Kerugian dapat melebihi jumlah $C_U$ tetapi perusahaan asuransi hanya memiliki $C_U$ dalam catatannya sebagai jumlah yang dibayarkan dan tidak memiliki jumlah kerugian aktual $X$ dalam catatannya. Sama halnya dengan penyensoran kiri, dapat mencatat yang lebih besar dari variabel yang diminati dan variabel yang disensor. Jika $C_L$ digunakan untuk merepresentasikan jumlah penyensoran, maka mencatat $X_L^*=max(X,C_L)$ bersama dengan indikator penyensoran $δ_L=I(X&gt;C_L)$. Sebagai contoh, reasuradur akan menanggung kerugian penanggung yang lebih besar dari $C_L$ ini berarti reasuradur bertanggung jawab atas kelebihan $X_L^*$ pada $C_L$. Dengan menggunakan notasi, kerugian reasuradur adalah $Y = X_L^*L-C_L$ Untuk melihat hal ini, pertama-tama pertimbangkan kasus di mana pemegang polis mengalami kerugian $X &lt; C_L$. Kemudian, penanggung akan membayar seluruh klaim dan $Y=C_L-C_L=0$ tidak ada kerugian bagi reasuradur. Sebaliknya, jika kerugian $X≥C_L$ maka $Y = X-C_L$ merupakan klaim yang ditahan oleh reasuradur. Dengan kata lain, jika terjadi kerugian, reasuradur mencatat jumlah sebenarnya jika melebihi batas $C_L$ dan jika tidak, hanya mencatat akan mengalami kerugian sebesar 0. #### Truncated data Pengamatan yang disensor dicatat untuk studi, meskipun dalam bentuk yang terbatas. Sebaliknya, hasil yang terpotong adalah jenis data yang hilang. Sebuah hasil berpotensi terpotong ketika ketersediaan pengamatan bergantung pada hasil. Dalam asuransi, biasanya pengamatan terpotong kiri pada $C_L$ ketika jumlahnya adalah $$ \\begin{aligned} Y &amp;= \\left\\{ \\begin{array}{cl} \\text{we do not observe }X &amp; X \\le C_L \\\\ X &amp; X &gt; C_L \\end{array} \\right.\\end{aligned} $$ Dengan kata lain, jika X kurang dari ambang batas $C_L$ maka ia tidak teramati. $C_L$ dapat merepresentasikan deductible dari sebuah polis asuransi. Jika kerugian yang diasuransikan kurang dari deductible, maka perusahaan asuransi mungkin tidak mengamati atau mencatat kerugian sama sekali. Jika kerugian melebihi deductible, maka kelebihan $X-C_L$ adalah klaim yang ditanggung oleh penanggung. Dimana dapat didefinisikan kerugian per pembayaran sebagai $$ \\begin{aligned} Y^{P} = \\left\\{ \\begin{matrix} \\text{Undefined} &amp; X \\le d \\\\ X - d &amp; X &gt; d \\end{matrix} \\right. \\end{aligned} $$ sehingga jika kerugian melebihi deductible, kami mencatat jumlah kelebihan $X-d$. Hal ini sangat penting ketika mempertimbangkan jumlah yang akan dibayarkan oleh perusahaan asuransi. Namun, untuk tujuan estimasi pada bagian ini, tidak terlalu penting jika kita mengurangkan konstanta yang diketahui seperti $C_L = d$. Sehingga, untuk variabel terpotong $Y$ kita menggunakan konvensi yang lebih sederhana dan tidak mengurangkan $d$. Demikian pula untuk data terpotong kanan, jika X melebihi ambang batas $C_U$ maka data tersebut tidak diobservasi. Dalam hal ini, jumlahnya adalah $$ \\begin{aligned} Y &amp;= \\left\\{ \\begin{array}{cl} X &amp; X \\le C_U \\\\ \\text{we do not observe }X &amp; X &gt; C_U. \\end{array} \\right.\\end{aligned} $$ Contoh klasik dari pemotongan dari kanan termasuk X sebagai ukuran jarak ke bintang. Ketika jaraknya melebihi tingkat tertentu $C_U$ maka bintang tersebut tidak lagi dapat diamati. Gambar dibawah ini membandingkan pengamatan yang terpotong dan tersensor. Nilai-nilai X yang lebih besar dari batas penyensoran &quot;atas&quot; $C_U$ tidak teramati sama sekali (tersensor kanan), sedangkan nilai X yang lebih kecil dari batas pemotongan &quot;bawah&quot; $C_L$ tetap diamati, tetapi diamati sebagai $C_L$ daripada nilai X yang sebenarnya (tersensor kiri). #### Parametric Estimation using Cencored and Truncated data Untuk mempermudah, dapat diasumsikan jumlah penyensoran tidak acak dan hasil yang kontinu X . Sebagai permulaan, pertimbangkan kasus data tersensor kanan di mana merekam $X_U^* = min(X, C_U) = X∧C_U$) dan indikator penyensoran $δ = I(X≤C_U)$ . Jika penyensoran terjadi sehingga $δ=0$ maka $X&gt;C_U$ dan peluangnya adalah $Pr(X&gt;C_U)=1-F(C_U)$. Jika penyensoran tidak terjadi sehingga $δ = 1$ maka $X≤C_U$ dan likelihoodnya adalah $f(x)$ . Ringkasnya, didapatkan likelihood dari sebuah pengamatan tunggal sebagai $$ \\begin{aligned} \\left\\{ \\begin{array}{ll} 1-F(C_U) &amp; \\text{if }\\delta=0 \\\\ f(x) &amp; \\text{if } \\delta = 1 \\end{array} \\right. = \\left\\{ f(x)\\right\\}^{\\delta} \\left\\{1-F(C_U)\\right\\}^{1-\\delta} . \\end{aligned} $$ Ekspresi ruas kanan memungkinkan dalam menyajikan peluang dengan lebih ringkas. Sekarang, untuk sampel ke-i dengan ukuran n , peluangnya adalah $$ \\begin{aligned} L(\\theta) = \\prod_{i=1}^n \\left\\{ f(x_i)\\right\\}^{\\delta_i} \\left\\{1-F(C_{Ui})\\right\\}^{1-\\delta_i} = \\prod_{\\delta_i=1} f(x_i) \\prod_{\\delta_i=0} \\{1-F(C_{Ui})\\} \\end{aligned} $$ dengan waktu penyensoran potensial ${(C_{U1},...,C_{Un})}$ . Di sini, notasi &quot;$∏{δi} = 1$&quot; berarti mengambil hasil kali dari pengamatan yang tidak disensor, dan demikian pula untuk &quot;$∏{δi} = 0$ &quot; Di sisi lain, data terpotong ditangani dalam inferensi kemungkinan melalui probabilitas bersyarat. Secara khusus, kontribusi likelihood dapat disesuaikan dengan membaginya dengan probabilitas bahwa variabel tersebut diamati. Sebagai rangkuman, kami memiliki kontribusi berikut pada fungsi likelihood untuk enam jenis hasil: src=&quot;https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/table4-1.png?raw=true&quot; width=&quot;300&quot; height=&quot;300&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; margin-top: 10px;&quot;&gt; Untuk hasil yang diketahui dan data yang disensor, kemungkinannya adalah \\[ \\begin{aligned} L(\\theta) = \\prod_{E} f(x_i) \\prod_{R} \\{1-F(C_{Ui})\\} \\prod_{L} F(C_{Li}) \\prod_{I} (F(C_{Ui})-F(C_{Li})), \\end{aligned} \\] di mana \\(&quot;∏_E&quot;\\) adalah hasil kali pengamatan dengan nilai Exact, dan demikian pula untuk Right-,Left- and Interval-censoring. Untuk data yang disensor kanan dan terpotong kiri, kemungkinannya adalah \\[ \\begin{aligned} L(\\theta) = \\prod_{E} \\frac{f(x_i)}{1-F(C_{Li})} \\prod_{R} \\frac{1-F(C_{Ui})}{1-F(C_{Li})}, \\end{aligned} \\] dan juga untuk kombinasi lainnya. Example 4.3.2. Actuarial Exam Question Diberikan data : Sebuah contoh kerugian adalah: 600 700 900 Tidak ada informasi yang tersedia mengenai kerugian sebesar 500 atau kurang. Kerugian diasumsikan mengikuti distribusi eksponensial dengan rata-rata \\(θ\\). Hitung estimasi kemungkinan maksimum dari \\(θ\\) Pengamatan ini terpotong pada angka 500. Kontribusi dari setiap pengamatan terhadap fungsi likelihood adalah \\(\\frac{f(x)}{1-F(500)} = \\frac{\\theta^{-1}e^{-x/\\theta}}{e^{-500/\\theta}}\\) Lalu Fungsi Likelihoodnya adalah \\(L(\\theta)= \\frac{\\theta^{-1} e^{-600/\\theta} \\theta^{-1} e^{-700/\\theta} \\theta^{-1} e^{-900/\\theta}}{(e^{-500/\\theta})^3} = \\theta^{-3}e^{-700/\\theta}\\) Log-Likehoodnya adalah \\(l(\\theta) = \\log L(\\theta) = -3 \\log \\theta - 700 \\theta^{-1}\\) Memaksimalkan ekspresi ini dengan menetapkan turunan terhadap θ sama dengan 0, Maka memiliki \\(L&#39;(\\theta) = -3 \\theta^{-1} + 700 \\theta^{-2} = 0 \\ \\Rightarrow \\ \\hat{\\theta} = \\frac{700}{3} = 233.33 .\\) 4.3.2 Nonparametric Estimation using Modified Data Estimator nonparametrik memberikan tolok ukur yang berguna, sehingga akan sangat membantu untuk memahami prosedur estimasi untuk data yang dikelompokkan, disensor, dan dipotong 4.3.2.1 Grouped Data Pengamatan dapat dikelompokkan (juga disebut sebagai interval tersensor) dalam arti bahwa pengamatan sebagai bagian dari salah satu dari k interval dalam bentuk \\((c_{j-1},c_j)\\) , untuk \\(j = 1,...,k\\) . Pada batas-batasnya, fungsi distribusi empiris didefinisikan dengan cara yang biasa: \\[ \\begin{aligned} F_n(c_j) = \\frac{\\text{number of observations } \\le c_j}{n} \\end{aligned} \\] Ogive Estimator Untuk nilai lain dari \\(x∈(c_{j-1},c_j)\\) dapat mengestimasi fungsi distribusi dengan ogive estimator yang menginterpolasi secara linear antara \\(F_n(c_{j-1})\\) dan \\(Fn_(c_j)\\) yaitu nilai dari batas-batas \\(F_n(c_{j-1})\\) dan \\(Fn_(c_j)\\) dihubungkan dengan sebuah garis lurus. Hal ini secara formal dapat dinyatakan sebagai \\[ \\begin{aligned} F_n(x) = \\frac{c_j-x}{c_j-c_{j-1}} F_n(c_{j-1}) + \\frac{x-c_{j-1}}{c_j-c_{j-1}} F_n(c_j) \\ \\ \\ \\text{for } c_{j-1} \\le x &lt; c_j \\end{aligned} \\] Sehinga Densitas yang sesuai adalah \\[ \\begin{aligned} f_n(x) = F^{\\prime}n(x) = \\frac{F_n(c_j)-F_n(c{j-1})}{c_j - c_{j-1}} \\ \\ \\ \\text{for } c_{j-1} &lt; x &lt; c_j . \\end{aligned} \\] Example 4.3.4. Actuarial Exam Question Diberikan informasi berikut ini mengenai jumlah klaim untuk 100 klaim: src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/example4.3.4-1.png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Dengan menggunakan ogive, hitunglah estimasi probabilitas bahwa klaim yang dipilih secara acak adalah antara 2000 dan 6000. Pada batas-batasnya, fungsi distribusi empiris didefinisikan dengan cara yang biasa, sehingga memiliki \\(F_{100}(1000) = 0.16, \\ F_{100}(3000)=0.38, \\ F_{100}(5000)=0.63, \\ F_{100}(10000)=0.81\\) Untuk ukuran klaim lainnya, penaksir ogive melakukan interpolasi linier di antara nilai-nilai ini: \\[ \\begin{array}{ll} F_{100}(2000) &amp;= 0.5F_{100}(1000) + 0.5F_{100}(3000) = 0.5(0.16)+0.5(0.38)=0.27 \\\\ F_{100}(6000) &amp;=0.8F_{100}(5000)+0.2F_{100}(10000) = 0.8(0.63)+0.2(0.81)=0.666 \\end{array} \\] Dengan demikian, probabilitas klaim antara 2000 dan 6000 adalah \\(F_{100}(6000) - F_{100}(2000) = 0.666-0.27 = 0.396\\) 4.3.2.2 Right-Censored Empirical Distribution Function Akan sangat berguna untuk mengkalibrasi penaksir parametrik dengan metode nonparametrik yang tidak bergantung pada bentuk parametrik distribusi. Penaksir batas produk menurut (Kaplan dan Meier 1958) merupakan penaksir yang terkenal untuk fungsi distribusi dengan adanya penyensoran. Motivasi untuk Penaksir Batas Produk Kaplan-Meier Untuk menjelaskan mengapa product-limit bekerja dengan sangat baik dengan observasi tersensor, pertama-tama dapat melihat ke kasus tanpa penyensoran. Di sini, fungsi distribusi empiris \\(F_n(x)\\) adalah penaksir tak bias dari fungsi distribusi \\(F(x)\\) . Hal ini karena \\(F_n(x)\\) adalah rata-rata dari variabel indikator yang masing-masing tidak bias, yaitu, \\(E [I(X_i≤x)]=Pr(X_i≤x)=F(x)\\) Sekarang misalkan hasil acak disensor di sebelah kanan dengan jumlah yang membatasi, katakanlah, CU sehingga dapat mencatat yang lebih kecil dari keduanya, \\(X^* = min(X, C_U)\\) . Untuk nilai-nilai \\(x\\) yang lebih kecil dari \\(C_U\\), variabel indikator masih memberikan penaksir yang tidak bias terhadap fungsi distribusi sebelum kita mencapai batas penyensoran. Artinya, \\(E [I(X^∗≤x)]=F(x)\\) karena \\(I(X^∗≤x)=I(X≤x)\\) untuk \\(x&lt;C_U\\) . Dengan cara yang sama, \\(E[I(X^∗&gt;x)]=1-F(x)=S(x)\\) . Tetapi, untuk \\(x&gt;C_U\\) , \\(I(X^∗≤x)\\) secara umum bukan merupakan penaksir tak bias dari F(x). Sebagai alternatif, pertimbangkan dua peubah acak yang memiliki batas penyensoran yang berbeda. Sebagai ilustrasi, misalkan kita mengamati \\(X^∗1=min(X_1,5)\\) dan \\(X^∗2 = min(X_2,10)\\) di mana \\(X_1\\) dan \\(X_2\\) adalah undian independen dari distribusi yang sama. Untuk \\(x≤5\\) fungsi distribusi empiris \\(F_2(x)\\) adalah penaksir tak bias dari \\(F(x)\\). Akan tetapi, untuk \\(5&lt;x≤10\\) pengamatan pertama tidak dapat digunakan untuk fungsi distribusi karena adanya batasan penyensoran. Sebagai gantinya, strategi yang dikembangkan oleh (Kaplan dan Meier 1958) adalah dengan menggunakan \\(S_2(5)\\) sebagai penaksir dari \\(S(5)\\) dan kemudian menggunakan observasi kedua untuk mengestimasi fungsi survival bersyarat pada kelangsungan hidup hingga waktu ke-5, \\(Pr(X&gt;x|X&gt;5)=\\frac{S(x)}{S(5)}\\) . Secara khusus, untuk \\(5&lt;x≤10\\) penaksir dari fungsi survival adalah \\[ \\begin{aligned} \\hat{S}(x) = S_2(5) \\times I(X_2^* &gt; x ) \\end{aligned} \\] Kaplan-Meier Product Limit Estimator Dengen memperluas ide dalam setiap observasi i,dengan ui menjadi batas atas penyensoran \\((=∞) jikatidakadapenyensoran\\). Dengan demikian, nilai yang tercatat adalah xi dalam kasus tidak ada penyensoran dan ui jika ada penyensoran. Dengan \\(t_1&lt;⋯&lt;t_k\\)menjadi k titik berbeda di mana kerugian yang tidak disensor terjadi, dan biarkan \\(s_j\\) adalah jumlah kerugian yang tidak tersensor \\(x_i\\) yang tidak tersensor pada \\(t_j\\). Himpunan risiko yang sesuai adalah jumlah observasi yang aktif (tidak tersensor) pada nilai yang kurang dari \\(t_j\\) yang dinotasikan sebagai \\(R_j = \\sum_{i=1}^n I(x_i \\geq t_{j}) + \\sum_{i=1}^n I(u_i \\geq t_{j})\\) Dengan notasi ini, penaksir product-limit dari fungsi distribusi \\[ \\begin{equation} \\hat{F}(x) = \\left\\{ \\begin{array}{ll} 0 &amp; x&lt;t_{1} \\\\ 1-\\prod_{j:t_{j} \\leq x}\\left( 1-\\frac{s_j}{R_{j}}\\right) &amp; x \\geq t_{1} \\end{array} \\right. . \\tag{4.6} \\end{equation} \\] Sebagai contohnya, jika x lebih kecil dari kerugian terkecil yang tidak tersensor, maka \\(x&lt;t1\\) dan \\(F^(x)=0\\) . Sebagai contoh lain, jika \\(x\\) berada di antara kerugian tersensor terkecil kedua dan ketiga, maka \\(x∈(t_2,t_3]\\) dan \\(\\hat{F}(x) = 1 - \\left(1- \\frac{s_1}{R_{1}}\\right)\\left(1- \\frac{s_2}{R_{2}}\\right)\\) .Taksiran yang sesuai dari fungsi survival adalah \\(\\hat{S}(x) = 1 - \\hat{F}(x)\\) 4.3.3 Example 4.3.5. Actuarial Exam Question. Berikut ini adalah contoh dari 10 pembayaran: \\[ 4 \\space \\space 4 \\space \\space 5+\\space \\space 5+ \\space\\space 5+ \\space\\space 8 \\space\\space 10+ \\space\\space 10+ \\space\\space 12 \\space\\space 15 \\] dimana + menunjukkan bahwa kerugian telah melebihi batas polis. Dengan menggunakan estimator batas produk Kaplan-Meier, hitunglah probabilitas bahwa kerugian pada suatu polis melebihi 11, \\(\\hat{S}(11)\\) Terdapat empat waktu kejadian (pengamatan yang tidak disensor). Untuk setiap waktu tj kita dapat menghitung jumlah kejadian sj dan himpunan risiko \\(R_j\\) sebagai berikut: src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/example4.3.6-1.png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Dengan demikian, estimasi Kaplan-Meier dari S(11) adalah \\[ \\begin{aligned} \\hat{S}(11) &amp;= \\prod_{j:t_j\\leq 11} \\left( 1- \\frac{s_j}{R_j} \\right) = \\prod_{j=1}^{2} \\left( 1- \\frac{s_j}{R_j} \\right)\\\\ &amp;= \\left(1-\\frac{2}{10} \\right) \\left(1-\\frac{1}{5} \\right) = (0.8)(0.8)= 0.64. \\\\ \\end{aligned} \\] Right-Censored, Left-Truncated Empirical Distribution Function Selain penyensoran kanan, selanjutnya adalah memperluas kerangka kerja untuk memungkinkan data terpotong ke kiri. Seperti sebelumnya, untuk setiap observasi i , dengan \\(u_i\\) menjadi batas penyensoran atas ( \\(=∞\\) jika tidak ada penyensoran). Selanjutnya, \\(d_i\\) merupakan batas pemotongan bawah (0 jika tidak ada pemotongan). Dengan demikian, nilai yang tercatat (jika lebih besar dari \\(d_i\\) ) adalah \\(x_i\\) dalam kasus tidak ada penyensoran dan \\(u_i\\) jika ada penyensoran. Lalu untuk $t_1&lt;⋯&lt;t_k $menjadi \\(k\\) titik-titik yang berbeda di mana sebuah kejadian yang menarik terjadi, dan biarkan \\(s_j\\) adalah jumlah kejadian yang terekam \\(x_i\\) pada titik waktu \\(t_j\\). Himpunan risiko yang sesuai adalah \\(R_j = \\sum_{i=1}^n I(x_i \\geq t_{j}) + \\sum_{i=1}^n I(u_i \\geq t_{j}) - \\sum_{i=1}^n I(d_i \\geq t_{j}).\\) Dengan definisi baru dari himpunan risiko ini, penaksir batas hasil kali dari fungsi distribusi adalah seperti pada persamaan product limit estimator. Rumus Greenwood (Greenwood 1926) menurunkan rumus untuk estimasi varians dari penaksir batas-produk menjadi \\(\\widehat{Var}(\\hat{F}(x)) = (1-\\hat{F}(x))^{2} \\sum {j:t{j} \\leq x} \\dfrac{s_j}{R_{j}(R_{j}-s_j)}.\\) Seperti biasa, dapat mengacu pada akar kuadrat dari estimasi varians sebagai kesalahan standar, sebuah kuantitas yang secara rutin digunakan dalam interval kepercayaan dan untuk pengujian hipotesis. Untuk menghitungnya, metode survfit R mengambil sebuah objek data survival dan membuat sebuah objek baru yang berisi estimasi Kaplan-Meier dari fungsi survival bersama dengan interval kepercayaan. Metode Kaplan-Meier (type='kaplan-meier') digunakan secara default untuk membuat estimasi kurva survival. Fungsi survival diskrit yang dihasilkan memiliki massa titik pada waktu kejadian yang diamati (tanggal pelepasan) \\(t_j\\) dimana probabilitas suatu kejadian yang diberi ketahanan hidup pada durasi tersebut diestimasi sebagai jumlah kejadian yang diamati pada durasi sj dibagi dengan jumlah subjek yang terpapar atau ‘berisiko’ sesaat sebelum durasi kejadian \\(R_j\\). Penaksir Alternatif Dua jenis estimasi alternatif juga tersedia untuk metode survfit. Alternatif pertama (type='fh2') menangani hubungan, pada dasarnya, dengan mengasumsikan bahwa beberapa kejadian pada durasi yang sama terjadi dalam urutan yang berubah-ubah. Alternatif lain (type='fleming-harrington') menggunakan estimasi Nelson-Aalen (Aalen 1978) dari fungsi hazard kumulatif untuk mendapatkan estimasi fungsi survival. Estimasi bahaya kumulatif \\(H^(x)\\) dimulai dari nol dan bertambah pada setiap durasi kejadian yang diamati \\(t_j\\) dengan jumlah kejadian \\(s_j\\) dibagi dengan jumlah yang berisiko \\(R_j\\). Dengan notasi yang sama seperti di atas, penaksir Nelson-Äalen dari fungsi distribusi adalah \\[ \\begin{aligned} \\hat{F}_{NA}(x) &amp;= \\left\\{ \\begin{array}{ll} 0 &amp; x&lt;t_{1} \\\\ 1- \\exp \\left(-\\sum_{j:t_{j} \\leq x}\\frac{s_j}{R_j} \\right) &amp; x \\geq t_{1} \\end{array} \\right. .\\end{aligned} \\] Itu merupakan hasil dari estimator Nelson-Äalen dari fungsi hazard kumulatif \\(\\hat{H}(x)=\\sum_{j:t_j\\leq x} \\frac{s_j}{R_j}\\) dan hubungan antara fungsi survival dan fungsi hazard kumulatif, \\(\\hat{S}_{NA}(x)=e^{-\\hat{H}(x)}\\) 4.4 Bayesian Inference Penjelasan pada subbab ini: Jelaskan model Bayesian sebagai alternatif dari pendekatan frequentist dan rangkum lima komponen dari pendekatan pemodelan ini. Ringkas distribusi parameter posterior dan gunakan distribusi posterior ini untuk memprediksi hasil baru. Gunakan distribusi konjugat untuk menentukan distribusi parameter posterior. 4.4.1 Introduction to Bayesian Inference Sampai saat ini, metode inferensial kami berfokus pada pengaturan frequentist , di mana sampel diambil berulang kali dari suatu populasi. Vektor parameter θ adalah tetap belum diketahui, sedangkan hasil X adalah realisasi variabel acak. Sebaliknya, di bawah kerangka Bayesian , kami melihat parameter model dan data sebagai variabel acak. Kami tidak yakin tentang parameternya θ dan gunakan alat probabilitas untuk mencerminkan ketidakpastian ini. Dibawah ini merupakan rumus aturan bayes: src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.4.1-1.png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Di mana, Pr(parameters): adalah distribusi parameter, yang dikenal sebagai distribusi sebelumnya . Pr(data|parameters): adalah distribusi sampling. Dalam konteks frequentist, ini digunakan untuk membuat kesimpulan tentang parameter dan dikenal sebagai kemungkinan . Pr(parameters|data):adalah distribusi parameter setelah mengamati data, yang dikenal sebagai distribusi posterior . Pr(data): adalah distribusi marjinal dari data. Ini umumnya diperoleh dengan mengintegrasikan (atau menjumlahkan) distribusi gabungan data dan parameter di atas nilai parameter. 4.4.2 Bayesian Model 4.4.3 Bayesian Inference 4.4.3.1 Summarizing the Posterior Distributiob of Paremeters 4.4.3.2 Bayesian Predictive Distribution 4.4.4 Conjugate Distributions "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
>>>>>>> b4abc2c5d190cd1ea18e0207c8734a0abd5b5641
>>>>>>> 6fa4781668e337b176fc85db84f4a9eb21b326c1
>>>>>>> 94f6b9f0db7af77d5a5675660e57daf7e0885b5b
>>>>>>> e1f3f004d9ab8401844bb1181aedd14e3f533f81
