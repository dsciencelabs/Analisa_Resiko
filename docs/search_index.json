[["model-selection-and-estimation.html", "Bab 4 Model Selection and Estimation 4.1 Nonparametric Inference 4.2 Model Selection 4.3 Estimasi Menggunakan Data Modifikasi 4.4 Bayesian Inference", " Bab 4 Model Selection and Estimation 4.1 Nonparametric Inference Di bagian ini, Anda mempelajari cara: Perkirakan momen, kuantil, dan distribusi tanpa mengacu pada distribusi parametrik Ringkas data secara grafis tanpa mengacu pada distribusi parametrik Tentukan ukuran yang meringkas penyimpangan parametrik dari kecocokan nonparametrik Gunakan estimator nonparametrik untuk memperkirakan parameter yang dapat digunakan untuk memulai prosedur estimasi parametrik 4.1.1 Estimasi Nonparametrik Pada bagian pembahasan sebelumnya telah mempelajari cara meringkas distribusi dengan cara menghitung, varians, kuantil/persentil, dan sebagainya. Untuk memperkirakan langkah-langkah ringkasan menggunakan kumpulan data, salah satu strateginya adalah: menganggap bentuk parametrik untuk distribusi, seperti binomial negatif untuk frekuensi atau distribusi gamma untuk tingkat keparahan, memperkirakan parameter distribusi itu, gunakan distribusi dengan estimasi parameter untuk menghitung ukuran ringkasan yang diinginkan. Ini adalah pendekatan parametrik . Strategi lain adalah memperkirakan ukuran ringkasan yang diinginkan langsung dari pengamatan tanpa mengacu pada model parametrik. Tidak mengherankan, ini dikenal sebagai pendekatan nonparametrik mempertimbangkan jenis skema pengambilan sampel yang paling dasar dan mengasumsikan bahwa observasi adalah realisasi dari serangkaian variabel acak \\(X_1, \\ldots, X_n\\) yang iid menarik dari distribusi populasi yang tidak diketahui \\(F( ⋅ )\\). Cara yang setara untuk mengatakan ini adalah itu \\(X_1, \\ldots, X_n\\), adalah sampel acak (dengan penggantian) dari F( ⋅) .Kemudian menjelaskan estimator nonparametrik dari banyak ukuran penting yang meringkas sebuah distribusi. 4.1.1.1 Estimator Momen Pada bagian 2.2.2. telah mendefinisikan momen untuk frekuensi dan pada bagian 3.1.1 untuk keparahan. Secara khusus, k -momen ke-, \\(\\mathrm{E~}[X^k] = \\mu^{\\prime}_k\\) , merangkum banyak aspek distribusi untuk berbagai pilihan k . Di Sini, μ′k kadang-kadang disebut k th momen populasi untuk membedakannya dari k momen sampel, \\[\\frac{1}{n} \\sum_{i=1}^n X_i^k ,\\] yang merupakan estimator nonparametrik yang sesuai. Dalam aplikasi tipikal, k adalah bilangan bulat positif, meskipun tidak perlu dalam teori. Kasus khusus yang penting adalah momen pertama di mana \\(k = 1\\) . Dalam hal ini, simbol prima ( \\(\\prime\\) ) dan 1 subskrip biasanya dijatuhkan dan satu digunakan \\(\\mu=\\mu^{\\prime}_1\\) untuk menunjukkan mean populasi, atau hanya mean . Estimator sampel yang sesuai untuk \\(μ\\) disebut rata-rata sampel , dilambangkan dengan bilah di atas variabel acak: \\[\\overline{X} =\\frac{1}{n} \\sum_{i=1}^n X_i .\\] Jenis ringkasan ukuran minat lainnya adalah k -momen pusat ke- , \\(\\mathrm{E~} [(X-\\mu)^k] = \\mu_k\\) . (Kadang-kadang, \\(\\mu^{\\prime}_k\\) disebut k -th momen mentah untuk membedakannya dari momen sentral μk .). Estimator nonparametrik, atau sampel, dari \\(\\mu_k\\) adalah \\[\\frac{1}{n} \\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^k .\\] Momen pusat kedua ( \\(k = 2\\) ) adalah kasus penting yang biasanya akan diberikan simbol baru, \\(\\sigma^2 = \\mathrm{E~} [(X-\\mu)^2]\\) , dikenal sebagai varians . Sifat penduga momen sampel dari varians seperti \\(n^{-1}\\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^2\\) telah dipelajari secara ekstensif tetapi bukan satu-satunya estimator yang mungkin. Versi yang paling banyak digunakan adalah versi di mana ukuran sampel efektif dikurangi satu, jadi kami mendefinisikannya \\[s^2 = \\frac{1}{n-1} \\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^2.\\] Membagi dengan \\(n − 1\\) alih-alih N masalah kecil ketika Anda memiliki ukuran sampel yang besar \\(N\\) seperti yang umum dalam aplikasi asuransi. Estimator varians sampel \\(s^2\\) tidak memihak dalam arti bahwa \\(\\mathrm{E~} [s^2] = \\sigma^2\\) , properti yang diinginkan terutama saat menginterpretasikan hasil analisis. 4.1.1.2 Fungsi Distribusi Empiris Kita telah melihat bagaimana menghitung estimator nonparametrik dari k saat ini \\(\\mathrm{E~} [X^k]\\) . Dengan cara yang sama, untuk fungsi apa pun yang diketahui g (⋅) , kita dapat memperkirakan \\(\\mathrm{E~} [\\mathrm{g}(X)]\\) menggunakan\\(n^{-1}\\sum_{i=1}^n \\mathrm{g}(X_i)\\) Sekarang perhatikan fungsinya \\(\\mathrm{g}(X) = I(X \\le x)\\) untuk tetap \\(X\\) . Di sini, notasi $I( ⋅ \\() adalah fungsi indikator ; itu mengembalikan 1 jika acara ( ⋅ ) benar dan 0 sebaliknya. Perhatikan bahwa sekarang variabel acak\\) g (X$) memiliki distribusi Bernoulli (distribusi binomial dengan \\(n = 1\\) ). Kita dapat menggunakan distribusi ini untuk dengan mudah menghitung jumlah seperti rata-rata dan varians. Misalnya, untuk pilihan ini \\(g (⋅)\\) , nilai harapannya adalah \\(\\mathrm{E~} [I(X \\le x)] = \\Pr(X \\le x) = F(x)\\) , fungsi distribusi dievaluasi pada \\(X\\) . Menggunakan prinsip analog , kami mendefinisikan estimator nonparametrik dari fungsi distribusi \\[ \\begin{aligned} F_n(x) &amp;= \\frac{1}{n} \\sum_{i=1}^n I\\left(X_i \\le x\\right) \\\\ &amp;= \\frac{\\text{number of observations less than or equal to }x}{n} . \\end{aligned} \\] Sebagai $F_N( ⋅ $) didasarkan hanya pada pengamatan dan tidak mengasumsikan keluarga parametrik untuk distribusi, itu nonparametrik dan juga dikenal sebagai fungsi distribusi empiris . Ia juga dikenal sebagai fungsi distribusi kumulatif empiris dan, dalam R, seseorang dapat menggunakan ecdf(.) fungsi tersebut untuk menghitungnya. Contoh 4.1.1. Kumpulan Data Mainan . Sebagai ilustrasi, pertimbangkan kumpulan data fiktif, atau “mainan”. \\(n = 10\\) observasi. Tentukan fungsi distribusi empiris. \\[ {\\small \\begin{array}{c|cccccccccc} \\hline i &amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;10 \\\\ X_i&amp; 10 &amp;15 &amp;15 &amp;15 &amp;20 &amp;23 &amp;23 &amp;23 &amp;23 &amp;30\\\\ \\hline \\end{array} }\\] Kemudian memeriksa bahwa rata-rata sampel adalah \\(\\overline{X} = 19.7\\) dan bahwa varians sampel adalah \\(S^2= 34,45556\\) . Fungsi distribusi empiris yang sesuai adalah \\[ \\begin{aligned} F_n(x) &amp;= \\left\\{ \\begin{array}{ll} 0 &amp; \\text{ for }\\ x&lt;10 \\\\ 0.1 &amp; \\text{ for }\\ 10 \\leq x&lt;15 \\\\ 0.4 &amp; \\text{ for }\\ 15 \\leq x&lt;20 \\\\ 0.5 &amp; \\text{ for }\\ 20 \\leq x&lt;23 \\\\ 0.9 &amp; \\text{ for }\\ 23 \\leq x&lt;30 \\\\ 1 &amp; \\text{ for }\\ x \\geq 30, \\end{array} \\right.\\end{aligned}\\] (xExample &lt;- c(10,rep(15,3),20,rep(23,4),30)) PercentilesxExample &lt;- ecdf(xExample) plot(PercentilesxExample, main=&quot;&quot;,xlab=&quot;x&quot;) 4.1.1.3 Quartiles, Percentiles and Quantiles Pada bagian 3.1.1 median , yaitu angka yang kira-kira setengah dari kumpulan data berada di bawah (atau di atasnya) . Kuartil pertama adalah angka yang kira-kira 25% datanya berada di bawahnya dan kuartil ketiga adalah angka yang kira-kira 75% datanya berada di bawahnya. 100 hal persentil adalah angka sehingga \\(100×p\\) persen dari data di bawahnya. Untuk menggeneralisasi konsep ini, pertimbangkan fungsi distribusi \\(F(⋅\\)) , yang mungkin kontinu atau tidak, dan biarkan Q menjadi pecahan sehingga \\(0 &lt; q&lt; 1\\) . Kami ingin mendefinisikan quantile , katakanlah \\(q_F\\) , menjadi bilangan sedemikian sehingga \\(F(q_F) \\approx q\\) . Perhatikan bahwa ketika \\(q=0.5\\) , \\(q_F\\) adalah median; Kapan \\(q=0.25\\) , \\(q_F\\) adalah kuartil pertama, dan seterusnya. Dengan cara yang sama, ketika \\(q = 0, 0.01, 0.02, \\ldots, 0.99, 1.00\\) , yang dihasilkan QF adalah persentil. Jadi, kuantil menggeneralisasikan konsep median, kuartil, dan persentil. Lebih tepatnya, untuk diberikan \\(0 &lt; q&lt; 1\\) , tentukan q kuantil \\(q_F\\) untuk menjadi nomor yang memenuhi: \\[ \\begin{equation} F(q_F-) \\le q \\le F(q_F) \\tag{4.1} \\end{equation}\\] Untuk mendapatkan pemahaman yang lebih baik tentang definisi ini, mari kita lihat beberapa kasus khusus. Pertama, pertimbangkan kasus di mana X adalah variabel acak kontinu sehingga fungsi distribusi \\(F(⋅)\\) tidak memiliki titik lompatan, seperti yang diilustrasikan pada Gambar 4.2 . Pada gambar ini, beberapa pecahan, Q1 , Q2 , Dan Q3 ditunjukkan dengan kuantil yang sesuai \\(q_{F,1} , q_{F,2} , dan q_{F,3}\\) . Dalam setiap kasus, dapat dilihat bahwa \\(F(q_F-)= F(q_F)\\) sehingga ada kuantil unik. Karena kita dapat menemukan invers unik dari fungsi distribusi di mana saja \\(0 &lt; q&lt; 1\\) , kita bisa menulis \\(q_F= F^{-1}(q)\\) Gambar 4.3 menunjukkan tiga kasus untuk fungsi distribusi. Panel kiri sesuai dengan kasus kontinu yang baru saja dibahas. Panel tengah menampilkan titik lompatan yang serupa dengan yang telah kita lihat dalam fungsi distribusi empiris Gambar 4.1 . Untuk nilai \\(q\\) ditampilkan di panel ini, kami masih memiliki nilai kuantil yang unik \\(q_F\\) . Meskipun ada banyak nilai Q seperti yang \\(F(q_F-) \\le q \\le F(q_F)\\) , untuk nilai tertentu dari \\(q\\) , hanya ada satu solusi untuk persamaan (4.1) . Panel kanan menggambarkan situasi di mana kuantil tidak dapat ditentukan secara unik untuk \\(q\\) ditampilkan karena ada berbagai \\(q_F\\) persamaan yang memuaskan (4.1) . Contoh 4.1.2. Kumpulan Data Mainan: Lanjutan. Tentukan kuantil yang sesuai dengan persentil ke-20, ke-50, dan ke-95. Solusi . Perhatikan Gambar 4.1 . Kasus \\(q=0.20\\) sesuai dengan panel tengah Gambar Gambar 4.3 , jadi persentil ke-20 adalah 15. Kasus \\(q=0.50\\) sesuai dengan panel kanan, jadi mediannya adalah angka antara 20 dan 23 inklusif. Banyak paket perangkat lunak menggunakan rata-rata 21,5 (misalnya R, seperti yang terlihat di bawah). Untuk persentil ke-95, solusinya adalah 30. Kita dapat melihat dari Gambar 4.1 bahwa 30 juga sesuai dengan persentil ke-99 dan ke-99,99. quantile(xExample, probs=c(0.2, 0.5, 0.95), type=6) Dengan mengambil rata-rata tertimbang antara pengamatan data, kuantil empiris yang dihaluskan dapat menangani kasus seperti panel kanan pada Gambar 4.3 . Itu Q kuantil empiris yang dihaluskan didefinisikan sebagai \\[\\hat{\\pi}_q = (1-h) X_{(j)} + h X_{(j+1)}\\] Di mana \\(j=\\lfloor(n+1)q\\rfloor\\) , Dan\\(X_{(1)}, \\ldots, X_{(n)}\\) adalah nilai yang diurutkan (dikenal sebagai statistik urutan ) yang sesuai dengan \\(X_1, \\ldots, X_n\\). (Ingat bahwa tanda kurung ⌊ ⋅ ⌋ adalah fungsi lantai yang menunjukkan nilai bilangan bulat terbesar.) Perhatikan bah wa \\(\\hat{\\pi}_q\\)$ hanyalah sebuah interpolasi linear antara \\(X_{( j )}\\) dan \\(X_{(j+1)}\\). Contoh 4.1.3. Kumpulan Data Mainan: Lanjutan. Tentukan persentil yang dihaluskan ke-50 dan ke-20. Solusi Ambil \\(n = 10\\) Dan \\(q= 0,5\\). Kemudian, \\(j=\\lfloor(11)(0.5) \\rfloor= \\lfloor 5.5 \\rfloor=5\\), . Maka kuantil empiris yang dihaluskan ke-0,5 adalah \\[\\hat{\\pi}_{0.5} = (1-0.5) X_{(5)} + (0.5) X_{(6)} = 0.5 (20) + (0.5)(23) = 21.5.\\] Sekarang ambil \\(n = 10\\) Dan \\(q= 0,2\\) . Pada kasus ini, \\(j=\\lfloor(11)(0.2)\\rfloor=\\lfloor 2.2 \\rfloor=2\\) . Maka kuantil empiris yang dihaluskan ke-0,2 adalah \\[\\hat{\\pi}_{0.2} = (1-0.2) X_{(2)} + (0.2) X_{(3)} = 0.8 (15) + (0.2)(15) = 15.\\] 4.1.1.4 Penduga Kepadatan Variabel Diskrit. Ketika variabel acak adalah diskrit, memperkirakan fungsi massa probabilitas \\(f(x) = \\Pr(X=x)\\) mudah. Kami hanya menggunakan rata-rata sampel, yang didefinisikan sebagai \\[f_n(x) = \\frac{1}{n} \\sum_{i=1}^n I(X_i = x),\\] yang merupakan proporsi sampel sama dengan X Variabel Berkelanjutan dalam Grup. Untuk variabel acak kontinu, pertimbangkan formulasi diskrit di mana domain dari F( ⋅ ) dipartisi oleh konstanta \\(\\{c_0 &lt; c_1 &lt; \\cdots &lt; c_k\\}\\) ke dalam interval bentuk \\([c_{j-1}, c_j)\\) , untuk \\(j=1, \\ldots, k\\) . Pengamatan data dengan demikian “dikelompokkan” berdasarkan interval di mana mereka jatuh. Kemudian, kita dapat menggunakan definisi dasar dari fungsi massa empiris, atau variasi seperti \\[f_n(x) = \\frac{n_j}{n \\times (c_j - c_{j-1})} \\ \\ \\ \\ \\ \\ c_{j-1} \\le x &lt; c_j,\\] Di mana \\(N_J\\) adalah jumlah pengamatan ( \\(X_i\\) ) yang termasuk dalam interval \\([c_{j-1}, c_j)\\). Variabel Berkelanjutan (tidak dikelompokkan). Memperluas gagasan ini ke contoh di mana kami mengamati data individual, perhatikan bahwa kami selalu dapat membuat pengelompokan arbitrer dan menggunakan rumus ini. Lebih formal, biarkan \\(b &gt; 0\\) menjadi konstanta positif kecil, yang dikenal sebagai bandwidth , dan menentukan penaksir kepadatan menjadi \\[\\begin{equation} f_n(x) = \\frac{1}{2nb} \\sum_{i=1}^n I(x-b &lt; X_i \\le x + b) \\tag{4.2} \\end{equation}\\] Secara lebih umum, tentukan penaksir kerapatan kernel dari pdf di X sebagai \\[\\begin{equation} f_n(x) = \\frac{1}{nb} \\sum_{i=1}^n w\\left(\\frac{x-X_i}{b}\\right) , \\tag{4.3} \\end{equation}\\] Di mana w adalah fungsi kerapatan probabilitas yang berpusat di sekitar 0. Perhatikan bahwa persamaan (4.2) adalah kasus khusus penduga kerapatan kernel di mana \\(w(x) = \\frac{1}{2}I(-1 &lt; x \\le 1)\\) , juga dikenal sebagai kernel seragam . Pilihan populer lainnya ditunjukkan pada Tabel 4.1 . \\[{\\small \\begin{matrix} \\begin{array}{l|cc} \\hline \\text{Kernel} &amp; w(x) \\\\ \\hline \\text{Uniform } &amp; \\frac{1}{2}I(-1 &lt; x \\le 1) \\\\ \\text{Triangle} &amp; (1-|x|)\\times I(|x| \\le 1) \\\\ \\text{Epanechnikov} &amp; \\frac{3}{4}(1-x^2) \\times I(|x| \\le 1) \\\\ \\text{Gaussian} &amp; \\phi(x) \\\\ \\hline \\end{array}\\end{matrix} }\\] Di Sini, \\(\\phi(\\cdot)\\) adalah fungsi kepadatan normal standar. Seperti yang akan kita lihat pada contoh berikut, pilihan bandwidth \\(B\\) hadir dengan tradeoff bias-varians antara mencocokkan fitur distribusi lokal dan mengurangi volatilitas. Contoh 4.1.4. Dana Properti. Gambar 4.4 menunjukkan histogram (dengan persegi panjang abu-abu yang diarsir) dari klaim properti logaritmik dari tahun 2010. Kurva tebal (biru) mewakili kerapatan kernel Gaussian di mana bandwidth dipilih secara otomatis menggunakan aturan ad hoc berdasarkan ukuran sampel dan volatilitas data ini . Untuk dataset ini, bandwidth ternyata b = 0,3255 . Sebagai perbandingan, kurva putus-putus (merah) menunjukkan penaksir densitas dengan lebar pita sama dengan 0,1 dan kurva halus berwarna hijau menggunakan lebar pita 1. Sebagaimana diantisipasi, lebar pita yang lebih kecil (0,1) menunjukkan mengambil rata-rata lokal dengan data yang lebih sedikit sehingga kita mendapatkan ide yang lebih baik dari rata-rata lokal, tetapi dengan harga volatilitas yang lebih tinggi. Sebaliknya, bandwidth yang lebih besar (1) memperhalus fluktuasi lokal, menghasilkan kurva yang lebih halus yang mungkin melewatkan gangguan pada rata-rata lokal. Untuk aplikasi aktuaria, kami terutama menggunakan estimator densitas kernel untuk mendapatkan kesan visual cepat dari data. Dari perspektif ini, Anda cukup menggunakan aturan ad hoc default untuk pemilihan bandwidth, mengetahui bahwa Anda memiliki kemampuan untuk mengubahnya tergantung pada situasi yang dihadapi. ClaimLev &lt;- read.csv(&quot;Data/CLAIMLEVEL.csv&quot;, header=TRUE); #nrow(ClaimLev); # 6258 ClaimData&lt;-subset(ClaimLev,Year==2010); #2010 subset #Density Comparison hist(log(ClaimData$Claim), main=&quot;&quot;, ylim=c(0,.35),xlab=&quot;Log Expenditures&quot;, freq=FALSE, col=&quot;lightgray&quot;) lines(density(log(ClaimData$Claim)), col=&quot;blue&quot;,lwd=2.5) lines(density(log(ClaimData$Claim), bw=1), col=&quot;green&quot;) lines(density(log(ClaimData$Claim), bw=.1), col=&quot;red&quot;, lty=3) legend(&quot;topright&quot;, c(&quot;b=0.3255 (default)&quot;, &quot;b=0.1&quot;, &quot;b=1.0&quot;), lty=c(1,3,1), lwd=c(2.5,1,1), col=c(&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;), cex=1) #density(log(ClaimData$Claim))$bw ##default bandwidth Estimator densitas nonparametrik, seperti estimator kernel, sering digunakan dalam praktik. Konsep ini juga dapat diperluas untuk memberikan versi halus dari fungsi distribusi empiris. Mengingat definisi penaksir densitas kernel, penaksir kernel dari fungsi distribusi dapat ditemukan sebagai \\[\\begin{aligned} \\tilde{F}_n(x) = \\frac{1}{n} \\sum_{i=1}^n W\\left(\\frac{x-X_i}{b}\\right).\\end{aligned}\\] Di mana \\(W\\) adalah fungsi distribusi yang terkait dengan densitas kernel \\(w\\) . Sebagai ilustrasi, untuk kernel yang seragam, kita punya \\(w(y) = \\frac{1}{2}I(-1 &lt; y \\le 1)\\) , Jadi \\[\\begin{aligned} W(y) = \\begin{cases} 0 &amp; y&lt;-1\\\\ \\frac{y+1}{2}&amp; -1 \\le y &lt; 1 \\\\ 1 &amp; y \\ge 1 \\\\ \\end{cases}\\end{aligned} .\\] Contoh 4.1.5. Soal Ujian Aktuaria. Anda mempelajari lima nyawa untuk memperkirakan waktu dari timbulnya penyakit hingga kematian. Waktu kematian adalah: \\[\\begin{array}{ccccc} 2 &amp; 3 &amp; 3 &amp; 3 &amp; 7 \\\\ \\end{array}\\] Menggunakan kernel segitiga dengan bandwidth 2 , hitung taksiran fungsi densitas pada 2,5. Solusi. Untuk perkiraan kepadatan kernel, kami punya \\[f_n(x) = \\frac{1}{nb} \\sum_{i=1}^n w\\left(\\frac{x-X_i}{b}\\right),\\] Di mana \\(n = 5\\) , \\(b = 2\\) , Dan \\(x = 2,5\\) . Untuk inti segitiga, \\(w(x) = (1-|x|)\\times I(|x| \\le 1)\\) . Dengan demikian, \\[\\begin{array}{c|c|c} \\hline X_i &amp; \\frac{x-X_i}{b} &amp; w\\left(\\frac{x-X_i}{b} \\right) \\\\ \\hline 2 &amp; \\frac{2.5-2}{2}=\\frac{1}{4} &amp; (1-\\frac{1}{4})(1) = \\frac{3}{4} \\\\ \\hline 3 &amp; &amp; \\\\ 3 &amp; \\frac{2.5-3}{2}=\\frac{-1}{4} &amp; \\left(1-\\left| \\frac{-1}{4} \\right| \\right)(1) = \\frac{3}{4} \\\\ 3 &amp; &amp; \\\\ \\hline 7 &amp; \\frac{2.5-7}{2}=-2.25 &amp; (1-|-2.25|)(0) = 0\\\\ \\hline \\end{array}\\] Kemudian perkiraan densitas kernel di \\(x = 2,5\\) adalah \\[f_n(2.5) = \\frac{1}{5(2)}\\left( \\frac{3}{4} + (3) \\frac{3}{4} + 0 \\right) = \\frac{3}{10}\\] 4.1.1.5 Plug-in Principle Salah satu cara untuk membuat penaksir nonparametrik dari beberapa kuantitas adalah dengan menggunakan prinsip analog atau plug-in di mana seseorang menggantikan cdf yang tidak diketahui \\(F\\) dengan estimasi yang diketahui seperti cdf empiris \\(F_N\\) . Jadi, jika kita mencoba memperkirakan \\(\\mathrm{E}~[\\mathrm{g}(X)]=\\mathrm{E}_F~[\\mathrm{g}(X)]\\) untuk fungsi generik g , maka kami mendefinisikan estimator nonparametrik menjadi \\(\\mathrm{E}_{F_n}~[\\mathrm{g}(X)]=n^{-1}\\sum_{i=1}^n \\mathrm{g}(X_i)\\). Untuk melihat cara kerjanya, sebagai kasus khusus dari g , kami menganggap kerugian per variabel acak pembayaran \\(Y = (X-d)_+\\) dan rasio eliminasi kerugian yang diperkenalkan di Bagian 3.4.1. Kita dapat mengungkapkan ini sebagai \\[LER(d) = \\frac{\\mathrm{E~}[X - (X-d)_+]}{\\mathrm{E~}[X]} =\\frac{\\mathrm{E~}[\\min(X,d)]}{\\mathrm{E~}[X]} ,\\] Contoh. 4.1.6. Klaim Cidera Tubuh dan Rasio Penghapusan Kerugian Kami menggunakan sampel 432 klaim mobil tertutup dari Boston dari Derrig, Ostaszewski, dan Rempala ( 2001 ) . Kerugian dicatat untuk pembayaran karena cedera tubuh dalam kecelakaan mobil. Kerugian tidak dapat dikurangkan tetapi dibatasi oleh berbagai jumlah pertanggungan maksimum yang juga tersedia dalam data. Ternyata hanya 17 dari 432 ( ≈ 4%) tunduk pada batasan kebijakan ini sehingga kami mengabaikan data ini untuk ilustrasi ini. Kerugian rata-rata yang dibayarkan adalah 6906 dalam dolar AS. Gambar 4.5 menunjukkan aspek lain dari distribusi. Secara khusus, panel sebelah kiri menunjukkan fungsi distribusi empiris, panel sebelah kanan memberikan plot kepadatan nonparametrik. Dampak kerugian cedera tubuh dapat dikurangi dengan pengenaan limit atau pembelian polis reasuransi (lihat Bagian 10.3). Untuk mengukur dampak dari alat mitigasi risiko ini, biasanya menghitung rasio eliminasi kerugian (LER) seperti yang diperkenalkan di Bagian 3.4.1. Fungsi distribusi tidak tersedia sehingga harus diestimasi dengan cara tertentu. Menggunakan prinsip plug-in, estimator nonparametrik dapat didefinisikan sebagai \\[LER_n(d) = \\frac{n^{-1} \\sum_{i=1}^n \\min(X_i,d)}{n^{-1} \\sum_{i=1}^n X_i} = \\frac{\\sum_{i=1}^n \\min(X_i,d)}{\\sum_{i=1}^n X_i} .\\] Gambar 4.6 menunjukkan estimator \\(LER_n(d)\\) untuk berbagai pilihan \\(d\\) . Misalnya, di \\(d= 1.000\\) dan punya \\(LER_n( 1000 ) ≈ 0,1442\\). Dengan demikian, memberlakukan batas 1.000 berarti ekspektasi klaim yang ditahan 14,42 persen lebih rendah bila dibandingkan dengan ekspektasi klaim dengan deductible nol. 4.1.2 Tools for Model Selection and Diagnostics Bagian sebelumnya memperkenalkan estimator nonparametrik di mana tidak ada bentuk parametrik yang diasumsikan tentang distribusi yang mendasarinya. Namun, dalam banyak aplikasi aktuaria, analis berusaha menggunakan kecocokan parametrik dari distribusi untuk kemudahan penjelasan dan kemampuan untuk memperluasnya ke situasi yang lebih kompleks seperti memasukkan variabel penjelas dalam pengaturan regresi. Saat memasang distribusi parametrik, seorang analis mungkin mencoba menggunakan distribusi gamma untuk mewakili sekumpulan data kerugian. Namun, analis lain mungkin lebih suka menggunakan distribusi Pareto. Bagaimana cara menentukan model mana yang akan dipilih? Alat nonparametrik dapat digunakan untuk menguatkan pemilihan model parametrik. Pada dasarnya, pendekatannya adalah untuk menghitung langkah-langkah ringkasan yang dipilih di bawah model parametrik yang dipasang dan membandingkannya dengan kuantitas yang sesuai di bawah model nonparametrik. Karena model nonparametrik tidak mengasumsikan distribusi tertentu dan hanya merupakan fungsi dari data, model ini digunakan sebagai tolok ukur untuk menilai seberapa baik distribusi/model parametrik mewakili data. Juga, ketika ukuran sampel meningkat, distribusi empiris hampir pasti menyatu dengan distribusi populasi yang mendasarinya (berdasarkan hukum jumlah besar yang kuat). Dengan demikian distribusi empiris adalah proksi yang baik untuk populasi. Perbandingan estimator parametrik dengan nonparametrik dapat mengingatkan analis akan kekurangan dalam model parametrik dan terkadang menunjukkan cara untuk meningkatkan spesifikasi parametrik. Prosedur diarahkan menilai validitas model yang dikenal sebagaidiagnostik model . 4.1.2.1 Perbandingan Grafik Distribusi Kita telah melihat teknik overlay grafik untuk tujuan perbandingan. Untuk memperkuat penerapan teknik ini, Gambar 4.7membandingkan distribusi empiris dengan dua distribusi pas parametrik. Panel kiri menunjukkan fungsi distribusi distribusi klaim. Titik-titik yang membentuk kurva “berbentuk S” mewakili fungsi distribusi empiris pada setiap pengamatan. Kurva biru tebal memberikan nilai yang sesuai untuk distribusi gamma yang pas dan ungu muda untuk distribusi Pareto yang pas. Karena Pareto lebih dekat dengan fungsi distribusi empiris daripada gamma, ini memberikan bukti bahwa Pareto adalah model yang lebih baik untuk kumpulan data ini. Panel kanan memberikan informasi serupa untuk fungsi kerapatan dan memberikan pesan yang konsisten. Berdasarkan (hanya) angka-angka ini, distribusi Pareto adalah pilihan yang jelas bagi analis. Untuk cara lain untuk membandingkan kesesuaian dua model yang cocok, pertimbangkan plot probabilitas-probabilitas (\\(pp\\)) . A \\[pp\\] plot membandingkan probabilitas kumulatif di bawah dua model. Untuk tujuan kami, kedua model ini adalah fungsi distribusi empiris nonparametrik dan model pas parametrik. Gambar 4.8 menunjukkan \\(pp\\) plot untuk data Dana Properti yang diperkenalkan di Bagian 1.3 . Gamma yang dipasang di sebelah kiri dan Pareto yang dipasang di sebelah kanan, dibandingkan dengan fungsi distribusi data empiris yang sama. Garis lurus mewakili kesetaraan antara dua distribusi yang dibandingkan, sehingga titik yang dekat dengan garis diinginkan. Seperti yang terlihat pada demonstrasi sebelumnya, Pareto jauh lebih dekat dengan distribusi empiris daripada gamma, memberikan bukti tambahan bahwa Pareto adalah model yang lebih baik. Itu QQ plot membandingkan dua model yang dipasang melalui kuantilnya. Seperti hal hal plot, kami membandingkan nonparametrik dengan model pas parametrik. Kuantil dapat dievaluasi pada setiap titik kumpulan data, atau pada kisi (misalnya, di 0 , 0,001 , 0,002 , … , 0,999 , 1,000 ), tergantung aplikasinya. Pada Gambar 4.9 , untuk setiap titik pada kisi tersebut, sumbu horizontal menampilkan kuantil empiris dan sumbu vertikal menampilkan kuantil parametrik yang sesuai (gamma untuk dua panel atas, Pareto untuk dua panel bawah). Kuantil diplot pada skala asli di panel kiri dan pada skala log di panel kanan untuk memungkinkan kita melihat di mana kekurangan distribusi yang pas. Garis lurus mewakili kesetaraan antara distribusi empiris dan distribusi pas. Dari plot ini, kita sekali lagi melihat bahwa Pareto secara keseluruhan lebih cocok daripada gamma. Selain itu, panel kanan bawah menunjukkan bahwa distribusi Pareto bekerja dengan baik dengan klaim besar, tetapi memberikan kecocokan yang lebih buruk untuk klaim kecil. Contoh 4.1.7. Soal Ujian Aktuaria. Grafik di bawah ini menunjukkan \\(pp\\) plot distribusi pas dibandingkan dengan sampel. Solusi. Ekor dari distribusi yang pas terlalu tebal di sebelah kiri, terlalu tipis di sebelah kanan, dan distribusi yang pas memiliki probabilitas yang lebih kecil di sekitar median daripada sampel. Untuk melihat ini, ingat bahwa hal hal plot grafik distribusi kumulatif dari dua distribusi pada sumbunya (empiris pada sumbu x dan dipasang pada sumbu y dalam kasus ini). Untuk nilai kecil dari X , model yang dipasang memberikan probabilitas yang lebih besar untuk berada di bawah nilai itu daripada yang terjadi dalam sampel (mis F( x ) &gt;FN( x ) ). Ini menunjukkan bahwa model memiliki ekor kiri yang lebih berat daripada datanya. Untuk nilai besar dari X , model kembali memberikan probabilitas yang lebih besar untuk berada di bawah nilai itu dan dengan demikian lebih kecil kemungkinannya untuk berada di atas nilai itu (mis S( x ) &lt;SN( x ) ). Hal ini menunjukkan bahwa model memiliki ekor kanan yang lebih ringan dari pada data. Selain itu, saat kita mulai dari 0,4 hingga 0,6 pada sumbu horizontal (dengan demikian melihat 20% tengah data), hal hal plot meningkat dari sekitar 0,3 menjadi 0,4. Ini menunjukkan bahwa model hanya menempatkan sekitar 10% dari probabilitas dalam kisaran ini. 4.1.2.2 Graphical Comparison of Distributions Saat memilih model, akan sangat membantu untuk menampilkan tampilan grafis. Namun, untuk melaporkan hasil, melengkapi tampilan grafis dengan statistik terpilih yang meringkas kebaikan kesesuaian model dapat efektif. Tabel 4.2 menyediakan tiga statistik kebaikan yang umum digunakan . Dalam tabel ini, \\(F_N\\) adalah distribusi empiris, \\(F\\) adalah distribusi pas atau hipotesis, dan \\(F_i^* = F(x_i)\\) . \\[{\\small \\begin{matrix} \\begin{array}{l|cc} \\hline \\text{Statistic} &amp; \\text{Definition} &amp; \\text{Computational Expression} \\\\ \\hline \\text{Kolmogorov-} &amp; \\max_x |F_n(x) - F(x)| &amp; \\max(D^+, D^-) \\text{ where } \\\\ ~~~\\text{Smirnov} &amp;&amp; D^+ = \\max_{i=1, \\ldots, n} \\left|\\frac{i}{n} - F_i^*\\right| \\\\ &amp;&amp; D^- = \\max_{i=1, \\ldots, n} \\left| F_i^* - \\frac{i-1}{n} \\right| \\\\ \\text{Cramer-von Mises} &amp; n \\int (F_n(x) - F(x))^2 f(x) dx &amp; \\frac{1}{12n} + \\sum_{i=1}^n \\left(F_i^* - (2i-1)/n\\right)^2 \\\\ \\text{Anderson-Darling} &amp; n \\int \\frac{(F_n(x) - F(x))^2}{F(x)(1-F(x))} f(x) dx &amp; -n-\\frac{1}{n} \\sum_{i=1}^n (2i-1) \\log\\left(F_i^*(1-F_{n+1-i})\\right)^2 \\\\ \\hline \\end{array} \\\\ \\end{matrix} }\\] Statistik Kolmogorov-Smirnov adalah perbedaan absolut maksimum antara fungsi distribusi yang dipasang dan fungsi distribusi empiris. Alih-alih membandingkan perbedaan antara titik tunggal, statistik Cramer-von Mises mengintegrasikan perbedaan antara fungsi distribusi empiris dan pas pada seluruh rentang nilai. Statistik Anderson-Darling juga mengintegrasikan perbedaan ini pada rentang nilai, meskipun diboboti oleh kebalikan dari varian. Oleh karena itu lebih menekankan pada ekor distribusi (yaitu kapan \\(F( x )\\) atau \\(1-F(x)=S(x)\\) kecil). Contoh 4.1.8. Soal Ujian Aktuaria (dimodifikasi). Contoh pembayaran klaim adalah: \\[\\begin{array}{ccccc} 29 &amp; 64 &amp; 90 &amp; 135 &amp; 182 \\\\ \\end{array}\\] Bandingkan distribusi klaim empiris dengan distribusi eksponensial dengan rata-rata 100 dengan menghitung nilai statistik uji Kolmogorov-Smirnov. Solusi. Untuk distribusi eksponensial dengan rata-rata 100 , fungsi distribusi kumulatif adalah \\(F(x)=1-e^{-x/100}\\) . Dengan demikian, \\[\\begin{array}{ccccc} \\hline x &amp; F(x) &amp; F_n(x) &amp; F_n(x-) &amp; \\max(|F(x)-F_n(x)|,|F(x)-F_n(x-)|) \\\\ \\hline 29 &amp; 0.2517 &amp; 0.2 &amp; 0 &amp; \\max(0.0517, 0.2517) = 0.2517 \\\\ 64 &amp; 0.4727 &amp; 0.4 &amp; 0.2 &amp; \\max(0.0727, 0.2727) = 0.2727 \\\\ 90 &amp; 0.5934 &amp; 0.6 &amp; 0.4 &amp; \\max(0.0066, 0.1934) = 0.1934 \\\\ 135 &amp; 0.7408 &amp; 0.8 &amp; 0.6 &amp; \\max(0.0592, 0.1408) = 0.1408 \\\\ 182 &amp; 0.8380 &amp; 1 &amp; 0.8 &amp; \\max(0.1620, 0.0380) = 0.1620 \\\\ \\hline \\end{array}\\] Oleh karena itu, statistik uji Kolmogorov-Smirnov adalah \\[KS = \\max(0.2517, 0.2727, 0.1934, 0.1408, 0.1620) = 0.2727 .\\] 4.1.3 Starting Values Metode pencocokan momen dan persentil merupakan metode estimasi nonparametrik yang memberikan alternatif kemungkinan maksimum. Umumnya, kemungkinan maksimum adalah teknik yang lebih disukai karena menggunakan data secara lebih efisien. (Lihat Lampiran Bab 17 untuk definisi efisiensi yang tepat.) Namun, metode pencocokan momen dan persentil berguna karena lebih mudah diinterpretasikan dan karena itu memungkinkan aktuaris atau analis untuk menjelaskan prosedur kepada orang lain. Selain itu, prosedur estimasi numerik (misalnya jika dilakukan di R) untuk kemungkinan maksimum adalah iteratif dan membutuhkan nilai awal untuk memulai proses rekursif. Meskipun banyak masalah yang kuat untuk pemilihan nilai awal, untuk beberapa situasi kompleks, penting untuk memiliki nilai awal yang mendekati nilai optimal (tidak diketahui). Metode momen dan pencocokan persentil adalah teknik yang dapat menghasilkan perkiraan yang diinginkan tanpa investasi komputasi yang serius dan dengan demikian dapat digunakan sebagai nilai awal untuk menghitung kemungkinan maksimum. 4.1.3.1 Method of Moments Metode ini merupakan estimasi parameter populasi dengan pendekatan momen parametrik menggunakan momen sampel empiris. pada momen ini, momen distribusi parametrik menggunakan momen empiris atau nonparametrik kemudian dapat dipecahkan secara aljabar untuk estimasi parameter. Contoh 4.1.9. Dana Properti. Untuk dana properti 2010, ada \\(n = 1 , 377\\) klaim individu (dalam ribuan dolar) dengan \\[m_1 = \\frac{1}{n} \\sum_{i=1}^n X_i = 26.62259 \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ m_2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 = 136154.6 .\\] Sesuaikan parameter distribusi gamma dan Pareto menggunakan metode momen. Solusi. Agar sesuai dengan distribusi gamma, kami memiliki \\(\\mu_1 = \\alpha \\theta\\) Dan \\(\\mu_2^{\\prime} = \\alpha(\\alpha+1) \\theta^2\\) . Menyamakan keduanya menghasilkan metode penaksir momen, aljabar mudah menunjukkannya \\[\\alpha = \\frac{\\mu_1^2}{\\mu_2^{\\prime}-\\mu_1^2} \\ \\ \\ \\text{and} \\ \\ \\ \\theta = \\frac{\\mu_2^{\\prime}-\\mu_1^2}{\\mu_1}.\\] Jadi, metode penduga momen adalah \\[\\begin{aligned} \\hat{\\alpha} &amp;= \\frac{26.62259^2}{136154.6-26.62259^2} = 0.005232809 \\\\ \\hat{\\theta} &amp;= \\frac{136154.6-26.62259^2}{26.62259} = 5,087.629. \\end{aligned}\\] Sebagai perbandingan, nilai kemungkinan maksimum berubah menjadi \\(\\hat{\\alpha}_{MLE} = 0.2905959\\) Dan \\(\\hat{\\theta}_{MLE} = 91.61378\\) , jadi ada perbedaan besar antara dua prosedur estimasi. Ini adalah salah satu indikasi, seperti yang telah kita lihat sebelumnya, bahwa model gamma kurang cocok. Sebaliknya, sekarang asumsikan distribusi Pareto sehingga \\(\\mu_1 = \\theta/(\\alpha -1)\\) Dan \\(\\mu_2^{\\prime} = 2\\theta^2/((\\alpha-1)(\\alpha-2) )\\) . Perhatikan bahwa ungkapan ini untuk μ′2 hanya berlaku untuk α &gt; 2 . Pertunjukan aljabar yang mudah \\[\\alpha = 1+ \\frac{\\mu_2^{\\prime}}{\\mu_2^{\\prime}-\\mu_1^2} \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ \\ \\theta = (\\alpha-1)\\mu_1.\\] Jadi, metode penduga momen adalah \\[ \\begin{aligned} \\hat{\\alpha} &amp;= 1+ \\frac{136154.6}{136154.6-26,62259^2} = 2.005233 \\\\ \\hat{\\theta} &amp;= (2.005233-1) \\cdot 26.62259 = 26.7619 \\end{aligned}\\] Nilai kemungkinan maksimum berubah menjadi \\(\\hat{\\alpha}_{MLE} = 0.9990936\\) Dan \\(\\hat{\\theta}_{MLE} = 2.2821147\\) . Sangat menarik bahwa \\(\\hat{\\alpha}_{MLE}&lt;1\\) ; untuk distribusi Pareto, ingat itu \\(α &lt; 1\\) berarti rata-ratanya tak terhingga. Ini adalah indikasi lain bahwa kumpulan data klaim properti adalah distribusi ekor panjang. Seperti contoh di atas, ada fleksibilitas dengan metode momen. Misalnya, kita dapat mencocokkan momen kedua dan ketiga alih-alih yang pertama dan kedua, menghasilkan estimator yang berbeda. Selain itu, tidak ada jaminan bahwa solusi akan ada untuk setiap masalah. Untuk data yang disensor atau terpotong, momen pencocokan dimungkinkan untuk beberapa masalah, tetapi secara umum, ini adalah skenario yang lebih sulit. Terakhir, untuk distribusi di mana momen tidak ada atau tidak terbatas, metode momen tidak tersedia. Sebagai alternatif, seseorang dapat menggunakan teknik pencocokan persentil. 4.1.3.2 Percentile Matching Di bawah pencocokan persentil , kami memperkirakan kuantil atau persentil dari distribusi parametrik menggunakan kuantil atau persentil empiris (nonparametrik) yang dijelaskan di Bagian 4.1.1.3 . Contoh 4.1.10. Dana Properti. Untuk dana properti 2010, kami mengilustrasikan pencocokan pada kuantil. Secara khusus, distribusi Pareto secara intuitif menyenangkan karena solusi bentuk tertutup untuk kuantil. Ingatlah bahwa fungsi distribusi untuk distribusi Pareto adalah \\[F(x) = 1 - \\left(\\frac{\\theta}{x+\\theta}\\right)^{\\alpha}.\\] Aljabar mudah menunjukkan bahwa kita dapat menyatakan kuantil sebagai \\[F^{-1}(q) = \\theta \\left( (1-q)^{-1/\\alpha} -1 \\right).\\] untuk sebagian kecil q , \\(0 &lt; q&lt; 1\\). Tentukan estimasi parameter distribusi Pareto menggunakan kuantil empiris ke-25 dan ke-95. Solusi. Persentil ke-25 (kuartil pertama) ternyata adalah 0,78853 dan persentil ke-95 adalah 50.98293 (keduanya dalam ribuan dolar). Dengan dua persamaan \\[0.78853 = \\theta \\left( 1- (1-.25)^{-1/\\alpha} \\right) \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ 50.98293 = \\theta \\left( 1- (1-.75)^{-1/\\alpha} \\right)\\] dan dua yang tidak diketahui, solusinya adalah \\[\\hat{\\alpha} = 0.9412076 \\ \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ \\hat{\\theta} = 2.205617 .\\] Sehingga kesimpulannya adalah rutin numerik diperlukan untuk solusi ini karena tidak ada solusi analitik yang tersedia. Selanjutnya, ingatlah perkiraan kemungkinan maksimumadalah α^ML E= 0,9990936 Dan θ^ML E= 2,2821147 , sehingga pencocokan persentil memberikan perkiraan yang lebih baik untuk distribusi Pareto daripada metode momen. Contoh 4.1.11. Soal Ujian Aktuaria. Anda diberikan: Kerugian mengikuti distribusi loglogistik dengan fungsi distribusi kumulatif: \\[F(x) = \\frac{\\left(x/\\theta\\right)^{\\gamma}}{1+\\left(x/\\theta\\right)^{\\gamma}}\\] Contoh kerugiannya adalah: \\[\\begin{array}{ccccccccccc} 10 &amp;35 &amp;80 &amp;86 &amp;90 &amp;120 &amp;158 &amp;180 &amp;200 &amp;210 &amp;1500 \\\\ \\end{array}\\] Hitung estimasi dari \\(θ\\) dengan pencocokan persentil, menggunakan perkiraan persentil ke-40 dan ke-80 yang dihaluskan secara empiris. Solusi. Dengan 11 pengamatan, kami memiliki \\(j=\\lfloor(n+1)q\\rfloor = \\lfloor 12(0.4) \\rfloor = \\lfloor 4.8\\rfloor=4\\). Dengan interpolasi, perkiraan persentil ke-40 yang dihaluskan secara empiris adalah \\(\\hat{\\pi}_{0.4} = (1-h) X_{(j)} + h X_{(j+1)} = 0.2(86)+0.8(90)=89.2\\). Demikian pula, untuk perkiraan persentil yang dihaluskan secara empiris ke-80, kami memiliki \\(12 ( 0,8 ) = 9,6\\) jadi perkiraannya \\(\\hat{\\pi}_{0.8} = 0.4(200)+0.6(210)=206\\). Dengan menggunakan distribusi kumulatif loglogistik, kita perlu menyelesaikan dua persamaan berikut untuk parameter \\({\\hat{\\theta}}\\) Dan \\({\\hat{\\gamma}}\\) : \\[0.4=\\frac{(89.2/{\\hat{\\theta}})^{\\hat{\\gamma}}}{1+(89.2/{\\hat{\\theta}})^{\\hat{\\gamma}}} \\ \\ \\ \\text{and} \\ \\ \\ \\ 0.8=\\frac{(206/{\\hat{\\theta}})^{\\hat{\\gamma}}}{1+(206/{\\hat{\\theta}})^{\\hat{\\gamma}}} .\\] Pemecahan untuk setiap ekspresi kurung memberi \\(\\frac{2}{3}=(89.2/\\theta)^{\\hat{\\gamma}}\\) Dan \\(4=(206/{\\hat{\\theta}})^{\\hat{\\gamma}}\\) . Mengambil rasio persamaan kedua dengan yang pertama memberi \\(6=(206/89.2)^{\\hat{\\gamma}}\\Rightarrow {\\hat{\\gamma}}=\\frac{\\log(6)}{\\log(206/89.2)} = 2.1407\\). Kemudian \\(4^{1/2.1407}=206/{\\hat{\\theta}} \\Rightarrow {\\hat{\\theta}}=107.8\\). Seperti metode momen, pencocokan persentil hampir terlalu fleksibel dalam arti bahwa estimator dapat bervariasi tergantung pada persentil berbeda yang dipilih. Misalnya, seorang aktuaris dapat menggunakan estimasi pada persentil ke-25 dan ke-95 sedangkan yang lain menggunakan persentil ke-20 dan ke-80. Secara umum estimasi parameter akan berbeda dan tidak ada alasan kuat untuk memilih salah satu dari yang lain. Seperti halnya metode momen, pencocokan persentil menarik karena memberikan teknik yang dapat diterapkan dengan mudah dalam situasi tertentu dan memiliki dasar intuitif. Meskipun sebagian besar aplikasi aktuaria menggunakan estimator kemungkinan maksimum, akan lebih mudah untuk memiliki pendekatan alternatif seperti metode momen dan pencocokan persentil yang tersedia. 4.2 Model Selection Menjelaskan proses pemilihan model berdasarkan: dataset dalam sampel atau pelatihan, dataset out -of-sampel atau uji, dan metode yang menggabungkan pendekatan ini dikenal sebagai cross-validation . 4.2.1 Pemilihan Model Iteratif Dalam memeriksa data secara grafis, membuat hipotesis struktur model, dan membandingkan data dengan model kandidat untuk merumuskan model yang lebih baik. Box ( 1980 ) menggambarkan ini sebagai proses berulang yang ditunjukkan pada Gambar dibawah ini src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.2.1-1png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Proses berulang ini memberikan resep yang berguna untuk menyusun tugas menentukan model untuk mewakili satu set data. Langkah pertama, tahap perumusan model, dilakukan dengan memeriksa data secara grafis dan menggunakan pengetahuan hubungan sebelumnya, seperti dari teori ekonomi atau praktik industri. Langkah kedua dalam iterasi adalah fitting berdasarkan asumsi model yang ditentukan. Asumsi ini harus konsisten dengan data untuk menggunakan model secara valid. Langkah ketiga adalah pemeriksaan diagnostik ; data dan model harus konsisten satu sama lain sebelum kesimpulan tambahan dapat dibuat. Pengecekan diagnostik adalah bagian penting dari formulasi model; itu dapat mengungkapkan kesalahan yang dilakukan pada langkah sebelumnya dan memberikan cara untuk memperbaiki kesalahan ini. 4.2.2 Model Selection Based on a Training Dataset Biasanya merujuk ke kumpulan data yang digunakan untuk analisis sebagai kumpulan data dalam sampel atau pelatihan . Teknik yang tersedia untuk memilih model tergantung pada apakah hasilnya X diskrit, kontinu, atau campuran dari keduanya, meskipun prinsipnya sama. Grafik dan Tindakan Ringkasan Dasar lainnya. Mulailah dengan meringkas data secara grafis dan dengan statistik yang tidak bergantung pada bentuk parametrik tertentu. Tes Rasio Kemungkinan. Untuk membandingkan kecocokan model, jika satu model merupakan bagian dari model lainnya, maka uji rasio kemungkinan dapat digunakan; pendekatan umum untuk pengujian rasio kemungkinan Kebaikan Statistik Fit. Secara umum, model bukan himpunan bagian yang tepat satu sama lain sehingga statistik kecocokan secara keseluruhan sangat membantu untuk membandingkan model. Kriteria informasi adalah salah satu jenis kebaikan statistik. Untuk memilih distribusi yang sesuai, statistik yang membandingkan kecocokan parametrik dengan alternatif nonparametrik. 4.2.3 Model Selection Based on a Test Dataset Validasi model adalah proses konfirmasi bahwa model yang diusulkan sesuai, terutama mengingat tujuan penyelidikan. Keterbatasan penting dari proses pemilihan model hanya berdasarkan data dalam sampel adalah bahwa hal itu dapat rentan terhadap data-snooping , yaitu menyesuaikan sejumlah besar model ke satu set data. Memilih model hanya berdasarkan data dalam sampel juga tidak mendukung tujuan inferensi prediktif . 4.2.4 Model Selection Based on Cross-Validation Meskipun validasi out-of-sample adalah standar emas dalam pemodelan prediktif, tidak selalu praktis untuk melakukannya. Alasan utamanya adalah kita memiliki ukuran sampel yang terbatas dan kriteria pemilihan model di luar sampel dalam persamaan (4.4) bergantung pada pemisahan data secara acak . Ini berarti bahwa analis yang berbeda, bahkan ketika mengerjakan kumpulan data yang sama dan pendekatan pemodelan yang sama, dapat memilih model yang berbeda. Prosedur Validasi Silang. Sebagai alternatif, seseorang dapat menggunakan cross-validation , sebagai berikut. Prosedur dimulai dengan menggunakan mekanisme acak untuk membagi data menjadi K himpunan bagian dengan ukuran yang kira-kira sama yang dikenal sebagai lipatan , di mana analis biasanya menggunakan 5 hingga 10. Selanjutnya, yang satu menggunakan yang pertama K-1 subsampel untuk memperkirakan parameter model. Kemudian, “prediksi” hasil untuk K th subsampel dan gunakan ukuran seperti pada persamaan (4.4) untuk meringkas kecocokan. Sekarang, ulangi ini dengan menahan masing-masing K subsampel, meringkas dengan statistik out-of-sample. Jadi, rangkumlah ini K statistik, biasanya dengan rata-rata, untuk memberikan satu statistik keseluruhan untuk tujuan perbandingan. Ulangi langkah-langkah ini untuk beberapa model kandidat dan pilih model dengan statistik validasi silang terendah secara keseluruhan. 4.3 Estimasi Menggunakan Data Modifikasi Penjelasan pada subbab ini: Mendeskripsikan data yang dikelompokkan, disensor, dan terpotong Perkirakan distribusi parametrik berdasarkan data yang dikelompokkan, disensor, dan terpotong Perkirakan distribusi secara nonparametrik berdasarkan data yang dikelompokkan, disensor, dan terpotong 4.3.1 Estimasi Parametrik menggunakan Data Modifikasi Seperti yang kita ketahui bahwa Estimasi parametrik bersifat kuantitatif dan menggunakan statistik untuk menghitung perkiraan jumlah sumber daya yang dibutuhkan untuk menyelesaikan proyek Anda, baik itu biaya atau waktu, atau bahkan sumber daya manusia. Bagian 3.5 memperkenalkan konsep observasi yang “ dimodifikasi ” karena dua jenis batasan umum: penyensoran dan pemotongan. Misalnya, adalah umum untuk berpikir tentang asuransi yang dapat dikurangkan sebagai menghasilkan data yang terpotong (dari kiri) atau batasan polis sebagai menghasilkan data yang disensor (dari kanan). Sudut pandang ini dari perusahaan asuransi utama (penjual asuransi). Secara khusus, bagian ini akan membahas metode estimasi parametrik untuk tiga alternatif data individual, lengkap, dan tidak dimodifikasi: data dengan sensor interval hanya tersedia dalam kelompok, data yang terbatas ataudisensor , dan data yang tidak dapat diamati karena pemotongan . 4.3.1.1 Estimasi Parametrik menggunakan Data yang Dikelompokkan Pertimbangkan sampel ukuran N diamati dari distribusinya \\(F( ⋅ )\\), tetapi dalam kelompok sehingga kita hanya mengetahui kelompok tempat setiap pengamatan jatuh, bukan nilai pastinya. Ini disebut sebagai data yang dikelompokkan atau disensor interval . Memformalkan ide ini, misalkan ada k kelompok atau interval yang dibatasi oleh batas \\(C_0&lt;C_1&lt; ⋯ &lt;C_k.\\) Untuk setiap pengamatan, kami hanya mengamati interval jatuhnya \\(((C_{j − 1},C_J))\\), bukan nilai yang tepat. Dengan demikian, kita hanya mengetahui jumlah observasi pada setiap interval. Konstanta \\({C_0&lt;C_1&lt; ⋯ &lt;C_k}\\) membentuk beberapa partisi dari domain \\(F( ⋅ )\\). Kemudian probabilitas pengamatan \\(X_i\\) jatuh di \\(J\\)th interval ke- adalah \\[ Pr(X_i \\in (c_{j-1},c_j])=F(c_j)-F(c_{j-1}) \\] Fungsi massa probabilitas yang sesuai untuk pengamatan adalah src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.3.1-1.png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Sekarang, tentukan $N_J$ menjadi jumlah pengamatan yang termasuk dalam $J$th interval, $(C_{j − 1},C_J]$. Jadi, fungsi kemungkinan (sehubungan dengan parameter) $θ$) adalah src=&quot;https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.3.1-2.png?raw=true&quot; width=&quot;300&quot; height=&quot;300&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; margin-top: 10px;&quot;&gt; Dan fungsi log-kemungkinan adalah src=&quot;https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.3.1-3png?raw=true&quot; width=&quot;300&quot; height=&quot;300&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; margin-top: 10px;&quot;&gt; Diberikan data : 1. Kerugian mengikuti distribusi eksponensial dengan rata-rata $θ$. 2. Sebuah sampel acak dari 20 kerugian didistribusikan sebagai berikut: src=&quot;https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/example4.3-1.png?raw=true&quot; width=&quot;300&quot; height=&quot;300&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; margin-top: 10px;&quot;&gt; Hitung estimasi kemungkinan maksimum dari $θ$ $$ \\begin{aligned} L(\\theta) &amp;= F(1000)^7[F(2000)-F(1000)]^6[1-F(2000)]^7 \\\\ &amp;= (1-e^{-1000/\\theta})^7(e^{-1000/\\theta} - e^{-2000/\\theta})^6(e^{-2000/\\theta})^7 \\\\ &amp;= (1-p)^7(p-p^2)^6(p^2)^7 \\\\ &amp;= p^{20}(1-p)^{13} \\end{aligned} $$ di mana $p = e^{-1000/θ}$. Memaksimalkan ekspresi ini sehubungan dengan $p$ setara dengan memaksimalkan kemungkinan terhadap $θ$. Maksimum terjadi pada $p=\\frac{20}{33}$. sehingga $\\hat{\\theta}=\\frac{-1000}{\\log(20/33)}= 1996.90$ #### Cencored Data Penyensoran terjadi ketika kita hanya mencatat nilai yang terbatas dari sebuah observasi. Bentuk yang paling umum adalah penyensoran kanan, di mana kita mencatat nilai yang lebih kecil dari variabel dependen &quot;benar&quot; dan nilai penyensoran. Dengan menggunakan notasi, dengan `X` mewakili hasil yang diminati, seperti kerugian akibat kejadian yang diasuransikan atau waktu hingga kejadian. Dengan $C_U$ menyatakan jumlah penyensoran. Dengan pengamatan tersensor kanan, mencatat $X_U^* = min(X, C_U) = X∧C_U$. Lalu juga mencatat apakah penyensoran telah terjadi atau tidak. $δ_U = I(X≤C_U)$ adalah variabel biner yang bernilai 0 jika penyensoran terjadi dan 1 jika tidak, yaitu, $δ_U$ menunjukkan apakah X tidak disensor atau tidak. Sebagai contoh $C_U$ dapat merepresentasikan batas atas pertanggungan sebuah polis asuransi. Kerugian dapat melebihi jumlah $C_U$ tetapi perusahaan asuransi hanya memiliki $C_U$ dalam catatannya sebagai jumlah yang dibayarkan dan tidak memiliki jumlah kerugian aktual $X$ dalam catatannya. Sama halnya dengan penyensoran kiri, dapat mencatat yang lebih besar dari variabel yang diminati dan variabel yang disensor. Jika $C_L$ digunakan untuk merepresentasikan jumlah penyensoran, maka mencatat $X_L^*=max(X,C_L)$ bersama dengan indikator penyensoran $δ_L=I(X&gt;C_L)$. Sebagai contoh, reasuradur akan menanggung kerugian penanggung yang lebih besar dari $C_L$ ini berarti reasuradur bertanggung jawab atas kelebihan $X_L^*$ pada $C_L$. Dengan menggunakan notasi, kerugian reasuradur adalah $Y = X_L^*L-C_L$ Untuk melihat hal ini, pertama-tama pertimbangkan kasus di mana pemegang polis mengalami kerugian $X &lt; C_L$. Kemudian, penanggung akan membayar seluruh klaim dan $Y=C_L-C_L=0$ tidak ada kerugian bagi reasuradur. Sebaliknya, jika kerugian $X≥C_L$ maka $Y = X-C_L$ merupakan klaim yang ditahan oleh reasuradur. Dengan kata lain, jika terjadi kerugian, reasuradur mencatat jumlah sebenarnya jika melebihi batas $C_L$ dan jika tidak, hanya mencatat akan mengalami kerugian sebesar 0. #### Truncated data Pengamatan yang disensor dicatat untuk studi, meskipun dalam bentuk yang terbatas. Sebaliknya, hasil yang terpotong adalah jenis data yang hilang. Sebuah hasil berpotensi terpotong ketika ketersediaan pengamatan bergantung pada hasil. Dalam asuransi, biasanya pengamatan terpotong kiri pada $C_L$ ketika jumlahnya adalah $$ \\begin{aligned} Y &amp;= \\left\\{ \\begin{array}{cl} \\text{we do not observe }X &amp; X \\le C_L \\\\ X &amp; X &gt; C_L \\end{array} \\right.\\end{aligned} $$ Dengan kata lain, jika X kurang dari ambang batas $C_L$ maka ia tidak teramati. $C_L$ dapat merepresentasikan deductible dari sebuah polis asuransi. Jika kerugian yang diasuransikan kurang dari deductible, maka perusahaan asuransi mungkin tidak mengamati atau mencatat kerugian sama sekali. Jika kerugian melebihi deductible, maka kelebihan $X-C_L$ adalah klaim yang ditanggung oleh penanggung. Dimana dapat didefinisikan kerugian per pembayaran sebagai $$ \\begin{aligned} Y^{P} = \\left\\{ \\begin{matrix} \\text{Undefined} &amp; X \\le d \\\\ X - d &amp; X &gt; d \\end{matrix} \\right. \\end{aligned} $$ sehingga jika kerugian melebihi deductible, kami mencatat jumlah kelebihan $X-d$. Hal ini sangat penting ketika mempertimbangkan jumlah yang akan dibayarkan oleh perusahaan asuransi. Namun, untuk tujuan estimasi pada bagian ini, tidak terlalu penting jika kita mengurangkan konstanta yang diketahui seperti $C_L = d$. Sehingga, untuk variabel terpotong $Y$ kita menggunakan konvensi yang lebih sederhana dan tidak mengurangkan $d$. Demikian pula untuk data terpotong kanan, jika X melebihi ambang batas $C_U$ maka data tersebut tidak diobservasi. Dalam hal ini, jumlahnya adalah $$ \\begin{aligned} Y &amp;= \\left\\{ \\begin{array}{cl} X &amp; X \\le C_U \\\\ \\text{we do not observe }X &amp; X &gt; C_U. \\end{array} \\right.\\end{aligned} $$ Contoh klasik dari pemotongan dari kanan termasuk X sebagai ukuran jarak ke bintang. Ketika jaraknya melebihi tingkat tertentu $C_U$ maka bintang tersebut tidak lagi dapat diamati. Gambar dibawah ini membandingkan pengamatan yang terpotong dan tersensor. Nilai-nilai X yang lebih besar dari batas penyensoran &quot;atas&quot; $C_U$ tidak teramati sama sekali (tersensor kanan), sedangkan nilai X yang lebih kecil dari batas pemotongan &quot;bawah&quot; $C_L$ tetap diamati, tetapi diamati sebagai $C_L$ daripada nilai X yang sebenarnya (tersensor kiri). #### Parametric Estimation using Cencored and Truncated data Untuk mempermudah, dapat diasumsikan jumlah penyensoran tidak acak dan hasil yang kontinu X . Sebagai permulaan, pertimbangkan kasus data tersensor kanan di mana merekam $X_U^* = min(X, C_U) = X∧C_U$) dan indikator penyensoran $δ = I(X≤C_U)$ . Jika penyensoran terjadi sehingga $δ=0$ maka $X&gt;C_U$ dan peluangnya adalah $Pr(X&gt;C_U)=1-F(C_U)$. Jika penyensoran tidak terjadi sehingga $δ = 1$ maka $X≤C_U$ dan likelihoodnya adalah $f(x)$ . Ringkasnya, didapatkan likelihood dari sebuah pengamatan tunggal sebagai $$ \\begin{aligned} \\left\\{ \\begin{array}{ll} 1-F(C_U) &amp; \\text{if }\\delta=0 \\\\ f(x) &amp; \\text{if } \\delta = 1 \\end{array} \\right. = \\left\\{ f(x)\\right\\}^{\\delta} \\left\\{1-F(C_U)\\right\\}^{1-\\delta} . \\end{aligned} $$ Ekspresi ruas kanan memungkinkan dalam menyajikan peluang dengan lebih ringkas. Sekarang, untuk sampel ke-i dengan ukuran n , peluangnya adalah $$ \\begin{aligned} L(\\theta) = \\prod_{i=1}^n \\left\\{ f(x_i)\\right\\}^{\\delta_i} \\left\\{1-F(C_{Ui})\\right\\}^{1-\\delta_i} = \\prod_{\\delta_i=1} f(x_i) \\prod_{\\delta_i=0} \\{1-F(C_{Ui})\\} \\end{aligned} $$ dengan waktu penyensoran potensial ${(C_{U1},...,C_{Un})}$ . Di sini, notasi &quot;$∏{δi} = 1$&quot; berarti mengambil hasil kali dari pengamatan yang tidak disensor, dan demikian pula untuk &quot;$∏{δi} = 0$ &quot; Di sisi lain, data terpotong ditangani dalam inferensi kemungkinan melalui probabilitas bersyarat. Secara khusus, kontribusi likelihood dapat disesuaikan dengan membaginya dengan probabilitas bahwa variabel tersebut diamati. Sebagai rangkuman, kami memiliki kontribusi berikut pada fungsi likelihood untuk enam jenis hasil: src=&quot;https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/table4-1.png?raw=true&quot; width=&quot;300&quot; height=&quot;300&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; margin-top: 10px;&quot;&gt; Untuk hasil yang diketahui dan data yang disensor, kemungkinannya adalah \\[ \\begin{aligned} L(\\theta) = \\prod_{E} f(x_i) \\prod_{R} \\{1-F(C_{Ui})\\} \\prod_{L} F(C_{Li}) \\prod_{I} (F(C_{Ui})-F(C_{Li})), \\end{aligned} \\] di mana \\(&quot;∏_E&quot;\\) adalah hasil kali pengamatan dengan nilai Exact, dan demikian pula untuk Right-,Left- and Interval-censoring. Untuk data yang disensor kanan dan terpotong kiri, kemungkinannya adalah \\[ \\begin{aligned} L(\\theta) = \\prod_{E} \\frac{f(x_i)}{1-F(C_{Li})} \\prod_{R} \\frac{1-F(C_{Ui})}{1-F(C_{Li})}, \\end{aligned} \\] dan juga untuk kombinasi lainnya. Example 4.3.2. Actuarial Exam Question Diberikan data : Sebuah contoh kerugian adalah: 600 700 900 Tidak ada informasi yang tersedia mengenai kerugian sebesar 500 atau kurang. Kerugian diasumsikan mengikuti distribusi eksponensial dengan rata-rata \\(θ\\). Hitung estimasi kemungkinan maksimum dari \\(θ\\) Pengamatan ini terpotong pada angka 500. Kontribusi dari setiap pengamatan terhadap fungsi likelihood adalah \\(\\frac{f(x)}{1-F(500)} = \\frac{\\theta^{-1}e^{-x/\\theta}}{e^{-500/\\theta}}\\) Lalu Fungsi Likelihoodnya adalah \\(L(\\theta)= \\frac{\\theta^{-1} e^{-600/\\theta} \\theta^{-1} e^{-700/\\theta} \\theta^{-1} e^{-900/\\theta}}{(e^{-500/\\theta})^3} = \\theta^{-3}e^{-700/\\theta}\\) Log-Likehoodnya adalah \\(l(\\theta) = \\log L(\\theta) = -3 \\log \\theta - 700 \\theta^{-1}\\) Memaksimalkan ekspresi ini dengan menetapkan turunan terhadap θ sama dengan 0, Maka memiliki \\(L&#39;(\\theta) = -3 \\theta^{-1} + 700 \\theta^{-2} = 0 \\ \\Rightarrow \\ \\hat{\\theta} = \\frac{700}{3} = 233.33 .\\) 4.3.2 Nonparametric Estimation using Modified Data Estimator nonparametrik memberikan tolok ukur yang berguna, sehingga akan sangat membantu untuk memahami prosedur estimasi untuk data yang dikelompokkan, disensor, dan dipotong 4.3.2.1 Grouped Data Pengamatan dapat dikelompokkan (juga disebut sebagai interval tersensor) dalam arti bahwa pengamatan sebagai bagian dari salah satu dari k interval dalam bentuk \\((c_{j-1},c_j)\\) , untuk \\(j = 1,...,k\\) . Pada batas-batasnya, fungsi distribusi empiris didefinisikan dengan cara yang biasa: \\[ \\begin{aligned} F_n(c_j) = \\frac{\\text{number of observations } \\le c_j}{n} \\end{aligned} \\] Ogive Estimator Untuk nilai lain dari \\(x∈(c_{j-1},c_j)\\) dapat mengestimasi fungsi distribusi dengan ogive estimator yang menginterpolasi secara linear antara \\(F_n(c_{j-1})\\) dan \\(Fn_(c_j)\\) yaitu nilai dari batas-batas \\(F_n(c_{j-1})\\) dan \\(Fn_(c_j)\\) dihubungkan dengan sebuah garis lurus. Hal ini secara formal dapat dinyatakan sebagai \\[ \\begin{aligned} F_n(x) = \\frac{c_j-x}{c_j-c_{j-1}} F_n(c_{j-1}) + \\frac{x-c_{j-1}}{c_j-c_{j-1}} F_n(c_j) \\ \\ \\ \\text{for } c_{j-1} \\le x &lt; c_j \\end{aligned} \\] Sehinga Densitas yang sesuai adalah \\[ \\begin{aligned} f_n(x) = F^{\\prime}n(x) = \\frac{F_n(c_j)-F_n(c{j-1})}{c_j - c_{j-1}} \\ \\ \\ \\text{for } c_{j-1} &lt; x &lt; c_j . \\end{aligned} \\] Example 4.3.4. Actuarial Exam Question Diberikan informasi berikut ini mengenai jumlah klaim untuk 100 klaim: src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/example4.3.4-1.png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Dengan menggunakan ogive, hitunglah estimasi probabilitas bahwa klaim yang dipilih secara acak adalah antara 2000 dan 6000. Pada batas-batasnya, fungsi distribusi empiris didefinisikan dengan cara yang biasa, sehingga memiliki \\(F_{100}(1000) = 0.16, \\ F_{100}(3000)=0.38, \\ F_{100}(5000)=0.63, \\ F_{100}(10000)=0.81\\) Untuk ukuran klaim lainnya, penaksir ogive melakukan interpolasi linier di antara nilai-nilai ini: \\[ \\begin{array}{ll} F_{100}(2000) &amp;= 0.5F_{100}(1000) + 0.5F_{100}(3000) = 0.5(0.16)+0.5(0.38)=0.27 \\\\ F_{100}(6000) &amp;=0.8F_{100}(5000)+0.2F_{100}(10000) = 0.8(0.63)+0.2(0.81)=0.666 \\end{array} \\] Dengan demikian, probabilitas klaim antara 2000 dan 6000 adalah \\(F_{100}(6000) - F_{100}(2000) = 0.666-0.27 = 0.396\\) 4.3.2.2 Right-Censored Empirical Distribution Function Akan sangat berguna untuk mengkalibrasi penaksir parametrik dengan metode nonparametrik yang tidak bergantung pada bentuk parametrik distribusi. Penaksir batas produk menurut (Kaplan dan Meier 1958) merupakan penaksir yang terkenal untuk fungsi distribusi dengan adanya penyensoran. Motivasi untuk Penaksir Batas Produk Kaplan-Meier Untuk menjelaskan mengapa product-limit bekerja dengan sangat baik dengan observasi tersensor, pertama-tama dapat melihat ke kasus tanpa penyensoran. Di sini, fungsi distribusi empiris \\(F_n(x)\\) adalah penaksir tak bias dari fungsi distribusi \\(F(x)\\) . Hal ini karena \\(F_n(x)\\) adalah rata-rata dari variabel indikator yang masing-masing tidak bias, yaitu, \\(E [I(X_i≤x)]=Pr(X_i≤x)=F(x)\\) Sekarang misalkan hasil acak disensor di sebelah kanan dengan jumlah yang membatasi, katakanlah, CU sehingga dapat mencatat yang lebih kecil dari keduanya, \\(X^* = min(X, C_U)\\) . Untuk nilai-nilai \\(x\\) yang lebih kecil dari \\(C_U\\), variabel indikator masih memberikan penaksir yang tidak bias terhadap fungsi distribusi sebelum kita mencapai batas penyensoran. Artinya, \\(E [I(X^∗≤x)]=F(x)\\) karena \\(I(X^∗≤x)=I(X≤x)\\) untuk \\(x&lt;C_U\\) . Dengan cara yang sama, \\(E[I(X^∗&gt;x)]=1-F(x)=S(x)\\) . Tetapi, untuk \\(x&gt;C_U\\) , \\(I(X^∗≤x)\\) secara umum bukan merupakan penaksir tak bias dari F(x). Sebagai alternatif, pertimbangkan dua peubah acak yang memiliki batas penyensoran yang berbeda. Sebagai ilustrasi, misalkan kita mengamati \\(X^∗1=min(X_1,5)\\) dan \\(X^∗2 = min(X_2,10)\\) di mana \\(X_1\\) dan \\(X_2\\) adalah undian independen dari distribusi yang sama. Untuk \\(x≤5\\) fungsi distribusi empiris \\(F_2(x)\\) adalah penaksir tak bias dari \\(F(x)\\). Akan tetapi, untuk \\(5&lt;x≤10\\) pengamatan pertama tidak dapat digunakan untuk fungsi distribusi karena adanya batasan penyensoran. Sebagai gantinya, strategi yang dikembangkan oleh (Kaplan dan Meier 1958) adalah dengan menggunakan \\(S_2(5)\\) sebagai penaksir dari \\(S(5)\\) dan kemudian menggunakan observasi kedua untuk mengestimasi fungsi survival bersyarat pada kelangsungan hidup hingga waktu ke-5, \\(Pr(X&gt;x|X&gt;5)=\\frac{S(x)}{S(5)}\\) . Secara khusus, untuk \\(5&lt;x≤10\\) penaksir dari fungsi survival adalah \\[ \\begin{aligned} \\hat{S}(x) = S_2(5) \\times I(X_2^* &gt; x ) \\end{aligned} \\] Kaplan-Meier Product Limit Estimator Dengen memperluas ide dalam setiap observasi i,dengan ui menjadi batas atas penyensoran \\((=∞) jikatidakadapenyensoran\\). Dengan demikian, nilai yang tercatat adalah xi dalam kasus tidak ada penyensoran dan ui jika ada penyensoran. Dengan \\(t_1&lt;⋯&lt;t_k\\)menjadi k titik berbeda di mana kerugian yang tidak disensor terjadi, dan biarkan \\(s_j\\) adalah jumlah kerugian yang tidak tersensor \\(x_i\\) yang tidak tersensor pada \\(t_j\\). Himpunan risiko yang sesuai adalah jumlah observasi yang aktif (tidak tersensor) pada nilai yang kurang dari \\(t_j\\) yang dinotasikan sebagai \\(R_j = \\sum_{i=1}^n I(x_i \\geq t_{j}) + \\sum_{i=1}^n I(u_i \\geq t_{j})\\) Dengan notasi ini, penaksir product-limit dari fungsi distribusi \\[ \\begin{equation} \\hat{F}(x) = \\left\\{ \\begin{array}{ll} 0 &amp; x&lt;t_{1} \\\\ 1-\\prod_{j:t_{j} \\leq x}\\left( 1-\\frac{s_j}{R_{j}}\\right) &amp; x \\geq t_{1} \\end{array} \\right. . \\tag{4.6} \\end{equation} \\] Sebagai contohnya, jika x lebih kecil dari kerugian terkecil yang tidak tersensor, maka \\(x&lt;t1\\) dan \\(F^(x)=0\\) . Sebagai contoh lain, jika \\(x\\) berada di antara kerugian tersensor terkecil kedua dan ketiga, maka \\(x∈(t_2,t_3]\\) dan \\(\\hat{F}(x) = 1 - \\left(1- \\frac{s_1}{R_{1}}\\right)\\left(1- \\frac{s_2}{R_{2}}\\right)\\) .Taksiran yang sesuai dari fungsi survival adalah \\(\\hat{S}(x) = 1 - \\hat{F}(x)\\) 4.3.3 Example 4.3.5. Actuarial Exam Question. Berikut ini adalah contoh dari 10 pembayaran: \\[ 4 \\space \\space 4 \\space \\space 5+\\space \\space 5+ \\space\\space 5+ \\space\\space 8 \\space\\space 10+ \\space\\space 10+ \\space\\space 12 \\space\\space 15 \\] dimana + menunjukkan bahwa kerugian telah melebihi batas polis. Dengan menggunakan estimator batas produk Kaplan-Meier, hitunglah probabilitas bahwa kerugian pada suatu polis melebihi 11, \\(\\hat{S}(11)\\) Terdapat empat waktu kejadian (pengamatan yang tidak disensor). Untuk setiap waktu tj kita dapat menghitung jumlah kejadian sj dan himpunan risiko \\(R_j\\) sebagai berikut: src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/example4.3.6-1.png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Dengan demikian, estimasi Kaplan-Meier dari S(11) adalah \\[ \\begin{aligned} \\hat{S}(11) &amp;= \\prod_{j:t_j\\leq 11} \\left( 1- \\frac{s_j}{R_j} \\right) = \\prod_{j=1}^{2} \\left( 1- \\frac{s_j}{R_j} \\right)\\\\ &amp;= \\left(1-\\frac{2}{10} \\right) \\left(1-\\frac{1}{5} \\right) = (0.8)(0.8)= 0.64. \\\\ \\end{aligned} \\] Right-Censored, Left-Truncated Empirical Distribution Function Selain penyensoran kanan, selanjutnya adalah memperluas kerangka kerja untuk memungkinkan data terpotong ke kiri. Seperti sebelumnya, untuk setiap observasi i , dengan \\(u_i\\) menjadi batas penyensoran atas ( \\(=∞\\) jika tidak ada penyensoran). Selanjutnya, \\(d_i\\) merupakan batas pemotongan bawah (0 jika tidak ada pemotongan). Dengan demikian, nilai yang tercatat (jika lebih besar dari \\(d_i\\) ) adalah \\(x_i\\) dalam kasus tidak ada penyensoran dan \\(u_i\\) jika ada penyensoran. Lalu untuk $t_1&lt;⋯&lt;t_k $menjadi \\(k\\) titik-titik yang berbeda di mana sebuah kejadian yang menarik terjadi, dan biarkan \\(s_j\\) adalah jumlah kejadian yang terekam \\(x_i\\) pada titik waktu \\(t_j\\). Himpunan risiko yang sesuai adalah \\(R_j = \\sum_{i=1}^n I(x_i \\geq t_{j}) + \\sum_{i=1}^n I(u_i \\geq t_{j}) - \\sum_{i=1}^n I(d_i \\geq t_{j}).\\) Dengan definisi baru dari himpunan risiko ini, penaksir batas hasil kali dari fungsi distribusi adalah seperti pada persamaan product limit estimator. Rumus Greenwood (Greenwood 1926) menurunkan rumus untuk estimasi varians dari penaksir batas-produk menjadi \\(\\widehat{Var}(\\hat{F}(x)) = (1-\\hat{F}(x))^{2} \\sum {j:t{j} \\leq x} \\dfrac{s_j}{R_{j}(R_{j}-s_j)}.\\) Seperti biasa, dapat mengacu pada akar kuadrat dari estimasi varians sebagai kesalahan standar, sebuah kuantitas yang secara rutin digunakan dalam interval kepercayaan dan untuk pengujian hipotesis. Untuk menghitungnya, metode survfit R mengambil sebuah objek data survival dan membuat sebuah objek baru yang berisi estimasi Kaplan-Meier dari fungsi survival bersama dengan interval kepercayaan. Metode Kaplan-Meier (type='kaplan-meier') digunakan secara default untuk membuat estimasi kurva survival. Fungsi survival diskrit yang dihasilkan memiliki massa titik pada waktu kejadian yang diamati (tanggal pelepasan) \\(t_j\\) dimana probabilitas suatu kejadian yang diberi ketahanan hidup pada durasi tersebut diestimasi sebagai jumlah kejadian yang diamati pada durasi sj dibagi dengan jumlah subjek yang terpapar atau ‘berisiko’ sesaat sebelum durasi kejadian \\(R_j\\). Penaksir Alternatif Dua jenis estimasi alternatif juga tersedia untuk metode survfit. Alternatif pertama (type='fh2') menangani hubungan, pada dasarnya, dengan mengasumsikan bahwa beberapa kejadian pada durasi yang sama terjadi dalam urutan yang berubah-ubah. Alternatif lain (type='fleming-harrington') menggunakan estimasi Nelson-Aalen (Aalen 1978) dari fungsi hazard kumulatif untuk mendapatkan estimasi fungsi survival. Estimasi bahaya kumulatif \\(H^(x)\\) dimulai dari nol dan bertambah pada setiap durasi kejadian yang diamati \\(t_j\\) dengan jumlah kejadian \\(s_j\\) dibagi dengan jumlah yang berisiko \\(R_j\\). Dengan notasi yang sama seperti di atas, penaksir Nelson-Äalen dari fungsi distribusi adalah \\[ \\begin{aligned} \\hat{F}_{NA}(x) &amp;= \\left\\{ \\begin{array}{ll} 0 &amp; x&lt;t_{1} \\\\ 1- \\exp \\left(-\\sum_{j:t_{j} \\leq x}\\frac{s_j}{R_j} \\right) &amp; x \\geq t_{1} \\end{array} \\right. .\\end{aligned} \\] Itu merupakan hasil dari estimator Nelson-Äalen dari fungsi hazard kumulatif \\(\\hat{H}(x)=\\sum_{j:t_j\\leq x} \\frac{s_j}{R_j}\\) dan hubungan antara fungsi survival dan fungsi hazard kumulatif, \\(\\hat{S}_{NA}(x)=e^{-\\hat{H}(x)}\\) 4.4 Bayesian Inference Penjelasan pada subbab ini: Jelaskan model Bayesian sebagai alternatif dari pendekatan frequentist dan rangkum lima komponen dari pendekatan pemodelan ini. Ringkas distribusi parameter posterior dan gunakan distribusi posterior ini untuk memprediksi hasil baru. Gunakan distribusi konjugat untuk menentukan distribusi parameter posterior. 4.4.1 Introduction to Bayesian Inference Sampai saat ini, metode inferensial kami berfokus pada pengaturan frequentist , di mana sampel diambil berulang kali dari suatu populasi. Vektor parameter θ adalah tetap belum diketahui, sedangkan hasil X adalah realisasi variabel acak. Sebaliknya, di bawah kerangka Bayesian , kami melihat parameter model dan data sebagai variabel acak. Kami tidak yakin tentang parameternya θ dan gunakan alat probabilitas untuk mencerminkan ketidakpastian ini. Dibawah ini merupakan rumus aturan bayes: src=“https://github.com/dsciencelabs/Analisa_Resiko/blob/main/images/4.4.1-1.png?raw=true” width=“300” height=“300” style=“display: block; margin-left: auto; margin-right: auto; margin-top: 10px;”&gt; Di mana, Pr(parameters): adalah distribusi parameter, yang dikenal sebagai distribusi sebelumnya . Pr(data|parameters): adalah distribusi sampling. Dalam konteks frequentist, ini digunakan untuk membuat kesimpulan tentang parameter dan dikenal sebagai kemungkinan . Pr(parameters|data):adalah distribusi parameter setelah mengamati data, yang dikenal sebagai distribusi posterior . Pr(data): adalah distribusi marjinal dari data. Ini umumnya diperoleh dengan mengintegrasikan (atau menjumlahkan) distribusi gabungan data dan parameter di atas nilai parameter. 4.4.2 Bayesian Model 4.4.3 Bayesian Inference 4.4.3.1 Summarizing the Posterior Distributiob of Paremeters 4.4.3.2 Bayesian Predictive Distribution 4.4.4 Conjugate Distributions "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
