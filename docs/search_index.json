<<<<<<< HEAD
[["kata-pengantar.html", "Analisis Resiko Kata Pengantar Deskripsi Buku Ucapan Terima Kasih Kontributor Kritik &amp; Saran", " Analisis Resiko Bakti Siregar, M.Sc 2023-05-22 Kata Pengantar Deskripsi Buku Analisa Resiko adalah buku yang interaktif, online, dan tersedia secara gratis. Versi online berisi banyak objek interaktif (kuis, demonstrasi komputer, grafik interaktif, video, dan sejenisnya) yang dapat dipergunakan untuk menunjang pembelajaran lebih baik. Sebagian besar isi dari buku ini tersedia untuk dibaca offline dalam format pdf dan EPUB. Direncanakan akan tersedia dalam berbagai bahasa. Petunjuk Penggunaan Buku ini dapat dipergunakan dalam pembelajaran kurikulum aktuaria di seluruh dunia. Adapun cakupan pembelajarannya adalah analisa data kerugian dari berbagai organisasi aktuaria ternama didunia. Sehingga, buku ini cocok digunakan ditingkat universitas maupun pembelajar mandiri yang ingin lulus ujian aktuaria profesional. Selain itu, buku juga akan sangat berguna dalam pengembangan profesional berkelanjutan bagi para aktuaris maupun profesional lainnya di bidang asuransi dan industri terkait manajemen risiko keuangan. Manfaat Salah satu manfaat penting dari buku online ini adalah pemerataan akses pengetahuan, sehingga memungkinkan masyarakat yang lebih luas untuk belajar tentang profesi aktuaria. Selain itu, setiap orang memiliki kapasitas untuk melibatkan banyak pihak melalui pembelajaran aktif yang memperdalam proses pembelajaran, menghasilkan analis terbaik dalam melakukan pekerjaan aktuaria yang solid. Sekarang, pertanyaan besarnya adalah “Mengapa buku ini baik untuk mahasiswa dan dosen serta orang lain yang terlibat dalam proses pembelajaran?” Biaya adalah salah satu faktor yang sering disebut sebagai kendala utama bagi mahasiswa dan dosen dalam pemilihan buku teks. Selain itu, Mahasiswa sekarang ini lebih menyukai buku yang dapat dibawa secara secara elektronik (online). Mengapa Analisa Resiko? Tujuannya adalah agar buku ini pada akhirnya akan dapat dikembangkan secara serius kurikulum aktuaria. Mengingat perubahan era digital seperti sekarang ini akhirnya mendorong para aktuaris dalam melakukan analisa bergantung pada data yang dimiliki. Ide di balik nama Analisa Resiko adalah untuk mengintegrasikan model data kerugian klasik dari probabilitas yang diterapkan dengan alat analitik modern. Secara khusus, penulis menyadari bahwa big data (termasuk media sosial dan asuransi berbasis penggunaan) akan terus berkembang dan komputasi berkecepatan tinggi sudah tersedia. Ucapan Terima Kasih Kami juga ingin mengucapkan terima kasih yang sebesar-sebesar pada semua pihak yang terlibat dalam pengembangan buku ini, yakni; mahasiswa-i, dosen, dan Universitas Matana atas dukungan dalam upaya bersama kami untuk menyediakan konten pendidikan dalam bidang aktuaria. Kontributor Sebagian besar dari isi buku ini diadopsi dari Loss Data Analytics. Berikut ini adalah nama-nama dan biografi singkat para penulis: Bakti Siregar, M.Sc adalah Kepala Program Studi dan Dosen di Jurusan Statistika Universitas Matana. Beliau juga seorang dosen yang juga bekerja sebagai ilmuwan data lepas yang memiliki antusiasme untuk analitik data besar, pembelajaran mesin, Pemodelan, dan pemecahan masalah. Orang menganggap saya programmer Matematika karena saya memiliki kemampuan yang kuat dalam program Statistik seperti R Studio, dan Python, dan juga akrab dengan alat basis data seperti MySQL dan sistem data besar baik Spark maupun Hadoop. Selain itu, saya dapat mengoperasikan salah satu perangkat lunak analitik bisnis yang paling kuat seperti Tableau. Yosia adalah salah satu mahasiwa terbaik di jurusan Statistika Universitas Matana. Dia juga memiliki minat dalam pembelajaran sains data dan akuturia khususnya melakukan komputasi dengan menggunakan R dan Python. Yosia bercita-cita suatu saat nanti akan menjadi seseorang yang ahli dibidang aktuaria maupun sain data. Yosia adalah mahasiswi jurusan Statistik di Universitas Matana. Dia memiliki minat teoretis yang luas serta minat dalam komputasi, ia juga sudah pernah terlibat dalam menerbitkan di jurnal Pengabdian Kepada Masyarakat (PKM). Dia juga aktif dalam berbagai aktifitas organisasi kampus. Clara Della adalah mahasiswi jurusan Statistik di Universitas Matana. Dia memiliki minat teoretis yang luas serta minat dalam komputasi, ia juga sudah pernah terlibat dalam menerbitkan di jurnal Pengabdian Kepada Masyarakat (PKM). Dia juga aktif dalam berbagai aktifitas organisasi kampus. adalah mahasiswi jurusan Statistik di Universitas Matana. Dia memiliki minat teoretis yang luas serta minat dalam komputasi, ia juga sudah pernah terlibat dalam menerbitkan di jurnal Pengabdian Kepada Masyarakat (PKM). Dia juga aktif dalam berbagai aktifitas organisasi kampus. Karen adalah mahasiswi jurusan Statistik di Universitas Matana yang memiliki keahlian penelitiannya dengan menggunakan teori pemodelan, manajemen risiko, dan optimasi. adalah mahasiswi jurusan Statistik di Universitas Matana. Dia memiliki minat teoretis yang luas serta minat dalam komputasi, ia juga sudah pernah terlibat dalam menerbitkan di jurnal Pengabdian Kepada Masyarakat (PKM). Dia juga aktif dalam berbagai aktifitas organisasi kampus. Brigita adalah dosen senior di Macquarie University di Australia, di mana ia menjabat sebagai direktur program sarjana aktuaria sejak 2018. Ia memperoleh gelar Ph.D. pada tahun 2015 dari Nanyang Technological University di Singapura. Dia adalah seorang aktuaris yang berkualifikasi penuh, memegang kredensial dari US Society of Actuaries dan Australian Actuaries Institute. Minat penelitian utamanya adalah pemodelan kematian, manajemen risiko umur panjang, dan sistem bonus-malus. Naufal adalah seorang profesor di Universitas Matana. Dia memiliki gelar di bidang Matematika dan Ph.D. dalam Sains: Matematika, diperoleh di University of Antwerp. Selama Ph.D., ia berhasil mengambil Magister Asuransi dan Magister Teknik Keuangan dan Aktuaria, keduanya di KU Leuven. Penelitiannya berfokus pada adaptasi dan penerapan metode statistik yang kuat untuk data asuransi dan keuangan. adalah mahasiswi jurusan Statistik di Universitas Matana. Dia memiliki minat teoretis yang luas serta minat dalam komputasi, ia juga sudah pernah terlibat dalam menerbitkan di jurnal Pengabdian Kepada Masyarakat (PKM). Dia juga aktif dalam berbagai aktifitas organisasi kampus. Garry adalah Associate Professor di Departemen Manajemen Risiko, Asuransi, dan Kesehatan di Fox School of Business, Temple University? Dia adalah Associate dari Society of Actuaries. Dia mengajar mata kuliah Ilmu Aktuaria dan Manajemen Risiko di tingkat sarjana dan pascasarjana. Minat penelitiannya meliputi tata kelola perusahaan asuransi, manajemen modal, dan analisis sentimen. Dia menerima gelar Ph.D. dari The Wharton School of the University of Pennsylvania. Kritik &amp; Saran Buku teks interaktif yang tersedia secara gratis mewakili usaha baru dalam pendidikan aktuaria dan kami membutuhkan masukan Anda. Meskipun banyak upaya telah dilakukan untuk pengembangan, kami mengharapkan cegukan. Harap beri tahu instruktur Anda tentang peluang untuk peningkatan, hubungi kami melalui situs proyek kami, atau hubungi kontributor bab secara langsung dengan saran peningkatan. Berikut ini dilampirkan beberapa peninjau atau pembaca yang telah memberikan saran dan pendapat mengenai pengembangan buku ini, adalah: mahasiswa 1 mahasiswa 2 mahasiswa 3 mahasiswa 4 mahasiswa 5 mahasiswa 6 "],["introduction-to-loss-data-analytics.html", "Bab 1 Introduction to Loss Data Analytics", " Bab 1 Introduction to Loss Data Analytics Chapter Preview. This book introduces readers to methods of analyzing insurance data. Section 1.1 begins with a discussion of why the use of data is important in the insurance industry. Section 1.2 gives a general overview of the purposes of analyzing insurance data which is reinforced in the Section 1.3 case study. Naturally, there is a huge gap between the broad goals summarized in the overview and a case study application; this gap is covered through the methods and techniques of data analysis covered in the rest of the text. "],["frequency-modeling.html", "Bab 2 Frequency Modeling 2.1 Goodness of Fit", " Bab 2 Frequency Modeling 2.1 Goodness of Fit "],["modeling-loss-severity.html", "Bab 3 Modeling Loss Severity 3.1 mdmmm 3.2 mdmem 3.3 mdmm 3.4 modifikasi pertanggungan", " Bab 3 Modeling Loss Severity 3.1 mdmmm 3.2 mdmem 3.3 mdmm 3.4 modifikasi pertanggungan Coverage modifications atau modifikasi pertanggungan adalah perubahan yang dibuat pada syarat dan ketentuan polis asuransi. Perubahan ini dapat diprakarsai oleh pemegang polis atau perusahaan asuransi, dan dirancang untuk mengubah pertanggungan yang diberikan oleh polis. Modifikasi pertanggungan dapat dilakukan karena berbagai alasan. Sebagai contoh, pemegang polis mungkin ingin meningkatkan batas pertanggungan pada polis mereka untuk melindungi diri mereka sendiri dari potensi kerugian. Atau, mereka mungkin ingin menambah atau menghapus jenis pertanggungan tertentu, seperti menambahkan asuransi banjir pada polis pemilik rumah atau menghapus pertanggungan tabrakan dari polis mobil. "],["model-selection-and-estimation.html", "Bab 4 Model Selection and Estimation", " Bab 4 Model Selection and Estimation test "],["aggregate-loss-models.html", "Bab 5 Aggregate Loss Models", " Bab 5 Aggregate Loss Models Sub bab ini membahas mengenai pembangunan model probabilitas untuk menggambarkan klaim agregat oleh sistem asuransi yang terjadi dalam periode waktu tertentu. Sistem asuransi dapat berupa polis tunggal, kontrak asuransi kelompok, lini bisnis , atau seluruh buku bisnis perusahaan asuransi. Dalam bab ini, klaim agregat mengacu pada jumlah klaim dari portofolio kontrak asuransi. Pertimbangkan portofolio asuransi dari \\(N\\) kontrak individu, dan \\(S\\) menunjukkan kerugian agregat portofolio dalam jangka waktu tertentu. Ada dua pendekatan untuk memodelkan kerugian agregat \\(S\\) , model risiko individu dan model risiko kolektif. Model risiko individu menekankan kerugian dari masing-masing kontrak individu dan mewakili kerugian agregat sebagai: \\[S_n=X_1 +X_2 +\\cdots+X_n,\\] Di mana \\(X_i~(i=1,\\ldots,n)\\) diinterpretasikan sebagai jumlah kerugian dari \\(X_i\\) kontrak. \\(N\\) menunjukkan jumlah kontrak dalam portofolio dan dengan demikian merupakan angka tetap daripada variabel acak. Untuk model risiko individu, biasanya diasumsikan \\(X_i\\) ini independen. Karena fitur kontrak yang berbeda seperti cakupan dan paparan , \\(X_i\\) belum tentu terdistribusi secara identik. Fitur penting dari distribusi masing-masing \\(X_i\\) adalah massa probabilitas pada nol yang sesuai dengan peristiwa tidak adanya klai Model risiko kolektif mewakili kerugian agregat dalam hal distribusi frekuensi dan distribusi keparahan: \\[S_N=X_1 +X_2 + \\cdots + X_N .\\] Sejumlah klaim acak \\(N\\) yang dapat mewakili baik jumlah kerugian atau jumlah pembayaran. Sebaliknya, dalam model risiko individual biasanya menggunakan sejumlah kontrak tetap \\(N\\).\\(X_1, X_2, \\ldots, X_N\\) sebagai representasi dari jumlah masing-masing kerugian. Setiap kerugian mungkin atau mungkin tidak sesuai dengan kontrak unik. Misalnya, mungkin ada banyak klaim yang timbul dari satu kontrak. Itu wajar untuk dipikirkan \\(X_i&gt;0\\) karena jika \\(X_i=0\\) maka tidak ada klaim yang terjadi. Biasanya kita menganggap bahwa kondisional pada \\(X_{1},X_{2},\\ldots ,X_{n}\\) adalah iid variabel acak. Distribusi dari N dikenal sebagai distribusi frekuensi , dan distribusi umum dari \\(X\\) dikenal sebagai distribusi keparahan . Dengan berasumsi \\(N\\) Dan \\(X\\) sendiri. Dengan model risiko kolektif, sehingga dapat menguraikan kerugian agregat menjadi frekuensi \\(( N )\\) proses dan tingkat keparahan \\(( X )\\) model. Fleksibilitas ini memungkinkan analis untuk mengomentari dua komponen terpisah ini. Misalnya, pertumbuhan penjualan karena standar penjaminan emisi yang lebih rendah dapat menyebabkan frekuensi kerugian yang lebih tinggi tetapi mungkin tidak memengaruhi keparahan. Demikian pula, inflasi atau kekuatan ekonomi lainnya dapat berdampak pada keparahan tetapi tidak pada frekuensi. "],["simulation-and-resampling.html", "Bab 6 Simulation and Resampling", " Bab 6 Simulation and Resampling "],["premium-foundations.html", "Bab 7 Premium Foundations", " Bab 7 Premium Foundations "],["risk-classification.html", "Bab 8 Risk Classification", " Bab 8 Risk Classification "],["experience-rating-using-credibility-theory.html", "Bab 9 Experience Rating Using Credibility Theory", " Bab 9 Experience Rating Using Credibility Theory "],["insurance-portfolio-management-including-reinsurance.html", "Bab 10 Insurance Portfolio Management including Reinsurance", " Bab 10 Insurance Portfolio Management including Reinsurance "],["loss-reserving.html", "Bab 11 Loss Reserving", " Bab 11 Loss Reserving "],["experience-rating-using-bonus-malus.html", "Bab 12 Experience Rating using Bonus-Malus", " Bab 12 Experience Rating using Bonus-Malus "],["aggregate-loss-models-1.html", "Bab 13 Aggregate Loss Models", " Bab 13 Aggregate Loss Models "],["dependence-modeling.html", "Bab 14 Dependence Modeling", " Bab 14 Dependence Modeling "],["appendix-a-review-of-statistical-inference.html", "Bab 15 Appendix A: Review of Statistical Inference", " Bab 15 Appendix A: Review of Statistical Inference "],["appendix-b-iterated-expectations.html", "Bab 16 Appendix B: Iterated Expectations", " Bab 16 Appendix B: Iterated Expectations "],["appendix-c-maximum-likelihood-theory.html", "Bab 17 Appendix C: Maximum Likelihood Theory", " Bab 17 Appendix C: Maximum Likelihood Theory "],["appendix-d-summary-of-distributions.html", "Bab 18 Appendix D: Summary of Distributions", " Bab 18 Appendix D: Summary of Distributions "],["appendix-e-conventions-for-notation.html", "Bab 19 Appendix E: Conventions for Notation", " Bab 19 Appendix E: Conventions for Notation "],["bibliography.html", "Bibliography", " Bibliography "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
=======
[["simulation-and-resampling.html", "Bab 6 Simulation and Resampling", " Bab 6 Simulation and Resampling "],["cross-validation.html", "Bab 7 6.3 Cross Validation 7.1 6.3.1 k-Fold Cross-Validation 7.2 6.3.2 Leave-One-Out Cross-Validation 7.3 6.3.3 Cross-Validation and Bootstrap", " Bab 7 6.3 Cross Validation Dalam bagian ini, kita akan mempelajari caranya: Membandingkan dan membedakan validasi silang dengan teknik simulasi dan metode bootstrap. Menggunakan teknik validasi silang untuk pemilihan model Menjelaskan metode jackknife sebagai kasus khusus validasi silang dan menghitung estimasi bias dan kesalahan standar jackknife Validasi silang, yang diperkenalkan secara singkat pada Bagian 4.2.4, adalah teknik yang didasarkan pada hasil simulasi. Sekarang kita akan membandingkan dan membedakan validasi silang dengan teknik simulasi lain yang telah diperkenalkan dalam bab ini.” Simulasi, atau Monte-Carlo, yang diperkenalkan pada Bagian 6.1, memungkinkan kita untuk menghitung nilai ekspektasi dan rangkuman distribusi statistik lainnya, seperti nilai-p, dengan mudah. Bootstrap, dan metode resampling lainnya yang diperkenalkan pada Bagian 6.2, menyediakan estimator presisi, atau variabilitas, statistik. Validasi silang penting ketika menilai seberapa akurat model prediktif akan bekerja dalam praktiknya. Tumpang tindih memang ada, namun tetap saja akan sangat membantu untuk memikirkan tujuan luas yang terkait dengan setiap metode statistik. Untuk membahas validasi silang, mari kita ingat kembali dari Bagian 4.2 beberapa ide kunci dari validasi model. Ketika menilai, atau memvalidasi, sebuah model, kita melihat kinerja yang diukur pada data baru, atau setidaknya bukan data yang digunakan untuk mencocokkan model. Pendekatan klasik, yang dijelaskan di Bagian 4.2.3, adalah membagi sampel menjadi dua: satu bagian (dataset pelatihan) digunakan untuk menyesuaikan model dan bagian lainnya (dataset pengujian) digunakan untuk memvalidasi. Namun, keterbatasan dari pendekatan ini adalah bahwa hasilnya bergantung pada pembagian; meskipun keseluruhan sampel tetap, pembagian antara sub-sampel pelatihan dan pengujian bervariasi secara acak. Sampel pelatihan yang berbeda berarti parameter estimasi model akan berbeda. Parameter model yang berbeda dan sampel uji yang berbeda berarti statistik validasi akan berbeda. Dua orang analis dapat menggunakan data yang sama dan model yang sama, namun mencapai kesimpulan yang berbeda tentang kelayakan suatu model (berdasarkan pembagian acak yang berbeda), sebuah situasi yang membuat frustasi. 7.1 6.3.1 k-Fold Cross-Validation Untuk mengurangi kesulitan ini, biasanya digunakan pendekatan validasi silang seperti yang diperkenalkan di Bagian 4.2.4. Ide utamanya adalah meniru pendekatan pengujian/pelatihan dasar untuk validasi model dengan mengulanginya berkali-kali melalui rata-rata dari beberapa bagian data yang berbeda. Keuntungan utamanya adalah bahwa statistik validasi tidak terikat pada model parametrik (atau nonparametrik) tertentu - seseorang dapat menggunakan statistik nonparametrik atau statistik yang memiliki interpretasi ekonomi - sehingga dapat digunakan untuk membandingkan model yang tidak bersarang (tidak seperti prosedur rasio kemungkinan). Contoh 6.3.1. Dana Properti Wisconsin. Untuk data dana properti 2010 yang diperkenalkan pada Bagian 1.3, kami mencocokkan distribusi gamma dan Pareto dengan 1.377 data klaim. Untuk rincian kecocokan terkait, lihat Lampiran Bagian 15.4.4. Sekarang kita mempertimbangkan statistik Kolmogorov-Smirnov yang diperkenalkan di Bagian 4.1.2.2. Ketika seluruh dataset telah sesuai, statistik kecocokan Kolmogorov-Smirnov untuk distribusi gamma adalah 0,2639 dan untuk distribusi Pareto adalah 0,0478. Nilai yang lebih rendah untuk distribusi Pareto menunjukkan bahwa distribusi ini lebih cocok daripada gamma. Untuk melihat bagaimana validasi silang k-lipatan bekerja, kami membagi data secara acak menjadi \\(k=8\\) kelompok, atau lipatan, yang masing-masing memiliki sekitar \\(1377/8≈172\\) pengamatan. Kemudian, kami mencocokkan model gamma dan Pareto pada set data dengan tujuh lipatan pertama (sekitar $172⋅7 = 120$4 pengamatan), menentukan estimasi parameter, dan kemudian menggunakan model-model yang cocok dengan data yang ditahan untuk menentukan statistik Kolmogorov-Smirnov. library(VGAM) ## Loading required package: stats4 ## Loading required package: splines library(MASS) claim_lev &lt;- read.csv(&quot;data/CLAIMLEVEL.csv&quot;, header = TRUE) claim_data &lt;- subset(claim_lev, Year == 2010); # Randomly re-order the data - &quot;shuffle it&quot; n &lt;- nrow(claim_data) set.seed(12347) cvdata &lt;- claim_data[sample(n), ] # Number of folds k &lt;- 8 cvalvec &lt;- matrix(0,2,k) for (i in 1:k) { indices &lt;- (((i-1) * round((1/k)*nrow(cvdata))) + 1):((i*round((1/k) * nrow(cvdata)))) # Pareto fit.pareto &lt;- vglm(Claim ~ 1, paretoII, loc = 0, data = cvdata[-indices,]) ksResultPareto &lt;- ks.test(cvdata[indices,]$Claim, &quot;pparetoII&quot;, loc = 0, shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1])) cvalvec[1,i] &lt;- ksResultPareto$statistic # Gamma fit.gamma &lt;- glm(Claim ~ 1, data = cvdata[-indices,], family = Gamma(link = log)) gamma_theta &lt;- exp(coef(fit.gamma)) * gamma.dispersion(fit.gamma) alpha &lt;- 1 / gamma.dispersion(fit.gamma) ksResultGamma &lt;- ks.test(cvdata[indices,]$Claim, &quot;pgamma&quot;, shape = alpha, scale = gamma_theta) cvalvec[2,i] &lt;- ksResultGamma$statistic } KScv &lt;- rowSums(cvalvec)/k Hasilnya tampak pada Gambar 6.12 di mana sumbu horizontal adalah Fold=1. Proses ini diulangi untuk tujuh lipatan lainnya. Hasil yang dirangkum dalam Gambar 6.12 menunjukkan bahwa Pareto secara konsisten memberikan distribusi prediktif yang lebih dapat diandalkan daripada gamma. # Plot the statistics matplot(1:k,t(cvalvec),type=&quot;b&quot;, col=c(1,3), lty=1:2, ylim=c(0,0.4), pch = 0, xlab=&quot;Fold&quot;, ylab=&quot;KS Statistic&quot;) legend(&quot;left&quot;, c(&quot;Pareto&quot;, &quot;Gamma&quot;), col=c(1,3),lty=1:2, bty=&quot;n&quot;) “Figure 6.2:” Statistik Kolmogorov-Smirnov (KS) yang telah divalidasi silang untuk Data Klaim Dana Asuransi. Garis hitam solid untuk distribusi Pareto, garis putus-putus hijau untuk distribusi gamma. Statistik KS mengukur deviasi terbesar antara distribusi yang sesuai dengan distribusi empiris untuk masing-masing dari 8 kelompok, atau lipatan, data yang dipilih secara acak. 7.2 6.3.2 Leave-One-Out Cross-Validation Kasus khusus di mana \\(k=n\\) dikenal sebagai validasi silang tinggalkan-satu-keluar. Kasus ini secara historis sangat menonjol dan terkait erat dengan jackknifestatistik yang merupakan pendahulu dari teknik bootstrap. Meskipun kita menyajikannya sebagai kasus khusus validasi silang, akan sangat membantu jika kami memberikan definisi eksplisit. Pertimbangkan sebuah statistik umum \\(θˆ = t(x)\\) yang merupakan penaksir untuk sebuah parameter yang diminati \\(θ\\). Ide dari jackknife adalah menghitung n nilai \\(θˆ_{-i} = t(x-i)\\), di mana \\(x-i\\) adalah subsampel dari \\(x\\) dengan nilai \\(ke-i\\) dihilangkan. Rata-rata dari nilai-nilai ini dilambangkan sebagai \\[\\overline{\\widehat{\\theta}}_{(\\cdot)}=\\frac{1}{n}\\sum_{i=1}^n \\widehat{\\theta}_{-i} .\\] Nilai-nilai ini dapat digunakan untuk membuat estimasi bias dari statistik \\(\\hatθ\\) \\[\\begin{equation} Bias_{jack} = (n-1) \\left(\\overline{\\widehat{\\theta}}_{(\\cdot)} - \\widehat{\\theta}\\right) \\tag{6.3} \\end{equation}\\] serta estimasi standar deviasi \\[\\begin{equation} s_{jack} =\\sqrt{\\frac{n-1}{n}\\sum_{i=1}^n \\left(\\widehat{\\theta}_{-i} -\\overline{\\widehat{\\theta}}_{(\\cdot)}\\right)^2} ~. \\tag{6.4} \\end{equation}\\] Contoh 6.3.2. Koefisien Variasi. Sebagai ilustrasi, pertimbangkan sebuah sampel fiktif kecil \\(x = {x_1,...,x_n}\\) dengan realisasi sample_x &lt;- c(2.46,2.80,3.28,3.86,2.85,3.67,3.37,3.40, 5.22,2.55,2.79,4.50,3.37,2.88,1.44,2.56,2.00,2.07,2.19,1.77) Misalkan kita tertarik dengan \\(\\theta = CV = \\sqrt{\\mathrm{Var~}[X]}/\\mathrm{E~}[X]\\) Dengan dataset ini, estimator koefisien variasi menjadi 0,31196. Namun, seberapa handalkah estimasi tersebut? Untuk menjawab pertanyaan ini, kita dapat menghitung estimator pisau lipat dari bias dan deviasi standarnya. Kode berikut ini menunjukkan bahwa penaksir jackknife untuk bias adalah \\(Bias_{jack} = -0,00627\\) dan standar deviasi jackknife adalah \\(s_{jack} = 0,01293\\). CVar &lt;- function(x) sqrt(var(x))/mean(x) JackCVar &lt;- function(i) sqrt(var(sample_x[-i]))/mean(sample_x[-i]) JackTheta &lt;- Vectorize(JackCVar)(1:length(sample_x)) BiasJack &lt;- (length(sample_x)-1)*(mean(JackTheta) - CVar(sample_x)) sd(JackTheta) ## [1] 0.01293001 Contoh 6.3.3. Klaim Cidera Badan dan Rasio Eliminasi Kerugian. Pada Contoh 6.2.1, kita telah menunjukkan bagaimana menghitung estimasi bootstrap dari bias dan deviasi standar untuk rasio eliminasi kerugian dengan menggunakan data klaim cedera badan pada Contoh 4.1.11. Sekarang kita menindaklanjuti dengan memberikan jumlah yang sebanding dengan menggunakan statistik jackknife. Tabel 6.7 merangkum hasil estimasi jackknife. Tabel ini menunjukkan bahwa estimasi jackknife terhadap bias dan deviasi standar dari rasio eliminasi kerugian \\(E [min (X, d)]/E [X]\\) sebagian besar konsisten dengan metodologi bootstrap. Selain itu, kita dapat menggunakan standar deviasi untuk membangun interval kepercayaan berbasis normal, yang berpusat di sekitar penaksir yang dikoreksi bias. Sebagai contoh, pada \\(d = 14000\\), kita melihat pada Contoh 4.1.11 bahwa estimasi nonparametrik dari \\(LER\\) adalah 0.97678. Estimasi ini memiliki bias sebesar 0,00010, sehingga menghasilkan estimator terkoreksi-bias sebesar 0,97688. Interval kepercayaan 95% dihasilkan dengan membuat interval dua kali panjang 1,96 deviasi standar jackknife, yang berpusat pada estimator terkoreksi bias (1,96 adalah perkiraan kuantil ke-97,5 dari distribusi normal standar). library(boot) ## ## Attaching package: &#39;boot&#39; ## The following objects are masked from &#39;package:VGAM&#39;: ## ## logit, simplex # Example from Derrig et al BIData &lt;- read.csv(&quot;data/DerrigResampling.csv&quot;, header =T) BIData$Censored &lt;- 1*(BIData$AmountPaid &gt;= BIData$PolicyLimit) BIDataUncensored &lt;- subset(BIData, Censored == 0) LER.boot &lt;- function(ded, data, indices){ resample.data &lt;- data[indices,] sumClaims &lt;- sum(resample.data$AmountPaid) sumClaims_d &lt;- sum(pmin(resample.data$AmountPaid,ded)) LER &lt;- sumClaims_d/sumClaims return(LER) } x &lt;- BIDataUncensored$AmountPaid LER.jack&lt;- function(ded,i){ LER &lt;- sum(pmin(x[-i],ded))/sum(x[-i]) return(LER) } LER &lt;- function(ded) sum(pmin(x,ded))/sum(x) ##Derrig et al set.seed(2019) dVec2 &lt;- c(4000, 5000, 10500, 11500, 14000, 18500) OutJack &lt;- matrix(0,length(dVec2),8) for (j in 1:length(dVec2)) { OutJack[j,1] &lt;- dVec2[j] results &lt;- boot(data=BIDataUncensored, statistic=LER.boot, R=1000, ded=dVec2[j]) OutJack[j,2] &lt;- results$t0 biasboot &lt;- mean(results$t)-results$t0 -&gt; OutJack[j,3] sdboot &lt;- sd(results$t) -&gt; OutJack[j,4] temp &lt;- boot.ci(results) LER.jack.ded&lt;- function(i) LER.jack(ded=dVec2[j],i) JackTheta.ded &lt;- Vectorize(LER.jack.ded)(1:length(x)) OutJack[j,5] &lt;- BiasJack.ded &lt;- (length(x)-1)*(mean(JackTheta.ded) - LER(ded=dVec2[j])) OutJack[j,6] &lt;- sd(JackTheta.ded) OutJack[j,7:8] &lt;- mean(JackTheta.ded)+qt(c(0.025,0.975),length(x)-1)*OutJack[j,6] } Table 6.7. Estimasi Jackknife dari LER pada Deductible yang Dipilih d NP Estimate Bootstrap Bias Bootstrap SD Jackknife Bias Jackknife SD Lower Jackknife 95% CI Upper Jackknife 95% CI 4000 0.54113 0.00011 0.01237 0.00031 0.00061 0.53993 0.54233 5000 0.64960 0.00027 0.01412 0.00033 0.00068 0.64825 0.65094 10500 0.93563 0.00004 0.01017 0.00019 0.00053 0.93460 0.93667 11500 0.95281 -0.00003 0.00941 0.00016 0.00047 0.95189 0.95373 14000 0.97678 0.00016 0.00687 0.00010 0.00034 0.97612 0.97745 18500 0.99382 0.00014 0.00331 0.00003 0.00017 0.99350 0.99415 Diskusi. Salah satu dari banyak hal menarik tentang kasus khusus leave-one-out adalah kemampuan untuk mereplikasi estimasi dengan tepat. Artinya, ketika ukuran lipatan hanya satu, maka tidak ada ketidakpastian tambahan yang disebabkan oleh validasi silang. Ini berarti bahwa para analis dapat mereplikasi pekerjaan satu sama lain dengan tepat, sebuah pertimbangan yang penting. Statistik Jackknife dikembangkan untuk memahami ketepatan estimator, menghasilkan estimator bias dan deviasi standar pada persamaan (6.3) dan (6.4). Hal ini sesuai dengan tujuan yang telah kita kaitkan dengan teknik bootstrap, bukan metode validasi silang. Hal ini menunjukkan bagaimana teknik statistik dapat digunakan untuk mencapai tujuan yang berbeda. 7.3 6.3.3 Cross-Validation and Bootstrap Bootstrap berguna untuk memberikan estimator presisi, atau variabilitas, dari statistik. Hal ini juga berguna untuk validasi model. Pendekatan bootstrap untuk validasi model mirip dengan prosedur validasi leave-one-out dan k-fold: Buat sampel bootstrap dengan mengambil sampel ulang (dengan penggantian) \\(n\\) indeks dalam \\({1, ⋯, n}\\). Ini akan menjadi sampel pelatihan kita. Perkirakan model yang sedang dipertimbangkan berdasarkan sampel ini. Uji, atau sampel validasi, terdiri dari pengamatan yang tidak dipilih untuk pelatihan. Mengevaluasi model yang cocok (berdasarkan data pelatihan) dengan menggunakan data uji. Ulangi proses ini beberapa kali (katakanlah \\(B\\)). Ambil rata-rata dari hasil-hasilnya dan pilih model berdasarkan statistik evaluasi rata-rata. Contoh 6.3.4. Dana Properti Wisconsin. Kembali ke Contoh 6.3.1 di mana kita menyelidiki kecocokan distribusi gamma dan Pareto pada data dana properti. Kita kembali membandingkan kinerja prediksi menggunakan statistik Kolmogorov-Smirnov (KS), namun kali ini menggunakan prosedur bootstrap untuk membagi data antara sampel pelatihan dan pengujian. Berikut ini adalah kode ilustrasinya. library(goftest) n &lt;- nrow(claim_data) set.seed(12347) indices &lt;- 1:n # Number of Bootstrap Samples B &lt;- 100 cvalvec &lt;- matrix(0,2,B) for (i in 1:B) { bootindex &lt;- unique(sample(indices, size=n, replace= TRUE)) traindata &lt;- claim_data[bootindex,] testdata &lt;- claim_data[-bootindex,] # Pareto fit.pareto &lt;- vglm(Claim ~ 1, paretoII, loc = 0, data = traindata) ksResultPareto &lt;- ks.test(testdata$Claim, &quot;pparetoII&quot;, loc = 0, shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1])) cvalvec[1,i] &lt;- ksResultPareto$statistic # Gamma fit.gamma &lt;- glm(Claim ~ 1, data = traindata, family = Gamma(link = log)) gamma_theta &lt;- exp(coef(fit.gamma)) * gamma.dispersion(fit.gamma) alpha &lt;- 1 / gamma.dispersion(fit.gamma) ksResultGamma &lt;- ks.test(testdata$Claim, &quot;pgamma&quot;, shape = alpha, scale = gamma_theta) cvalvec[2,i] &lt;- ksResultGamma$statistic } KSBoot &lt;- rowSums(cvalvec)/B Kami melakukan pengambilan sampel dengan menggunakan B= 100 ulangan. Statistik KS rata-rata untuk distribusi Pareto adalah 0,058 dibandingkan dengan rata-rata untuk distribusi gamma, 0,262. Hal ini konsisten dengan hasil sebelumnya dan memberikan bukti lain bahwa Pareto adalah model yang lebih baik untuk data ini dibandingkan dengan gamma. "],["importance-sampling.html", "Bab 8 6.4 Importance Sampling", " Bab 8 6.4 Importance Sampling Bagian 6.1 memperkenalkan teknik Monte Carlo dengan menggunakan teknik inversi: untuk membangkitkan sebuah variabel acak \\(X\\) dengan distribusi \\(F\\), terapkan \\(F^{-1}\\) pada pemanggilan sebuah generator acak (seragam pada interval satuan). Bagaimana jika kita ingin menggambar sesuai dengan \\(X\\), dengan syarat \\(X∈[a,b]\\)? Seseorang dapat menggunakan mekanisme terima-tolak: menarik \\(x\\) dari distribusi \\(F\\) jika \\(x\\in[a,b]\\): simpan (“terima”) jika \\(x\\notin[a,b]\\): gambar yang lain (“tolak”) Amati bahwa dari n nilai yang awalnya dihasilkan, kita simpan di sini hanya \\([F(b)-F(a)] ⋅ n\\) hasil imbang, rata-rata. Contoh 6.4.1. Penarikan dari Distribusi Normal. Misalkan kita menggambar dari distribusi normal dengan rata-rata 2,5 dan varians 1, \\(N(2,5,1)\\), tetapi hanya tertarik pada gambar yang lebih besar dari \\(a≥2\\) dan kurang dari \\(b≤4\\). Artinya, kita hanya dapat menggunakan \\(F(4)-F(2)=Φ(4-2.5)-Φ(2-2.5) = 0.9332 - 0.3085 = 0.6247\\) proporsi undian. Gambar 6.13 menunjukkan bahwa beberapa hasil undian berada di dalam interval \\((2,4)\\) dan beberapa di luarnya. mu = 2.5 sigma = 1 a = 2 b = 4 Fa = pnorm(a,mu,sigma) Fb = pnorm(b,mu,sigma) pic_ani = function(){ u=seq(0,5,by=.01) plot(u,pnorm(u,mu,sigma),col=&quot;white&quot;,ylab=&quot;&quot;,xlab=&quot;&quot;) rect(-1,-1,6,2,col=rgb(1,0,0,.2),border=NA) rect(a,Fa,b,Fb,col=&quot;white&quot;,border=NA) lines(u,pnorm(u,mu,sigma),lwd=2) abline(v=c(a,b),lty=2,col=&quot;red&quot;) ru &lt;- runif(1) clr &lt;- &quot;red&quot; if((qnorm(ru,mu,sigma)&gt;=a)&amp;(qnorm(ru,mu,sigma)&lt;=b)) clr &lt;- &quot;blue&quot; segments(-1,ru,qnorm(ru,mu,sigma),ru,col=clr,lwd=2) arrows(qnorm(ru,mu,sigma),ru,qnorm(ru,mu,sigma),0,col=clr,lwd=2,length = .1) } for (i in 1:numAnimation) {pic_ani()} Sebagai gantinya, seseorang dapat menggambar menurut distribusi bersyarat \\(F^⋆\\) yang didefinisikan sebagai \\[F^{\\star}(x) = \\Pr(X \\le x | a &lt; X \\le b) =\\frac{F(x)-F(a)}{F(b)-F(a)}, \\ \\ \\ \\text{for } a &lt; x \\le b .\\] Dengan menggunakan metode inverse transform pada Bagian 6.1.2, kita mendapatkan hasil imbang \\[X^\\star=F^{\\star-1}\\left( U \\right) = F^{-1}\\left(F(a)+U\\cdot[F(b)-F(a)]\\right)\\] memiliki distribusi \\(F⋆^\\). Dinyatakan dengan cara lain, definisikan \\[\\tilde{U} = (1-U)\\cdot F(a)+U\\cdot F(b)\\] dan kemudian gunakan \\(F^{-1}(\\tilde{U})\\). Dengan pendekatan ini, setiap undian dihitung. Hal ini dapat dikaitkan dengan mekanisme pengambilan sampel kepentingan: kita menarik lebih sering di wilayah yang kita harapkan memiliki kuantitas yang memiliki kepentingan. Transformasi ini dapat dianggap sebagai “perubahan ukuran.” pic_ani = function(){ u=seq(0,5,by=.01) plot(u,pnorm(u,mu,sigma),col=&quot;white&quot;,ylab=&quot;&quot;,xlab=&quot;&quot;) rect(-1,-1,6,2,col=rgb(1,0,0,.2),border=NA) rect(a,Fa,b,Fb,col=&quot;white&quot;,border=NA) lines(u,pnorm(u,mu,sigma),lwd=2) abline(h=pnorm(c(a,b),mu,sigma),lty=2,col=&quot;red&quot;) ru &lt;- runif(1) rutilde &lt;- (1-ru)*Fa+ru*Fb segments(-1,rutilde,qnorm(rutilde,mu,sigma),rutilde,col=&quot;blue&quot;,lwd=2) arrows(qnorm(rutilde,mu,sigma),rutilde,qnorm(rutilde,mu,sigma),0,col=&quot;blue&quot;,lwd=2,length = .1) } for (i in 1:numAnimation) {pic_ani()} Pada Contoh 6.4.1., kebalikan dari distribusi normal sudah tersedia (dalam R, fungsinya adalah qnorm). Namun, untuk aplikasi lain, hal ini tidak terjadi. Kemudian, kita cukup menggunakan metode numerik untuk menentukan \\(X^⋆\\) sebagai solusi dari persamaan \\(F(X^\\star) =\\tilde{U}\\) di mana \\(\\tilde{U}=(1-U)\\cdot F(a)+U\\cdot F(b)\\)). Lihat kode ilustrasi berikut ini. pic_ani = function(){ u=seq(0,5,by=.01) plot(u,pnorm(u,mu,sigma),col=&quot;white&quot;,ylab=&quot;&quot;,xlab=&quot;&quot;) rect(-1,-1,6,2,col=rgb(1,0,0,.2),border=NA) rect(2,-1,4,2,col=&quot;white&quot;,border=NA) lines(u,pnorm(u,mu,sigma),lty=2) pnormstar &lt;- Vectorize(function(x){ y=(pnorm(x,mu,sigma)-Fa)/(Fb-Fa) if(x&lt;=a) y &lt;- 0 if(x&gt;=b) y &lt;- 1 return(y) }) qnormstar &lt;- function(u) as.numeric(uniroot((function (x) pnormstar(x) - u), lower = 2, upper = 4)[1]) lines(u,pnormstar(u),lwd=2) abline(v=c(2,4),lty=2,col=&quot;red&quot;) ru &lt;- runif(1) segments(-1,ru,qnormstar(ru),ru,col=&quot;blue&quot;,lwd=2) arrows(qnormstar(ru),ru,qnormstar(ru),0,col=&quot;blue&quot;,lwd=2,length = .1) } for (i in 1:numAnimation) {pic_ani()} "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
>>>>>>> 479519a4549eebce8afc2cd7c6cc44df854e36e3
