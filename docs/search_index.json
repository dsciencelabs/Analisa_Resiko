[["kata-pengantar.html", "Analisis Resiko Kata Pengantar Deskripsi Buku Ucapan Terima Kasih Kontributor Kritik &amp; Saran", " Analisis Resiko Bakti Siregar, M.Sc 2023-06-11 Kata Pengantar Deskripsi Buku Analisa Resiko adalah buku yang interaktif, online, dan tersedia secara gratis. Versi online berisi banyak objek interaktif (kuis, demonstrasi komputer, grafik interaktif, video, dan sejenisnya) yang dapat dipergunakan untuk menunjang pembelajaran lebih baik. Sebagian besar isi dari buku ini tersedia untuk dibaca offline dalam format pdf dan EPUB. Direncanakan akan tersedia dalam berbagai bahasa. Petunjuk Penggunaan Buku ini dapat dipergunakan dalam pembelajaran kurikulum aktuaria di seluruh dunia. Adapun cakupan pembelajarannya adalah analisa data kerugian dari berbagai organisasi aktuaria ternama didunia. Sehingga, buku ini cocok digunakan ditingkat universitas maupun pembelajar mandiri yang ingin lulus ujian aktuaria profesional. Selain itu, buku juga akan sangat berguna dalam pengembangan profesional berkelanjutan bagi para aktuaris maupun profesional lainnya di bidang asuransi dan industri terkait manajemen risiko keuangan. Manfaat Salah satu manfaat penting dari buku online ini adalah pemerataan akses pengetahuan, sehingga memungkinkan masyarakat yang lebih luas untuk belajar tentang profesi aktuaria. Selain itu, setiap orang memiliki kapasitas untuk melibatkan banyak pihak melalui pembelajaran aktif yang memperdalam proses pembelajaran, menghasilkan analis terbaik dalam melakukan pekerjaan aktuaria yang solid. Sekarang, pertanyaan besarnya adalah “Mengapa buku ini baik untuk mahasiswa dan dosen serta orang lain yang terlibat dalam proses pembelajaran?” Biaya adalah salah satu faktor yang sering disebut sebagai kendala utama bagi mahasiswa dan dosen dalam pemilihan buku teks. Selain itu, Mahasiswa sekarang ini lebih menyukai buku yang dapat dibawa secara secara elektronik (online). Mengapa Analisa Resiko? Tujuannya adalah agar buku ini pada akhirnya akan dapat dikembangkan secara serius kurikulum aktuaria. Mengingat perubahan era digital seperti sekarang ini akhirnya mendorong para aktuaris dalam melakukan analisa bergantung pada data yang dimiliki. Ide di balik nama Analisa Resiko adalah untuk mengintegrasikan model data kerugian klasik dari probabilitas yang diterapkan dengan alat analitik modern. Secara khusus, penulis menyadari bahwa big data (termasuk media sosial dan asuransi berbasis penggunaan) akan terus berkembang dan komputasi berkecepatan tinggi sudah tersedia. Ucapan Terima Kasih Kami juga ingin mengucapkan terima kasih yang sebesar-sebesar pada semua pihak yang terlibat dalam pengembangan buku ini, yakni; mahasiswa-i, dosen, dan Universitas Matana atas dukungan dalam upaya bersama kami untuk menyediakan konten pendidikan dalam bidang aktuaria. Kontributor Sebagian besar dari isi buku ini diadopsi dari Loss Data Analytics. Berikut ini adalah nama-nama dan biografi singkat para penulis: Bakti Siregar, M.Sc adalah Kepala Program Studi dan Dosen di Jurusan Statistika Universitas Matana. Beliau juga seorang dosen yang juga bekerja sebagai ilmuwan data lepas yang memiliki antusiasme untuk analitik data besar, pembelajaran mesin, Pemodelan, dan pemecahan masalah. Orang menganggap saya programmer Matematika karena saya memiliki kemampuan yang kuat dalam program Statistik seperti R Studio, dan Python, dan juga akrab dengan alat basis data seperti MySQL dan sistem data besar baik Spark maupun Hadoop. Selain itu, saya dapat mengoperasikan salah satu perangkat lunak analitik bisnis yang paling kuat seperti Tableau. Yosia adalah salah satu mahasiwa terbaik di jurusan Statistika Universitas Matana. Dia juga memiliki minat dalam pembelajaran sains data dan akuturia khususnya melakukan komputasi dengan menggunakan R dan Python. Yosia bercita-cita suatu saat nanti akan menjadi seseorang yang ahli dibidang aktuaria maupun sain data. Yosia adalah mahasiswi jurusan Statistik di Universitas Matana. Dia memiliki minat teoretis yang luas serta minat dalam komputasi, ia juga sudah pernah terlibat dalam menerbitkan di jurnal Pengabdian Kepada Masyarakat (PKM). Dia juga aktif dalam berbagai aktifitas organisasi kampus. Clara Della adalah mahasiswi jurusan Statistik di Universitas Matana. Dia memiliki minat teoretis yang luas serta minat dalam komputasi, ia juga sudah pernah terlibat dalam menerbitkan di jurnal Pengabdian Kepada Masyarakat (PKM). Dia juga aktif dalam berbagai aktifitas organisasi kampus. adalah mahasiswi jurusan Statistik di Universitas Matana. Dia memiliki minat teoretis yang luas serta minat dalam komputasi, ia juga sudah pernah terlibat dalam menerbitkan di jurnal Pengabdian Kepada Masyarakat (PKM). Dia juga aktif dalam berbagai aktifitas organisasi kampus. Karen adalah mahasiswi jurusan Statistik di Universitas Matana yang memiliki keahlian penelitiannya dengan menggunakan teori pemodelan, manajemen risiko, dan optimasi. adalah mahasiswi jurusan Statistik di Universitas Matana. Dia memiliki minat teoretis yang luas serta minat dalam komputasi, ia juga sudah pernah terlibat dalam menerbitkan di jurnal Pengabdian Kepada Masyarakat (PKM). Dia juga aktif dalam berbagai aktifitas organisasi kampus. Brigita adalah dosen senior di Macquarie University di Australia, di mana ia menjabat sebagai direktur program sarjana aktuaria sejak 2018. Ia memperoleh gelar Ph.D. pada tahun 2015 dari Nanyang Technological University di Singapura. Dia adalah seorang aktuaris yang berkualifikasi penuh, memegang kredensial dari US Society of Actuaries dan Australian Actuaries Institute. Minat penelitian utamanya adalah pemodelan kematian, manajemen risiko umur panjang, dan sistem bonus-malus. Naufal adalah seorang profesor di Universitas Matana. Dia memiliki gelar di bidang Matematika dan Ph.D. dalam Sains: Matematika, diperoleh di University of Antwerp. Selama Ph.D., ia berhasil mengambil Magister Asuransi dan Magister Teknik Keuangan dan Aktuaria, keduanya di KU Leuven. Penelitiannya berfokus pada adaptasi dan penerapan metode statistik yang kuat untuk data asuransi dan keuangan. adalah mahasiswi jurusan Statistik di Universitas Matana. Dia memiliki minat teoretis yang luas serta minat dalam komputasi, ia juga sudah pernah terlibat dalam menerbitkan di jurnal Pengabdian Kepada Masyarakat (PKM). Dia juga aktif dalam berbagai aktifitas organisasi kampus. Garry adalah Associate Professor di Departemen Manajemen Risiko, Asuransi, dan Kesehatan di Fox School of Business, Temple University? Dia adalah Associate dari Society of Actuaries. Dia mengajar mata kuliah Ilmu Aktuaria dan Manajemen Risiko di tingkat sarjana dan pascasarjana. Minat penelitiannya meliputi tata kelola perusahaan asuransi, manajemen modal, dan analisis sentimen. Dia menerima gelar Ph.D. dari The Wharton School of the University of Pennsylvania. Kritik &amp; Saran Buku teks interaktif yang tersedia secara gratis mewakili usaha baru dalam pendidikan aktuaria dan kami membutuhkan masukan Anda. Meskipun banyak upaya telah dilakukan untuk pengembangan, kami mengharapkan cegukan. Harap beri tahu instruktur Anda tentang peluang untuk peningkatan, hubungi kami melalui situs proyek kami, atau hubungi kontributor bab secara langsung dengan saran peningkatan. Berikut ini dilampirkan beberapa peninjau atau pembaca yang telah memberikan saran dan pendapat mengenai pengembangan buku ini, adalah: mahasiswa 1 mahasiswa 2 mahasiswa 3 mahasiswa 4 mahasiswa 5 mahasiswa 6 "],["pengantar-analitika-data-kerugian.html", "Bab 1 Pengantar Analitika Data Kerugian 1.1 Relevansi Analitika dalam Aktivitas Asuransi 1.2 Operasi Perusahaan Asuransi", " Bab 1 Pengantar Analitika Data Kerugian Preview Bab. Buku ini memperkenalkan pada metode analisis data asuransi. Bagian 1.1 dimulai dengan pembahasan mengapa penggunaan data itu penting dalam industri asuransi. Bagian 1.2 memberikan gambaran umum tentang tujuan analisis data asuransi yang diperkuat dalam studi kasus Bagian 1.3. Secara alami, ada kesenjangan besar antara tujuan umum yang dirangkum dalam gambaran dan aplikasi studi kasus. Kesenjangan ini dibahas melalui metode dan teknik analisis data yang tercakup dalam penjelasan berikutnya. 1.1 Relevansi Analitika dalam Aktivitas Asuransi yang akan dipelajari dalam bab ini yaitu: Meringkas pentingnya asuransi bagi konsumen dan ekonomi Menggambarkan analitika Mengidentifikasi peristiwa penghasil data yang terkait dengan jangka waktu kontrak asuransi yang umum 1.1.1 Sifat dan Relevansi Asuransi Buku ini memperkenalkan proses penggunaan data untuk mengambil keputusan dalam konteks asuransi. Buku ini tidak berasumsi bahwa pembaca sudah familiar dengan asuransi, tetapi memperkenalkan konsep-konsep asuransi sesuai kebutuhan. Jika baru mengenal asuransi, mungkin yang paling mudah adalah memikirkan sebuah polis asuransi yang mencakup isi apartemen atau rumah yang dapat di sewa (dikenal sebagai asuransi penyewa) atau isi dan properti dari bangunan yang dimiliki pribadi atau seorang teman (dikenal sebagai asuransi pemilik rumah). Contoh umum lainnya adalah asuransi mobil. Dalam kejadian kecelakaan, polis ini dapat mencakup kerusakan pada kendaraan pribadi, kerusakan pada kendaraan lain dalam kecelakaan tersebut, serta biaya medis bagi mereka yang terluka dalam kecelakaan. Salah satu cara untuk memahami sifat asuransi adalah dengan melihat siapa yang membelinya. Asuransi penyewa, pemilik rumah, dan asuransi mobil adalah contoh asuransi personal, karena polis-polis ini diterbitkan untuk individu. Bisnis juga membeli asuransi, seperti perlindungan atas properti mereka, dan ini dikenal sebagai asuransi komersial. Penjualnya, perusahaan asuransi, juga dikenal sebagai penanggung. Bahkan perusahaan asuransi pun membutuhkan asuransi. Hal ini dikenal sebagai reasuransi. Cara lain untuk memahami sifat asuransi adalah dengan jenis risiko yang dicakup. Di Amerika Serikat, kebijakan seperti asuransi penyewa dan pemilik rumah dikenal sebagai asuransi properti, sedangkan kebijakan seperti asuransi mobil yang mencakup kerusakan medis pada orang disebut asuransi kecelakaan. Di negara lain, keduanya dikenal sebagai asuransi non-hidup atau umum, untuk membedakannya dari asuransi jiwa. Baik asuransi jiwa maupun asuransi non-hidup adalah komponen penting dalam ekonomi dunia. Institut Informasi Asuransi (2016) memperkirakan premi asuransi langsung di dunia untuk tahun 2014 sebesar 2.654.549 juta dolar AS untuk asuransi jiwa dan 2.123.699 juta dolar AS untuk asuransi non-hidup; angka-angka ini dalam jutaan dolar AS. Total tersebut mewakili 6,2% dari produk domestik bruto (PDB) dunia. Dengan kata lain, asuransi jiwa menyumbang 55,5% dari premi asuransi dan 3,4% dari PDB dunia, sedangkan asuransi non-hidup menyumbang 44,5% dari premi asuransi dan 2,8% dari PDB dunia. Baik asuransi jiwa maupun asuransi non-hidup merupakan kegiatan ekonomi penting. Asuransi mungkin tidak semenarik industri olahraga, tetapi asuransi memengaruhi kehidupan keuangan banyak orang. Dalam segala ukuran, asuransi merupakan kegiatan ekonomi yang besar. Seperti yang telah disebutkan sebelumnya, secara global, premi asuransi mencakup sekitar 6,2% dari PDB dunia pada tahun 2014 (Institut Informasi Asuransi 2016). Sebagai contoh, premi asuransi menyumbang 18,9% dari PDB di Taiwan (yang tertinggi dalam studi ini) dan mewakili 7,3% dari PDB di Amerika Serikat. Pada tingkat pribadi, hampir setiap orang yang memiliki rumah memiliki asuransi untuk melindungi diri mereka dalam kejadian kebakaran, badai es, atau peristiwa bencana lainnya. Hampir setiap negara mengharuskan asuransi bagi mereka yang mengemudikan mobil. Secara keseluruhan, meskipun tidak terlalu menghibur, asuransi memainkan peran penting dalam perekonomian negara-negara dan kehidupan individu. 1.1.2 Apa itu Analitika? Asuransi merupakan industri yang mengandalkan data. Seperti perusahaan-perusahaan besar dan organisasi lainnya, perusahaan asuransi menggunakan data ketika mencoba untuk menentukan berapa banyak yang harus dibayarkan kepada karyawan, berapa banyak karyawan yang harus dipertahankan, bagaimana cara memasarkan layanan dan produk mereka, bagaimana meramalkan tren keuangan, dan sebagainya. Hal ini mewakili bidang-bidang aktivitas umum yang tidak spesifik hanya untuk industri asuransi. Meskipun setiap industri memiliki nuansa dan kebutuhan data yang berbeda, pengumpulan, analisis, dan penggunaan data merupakan kegiatan yang dibagikan oleh semua, mulai dari raksasa internet hingga bisnis kecil, oleh organisasi publik dan pemerintah, dan tidak spesifik hanya untuk industri asuransi. Anda akan menemukan bahwa metode dan alat pengumpulan dan analisis data yang diperkenalkan dalam teks ini relevan untuk semua industri. Dalam industri yang mengandalkan data, analitika merupakan kunci untuk mendapatkan dan mengekstraksi informasi dari data. Namun, apa itu analitika? Pengambilan keputusan bisnis yang didasarkan pada data telah dijelaskan sebagai analitika bisnis, inteligensi bisnis, dan ilmu data. Istilah-istilah ini, antara lain, kadang-kadang digunakan secara bergantian dan kadang-kadang mengacu pada aplikasi yang berbeda. Inteligensi bisnis mungkin fokus pada proses pengumpulan data, sering kali melalui basis data dan gudang data, sedangkan analitika bisnis menggunakan alat dan metode untuk analisis statistik data. Berbeda dengan dua istilah tersebut yang menekankan aplikasi bisnis, istilah ilmu data dapat mencakup aplikasi data yang lebih luas dalam berbagai domain ilmiah. Untuk tujuan kami, kami menggunakan istilah analitika untuk merujuk pada proses penggunaan data dalam pengambilan keputusan. Proses ini melibatkan pengumpulan data, pemahaman konsep dan model ketidakpastian, membuat inferensi umum, dan mengkomunikasikan hasil. Ketika memperkenalkan metode data dalam teks ini, kami fokus pada kerugian yang timbul dari, atau terkait dengan, kewajiban dalam kontrak asuransi. Hal ini bisa berupa jumlah kerusakan pada apartemen seseorang dalam perjanjian asuransi penyewa, jumlah yang dibutuhkan untuk mengganti rugi seseorang yang Anda cedera dalam kecelakaan berkendara, dan sejenisnya. Kami menyebut jenis kewajiban ini sebagai klaim asuransi. Dengan fokus ini, kami dapat memperkenalkan dan langsung menggunakan alat dan teknik statistik yang umumnya berlaku. 1.1.3 Proses Asuransi Cara lain untuk memahami sifat asuransi adalah melalui durasi kontrak asuransi, yang dikenal sebagai masa berlaku. Teks ini akan berfokus pada kontrak asuransi jangka pendek. Dalam konteks jangka pendek, kontrak asuransi umumnya memberikan perlindungan selama satu tahun atau enam bulan. Sebagian besar kontrak komersial dan personal berlaku selama satu tahun, sehingga itu adalah durasi default kami. Namun, terdapat pengecualian penting seperti kebijakan asuransi mobil di Amerika Serikat yang sering kali berlaku selama enam bulan. Sebaliknya, biasanya kita menganggap asuransi jiwa sebagai kontrak jangka panjang di mana durasi default adalah beberapa tahun. Sebagai contoh, jika seseorang berusia 25 tahun membeli polis asuransi jiwa yang memberikan pembayaran saat meninggalnya tertanggung dan orang tersebut tidak meninggal sampai usia 100 tahun, maka kontrak tersebut berlaku selama 75 tahun. Terdapat perbedaan penting lainnya antara produk asuransi jiwa dan non-jiwa. Dalam asuransi jiwa, jumlah manfaat sering ditetapkan dalam ketentuan kontrak. Sebaliknya, sebagian besar kontrak non-jiwa memberikan kompensasi atas kerugian tertanggung yang tidak diketahui sebelum terjadinya kecelakaan. (Biasanya terdapat batasan jumlah kompensasi yang ditetapkan.) Dalam kontrak asuransi jiwa yang berlaku selama bertahun-tahun, nilai waktu uang memainkan peran penting. Dalam kontrak non-jiwa, jumlah kompensasi yang acak menjadi prioritas. Baik dalam asuransi jiwa maupun asuransi non-jiwa, frekuensi klaim sangat penting. Untuk banyak kontrak asuransi jiwa, peristiwa yang diasuransikan (seperti kematian) hanya terjadi sekali. Sebaliknya, dalam asuransi non-jiwa seperti asuransi mobil, umum bagi individu (terutama pengemudi pria muda) untuk mengalami lebih dari satu kecelakaan dalam setahun. Oleh karena itu, model-model kita perlu mencerminkan pengamatan ini; kami memperkenalkan berbagai model frekuensi yang juga mungkin Anda temui saat mempelajari asuransi jiwa. Untuk asuransi jangka pendek, kerangka model probabilistiknya sederhana. Hanya mempertimbangkan model satu periode (panjang periode, misalnya satu tahun, akan ditentukan dalam situasi tersebut). Pada awal periode, tertanggung membayar premi yang diketahui kepada perusahaan asuransi sesuai kesepakatan antara kedua belah pihak dalam kontrak. Pada akhir periode, perusahaan asuransi mengganti rugi tertanggung atas kerugian (mungkin multivariat) yang acak. Kerangka kerja ini akan dikembangkan seiring berjalannya waktu, tetapi yang pertama fokus pada mengintegrasikan kerangka kerja ini dengan kekhawatiran tentang bagaimana data dapat muncul. Dari sudut pandang perusahaan asuransi, kontrak mungkin hanya berlaku selama satu tahun tetapi cenderung diperpanjang. Selain itu, pembayaran yang timbul dari klaim selama setahun dapat meluas jauh melebihi satu tahun. Salah satu cara untuk menggambarkan data yang muncul dari operasional perusahaan asuransi adalah dengan menggunakan pendekatan granular dengan garis waktu. Pendekatan proses memberikan gambaran keseluruhan tentang peristiwa yang terjadi selama masa berlaku kontrak asuransi, dan sifatnya - acak atau direncanakan, peristiwa kerugian (klaim) dan peristiwa perubahan kontrak, dan sebagainya. Dalam pandangan mikro ini, kita dapat memikirkan apa yang terjadi pada suatu kontrak pada berbagai tahap keberadaannya. Gambar 1.1 menggambarkan garis waktu dari suatu kontrak asuransi yang khas. Sepanjang masa berlakunya kontrak, perusahaan secara rutin memproses peristiwa seperti pengumpulan premi dan penilaian, yang dijelaskan dalam Bagian 1.2; peristiwa-peristiwa ini ditandai dengan tanda x pada garis waktu. Peristiwa-peristiwa yang tidak teratur dan tak terduga juga terjadi. Sebagai contoh, \\(t_2\\) dan \\(t_4\\) menandai peristiwa klaim asuransi (beberapa kontrak, seperti asuransi jiwa, mungkin hanya memiliki satu klaim). Waktu \\(t_3\\) dan \\(t_5\\) menandai peristiwa ketika pemegang polis ingin mengubah fitur-fitur tertentu dalam kontrak, seperti pilihan klaim dan jumlah perlindungan. Dari perspektif perusahaan, kita bahkan dapat mempertimbangkan inisiasi kontrak (kedatangan, waktu \\(t_1\\)) dan terminasi kontrak (kepergian, waktu \\(t_6\\)) sebagai peristiwa yang tidak pasti. (Sebagai alternatif, untuk beberapa tujuan, Anda dapat mengkondisikan peristiwa-peristiwa ini dan memperlakukannya sebagai pasti.) ( Gambar 1) 1.2 Operasi Perusahaan Asuransi pada bab ini yang akan dipelajari adalah: Menggambarkan lima area operasional utama perusahaan asuransi. Mengidentifikasi peran data dan peluang analitik dalam setiap area operasional. Berdasarkan data asuransi, tujuan akhirnya adalah menggunakan data untuk mengambil keputusan. Kita akan mempelajari lebih lanjut tentang metode analisis dan ekstrapolasi data di bab-bab berikutnya. Untuk memulainya, mari kita pikirkan mengapa kita ingin melakukan analisis. Kita mengambil sudut pandang perusahaan asuransi (bukan orang yang diasuransikan) dan memperkenalkan cara untuk mendapatkan uang, membayarnya, mengelola biaya, dan memastikan bahwa kita memiliki cukup uang untuk memenuhi kewajiban. Penekanannya adalah pada operasi khusus asuransi daripada pada kegiatan bisnis umum seperti periklanan, pemasaran, dan manajemen sumber daya manusia. Secara khusus, dalam banyak perusahaan asuransi, umumnya praktik untuk menggabungkan proses asuransi yang terperinci ke dalam unit operasional yang lebih besar; banyak perusahaan menggunakan area fungsional ini untuk memisahkan aktivitas karyawan dan tanggung jawab area tertentu. Aktuaris, analis keuangan lainnya, dan regulator asuransi bekerja dalam unit-unit ini dan menggunakan data untuk kegiatan-kegiatan berikut ini: 1. Memulai Asuransi. Pada tahap ini, perusahaan membuat keputusan untuk menerima atau menolak risiko (tahap underwriting) dan menentukan premi yang sesuai (atau tarif). Analitik asuransi memiliki akar aktuaria dalam pembuatan tarif, di mana para analis berusaha untuk menentukan harga yang tepat untuk risiko yang tepat. 2. Memperbaharui Asuransi. Banyak kontrak, terutama dalam asuransi umum, memiliki durasi yang relatif pendek seperti 6 bulan atau 1 tahun. Meskipun ada harapan implisit bahwa kontrak-kontrak tersebut akan diperbaharui, perusahaan asuransi memiliki kesempatan untuk menolak jaminan dan menyesuaikan premi. Analitik juga digunakan pada tahap perpanjangan polis ini di mana tujuannya adalah untuk mempertahankan pelanggan yang menguntungkan. 3. Manajemen Klaim. Analitik telah lama digunakan dalam (1) mendeteksi dan mencegah penipuan klaim, (2) mengelola biaya klaim, termasuk mengidentifikasi dukungan yang tepat untuk biaya penanganan klaim, serta (3) memahami lapisan kelebihan untuk reasuransi dan retensi. 4. Reserving Kerugian. Alat analitik digunakan untuk memberikan perkiraan yang tepat kepada manajemen mengenai kewajiban di masa depan dan untuk mengkuantifikasi ketidakpastian perkiraan tersebut. 5. Kesolvenan dan Alokasi Modal. Menentukan jumlah modal yang diperlukan dan cara mengalokasikan modal di antara investasi-alternatif juga merupakan kegiatan analitik penting. Perusahaan harus memahami berapa banyak modal yang dibutuhkan agar mereka memiliki aliran kas yang cukup tersedia untuk memenuhi kewajiban mereka pada saat yang diharapkan (kesolvenan). Ini adalah pertanyaan penting yang tidak hanya menyangkut manajer perusahaan tetapi juga pelanggan, pemegang saham perusahaan, otoritas pengawas, serta masyarakat secara umum. Terkait dengan isu berapa banyak modal adalah pertanyaan tentang bagaimana mengalokasikan modal ke proyek keuangan yang berbeda, biasanya untuk memaksimalkan pengembalian investor. Meskipun pertanyaan ini dapat muncul pada beberapa tingkat, perusahaan asuransi umumnya tertarik dengan cara mengalokasikan modal ke berbagai lini bisnis dalam perusahaan dan ke anak perusahaan dari perusahaan induk. Meskipun data merupakan komponen penting dari kesolvenan dan alokasi modal, komponen lain termasuk kerangka ekonomi lokal dan global, lingkungan investasi keuangan, dan persyaratan yang sangat spesifik sesuai dengan lingkungan regulasi saat ini, juga penting. Karena latar belakang yang diperlukan untuk mengatasi komponen-komponen ini, kami tidak membahas isu-isu kesolvenan, alokasi modal, dan regulasi dalam teks ini. Namun demikian, untuk semua fungsi operasional, kami menekankan bahwa analitik dalam industri asuransi bukanlah latihan yang dapat dilakukan oleh sekelompok kecil analis sendiri. Hal ini membutuhkan perusahaan asuransi untuk melakukan investasi yang signifikan dalam teknologi informasi, pemasaran, underwriting, dan aktuaria. Karena area-area ini merupakan tujuan akhir utama dari analisis data, penjelasan tambahan tentang masing-masing unit operasional diberikan dalam subseksi berikutnya. 1.2.1 Memulai Asuransi Menentukan harga produk asuransi bisa menjadi masalah yang membingungkan. Hal ini berbeda dengan industri lain seperti manufaktur di mana biaya suatu produk (relatif) diketahui dan menjadi acuan untuk menilai harga permintaan pasar. Demikian pula, dalam bidang layanan keuangan lainnya, harga pasar tersedia dan menjadi dasar untuk struktur penetapan harga yang sesuai dengan pasar produk. Namun, untuk banyak jenis asuransi, biaya suatu produk tidak pasti dan harga pasar tidak tersedia. Harapan atas biaya acak adalah tempat yang wajar untuk memulai penetapan harga. (Jika Anda telah mempelajari keuangan, maka Anda akan mengingat bahwa harapan adalah harga optimal untuk perusahaan asuransi yang netral terhadap risiko.) Dalam penetapan harga asuransi, sudah menjadi tradisi untuk memulai dengan biaya yang diharapkan. Perusahaan asuransi kemudian menambahkan margin untuk memperhitungkan tingkat risiko produk, biaya yang dikeluarkan dalam pelayanan produk, dan alokasi keuntungan/surplus perusahaan. Penggunaan biaya yang diharapkan sebagai dasar penetapan harga umum terjadi dalam beberapa jenis bisnis asuransi. Ini termasuk asuransi mobil dan asuransi pemilik rumah. Dalam jenis asuransi ini, analitik telah membantu meningkatkan ketepatan perhitungan biaya yang diharapkan dari produk. Ketersediaan internet yang semakin luas bagi konsumen juga mendorong transparansi dalam penetapan harga; di pasar saat ini, konsumen memiliki akses mudah ke kutipan kompetitif dari sejumlah perusahaan asuransi. Perusahaan asuransi berusaha meningkatkan pangsa pasarnya dengan menyempurnakan sistem klasifikasi risiko mereka, sehingga mencapai perkiraan yang lebih baik mengenai harga produk dan memungkinkan strategi underwriting yang selektif (“cream-skimming” adalah istilah yang digunakan ketika perusahaan asuransi hanya mengasuransikan risiko terbaik). Survei (misalnya, Earnix (2013)) menunjukkan bahwa penetapan harga adalah penggunaan analitik yang paling umum di kalangan perusahaan asuransi. Underwriting, yaitu proses mengklasifikasikan risiko ke dalam kategori homogen dan mengalokasikan pemegang polis ke dalam kategori-kategori tersebut, merupakan inti dari penetapan tarif. Pemegang polis dalam satu kelas (kategori) memiliki profil risiko yang serupa sehingga dikenakan tarif asuransi yang sama. Ini adalah konsep premi yang adil secara aktuaria; adil untuk mengenakan tarif yang berbeda kepada pemegang polis hanya jika mereka dapat dibedakan berdasarkan faktor risiko yang dapat diidentifikasi. Sebuah artikel awal, Two Studies in Automobile Insurance Ratemaking (Bailey dan LeRoy 1960), memberikan dorongan bagi penerimaan metode analitik dalam industri asuransi. Makalah ini membahas masalah klasifikasi penetapan tarif asuransi mobil. Artikel tersebut menggambarkan contoh asuransi mobil yang memiliki lima kelas penggunaan yang saling berkaitan dengan empat kelas rating prestasi. Pada saat itu, kontribusi premi untuk kelas penggunaan dan rating prestasi ditentukan secara independen satu sama lain. Memikirkan efek interaksi dari variabel klasifikasi yang berbeda merupakan masalah yang lebih rumit. Saat risiko pertama kali diperoleh, kewajiban perusahaan asuransi dapat dikelola dengan menerapkan parameter kontrak yang mengubah pembayaran kontrak. Bab 3 menjelaskan modifikasi umum termasuk co-insurance, deduktibel, dan batas atas kebijakan. 1.2.2 Memperbarui Asuransi Asuransi merupakan jenis layanan keuangan dan, seperti banyak kontrak layanan lainnya, cakupan asuransi sering kali disepakati untuk jangka waktu terbatas di mana komitmen cakupan diselesaikan. Terutama untuk asuransi umum, kebutuhan akan cakupan terus berlanjut, dan oleh karena itu upaya dilakukan untuk mengeluarkan kontrak baru yang memberikan cakupan serupa ketika kontrak yang ada mencapai akhir masa berlakunya. Ini disebut perpanjangan polis. Isu perpanjangan juga dapat muncul dalam asuransi jiwa, misalnya, asuransi jiwa berjangka (sementara). Pada saat yang sama, kontrak lain seperti pensiun dini berakhir pada saat kematian tertanggung, sehingga masalah perpanjangan tidak relevan. Dalam ketiadaan pembatasan hukum, saat perpanjangan polis, perusahaan asuransi memiliki kesempatan untuk: menerima atau menolak menerbitkan risiko tersebut; dan menentukan premi baru, mungkin dengan melakukan klasifikasi ulang terhadap risiko tersebut. Klasifikasi risiko dan penilaian saat perpanjangan didasarkan pada dua jenis informasi. Pertama, pada tahap awal, perusahaan asuransi memiliki banyak variabel penilaian yang dapat digunakan untuk pengambilan keputusan. Banyak variabel yang kemungkinan tidak akan berubah, misalnya jenis kelamin, sedangkan yang lain kemungkinan akan berubah, misalnya usia, dan yang lainnya mungkin berubah atau tidak, misalnya skor kredit. Kedua, berbeda dengan tahap awal, saat perpanjangan, perusahaan asuransi memiliki riwayat pengalaman kerugian dari pemegang polis, dan riwayat ini dapat memberikan wawasan tentang pemegang polis yang tidak tersedia dari variabel penilaian. Modifikasi premi berdasarkan riwayat klaim dikenal sebagai penilaian berdasarkan pengalaman, juga kadang-kadang disebut sebagai penilaian berdasarkan prestasi. Metode penilaian berdasarkan pengalaman dapat diterapkan secara retrospektif atau prospektif. Dengan metode retrospektif, sebagian premi dikembalikan kepada pemegang polis dalam kejadian pengalaman yang menguntungkan bagi perusahaan asuransi. Premi retrospektif umum dalam perjanjian asuransi jiwa (di mana pemegang polis memperoleh dividen di Amerika Serikat, bonus di Inggris, dan pembagian keuntungan dalam cakupan asuransi jiwa sementara di Israel). Pada umumnya, metode prospektif lebih umum digunakan dalam asuransi umum, di mana pengalaman yang menguntungkan bagi pemegang polis akan dihargai melalui premi perpanjangan yang lebih rendah. Riwayat klaim dapat memberikan informasi tentang minat risiko pemegang polis. Sebagai contoh, dalam asuransi personal, umumnya digunakan variabel untuk menunjukkan apakah klaim telah terjadi dalam tiga tahun terakhir atau tidak. Sebagai contoh lain, dalam asuransi komersial seperti asuransi kecelakaan kerja, dapat dilihat frekuensi atau tingkat keparahan klaim rata-rata pemegang polis selama tiga tahun terakhir. Riwayat klaim dapat mengungkapkan informasi yang sebaliknya tersembunyi (bagi perusahaan asuransi) tentang pemegang polis. 1.2.3 Pengelolaan Klaim dan Produk Dalam beberapa jenis asuransi, proses pembayaran klaim untuk peristiwa yang diasuransikan relatif sederhana. Misalnya, dalam asuransi jiwa, sertifikat kematian sederhana sudah cukup untuk membayarkan jumlah manfaat sesuai dengan kontrak. Namun, dalam bidang asuransi properti dan kecelakaan, prosesnya bisa jauh lebih kompleks. Bayangkan peristiwa yang diasuransikan yang relatif sederhana seperti kecelakaan mobil. Di sini, seringkali diperlukan untuk menentukan pihak yang bertanggung jawab dan kemudian perlu mengevaluasi kerusakan pada semua kendaraan dan orang yang terlibat dalam insiden, baik yang diasuransikan maupun yang tidak diasuransikan. Selain itu, biaya yang dikeluarkan dalam mengevaluasi kerusakan juga harus dinilai, dan sebagainya. Proses menentukan cakupan, tanggung jawab hukum, dan penyelesaian klaim dikenal sebagai penyesuaian klaim. Manajer asuransi terkadang menggunakan istilah “kebocoran klaim” untuk merujuk pada dolar yang hilang melalui ketidakefisienan pengelolaan klaim. Ada banyak cara di mana analitik dapat membantu mengelola proses klaim, lihat Gorman dan Swenson (2013). Secara historis, yang paling penting adalah pendeteksian penipuan. Proses penyesuaian klaim melibatkan mengurangi asimetri informasi (klaiman mengetahui apa yang terjadi; perusahaan mengetahui sebagian dari apa yang terjadi). Mengurangi penipuan adalah bagian penting dari proses pengelolaan klaim. Deteksi penipuan hanyalah satu aspek dari pengelolaan klaim. Lebih luas lagi, kita dapat mempertimbangkan pengelolaan klaim terdiri dari komponen-komponen berikut: - Pemilahan klaim. Seperti dalam dunia medis, identifikasi dini dan penanganan yang tepat terhadap klaim dengan biaya tinggi (pasien, dalam dunia medis) dapat menghasilkan penghematan yang dramatis. Misalnya, dalam asuransi kecelakaan kerja, perusahaan asuransi berupaya untuk mengidentifikasi secara dini klaim-klaim yang berisiko mengakibatkan biaya medis tinggi dan periode pembayaran yang lama. Intervensi dini dalam kasus-kasus ini dapat memberikan perusahaan asuransi lebih banyak kendali atas penanganan klaim, perawatan medis, dan biaya secara keseluruhan dengan kembalinya pekerja lebih awal. - Pengolahan klaim. Tujuannya adalah menggunakan analitik untuk mengidentifikasi situasi rutin yang diperkirakan memiliki pembayaran kecil. Situasi yang lebih kompleks mungkin memerlukan penyesuaian oleh penyelesaian klaim yang berpengalaman dan bantuan hukum untuk menangani klaim dengan potensi pembayaran besar. - Keputusan penyesuaian. Setelah klaim kompleks diidentifikasi dan ditugaskan kepada penyelesaian klaim, rutinitas yang didorong oleh analitik dapat dibentuk untuk membantu proses pengambilan keputusan berikutnya. Proses tersebut juga dapat membantu penyelesaian klaim dalam mengembangkan cadangan kasus, perkiraan kewajiban finansial perusahaan asuransi di masa depan. Ini adalah masukan penting untuk cadangan kerugian perusahaan asuransi, yang dijelaskan dalam Bagian 1.2.4. Selain penggantian kerugian kepada tertanggung, perusahaan asuransi juga perlu memperhatikan sumber lain yang mengalirkan pendapatan, yaitu biaya. Biaya penyesuaian kerugian merupakan bagian dari biaya pengelolaan klaim oleh perusahaan asuransi. Analitik dapat digunakan untuk mengurangi biaya yang terkait langsung dengan penanganan klaim (dialokasikan) serta waktu staf umum untuk mengawasi proses klaim (tidak dialokasikan). Industri asuransi memiliki biaya operasional yang tinggi dibandingkan dengan sektor jasa keuangan lainnya. Selain pembayaran klaim, terdapat banyak cara lain di mana perusahaan asuransi menggunakan data untuk mengelola produk mereka. Kami telah membahas kebutuhan analitik dalam penjaminan, yaitu klasifikasi risiko pada tahap akuisisi awal dan tahap perpanjangan. Perusahaan asuransi juga tertarik untuk mengetahui pemegang polis mana yang memilih untuk memperpanjang kontrak mereka dan, seperti halnya dengan produk lainnya, memantau loyalitas pelanggan. Analitik juga dapat digunakan untuk mengelola portofolio atau kumpulan risiko yang telah diperoleh oleh perusahaan asuransi. Seperti yang dijelaskan dalam Bab 10, setelah kontrak disepakati dengan tertanggung, perusahaan asuransi masih dapat mengubah kewajibannya bersih dengan melakukan perjanjian reasuransi. Jenis perjanjian ini melibatkan perusahaan reasuransi, yaitu perusahaan asuransi bagi perusahaan asuransi lainnya. Umum bagi perusahaan asuransi untuk membeli asuransi atas portofolio risiko mereka guna mendapatkan perlindungan dari peristiwa yang tidak biasa, sama seperti individu dan perusahaan lainnya. 1.2.4 Penyisihan Klaim Fitur penting yang membedakan asuransi dari sektor lain dalam ekonomi adalah waktu pertukaran pertimbangan. Dalam industri manufaktur, pembayaran untuk barang umumnya dilakukan pada saat transaksi. Sebaliknya, dalam asuransi, uang yang diterima dari pelanggan terjadi sebelum manfaat atau layanan diberikan; ini diberikan pada tanggal yang lebih kemudian jika terjadi peristiwa tertanggung. Hal ini mengakibatkan kebutuhan untuk memiliki cadangan kekayaan guna memenuhi kewajiban di masa depan sehubungan dengan kewajiban yang dibuat, dan untuk memperoleh kepercayaan dari tertanggung bahwa perusahaan akan mampu memenuhi komitmennya. Besarannya cadangan kekayaan ini, dan pentingnya memastikan kecukupannya, merupakan perhatian utama bagi industri asuransi. Mengalokasikan uang untuk klaim yang belum dibayar dikenal sebagai penyisihan kerugian; di beberapa yurisdiksi, cadangan ini juga dikenal sebagai ketentuan teknis. Seperti yang terlihat pada Gambar 1.1 beberapa kali di mana perusahaan merangkum posisi keuangannya; waktu-waktu ini dikenal sebagai tanggal penilaian. Klaim yang muncul sebelum tanggal penilaian telah dibayarkan, sedang dalam proses pembayaran, atau akan segera dibayarkan; klaim di masa depan setelah tanggal penilaian ini tidak diketahui. Perusahaan harus memperkirakan kewajiban yang belum diselesaikan ini ketika menentukan kekuatan keuangannya. Menentukan dengan tepat penyisihan kerugian penting bagi perusahaan asuransi atas banyak alasan. Penyisihan kerugian merupakan klaim yang diantisipasi yang perusahaan asuransi harus bayarkan kepada pelanggannya. Ketidakcukupan penyisihan dapat mengakibatkan ketidakmampuan untuk memenuhi kewajiban klaim. Sebaliknya, perusahaan asuransi dengan penyisihan berlebihan dapat menunjukkan perkiraan kelebihan cadangan yang konservatif dan dengan demikian menggambarkan posisi keuangan yang lebih lemah daripada yang sebenarnya. Penyisihan memberikan perkiraan untuk biaya asuransi yang belum dibayar yang dapat digunakan untuk penetapan harga kontrak. Penyisihan kerugian ini diperlukan oleh hukum dan peraturan. Masyarakat memiliki kepentingan yang kuat terhadap kekuatan keuangan dan solvabilitas perusahaan asuransi. Selain regulator, pemangku kepentingan lain seperti manajemen perusahaan asuransi, investor, dan pelanggan membuat keputusan yang bergantung pada penyisihan kerugian perusahaan. Sementara regulator dan pelanggan menghargai perkiraan yang konservatif terkait klaim yang belum dibayar, manajer dan investor mencari perkiraan yang lebih objektif untuk mewakili kesehatan keuangan yang sebenarnya dari perusahaan. Penyisihan kerugian adalah topik di mana terdapat perbedaan yang signifikan antara asuransi jiwa dan asuransi umum (juga dikenal sebagai asuransi harta benda dan kecelakaan, atau asuransi non-jiwa). Dalam asuransi jiwa, tingkat keparahan (besaran kerugian) seringkali bukan sumber ketidakpastian karena pembayaran telah ditentukan dalam kontrak. Tingkat kejadian, yang dipengaruhi oleh kematian tertanggung, menjadi perhatian. Namun, karena waktu yang lama untuk penyelesaian kontrak asuransi jiwa, ketidakpastian nilai waktu uang yang diukur dari penerbitan hingga tanggal pembayaran dapat mendominasi perhatian tingkat kejadian. Sebagai contoh, bagi seorang tertanggung yang membeli kontrak asuransi jiwa pada usia 20 tahun, tidak jarang kontrak masih berlaku setelah 60 tahun, ketika tertanggung merayakan ulang tahun ke-80. Lihat, misalnya, Bowers et al. (1986) atau Dickson, Hardy, dan Waters (2013) untuk pengenalan terhadap penyisihan kerugian dalam asuransi jiwa. Sebaliknya, untuk sebagian besar jenis bisnis asuransi non-jiwa, tingkat keparahan adalah sumber ketidakpastian utama dan durasi kontrak cenderung lebih singkat. "],["frequency-modeling.html", "Bab 2 Frequency Modeling 2.1 Goodness of Fit", " Bab 2 Frequency Modeling 2.1 Goodness of Fit Coverage modifications atau modifikasi pertanggungan adalah perubahan yang dibuat pada syarat dan ketentuan polis asuransi. Perubahan ini dapat diprakarsai oleh pemegang polis atau perusahaan asuransi, dan dirancang untuk mengubah pertanggungan yang diberikan oleh polis. Modifikasi pertanggungan dapat dilakukan karena berbagai alasan. Sebagai contoh, pemegang polis mungkin ingin meningkatkan batas pertanggungan pada polis mereka untuk melindungi diri mereka sendiri dari potensi kerugian. Atau, mereka mungkin ingin menambah atau menghapus jenis pertanggungan tertentu, seperti menambahkan asuransi banjir pada polis pemilik rumah atau menghapus pertanggungan tabrakan dari polis mobil. "],["modeling-loss-severity.html", "Bab 3 Modeling Loss Severity 3.1 Basic Distributional Quantities 3.2 Distribusi Kontinu untuk Memodelkan Tingkat Keparahan dari Kerugian 3.3 Methods of Creating New Distributions 3.4 modifikasi pertanggungan 3.5 Maximum Likelihood Estimation", " Bab 3 Modeling Loss Severity 3.1 Basic Distributional Quantities 3.1.1 Moments Dengan memisalkan X merupakan suatu variabel acak kontinu dengan probability density function (pdf) \\(f_X(x)\\) dan fungsi ditribusi \\(F_X(x)\\). Momen baku ke-k dari X yang dinotasikan dengan \\(μ&#39;_k\\) adalah nilai ekspektasi dari pangkat ke-k dari X yang diharapkan, asalkan mempunyai nilai. Momen pertama \\(μ&#39;_1\\) adalah nilai tengah (mean) dari X yang biasanya dilambangkan dengan μ . Rumus untuk \\(μ&#39;_k\\) adalah sebagai berikut : \\[ \\begin{align} μ&#39;_k = E(X^k) = \\int_{0}^{∞}x^kf_X(x)dx \\end{align} \\] Untuk mendukung dari variabel acak X dapat diasumsikan sebagai nonnegatif dikarenakan pada kenyataannya jarang sekali bernilai negatif. Salah satu contohnya yang menunjukkan bahwa raw moments untuk variabel nonnegatif dapat dihitung menggunakan rumus : \\[ \\begin{align} μ&#39;_k = \\int_{0}^{∞}kx^{k-1}[1-F_X(x)]dx, \\end{align} \\] yang didasarkan pada fungsi survival \\(S_X(x)=1−F_X(x)\\). Pada rumus ini berguna disaat k=1. Central moment ke -k dari X yang dinotasikan dengan \\(μ_k\\) yang merupakan nilai yang diharapkan dari pangkat ke-k dari deviasi x dan dari mean μ. Maka untuk rumus \\(μ_k\\) adalah \\[ \\begin{align} μ_k = E[(X-μ)^k] = \\int_{0}^{∞}(x-μ)^kf_X(x)dx \\end{align} \\] Momen pusat kedua \\(μ_2\\) mendefinisikan varians dari X yang dinotasikan dengan \\(σ^2\\). Akar kuadrat dari varians adalah simpangan baku \\(σ\\) . Rasio momen sentral ketiga terhadap pangkat tiga dari deviasi standar \\((μ_3/σ^3)\\) yang mendefinisikan koefisien kemiringan yang merupakan ukuran simetri. Koefisien kemencengan yang positif menunjukkan bahwa distribusi condong ke kanan (condong ke kanan). Rasio momen sentral keempat dengan pangkat empat dari deviasi standar \\((μ_4/σ^4)\\) mendefinisikan koefisien kurtosis. Distribusi normal memiliki koefisien kurtosis 3. Distribusi dengan koefisien kurtosis lebih besar dari 3 memiliki ekor yang lebih berat daripada distribusi normal, sedangkan distribusi dengan koefisien kurtosis kurang dari 3 memiliki ekor yang lebih ringan dan lebih datar. 3.1.1.1 Example Mengasumsikan bahwa variabel acak X memiliki distribusi gamma dengan rata-rata 9 dan skewness 1. Maka dicari perhitungan varians dari X. pdf dari X adalah : \\[ \\begin{align} f_{X}\\left( x \\right) = \\frac{\\left( x / \\theta \\right)^{\\alpha}}{x ~\\Gamma\\left( \\alpha \\right)} e^{- x / \\theta} \\end{align} \\] Untuk \\(x&gt;0\\), dan \\(α&gt;0\\), maka momen baku ke-k adalah \\[ \\begin{align} \\mu_{k}^{\\prime} = \\mathrm{E}\\left( X^{k} \\right) = \\int_{0}^{\\infty}{\\frac{1}{\\Gamma\\left( \\alpha \\right)\\theta^{\\alpha}}x^{k + \\alpha - 1}e^{- x / \\theta} dx} = \\frac{\\Gamma\\left( k + \\alpha \\right)}{\\Gamma\\left( \\alpha \\right)}\\theta^{k} \\end{align} \\] Dengan memberikan \\(\\Gamma\\left( r + 1 \\right) = r\\Gamma\\left( r \\right)\\) dan \\(\\Gamma\\left( 1 \\right) = 1\\). Maka \\(\\mu_{1}^{\\prime} = \\mathrm{E}\\left( X \\right) = \\alpha\\theta\\) $$ \\[\\begin{align} \\mu_{2}^{\\prime} &amp;= \\mathrm{E}\\left( X^{2} \\right) = \\left( \\alpha + 1 \\right)\\alpha\\theta^{2} = \\mu_{3}^{\\prime} = \\mathrm{E}\\left( X^{3} \\right) = \\left( \\alpha + 2 \\right)\\left( \\alpha + 1 \\right)\\alpha\\theta^{3}\\\\ \\mathrm{Var}\\left( X \\right) &amp;= (\\alpha + 1)\\alpha\\theta^2 - (\\alpha\\theta)^2 = \\alpha\\theta^{2}\\\\ \\end{align}\\] \\[ \\] \\[\\begin{array}{ll} \\text{Skewness} &amp;= \\frac{\\mathrm{E}\\left\\lbrack {(X - \\mu_{1}^{\\prime})}^{3} \\right\\rbrack}{{\\left( \\mathrm{Var}X \\right)}^{3/2}} = \\frac{\\mu_{3}^{\\prime} - 3\\mu_{2}^{\\prime}\\mu_{1}^{\\prime} + 2{\\mu_{1}^{\\prime}}^{3}}{{\\left(\\mathrm{Var} X \\right)}^{3/2}} \\\\ &amp;= \\frac{\\left( \\alpha + 2 \\right)\\left( \\alpha + 1 \\right)\\alpha\\theta^{3} - 3\\left( \\alpha + 1 \\right)\\alpha^{2}\\theta^{3} + 2\\alpha^{3}\\theta^{3}}{\\left( \\alpha\\theta^{2} \\right)^{3/2}} \\\\ &amp;= \\frac{2}{\\alpha^{1/2}} = 1. \\end{array}\\] $$ Maka didapatkan hasil \\[ \\begin{align} α&amp;=4\\\\ E(X)&amp;=αθ=8\\\\ θ&amp;=2\\\\ Var(X)&amp;=αθ^2=16 \\end{align} \\] 3.1.2 Quantiles Kuantil dapat digunakan dalam menggambarkan karakteristik distribusi X. Ketika distribusi X kontinu, untuk suatu pecahan tertentu 0≤p≤1 kuantil yang sesuai adalah solusi dari persamaan \\[ \\begin{align} F_X(π_p)=p \\end{align} \\] Sebagai contoh, titik tengah distribusi, \\(π_{0.5}\\) adalah median. Persentil adalah jenis kuantil; persentil \\(100p\\) yang merupakan angka sedemikian rupa sehingga \\(100×p\\) persen dari data berada di bawahnya. 3.1.3 Moment Generating Function Fungsi pembangkit momen (mgf)dilambangkan dengan MX(t) secara unik mencirikan distribusi dari X . Meskipun ada kemungkinan dua distribusi yang berbeda memiliki momen yang sama namun tetap berbeda, tidak demikian halnya dengan fungsi pembangkit momen. Artinya, jika dua variabel acak memiliki fungsi pembangkit momen yang sama, maka keduanya memiliki distribusi yang sama. Fungsi pembangkit momen diberikan oleh untuk semua nilai t yang memiliki nilai ekspektasi. \\[ \\begin{align} M_X(t) = E(e^{tX}) = \\int_{0}^{∞}e^{tX}f_X(x)dx \\end{align} \\] MGF adalah fungsi real yang turunan ke-k pada nol sama dengan momen mentah ke-k dari X . Dalam simbol, ini adalah \\[ \\begin{align} \\frac{d^k}{dt^k}MX(t)\\Bigr|_{t=0} = E(X^{k}) \\end{align} \\] 3.1.3.1 Example Variabel acak X memiliki distribusi eksponensial dengan mean \\(1/b\\). Maka dapat menacari b jika \\(M_{X}\\left( - b^{2} \\right) = 0.2\\). \\[ \\begin{align} M_{X}(t) = \\mathrm{E}\\left( e^{tX} \\right) = \\int_{0}^{\\infty}{e^{\\text{tx}}be^{- bx} dx} = \\int_{0}^{\\infty}{be^{- x\\left( b - t \\right)} dx} = \\frac{b}{\\left( b - t \\right)}. \\end{align} \\] Maka \\[ \\begin{align} M_{X}\\left( - b^{2} \\right) = \\frac{b}{\\left( b + b^{2} \\right)} = \\frac{1}{\\left( 1 + b \\right)} = 0.2 \\end{align} \\] Maka akan didapatkan \\[ \\begin{align} \\frac{1}{\\left( 1 + b \\right)} &amp;= 0.2\\\\ 1&amp;=0.2(1+b)\\\\ 1&amp;=0.2+0.2b\\\\ 0.8&amp;=0.2b\\\\ 4&amp;=b\\\\ \\end{align} \\] Kita juga dapat menggunakan fungsi pembangkit momen untuk menghitung fungsi pembangkit probabilitas dengan \\[ \\begin{align} P_X(z)= E(z^X)=M_X(logz) \\end{align} \\] 3.2 Distribusi Kontinu untuk Memodelkan Tingkat Keparahan dari Kerugian Metode yang akan dibahas: Gamma Pareto Weibull Generalized beta distribution of the second kind 3.2.1 Gamma Distribution Pendekatan konvensional dalam memodelkan kerugian adalah dengan membuat model terpisah untuk frekuensi dan tingkat keparahan klaim. Ketika frekuensi dan tingkat keparahan dimodelkan secara terpisah, biasanya para aktuaris menggunakan distribusi Poisson untuk jumlah klaim dan distribusi gamma untuk memodelkan tingkat keparahan dari kerugian itu sendiri. Namun, dengan perkembangan, menjadi popule metode dengan membuat model tunggal untuk premi murni (biaya klaim secara rata-rata). Rumus gamma density function: \\[\\begin{equation} f_{X}\\left( x \\right) = \\frac{\\left( x/ \\theta \\right)^{\\alpha}}{x~ \\Gamma\\left( \\alpha \\right)}\\exp \\left( -x/ \\theta \\right) \\ \\ \\ \\text{for } x &gt; 0 . \\end{equation}\\] Dengan nilai \\(a&gt;0\\) dan \\(θ&gt;0\\) juga. Variabel kontinu \\(X\\) yang merupakan fungsi adalah variable yang mewakilkan distribusi gamma dengan shape parameter \\(a\\) dan scale parameter \\(θ\\). Perubahan dari perubahan scale dan shape parameter untuk gamma density function tertuang dalam grafik: Apabila \\(a=1\\) gamma akan mereduksi menjadi distribusi eksponensial, apabila \\(a=\\frac{n}2\\) dan \\(θ=2\\) maka gamma akan mereduksi menjadi distribusi chi-square dengan nilai n sebagai derajat kebebesan pada distribusi.Fungsi distribusi dari model gamma merupakan bentuk tidak sempurna atau lengkap dari fungsi gamma, dan dianotasikan sebagai \\(\\Gamma\\left(\\alpha; \\frac{x}{\\theta} \\right)\\),, dan bentuk persamaannya adalah: \\[\\begin{equation} F_{X}\\left( x \\right) = \\Gamma\\left( \\alpha; \\frac{x}{\\theta} \\right) = \\frac{1}{\\Gamma\\left( \\alpha \\right)}\\int_{0}^{x /\\theta}t^{\\alpha - 1}e^{- t}~dt , \\end{equation}\\] dengan \\(\\alpha &gt; 0,\\ \\theta &gt; 0\\) adalah integer untul \\(\\alpha\\), sehingga dapat dituliskan sebagai \\(\\Gamma\\left( \\alpha; \\frac{x}{\\theta} \\right) = 1 - e^{-x/\\theta}\\sum_{k = 0}^{\\alpha-1}\\frac{(x/\\theta)^k}{k!}\\) Sedangkan, untuk momen pada suatu \\(K\\) dari variable acak berdistribusi gamma untuk \\(k\\) positif dituliskan dengan persamaan: \\[\\begin{equation} \\mathrm{E}\\left( X^{k} \\right) = \\theta^{k} \\frac{\\Gamma\\left( \\alpha + k \\right)}{\\Gamma\\left( \\alpha \\right)} . \\end{equation}\\] rata-rata dan variansi yang diberikan oleh \\(\\mathrm{E}\\left( X \\right) = \\alpha\\theta\\) dan \\(\\mathrm{Var}\\left( X \\right) = \\alpha\\theta^{2}\\), secara berturut-turut Karena semua momen ada untuk setiap \\(K\\) positif distribusi gamma dianggap sebagai distribusi berekor ringan, yang mungkin tidak cocok untuk memodelkan aset berisiko karena tidak akan memberikan penilaian yang realistis tentang kemungkinan kerugian yang parah. 3.2.2 Distribusi Pareto Distribusi Pareto, yang dinamai menurut nama ekonom Italia Vilfredo Pareto (1843-1923), yang memiliki banyak kontribusi pada aplikasi ekonomi dan keuangan. Distribusi ini memiliki kemiringan positif dan heavy tailed yang membuatnya cocok untuk memodelkan pendapatan, klaim asuransi berisiko tinggi, dan tingkat keparahan kerugian korban yang besar. Fungsi survival dari distribusi Pareto yang meluruh perlahan-lahan menuju nol pertama kali digunakan untuk menggambarkan distribusi pendapatan di mana sebagian kecil dari populasi memiliki proporsi yang besar dari total kekayaan. Untuk klaim asuransi yang ekstrim, ekor dari distribusi keparahan (kerugian yang melebihi ambang batas) dapat dimodelkan dengan menggunakan distribusi Pareto yang digeneralisasi. Variabel kontinu \\(X\\) dikatakan memiliki dua parameter pada distribusi pareto, dimana ada shape parameter \\(a\\) dan scale parameter \\(θ\\), yang persamaannya adalah \\[\\begin{equation} f_{X}\\left( x \\right) = \\frac{\\alpha\\theta^{\\alpha}}{\\left( x + \\theta \\right)^{\\alpha + 1}} \\ \\ \\ x &gt; 0, \\ \\alpha &gt; 0, \\ \\theta &gt; 0. \\tag{3.1} \\end{equation}\\] Grafis di bawah menggambarkan efek dari perubahan scale dan shape parameter pada pareto density function. Fungsi distribusi dari distribusi pareto adalah: \\[\\begin{equation} F_{X}\\left( x \\right) = 1 - \\left( \\frac{\\theta}{x + \\theta} \\right)^{\\alpha} \\ \\ \\ x &gt; 0,\\ \\alpha &gt; 0,\\ \\theta &gt; 0. \\end{equation}\\] Hazard function dari distribusi pareto adalah fungsi penurunan dari fungsi \\(x\\), indikasi ini menunjukkan bahwa distribusi pareto heavy tailed menggunakan analogi dari pendapatan populasi, apabila hazard function menurun seiring waktu, populasi akan habis pada suatu waktu penurunan yang menghasilkan heavier tail pada distribusi. Hazard function juga memberikan informasi tentang tail distribution yang digunakan untuk distribusi model data pada analisis survival. Hazard function ini juga dapat diartikan sebagai potensi sesaat bahwa suatu event yang menarik dapat terjadi dalam jangka waktu yang sangat sempit. Model \\(K\\) pada distribusi parretto dalam suatu keadaan random variable, jika \\(a\\)&gt;\\(k\\), yang dapat ditulis sebagai: \\[\\begin{equation} \\mathrm{E}\\left( X^{k} \\right) = \\frac{\\theta^{k}~ k!}{\\left( \\alpha - 1 \\right)\\cdots\\left( \\alpha - k \\right)} \\ \\ \\ \\alpha &gt; k. \\end{equation}\\] rata-rata dan variansi yang diberikan oleh \\[\\begin{equation} \\mathrm{E}\\left( X \\right) = \\frac{\\theta}{\\alpha - 1} \\ \\ \\ \\text{for } \\alpha &gt; 1 \\end{equation}\\] dan \\[\\begin{equation} \\mathrm{Var}\\left( X \\right) = \\frac{\\alpha\\theta^{2}}{\\left( \\alpha - 1 \\right)^{2}\\left( \\alpha - 2 \\right)} \\ \\ \\ \\text{for } \\alpha &gt; 2, \\end{equation}\\] secara berturut-turut 3.2.3 Distribusi Weibull Distribusi ini diambil dari nama pemiliknya yang merupakan fisikawan Swedish Waloddi Weibull (1887 – 1979) yang secara luas digunakan dalam keandalan, analisis data kehidupan, prakiraan cuaca, dan klaim asuransi umum. Data terpotong sering muncul dalam studi asuransi. Distribusi Weibull telah digunakan untuk memodelkan kelebihan perjanjian kerugian atas asuransi mobil serta waktu antar kedatangan gempa bumi. Variabel kontinu \\(X\\) dikatakan memiliki distribusi weibul dengan syarat memiliki 2 parameter yang digunakan, yaitu scale parameter \\(a\\) dan shape parameter \\(θ\\). Yang persamaannay turun dari: $$\\[\\begin{equation} \\end{equation}\\]$$ Pada grafik menunjukkan efek dari scale dan shape parameter pada perubahan weibul density function. Fungsi distribusi dari weibul distribusi diberikan sebagai: \\[\\begin{equation} F_{X}\\left( x \\right) = 1 - \\exp\\left(- \\left( \\frac{x}{\\theta} \\right)^{\\alpha}~\\right) \\ \\ \\ x &gt; 0,\\ \\alpha &gt; 0,\\ \\theta &gt; 0. \\end{equation}\\] Dari rumus dapat ditarik kesimpulan bahwa \\(a\\) mendeskripsikan bentuk dari hazard function dari weibul distribution. Dimana hazard function akan menjadi fungsi penurunan apabila \\(a\\)&lt;1 (heavy tailed distribution), akan konstan Ketika \\(a\\)=1 dan akan menjadi fungsi naik Ketika \\(a\\)&gt;1 (light tailed distribution). Sifat dari hazard function ini membuat weibul distribusi cocok untuk digunakan pada model yang variety yang luas, contohnya fenomena alam, forecast cuaca, Teknik industry, model asuransi, dan analisis resiko financial. Momen \\(K\\) pada weibul dianotasikan dalam : \\[\\begin{equation} \\mathrm{E}\\left( X^{k} \\right) = \\theta^{k}~\\Gamma\\left( 1 + \\frac{k}{\\alpha} \\right) . \\end{equation}\\] rata-rata dan variansi yang diberikan oleh \\[\\begin{equation} \\mathrm{E}\\left( X \\right) = \\theta~\\Gamma\\left( 1 + \\frac{1}{\\alpha} \\right) \\end{equation}\\] dan \\[\\begin{equation} \\mathrm{Var}(X)= \\theta^{2}\\left( \\Gamma\\left( 1 + \\frac{2}{\\alpha} \\right) - \\left\\lbrack \\Gamma\\left( 1 + \\frac{1}{\\alpha} \\right) \\right\\rbrack ^{2}\\right), \\end{equation}\\] secara berturut-turut 3.2.3.1 Contoh Soal Misalkan distribusi probabilitas masa hidup penderita AIDS (dalam bulan) dari saat diagnosis digambarkan oleh distribusi Weibull dengan parameter bentuk 1.2 dan parameter skala 33.33. Temukan probabilitas bahwa orang yang dipilih secara acak dari populasi ini bertahan setidaknya 12 bulan. Sebuah sampel acak dari 10 pasien akan dipilih dari populasi ini. Berapa peluang bahwa paling banyak dua orang akan meninggal dalam waktu satu tahun setelah diagnosis. Temukan persentil ke-99 dari distribusi masa hidup. 3.2.3.2 Solusi Biarkan X menjadi seumur hidup pasien AIDS (dalam bulan) memiliki distribusi Weibull dengan parameter \\((1.2,33.33)\\). Kita punya, \\(\\Pr \\left( X \\geq 12 \\right) = S_{X} \\left( 12 \\right) = e^{- \\left( \\frac{12}{33.33} \\right)^{1.2}} = 0.746.\\) Biarkan \\(Y\\) adalah jumlah pasien yang meninggal dalam waktu satu tahun diagnosis. Lalu, \\(Y\\)∼ \\(Bin(10, 0,254)\\) dan \\(Pr(Y≤2)=0,514\\). Misalkan \\(π0,99\\) menunjukkan persentil ke-99 dari distribusi ini. Kemudian, \\(S_{X}\\left( \\pi_{0.99} \\right) = \\exp\\left\\{- \\left( \\frac{\\pi_{0.99}}{33.33} \\right)^{1.2}\\right\\} = 0.01.\\) Memecahkan untuk \\(π_{0,99}\\) kita mendapatkan \\(π_{0,99}=118,99\\). 3.2.4 Distribusi Beta Umum Jenis Kedua Generalized Beta Distribution of the Second Kind (GB2) diperkenalkan oleh Venter (1983) dalam konteks pemodelan kerugian asuransi dan oleh McDonald (1984) sebagai distribusi pendapatan dan kekayaan. Ini adalah distribusi empat parameter, sangat fleksibel, yang dapat memodelkan distribusi miring positif dan negatif. Variabel kontinu \\(X\\) dikatakan memiliki distribusi GB2 dengan parameter \\(σ\\), \\(θ\\), \\(a_1\\) dan \\(a_2\\) jika pdf-nya diberikan oleh : \\[\\begin{equation} f_{X}\\left( x \\right) = \\frac{(x/\\theta)^{\\alpha_2/\\sigma}}{x \\sigma~\\mathrm{B}\\left( \\alpha_1,\\alpha_2\\right)\\left\\lbrack 1 + \\left( x/\\theta \\right)^{1/\\sigma} \\right\\rbrack^{\\alpha_1 + \\alpha_2}} \\ \\ \\ \\text{for } x &gt; 0, \\tag{3.2} \\end{equation}\\] \\(\\sigma,\\theta,\\alpha_1,\\alpha_2 &gt; 0\\) dan dimana fungsi Beta \\(\\mathrm{B}\\left( \\alpha_1,\\alpha_2 \\right)\\) didefinisikan sebagai: \\[\\begin{equation} \\mathrm{B}\\left( \\alpha_1,\\alpha_2\\right) = \\int_{0}^{1}{t^{\\alpha_1 - 1}\\left( 1 - t \\right)^{\\alpha_2 - 1}}~ dt. \\end{equation}\\] GB2 menyediakan model untuk data berekor berat dan ringan. Ini termasuk eksponensial, gamma, Weibull, Burr, Lomax, F, chi-square, Rayleigh, lognormal dan log-logistik sebagai kasus khusus atau terbatas. Misalnya dengan mengatur parameter \\(σ=α_1=α_2=1\\), GB2 direduksi menjadi distribusi logistik log. Ketika \\(σ=1\\) dan \\(α_2→∞\\), ini direduksi menjadi distribusi gamma, dan ketika \\(α=1\\) dan \\(α_2→∞\\) itu direduksi menjadi distribusi Weibull. Variabel acak GB2 dapat dibangun sebagai berikut. Misalkan \\(G_1\\) dan \\(G_2\\) adalah variabel acak independen di mana \\(G_i\\) memiliki distribusi gamma dengan parameter bentuk \\(α_i\\) dan parameter skala 1. Kemudian, dapat ditunjukkan bahwa variabel acak \\(X=θ(\\frac{G_1}{G_2})^σ\\) memiliki distribusi GB2. Hasil teoritis ini memiliki beberapa implikasi. Sebagai contoh, ketika momen-momen itu ada, dapat ditunjukkan bahwa \\(k\\) momen mentah ke-th dari variabel acak terdistribusi GB2 diberikan oleh : \\(\\mathrm{E}\\left( X^{k} \\right) = \\frac{\\theta^{k}~\\mathrm{B}\\left( \\alpha_1 +k \\sigma,\\alpha_2 - k \\sigma \\right)}{\\mathrm{B}\\left( \\alpha_1,\\alpha_2 \\right)}, \\ \\ \\ k &gt; 0.\\) Seperti yang sudah dijelaskan, GB2 juga terkait dengan \\(F\\)-distribusi, hasil yang dapat berguna dalam simulasi dan analisis residual. Aplikasi GB2 sebelumnya adalah pada data pendapatan dan baru-baru ini telah digunakan untuk memodelkan data klaim berekor panjang. GB2 telah digunakan untuk memodelkan berbagai jenis klaim asuransi mobil, tingkat kerugian akibat kebakaran, serta data klaim asuransi kesehatan. 3.3 Methods of Creating New Distributions 3.3.1 Functions of Random Variables and their Distributions Sub bab ini membahas mengenai cara-cara untuk membuat distribusi probabilitas parametrik baru dari distribusi yang sudah ada. Secara khusus, misalkan X sebuah variabel acak kontinu dengan pdf (probability distribution function) yang diketahui \\(fX(x)\\) dan fungsi distribusi \\(FX(x)\\). Kemudian distribusi \\(Y = g(X)\\) , di mana \\(g(X)\\) adalah transformasi satu-ke-satu yang mendefinisikan variabel acak baru \\(Y\\) . Dengan demikian pada sub bab ini menerapkan teknik-teknik berikut untuk membuat keluarga distribusi baru: perkalian dengan sebuah konstanta pemangkatan, eksponensial, dan pencampuran. 3.3.2 Multiplication by a Constant Jika data klaim menunjukkan perubahan dari waktu ke waktu, maka transformasi tersebut dapat berguna untuk menyesuaikan inflasi. Jika tingkat inflasi positif maka biaya klaim meningkat, dan jika negatif maka biaya menurun. Untuk menyesuaikan dengan inflasi, maka mengalikan biaya X dengan 1+ tingkat inflasi (inflasi negatif adalah deflasi). Untuk memperhitungkan dampak mata uang terhadap biaya klaim, maka menggunakan transformasi untuk menerapkan konversi mata uang dari mata uang dasar ke mata uang lawan. Pertimbangkan transformasi \\(Y = c_X\\) , dimana \\(c&gt;0\\) , maka fungsi distribusi dari \\(Y\\) diberikan oleh \\[\\begin{equation} F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( cX \\leq y \\right) = \\Pr\\left( X \\leq \\frac{y}{c} \\right) = F_{X}\\left( \\frac{y}{c} \\right). \\end{equation}\\] Dengan menggunakan aturan rantai untuk diferensiasi, pdf bunga \\(f_Y(y)\\) dapat ditulis sebagai \\[\\begin{equation} f_{Y}\\left( y \\right) = \\frac{1}{c}f_{X}\\left( \\frac{y}{c} \\right). \\end{equation}\\] Misalkan X termasuk dalam himpunan distribusi parametrik tertentu dan mendefinisikan versi yang diskalakan \\(Y = c_X , c &gt; 0\\) . Jika \\(Y\\) berada dalam himpunan distribusi yang sama maka distribusi tersebut dikatakan sebagai distribusi skala. Ketika sebuah anggota dari distribusi skala dikalikan dengan sebuah konstanta \\(c\\) \\(( c&gt;0 )\\), parameter skala untuk distribusi skala ini memenuhi dua kondisi: Parameter diubah dengan mengalikan dengan c Semua parameter lainnya tetap tidak berubah Contoh 3.3.1. Pertanyaan Ujian Aktuaria. Kerugian Asuransi Mobil Eiffel dilambangkan dalam mata uang Euro dan mengikuti distribusi lognormal dengan \\(μ = 8\\) dan \\(σ = 2\\) . Mengingat bahwa 1 euro = 1,3 dolar, tentukan himpunan parameter lognormal yang menggambarkan distribusi kerugian Eiffel dalam dolar. solusi: Misalkan \\(X\\) dan \\(Y\\) menunjukkan total kerugian Eiffel Auto Insurance dalam mata uang euro dan dolar secara berturut-turut. Karena \\(Y = 1.3X\\), kita memiliki: \\[\\begin{equation} F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( 1.3X \\leq y \\right) = \\Pr\\left( X \\leq \\frac{y}{1.3} \\right) = F_{X}\\left( \\frac{y}{1.3} \\right). \\end{equation}\\] \\(X\\) mengikuti distribusi lognormal dengan parameter \\(μ=8\\) dan \\(σ=2\\). Fungsi kepadatan probabilitas (\\(pdf\\)) dari \\(X\\) diberikan oleh: \\[\\begin{equation} f_{X}\\left( x \\right) = \\frac{1}{x \\sigma \\sqrt{2\\pi}}\\exp \\left\\{- \\frac{1}{2}\\left( \\frac{\\log x - \\mu}{\\sigma} \\right)^{2}\\right\\} \\ \\ \\ \\text{for } x &gt; 0. \\end{equation}\\] Karena \\(\\left| \\frac{dx}{dy} \\right| = \\frac{1}{1.3}\\), \\(PDF\\) yang diinginkan \\(f_{Y}(y)\\) adalah: \\[\\begin{array}{ll} f_{Y}\\left( y \\right) &amp; = \\frac{1}{1.3}f_{X}\\left( \\frac{y}{1.3} \\right) \\\\ &amp;= \\frac{1}{1.3}\\frac{1.3}{y \\sigma \\sqrt{2\\pi}}\\exp \\left\\{- \\frac{1}{2}\\left( \\frac{\\log\\left( y/1.3 \\right) - \\mu}{\\sigma} \\right)^{2}\\right\\} \\\\ &amp;= \\frac{1}{y \\sigma\\sqrt{2\\pi}}\\exp \\left\\{- \\frac{1}{2}\\left( \\frac{\\log y - \\left( \\log 1.3 + \\mu \\right)}{\\sigma} \\right)^{2}\\right\\}. \\end{array}\\] Maka Y mengikuti distribusi lognormal dengan parameter \\(4log1.3+μ=8.26\\) dan \\(σ = 2.00\\). Jika \\(μ = log(m)\\), dengan mudah dapat dilihat bahwa \\(m = e^μ\\) adalah parameter skala yang dikalikan dengan 1,3 sedangkan σ adalah parameter bentuk yang tidak berubah. 3.3.3 Raising to a Power Pada Bagian 3.2.3, telah membahas tentang fleksibilitas distribusi Weibull dalam menyesuaikan data keandalan. Distribusi Weibull adalah transformasi pangkat dari distribusi eksponensial. Ini adalah aplikasi dari jenis transformasi lain yang melibatkan peningkatan variabel acak menjadi pangkat. Pertimbangkan transformasi \\(Y = X^τ\\) dengan \\(τ&gt;0\\) , maka fungsi distribusi dari \\(Y\\) diberikan oleh \\[\\begin{equation} F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( X^{\\tau} \\leq y \\right) = \\Pr\\left( X \\leq y^{1/ \\tau} \\right) = F_{X}\\left( y^{1/ \\tau} \\right). \\end{equation}\\] Oleh karena itu, \\(pdf\\) dari bunga \\(f_Y(y)\\) dapat ditulis sebagai \\[\\begin{equation} f_{Y}(y) = \\frac{1}{\\tau} y^{(1/ \\tau) - 1} f_{X}\\left( y^{1/ \\tau} \\right). \\end{equation}\\] Di sisi lain, jika \\(τ &lt; 0\\) maka fungsi distribusi dari \\(Y\\) diberikan oleh \\[\\begin{equation} F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( X^{\\tau} \\leq y \\right) = \\Pr\\left( X \\geq y^{1/ \\tau} \\right) = 1 - F_{X}\\left( y^{1/ \\tau} \\right), \\end{equation}\\] dan \\[\\begin{equation} f_{Y}(y) = \\left| \\frac{1}{\\tau} \\right|{y^{(1/ \\tau) - 1}f}_{X}\\left( y^{1/ \\tau} \\right). \\end{equation}\\] Contoh 3.3.3. Asumsikan bahwa \\(X\\) mengikuti distribusi eksponensial dengan rata-rata \\(θ\\) dan pertimbangkan variabel yang ditransformasi \\(Y = X^τ\\) . Tunjukkan bahwa \\(Y\\) mengikuti distribusi Weibull ketika \\(τ\\) positif dan tentukan parameter-parameter dari distribusi Weibull. solusi: Karena \\(X\\) mengikuti distribusi eksponensial dengan rata-rata \\(\\theta\\), kita memiliki: \\[\\begin{equation} f_{X}(x) = \\frac{1}{\\theta}e^{- x/ \\theta} \\ \\ \\ \\, x &gt; 0. \\end{equation}\\] Dalam menyelesaikan persamaan untuk \\(x\\), kita dapatkan \\(x = y^{1/\\tau}\\). Mengambil turunan, kita memiliki: \\[\\begin{equation} \\left| \\frac{dx}{dy} \\right| = \\frac{1}{\\tau}{y^{\\frac{1}{\\tau}-1}}. \\end{equation}\\] maka \\[\\begin{equation} f_{Y}\\left( y \\right) = \\frac{1}{\\tau}{y^{\\frac{1}{\\tau} - 1}f}_{X}\\left( y^{\\frac{1}{\\tau}} \\right) \\\\ = \\frac{1}{\\tau \\theta }y^{\\frac{1}{\\tau} - 1}e^{- \\frac{y^{\\frac{1}{\\tau}}}{\\theta}} = \\frac{\\alpha}{\\beta}\\left( \\frac{y}{\\beta} \\right)^{\\alpha - 1}e^{- \\left( y/ \\beta \\right)^{\\alpha}}. \\end{equation}\\] di mana \\(α = 1/τ\\) dan \\(β = θ^τ\\). Kemudian, \\(Y\\) mengikuti distribusi Weibull dengan parameter bentuk \\(α\\) dan parameter skala \\(β\\) . 3.3.4 Exponentiation Distribusi normal adalah model yang sangat populer untuk sejumlah besar aplikasi ketika ukuran sampel besar, distribusi ini dapat berfungsi sebagai distribusi perkiraan untuk model lainnya. Jika variabel acak X memiliki distribusi normal dengan rata-rata \\(μ\\) dan varians \\(σ^2\\) maka \\(Y = e^X\\) memiliki distribusi lognormal dengan parameter \\(μ\\) dan \\(σ^2\\) . Variabel acak lognormal memiliki batas bawah nol, condong ke kanan, dan memiliki ekor kanan yang panjang. Distribusi lognormal biasanya digunakan untuk menggambarkan distribusi aset keuangan seperti harga saham. Distribusi ini juga digunakan untuk menyesuaikan jumlah klaim untuk asuransi mobil dan kesehatan. Ini adalah contoh jenis transformasi lain yang melibatkan eksponensial. Secara umum, pertimbangkan transformasi \\(Y = e^X\\) . Kemudian, fungsi distribusi dari \\(Y\\) diberikan oleh \\[\\begin{equation} F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( e^{X} \\leq y \\right) = \\Pr\\left( X \\leq \\log y \\right) = F_{X}\\left( \\log y \\right). \\end{equation}\\] Dengan mengambil turunan bahwa pdf bunga \\(f_Y(y)\\) dapat ditulis sebagai \\[\\begin{equation} f_{Y}(y) = \\frac{1}{y}f_{X}\\left( \\log y \\right). \\end{equation}\\] Sebagai kasus khusus yang penting, misalkan \\(X\\) berdistribusi normal dengan rata-rata \\(μ\\) dan varians \\(σ^2\\) . Maka, distribusi dari \\(Y = e^X\\) adalah \\[\\begin{equation} f_{Y}(y) = \\frac{1}{y}f_{X}\\left( \\log y \\right) = \\frac{1}{y \\sigma \\sqrt{2 \\pi}} \\exp \\left\\{-\\frac{1}{2}\\left(\\frac{ \\log y - \\mu}{\\sigma}\\right)^2\\right\\}. \\end{equation}\\] Ini dikenal sebagai distribusi lognormal. Contoh 3.3.4. Pertanyaan Ujian Aktuaria. Asumsikan bahwa \\(X\\) memiliki distribusi seragam pada interval \\((0, c)\\) dan mendefinisikan \\(Y = e^X\\) . Tentukan distribusi dari \\(Y\\). solusi: Kita mulai dengan fungsi distribusi kumulatif (\\(cdf\\)) dari \\(Y\\), \\[\\begin{equation} F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( e^{X} \\leq y \\right) = \\Pr\\left( X \\leq \\log y \\right) = F_{X}\\left( \\log y \\right). \\end{equation}\\] Mengambil turunan, kita memperoleh: \\[\\begin{equation} f_{Y}\\left( y \\right) = \\frac{1}{y}f_{X}\\left(\\log y \\right) = \\frac{1}{cy} . \\end{equation}\\] karena \\(0 &lt; x &lt; c\\), maka \\(1 &lt; y &lt; e^{c}\\). 3.3.5 Finite Mixtures Distribusi campuran merupakan cara yang berguna untuk memodelkan data yang diambil dari populasi yang heterogen. Populasi induk ini dapat dianggap dibagi menjadi beberapa subpopulasi dengan distribusi yang berbeda. 3.3.5.1 Two-point Mixture Jika fenomena yang mendasari beragam dan sebenarnya dapat digambarkan sebagai dua fenomena yang mewakili dua subpopulasi de_ngan modus yang berbeda, dapat membangun variabel acak campuran dua titik \\(X\\) . Diberikan variabel acak \\(X_1\\) dan \\(X_2\\) dengan pdf \\(fX_1(x)\\) dan \\(fX_2(x)\\) masing-masing, pdf dari \\(X\\) adalah rata-rata tertimbang dari komponen pdf \\(fX_1(x)\\) dan \\(fX_2(x)\\). Pdf dan fungsi distribusi dari \\(X\\) diberikan oleh \\[\\begin{equation} f_{X}\\left( x \\right) = af_{X_{1}}\\left( x \\right) + \\left( 1 - a \\right)f_{X_{2}}\\left( x \\right), \\end{equation}\\] dan \\[\\begin{equation} F_{X}\\left( x \\right) = aF_{X_{1}}\\left( x \\right) + \\left( 1 - a \\right)F_{X_{2}}\\left( x \\right), \\end{equation}\\] untuk \\(0&lt;a&lt;1\\) , dengan parameter pencampuran \\(a\\) dan \\((1-a)\\) masing-masing mewakili proporsi titik data yang termasuk dalam masing-masing dua subpopulasi. Rata-rata tertimbang ini dapat diterapkan pada sejumlah besaran terkait distribusi lainnya. Momen mentah ke-k dan fungsi pembangkit momen dari \\(X\\) diberikan oleh \\[\\begin{equation} \\mathrm{E}\\left( X^{k} \\right) = a\\mathrm{E}\\left( X_{1}^{K} \\right) + \\left( 1 - a \\right)\\mathrm{E}\\left( X_{2}^{k} \\right) \\end{equation}\\] dan \\[\\begin{equation} M_{X}(t) = aM_{X_{1}}(t) + \\left( 1 - a \\right)M_{X_{2}}(t), \\end{equation}\\] masing-masing. Contoh 3.3.5. Pertanyaan Ujian Aktuaria. Kumpulan polis asuransi terdiri dari dua jenis. 25% polis adalah Tipe 1 dan 75% polis adalah Tipe 2. Untuk polis Tipe 1, jumlah kerugian per tahun mengikuti distribusi eksponensial dengan rata-rata 200, dan untuk polis Tipe 2, jumlah kerugian per tahun mengikuti distribusi Pareto dengan parameter \\(α = 3\\) dan \\(θ = 200\\) . Untuk sebuah polis yang dipilih secara acak dari seluruh kumpulan kedua jenis polis tersebut, tentukan probabilitas bahwa kerugian tahunan akan kurang dari 100, dan tentukan rata-rata kerugiannya. solusi: Dua jenis kerugian tersebut adalah variabel acak \\(X_1\\) dan \\(X_2\\). \\(X_1\\) memiliki distribusi eksponensial dengan rata-rata 100, sehingga \\(F_{X_1}\\left(100\\right)=1-e^{-\\frac{100}{200}}=0.393\\). \\(X_2\\) memiliki distribusi Pareto dengan parameter \\(\\alpha=3\\) dan \\(\\theta=200\\), sehingga \\(F_{X_1}\\left(100\\right)=1-\\left(\\frac{200}{100+200}\\right)^3=0.704\\). Oleh karena itu, \\(F_X\\left(100\\right)=\\left(0.25\\times0.393\\right)+\\left(0.75\\times0.704\\right)=0.626\\). Kerugian rata-rata diberikan oleh \\[\\begin{equation} \\mathrm{E}\\left(X\\right)=0.25\\mathrm{E}\\left(X_1\\right)+0.75\\mathrm{E}\\left(X_2\\right)=\\left(0.25\\times200\\right)+\\left(0.75\\times100\\right)=125 \\end{equation}\\] 3.3.5.2 k-point Mixture Dalam kasus distribusi campuran berhingga, variabel acak yang diminati \\(X\\) memiliki probabilitas \\(p_i\\) untuk terambil dari subpopulasi homogen \\(i\\) dengan \\(i = 1,2,...,k\\) dan \\(k\\) adalah jumlah subpopulasi yang ditentukan pada awalnya dalam campuran. Parameter pencampuran \\(p_i\\) merepresentasikan proporsi observasi dari subpopulasi \\(i\\) . Pertimbangkan variabel acak \\(X\\) yang dihasilkan dari k subpopulasi yang berbeda, di mana subpopulasi \\(i\\) dimodelkan dengan distribusi kontinu \\(fX_i(x)\\) . Distribusi probabilitas dari \\(X\\) diberikan oleh \\[\\begin{equation} \\mathrm{E}\\left(X\\right)=0.25\\mathrm{E}\\left(X_1\\right)+0.75\\mathrm{E}\\left(X_2\\right)=\\left(0.25\\times200\\right)+\\left(0.75\\times100\\right)=125 \\end{equation}\\] dimana \\(0 &lt; p_{i} &lt; 1\\) dan \\(\\sum_{i = 1}^{k} p_{i} = 1\\) Model ini sering disebut sebagai campuran terbatas atau campuran k-point mixture. Fungsi distribusi, r momen mentah ke-k dan fungsi pembangkit momen dari k-point mixture ke-k diberikan sebagai \\[\\begin{equation} F_{X}\\left( x \\right) = \\sum_{i = 1}^{k}{p_{i}F_{X_{i}}\\left( x \\right)}, \\end{equation}\\] \\[\\begin{equation} F_{X}\\left( x \\right) = \\sum_{i = 1}^{k}{p_{i}F_{X_{i}}\\left( x \\right)}, \\end{equation}\\] \\[\\begin{equation} M_{X}(t) = \\sum_{i = 1}^{k}{p_{i}M_{X_{i}}(t)}, \\end{equation}\\] masing-masing Contoh 3.3.6. Pertanyaan Ujian Aktuaria. \\(Y_1\\) adalah campuran dari \\(X_1\\) dan \\(X_2\\) dengan bobot-bobot pencampuran \\(a\\) dan \\((1-a)\\). \\(Y_2\\) adalah campuran dari \\(X_3\\) dan \\(X_4\\) dengan bobot pencampuran \\(b\\) dan \\((1-b)\\). \\(Z\\) adalah campuran dari \\(Y_1\\) dan \\(Y_2\\) dengan bobot pencampuran \\(c\\) dan \\((1-c)\\). Tunjukkan bahwa \\(Z\\) adalah campuran dari \\(X_1, X_2, X_3 dan X_4\\) dan tentukan bobot pencampurannya. solusi: Dengan menerapkan rumus untuk distribusi campuran (mixed distribution), kita dapatkan: \\[\\begin{equation} f_{Y_{1}}\\left( x \\right) = af_{X_{1}}\\left( x \\right) + \\left( 1 - a \\right)f_{X_{2}}\\left( x \\right) \\end{equation}\\] \\[\\begin{equation} f_{Y_{2}}\\left( x \\right) = bf_{X_{3}}\\left( x \\right) + \\left( 1 - b \\right)f_{X_{4}}\\left( x \\right) \\end{equation}\\] \\[\\begin{equation} f_{Z}\\left( x \\right) = cf_{Y_{1}}\\left( x \\right) + \\left( 1 - c \\right)f_{Y_{2}}\\left( x \\right) \\end{equation}\\] Dengan menggantikan persamaan pertama dan kedua ke dalam persamaan ketiga, kita dapatkan: \\[\\begin{equation} f_{Z}\\left( x \\right) = c\\left\\lbrack af_{X_{1}}\\left( x \\right) + \\left( 1 - a \\right)f_{X_{2}}\\left( x \\right) \\right\\rbrack + \\left( 1 - c \\right)\\left\\lbrack bf_{X_{3}}\\left( x \\right) + \\left( 1 - b \\right)f_{X_{4}}\\left( x \\right) \\right\\rbrack \\end{equation}\\] \\[\\begin{equation} = caf_{X_{1}}\\left( x \\right) + c\\left( 1 - a \\right)f_{X_{2}}\\left( x \\right) + \\left( 1 - c \\right)bf_{X_{3}}\\left( x \\right) + (1 - c)\\left( 1 - b \\right)f_{X_{4}}\\left( x \\right) \\end{equation}\\] Kemudian, \\(Z\\) adalah campuran dari \\(X_1\\), \\(X_2\\), \\(X_3\\), dan \\(X_4\\), dengan bobot campuran \\(ca\\), \\(c(1−a)\\), \\((1−c)b\\), dan \\((1−c)(1−b)\\) secara berturut-turut. Mudah dilihat bahwa jumlah bobot campuran tersebut adalah satu. 3.3.6 Continuous Mixtures Campuran dengan jumlah subpopulasi yang sangat banyak (k menuju tak terhingga) sering disebut sebagai campuran kontinu. Dalam campuran kontinu, subpopulasi tidak dibedakan oleh parameter pencampuran diskrit tetapi oleh variabel kontinu \\(Θ\\) dimana \\(Θ\\) memainkan peran sebagai \\(p_i\\) dalam campuran berhingga. Pertimbangkan variabel acak \\(X\\) dengan distribusi yang bergantung pada parameter \\(Θ\\) , dimana \\(Θ\\) itu sendiri adalah variabel acak kontinu. Deskripsi ini menghasilkan model berikut untuk \\(X\\). \\[\\begin{equation} f_{X}\\left( x \\right) = \\int_{-\\infty}^{\\infty}{f_{X}\\left(x \\left| \\theta \\right. \\right)g_{\\Theta}( \\theta )} d \\theta , \\end{equation}\\] di mana \\(f_X(x|θ)\\) adalah distribusi bersyarat dari \\(X\\) pada nilai tertentu dari \\(Θ = θ\\) dan \\(g_Θ(θ)\\) adalah pernyataan probabilitas yang dibuat tentang parameter \\(θ\\) yang tidak diketahui . Dalam konteks Bayesian (dijelaskan pada Bagian 4.4), hal ini dikenal sebagai distribusi prior dari \\(Θ\\) (informasi sebelumnya atau pendapat ahli yang akan digunakan dalam analisis). Fungsi distribusi, k momen mentah ke-k dan fungsi pembangkit momen dari campuran kontinu diberikan sebagai \\[\\begin{equation} F_{X}\\left( x \\right) = \\int_{-\\infty}^{\\infty}{F_{X}\\left(x \\left| \\theta \\right. \\right) g_{\\Theta}(\\theta)} d \\theta, \\end{equation}\\] \\[\\begin{equation} \\mathrm{E}\\left( X^{k} \\right) = \\int_{-\\infty}^{\\infty}{\\mathrm{E}\\left( X^{k}\\left| \\theta \\right. \\right)g_{\\Theta}(\\theta)}d \\theta, \\end{equation}\\] \\[\\begin{equation} M_{X}(t) = \\mathrm{E}\\left( e^{t X} \\right) = \\int_{-\\infty}^{\\infty}{\\mathrm{E}\\left( e^{ tx}\\left| \\theta \\right. \\right)g_{\\Theta}(\\theta)}d \\theta, \\end{equation}\\] masing-masing Momen mentah ke-k ke-k dari distribusi campuran dapat ditulis ulang sebagai \\[\\begin{equation} \\mathrm{E}\\left( X^{k} \\right) = \\int_{-\\infty}^{\\infty}{\\mathrm{E}\\left( X^{k}\\left| \\theta \\right. \\right)g_{\\Theta}(\\theta)}d\\theta ~=~ \\mathrm{E}\\left\\lbrack \\mathrm{E}\\left( X^{k}\\left| \\Theta \\right. \\right) \\right\\rbrack . \\end{equation}\\] Dengan menggunakan hukum ekspektasi berulang (lihat Lampiran Bab 16), dapat mendefinisikan rata-rata dan varians dari \\(X\\) sebagai \\[\\begin{equation} \\mathrm{E}\\left( X \\right) = \\mathrm{E}\\left\\lbrack \\mathrm{E}\\left( X\\left| \\Theta \\right. \\right) \\right\\rbrack \\end{equation}\\] dan \\[\\begin{equation} \\mathrm{Var}\\left( X \\right) = \\mathrm{E}\\left\\lbrack \\mathrm{Var}\\left( X\\left| \\Theta \\right. \\right) \\right\\rbrack + \\mathrm{Var}\\left\\lbrack \\mathrm{E}\\left( X\\left| \\Theta \\right. \\right) \\right\\rbrack . \\end{equation}\\] Contoh 3.3.7. Pertanyaan Ujian Aktuaria. \\(X\\) memiliki distribusi normal dengan mean sebesar \\(Λ\\) sebesar 1 dan variansi sebesar 1. \\(Λ\\) memiliki distribusi normal dengan mean 1 dan varians 1. Tentukan mean dan varians dari \\(X\\) . solusi: X adalah campuran kontinu dengan rata-rata \\[\\begin{equation} \\mathrm{E}\\left(X\\right)=\\mathrm{E}\\left[\\mathrm{E}\\left(X\\middle|\\Lambda\\right)\\right]=\\mathrm{E}\\left(\\Lambda\\right)=1 \\text{ and } \\mathrm{V}\\left(X\\right)=\\mathrm{V}\\left[\\mathrm{E}\\left(X\\middle|\\Lambda\\right)\\right]+\\mathrm{E}\\left[\\mathrm{V}\\left(X\\middle|\\Lambda\\right)\\right]=\\mathrm{V}\\left(\\Lambda\\right)+\\mathrm{E}\\left(1\\right)=1+1=2. \\end{equation}\\] 3.4 modifikasi pertanggungan Coverage modifications atau modifikasi pertanggungan adalah perubahan yang dibuat pada syarat dan ketentuan polis asuransi. Perubahan ini dapat diprakarsai oleh pemegang polis atau perusahaan asuransi, dan dirancang untuk mengubah pertanggungan yang diberikan oleh polis. Modifikasi pertanggungan dapat dilakukan karena berbagai alasan. Sebagai contoh, pemegang polis mungkin ingin meningkatkan batas pertanggungan pada polis mereka untuk melindungi diri mereka sendiri dari potensi kerugian. Atau, mereka mungkin ingin menambah atau menghapus jenis pertanggungan tertentu, seperti menambahkan asuransi banjir pada polis pemilik rumah atau menghapus pertanggungan tabrakan dari polis mobil. pada bagian ini membahas mengenai 3.4.1 policy deductibles pada polis deductible biasa, pemegang polis setuju untuk menanggung sejumlah klaim asuransi sebelum perusahaan asuransi membayarkan klaim. Sehingga bagian kerugian yang ditanggung dan menjadi tanggung jawab pemegang polis untuk membayar deductible dengan uang mereka sendiri. sebagai contoh jika sebuah polis memiliki daductible sebesar Rp.500 dan pemegang polis mengalami kerugian dengan biaya sebesar Rp.2500, maka perusahaan asuransi hanya akan membayar Rp.2000 (yaitu total biaya perbaikan dikurangi deductible Rp.500) deductible sendiri di notasikan dengan \\(d\\), maka jika kerugian melebihi \\(d\\) atau nilai deductible, maka perusahaan asuransi bertanggung jawab untuk menanggung total kerugian dikurangin dengan deductible atau \\(d\\) tergantung dengan perjanjiannya, deductible dapat berlaku untuk setiap kerugian atau total dari seluruh kerugian. jumlah dari deductible biasanya dipilih pada saat pemegang polis membeli polis dan disesuaikan dengan kebutuhannya selama masa berlaku polis. deductible yang lebih tinggi akan menghasilkan pembayaran premi yang lebih rendah, dikarenakan pemegang polis menanggung lebih banyak saat terjadinya kerugian. lalu jika \\(X\\) di notasikan sebagai kerugian yang diterima oleh pemegang polis dan \\(Y\\) dinotasikan sebagai jumlah klaim yang dibayarkan oleh perusahaan asuransi, maka ada dua variabel berdasarkan pembayarannya kepada pemegang polis. a. pembayaran per kerugian b. pembayaran per pembayaran pada variabel perbayaran per kerugian, dinotasikan sebagai \\(Y^L\\) atau \\((X-d)_+\\) atau left censor, atau ketika jumlah atau total kerugian yang dialami kurang dari deductible, maka dinilai sama dengan 0 atau tidak dilakukan pembayaran. maka variabel ini didefinisikan sebagai \\[ Y^{L} = \\left( X - d \\right)_{+} = \\left\\{ \\begin{array}{cc} 0 &amp; X \\le d, \\\\ X - d &amp; X &gt; d \\end{array} \\right. . \\] disisi lain, variabel pembayaran per pembayaran dinotasikan sebagai \\(Y^P\\) didefinisikan ketika hanya terjadinya pembayaran, terutama \\(Y^P\\) sama dengan \\(X-d\\) dengan syarat \\([X&gt;d]\\), atau dinotasikan sebagai \\(Y^P=X-d||X&gt;d\\) atau dituliskan sebagai \\[ Y^{P} = \\left\\{ \\begin{matrix} \\text{Undefined} &amp; X \\le d \\\\ X - d &amp; X &gt; d . \\end{matrix} \\right. \\] disini \\(Y^P\\) disebut juga sebagai left truncated atau variabel kerugian berlebih, karena klaim yang lebih kecil dari \\(d\\) tidak dilaporkan dan nilai dari \\(d\\) berubah sebesar \\(d\\) ketika nilai distribusi dari nilai kerugian bersifat kontinu, namun distribusi dari \\(Y^L\\) adalah gabungan kombinasi dari komponen nilai diskrit dan kontinu. bagian diskrit terletak pada \\(Y=0\\) atau saat \\((X \\leq d)\\) dan komponen nilai kontinu terletak pada interval \\(Y&gt;0\\) atau saat \\(X&gt;d\\) 3.4.2 Policy Limit policy limit atau batas polis adalah bentuk jumlah maksimum yang dibayarkan oleh perusahaan asuransi untuk pertanggungan tertentu berdasarkan polis asuransinya. sehingga kerugian yang ditanggung dinotasikan sebagai \\(X\\), dan batas pertanggunannya atau batas polisnya dinotasikan sebagai \\(u\\), jika kerugian melebihi batas polis \\(X-u\\) harus dibayar oleh pemegang polis sendiri. batas polis yang lebih tinggi berarti premi yang dibayar oleh pemegang polis semakin besar. sebagai contoh sebuah polis mungkin memiliki batas pertanggungan sebesar Rp100.000 per kejadian, yang berarti bahwa perusahaan asuransi tidak akan membayar lebih dari Rp100.000 untuk setiap klaim atau tanggung jawab yang menjadi bagian dari polis. dimana biaya kerugian pemegang polis dinotasikan sebagai \\(X\\) dan klaim yang dibayarkan oleh perusahaan asuransi dinotasikan sebagai \\(Y\\), dan variabel policy limit dinotasikan sebagai \\(X \\land u\\). atau disebut sebagai right censored variable dikarenakan nilai dari \\(u\\) di set sama dengan \\(u\\). maka variabel \\(Y\\) didefinisikan sebagai \\[ Y = X \\land u = \\left\\{ \\begin{matrix} X &amp; X \\leq u \\\\ u &amp; X &gt; u. \\\\ \\end{matrix} \\right.\\ \\] pada batas polis, perbedaan antara \\(Y^L\\) dan \\(Y^P\\) tidak dibutuhkan dikarenakan perusahaan asuransi akan selalu melakukan pembayaran. dengan \\((X-u)\\) dan \\((X \\land u)\\) maka expektasi dari pembayaran terjadi tanpa modifikasi pertangguangan \\(X\\). jumlah ekspektasi pembayaran dari deductible \\(u\\) dan limit \\(u\\) maka, \\(X=(X-u)_++(X\\land u)\\) jika kerugian merupakan subjek dari deductible \\(d\\) dan limit \\(u\\), maka didefinisikan sebagai \\[ Y^{L} = \\left\\{ \\begin{matrix} 0 &amp; X \\leq d \\\\ X - d &amp; d &lt; X \\leq u \\\\ u - d &amp; X &gt; u. \\\\ \\end{matrix} \\right.\\ \\] maka, \\(Y^L\\) dapat dinyatakan sebagai \\(Y^L=(X\\land u)-(X\\land u)\\). 3.4.3 policy deductible and policy limit pada policy deductible atau pengurangan polis, jika kerugian yang dialami oleh pemegang polis kurang dari nilai deductible maka perusahaan asuransi tidak akan membayarkan kerugian tersebut, dan jika lebih besar dari nilai deductible maka klaim yang dibayarkan merupakan total dari kerugian dikurangi dengan nilai deductible, sehingga sisa biaya kerugian ditanggung pemegang polis. semakin besar nilai deductible maka besar premi yang perlu dibayarkan oleh pemegang polis semakin rendah pada policy limit atau pembatasaan polis, jika kerugian yang dialami oleh pemegang polis lebih besar dari nilai limit maka perusahaan asuransi tidak akan membayarkan kerugian tersebut, dan jika masih dibawah dari batas limit maka perusahaan asuransi akan selalu membayarkan total kerugian tersebut. akan tetapi jika lebih besar dari limit sisa biaya kerugian ditanggung pemegang polis. semakin besar nilai limit maka besar premi yang dibayarkan oleh pemegang polis semakin besar 3.4.4 Coinsurance and inflation coinsurance atau koasuransi adalah jenis pengaturan asuransi di mana dua atau lebih perusahaan asuransi berbagi risiko yang terkait dengan satu polis. Dalam pengaturan koasuransi, setiap perusahaan asuransi mengasumsikan sebagian risiko yang terkait dengan polis dan bertanggung jawab untuk membayar bagian proporsional dari setiap klaim yang muncul. Coinsurance sering digunakan pada asuransi properti dan asuransi kecelakaan, di mana besarnya risiko dapat melebihi kapasitas penanggung tunggal untuk menanggungnya. pada Policy Deductibles jumlah kerugian yang ditanggung oleh pemegang polis sampai dengan nilai dari deductible \\(d\\). kerugian yang dapat ditanggung juga dapat berupa presentase dari klaim. presentase \\(\\alpha\\) sering disebut sebagai faktor koasuransi. jika polis merupakan subjek dari deductible dan limit polis, maka koasuransi mengacu pada presentase klaim yang harus ditanggung oleh perusahaan asuransi. setelah dilakukan deductible dan limit pada polis maka, variabel pembayaran per kerugiaan atau \\(Y^L\\) didefinisikan sebagai: \\[ Y^{L} = \\left\\{ \\begin{matrix} 0 &amp; X \\leq d, \\\\ \\alpha\\left( X - d \\right) &amp; d &lt; X \\leq u, \\\\ \\alpha\\left( u - d \\right) &amp; X &gt; u. \\\\ \\end{matrix} \\right.\\ \\] jumlah maksimum yang dapat dibayarkan oleh perusahaan asuransi adalah \\(\\alpha (u-d)\\), dimana u adalah maksimum klaim yang dibayarkan dan pada Policy limit ketika kerugian merupakan subjek pada deductible \\(d\\) dan limit \\(u\\) untuk variabel per kerugian atau \\(Y^L\\), maka dapat dinyatakan sebagai \\(Y^L=(X\\land u)-(X\\land d)\\), maka pada koasuransi \\(Y^L\\) dapat dinyatakan sebagai\\(Y^L=\\alpha[(X\\land u)-(X\\land d)]\\). 3.4.5 Reinsurance Reinsurance atau Reasuransi adalah jenis asuransi yang digunakan perusahaan asuransi untuk mengalihkan sebagian risiko yang telah mereka tanggung dalam menjamin polis asuransi kepada perusahaan asuransi lain. Dalam pengaturan reasuransi, perusahaan asuransi menyerahkan sebagian risiko yang terkait dengan polis atau portofolio polis kepada perusahaan reasuransi, yang mengasumsikan risiko tersebut dengan imbalan sebagian premi yang dibayarkan oleh pemegang polis. Reasuransi biasanya digunakan oleh perusahaan asuransi untuk melindungi diri mereka sendiri dari kerugian akibat bencana atau untuk mengelola eksposur mereka terhadap risiko di lini bisnis tertentu. pada Policy deductible berdasarkan polis tersebut, pemegang polis harus membayar semua kerugian hingga batas nilai deductible, dan perusahaan asuransi hanya membayar jumlah (jika ada) di atas batas nilai deductible. terdapat peraturan dimana di mana perusahaan asuransi mengalihkan sebagian risiko polis dengan mendapatkan pertanggungan dari perusahaan asuransi lain dengan membayarkan juga premi asuransi. Reasuransi adalah pengaturan kontrak di mana perusahaan asuransi mengalihkan sebagian dari risiko yang diasuransikan dengan mendapatkan pertanggungan dari perusahaan asuransi lain dengan imbalan premi reasuransi. Dalam kontrak tersebut, penanggung utama atau perusahaan asuransi awal harus melakukan semua pembayaran yang diperlukan kepada pemegang polis hingga total pembayaran penanggung utama mencapai deductible reasuransi yang telah ditetapkan. lalu Perusahaan asuransi lainnya kemudian hanya bertanggung jawab untuk membayar kerugian di atas deductible reasuransi. Jumlah maksimum yang dipertahankan oleh penanggung utama dalam perjanjian reasuransi disebut retensi. 3.4.6 Coinsurance and Reinsurance Perbedaan utama antara reasuransi dan koasuransi adalah arah pengalihan risiko. Dalam pengaturan reasuransi, perusahaan asuransi mengalihkan risiko kepada perusahaan asuransi lain, sedangkan dalam pengaturan koasuransi, beberapa perusahaan asuransi berbagi risiko yang terkait dengan satu polis. Selain itu, reasuransi biasanya digunakan untuk melindungi penanggung dari kerugian akibat bencana atau untuk mengelola eksposur risiko mereka, sementara koasuransi sering digunakan untuk memungkinkan penanggung menanggung polis yang lebih besar daripada yang dapat mereka tangani sendiri. 3.5 Maximum Likelihood Estimation 3.5.1 Maximum Likelihood Estimators for Complete Data Hingga saat ini, bab ini berfokus pada distribusi parametrik yang biasa digunakan dalam aplikasi asuransi. Namun, agar berguna dalam pekerjaan terapan, distribusi ini harus menggunakan nilai “realistis” untuk parameter dan untuk ini kita beralih ke data. Pada tingkat dasar, kami berasumsi bahwa analis telah menyediakan sampel acak \\(X_1,... , X_n\\) dari distribusi dengan fungsi distribusi \\(F_x\\) (untuk singkatnya, kita terkadang menjatuhkan subskrip \\(X\\)). Vektor \\(θ\\) untuk menunjukkan kumpulan parameter untuk \\(F\\). Sebelum menggambar dari distribusi, kami mempertimbangkan hasil potensial yang dirangkum oleh variabel acak \\(X_i\\) (\\(i\\) = 1,2,..,n). Dari Bab sebelumnya yang membahas perkiraan distribusi frekuensi, dimana \\(Pr(X_1 = x_1,...,X_n = x_n)\\) untuk mengukur “kemungkinan” menggambar sampel \\({x_1,...,x_n}\\) Dengan data kontinu, kami menggunakan fungsi kerapatan probabilitas bersama alih-alih probabilitas bersama. Dengan asumsi independensi, pdf bersama dapat ditulis sebagai produk pdf. Dengan demikian, kami mendefinisikan kemungkinannya menjadi \\[\\begin{equation} L(\\boldsymbol \\theta) = \\prod_{i=1}^n f(x_i) . \\end{equation}\\] Dari notasi, perhatikan bahwa kami menganggap ini sebagai fungsi dari parameter di \\(θ\\), dengan data \\({x_1,...,x_n}\\) iadakan tetap. Penaksir kemungkinan maksimum adalah nilai parameter di \\(θ\\) yang memaksimalkan \\(L(θ)\\). Dari kalkulus, kita tahu bahwa memaksimalkan fungsi menghasilkan hasil yang sama dengan memaksimalkan logaritma suatu fungsi (ini karena logaritma adalah fungsi monoton). Karena kita mendapatkan hasil yang sama, untuk memudahkan pertimbangan komputasi, adalah umum untuk mempertimbangkan kemungkinan logaritmik, yang dilambangkan sebagai \\[\\begin{equation} l(\\boldsymbol \\theta) = \\log L(\\boldsymbol \\theta) = \\sum_{i=1}^n \\log f(x_i) . \\end{equation}\\] Contoh 3.5.1. Soal Ujian Aktuaria. Anda diberikan lima pengamatan berikut: 521, 658, 702, 819, 1217. Anda menggunakan Pareto parameter tunggal dengan fungsi distribusi: \\[F(x) = 1- \\left(\\frac{500}{x}\\right)^{\\alpha}, ~~~~ x&gt;500 .\\] Dengan \\(n = 5\\), fungsi log-likelihood adalah \\[l(\\alpha) = \\sum_{i=1}^5 \\log f(x_i;\\alpha ) = 5 \\alpha \\log 500 + 5 \\log \\alpha-(\\alpha+1) \\sum_{i=1}^5 \\log x_i.\\] Gambar dibawah menunjukkan kemungkinan logaritmik sebagai fungsi parameter \\(α\\) Kita dapat menentukan nilai maksimum kemungkinan logaritmik dengan mengambil turunan dan mengaturnya sama dengan nol. Ini menghasilkan \\[\\begin{array}{ll} \\frac{ \\partial}{\\partial \\alpha } l(\\alpha ) &amp;= 5 \\log 500 + 5 / \\alpha - \\sum_{i=1}^5 \\log x_i =_{set} 0 \\Rightarrow \\\\ \\hat{\\alpha}_{MLE} &amp;= \\frac{5}{\\sum_{i=1}^5 \\log x_i - 5 \\log 500 } = 2.453 . \\end{array}\\] Secara alami, ada banyak masalah di mana tidak praktis menggunakan perhitungan tangan untuk optimasi. Untungnya ada banyak rutinitas statistik yang tersedia seperti fungsi .R optim c1 &lt;- log(521)+log(658)+log(702)+log(819)+log(1217) nloglike &lt;- function(alpha){-(5*alpha*log(500)+5*log(alpha)-(alpha+1)*c1)} MLE &lt;- optim(par=1, fn=nloglike)$par ## Warning in optim(par = 1, fn = nloglike): one-dimensional optimization by Nelder-Mead is unreliable: ## use &quot;Brent&quot; or optimize() directly MLE ## [1] 2.453125 \\(2.453125\\) mengkonfirmasi hasil perhitungan tangan kita di mana penaksir kemungkinan maksimum Contoh 3.5.4. Dana Properti Wisconsin. Untuk melihat bagaimana penaksir kemungkinan maksimum bekerja dengan data nyata, kami kembali ke data klaim 2010 Cuplikan kode berikut menunjukkan cara menyesuaikan eksponensial, gamma, Pareto, lognormal, dan \\(GB2\\) Model. Untuk konsistensi, kode menggunakan package R dan VGAM. Akronim adalah singkatan dari Vector Generalized Linear and Additive Models; Seperti yang disarankan oleh namanya, paket ini dapat melakukan jauh lebih dari sesuai dengan model-model ini meskipun cukup untuk tujuan kita. Satu-satunya pengecualian adalah GB2 kepadatan yang tidak banyak digunakan di luar aplikasi asuransi; Namun, kita dapat mengkodekan kepadatan ini dan menghitung penaksir kemungkinan maksimum menggunakan optim General Purpose Optimizer. #library(VGAM) #claim_lev &lt;- read.csv(&quot;CLAIMLEVEL.csv&quot;, header = TRUE) #claim_data &lt;- subset(claim_lev, Year == 2010); # #Inference assuming a GB2 Distribution - this is more complicated # #The likelihood function of GB2 distribution (negative for optimization) #lik_gb2 &lt;- function (param) { # a_1 &lt;- param[1] # a_2 &lt;- param[2] # mu &lt;- param[3] # sigma &lt;- param[4] # yt &lt;- (log(claim_data$Claim) - mu) / sigma # logexpyt &lt;- ifelse(yt &gt; 23, yt, log(1 + exp(yt))) # logdens &lt;- a_1 * yt - log(sigma) - log(beta(a_1,a_2)) - # (a_1+a_2) * logexpyt - log(claim_data$Claim) # return(-sum(logdens)) #} # &quot;optim&quot; is a general purpose minimization function #gb2_bop &lt;- optim(c(1, 1, 0, 1), lik_gb2, method = c(&quot;L-BFGS-B&quot;), # lower = c(0.01, 0.01, -500, 0.01), # upper = c(500, 500, 500, 500), hessian = TRUE) # #Nonparametric Plot #plot(density(log(claim_data$Claim)), main = &quot;&quot;, xlab = &quot;Log Expenditures&quot;, # ylim = c(0 ,0.37)) #x &lt;- seq(0, 15, by = 0.01) # #Exponential #fit.exp &lt;- vglm(Claim ~ 1, exponential, data = claim_data) #theta = 1 / exp(coef(fit.exp)) #fexp_ex &lt;- dgamma(exp(x), scale = exp(-coef(fit.exp)), shape = 1) * exp(x) #lines(x, fexp_ex, col = &quot;red&quot;, lty =2) # #Inference assuming a gamma distribution #fit.gamma &lt;- vglm(Claim ~ 1, family = gamma2, data = claim_data) #theta &lt;- exp(coef(fit.gamma)[1]) / exp(coef(fit.gamma)[2]) # theta = mu / alpha #alpha &lt;- exp(coef(fit.gamma)[2]) #fgamma_ex &lt;- dgamma(exp(x), shape = alpha, scale = theta) * exp(x) #lines(x, fgamma_ex, col = &quot;blue&quot;, lty =3) # #Pareto #fit.pareto &lt;- vglm(Claim ~ 1, paretoII, loc = 0, data = claim_data) #fpareto_ex &lt;- dparetoII(exp(x), loc = 0, shape = exp(coef(fit.pareto)[2]), # scale = exp(coef(fit.pareto)[1])) * exp(x) #lines(x, fpareto_ex, col = &quot;purple&quot;) # #Lognormal #fit.LN &lt;- vglm(Claim ~ 1, family = lognormal, data = claim_data) #flnorm_ex &lt;- dlnorm(exp(x), mean = coef(fit.LN)[1], # sd = exp(coef(fit.LN)[2])) * exp(x) #lines(x, flnorm_ex, col = &quot;lightblue&quot;) # #Density for GB II #gb2_density &lt;- function (x) { # a_1 &lt;- gb2_bop$par[1] # a_2 &lt;- gb2_bop$par[2] # mu &lt;- gb2_bop$par[3] # sigma &lt;- gb2_bop$par[4] # xt &lt;- (log(x) - mu) / sigma # logexpxt &lt;- ifelse (xt &gt; 23, yt, log(1 + exp(xt))) # logdens &lt;- a_1 * xt - log(sigma) - log(beta(a_1, a_2)) - # (a_1+a_2) * logexpxt -log(x) # exp(logdens) # } #fGB2_ex = gb2_density(exp(x)) * exp(x) #lines(x, fGB2_ex, col=&quot;green&quot;) #legend(&quot;topleft&quot;, c(&quot;log(Expend)&quot;, &quot;Exponential&quot;, &quot;Gamma&quot;, &quot;Pareto&quot;, # &quot;Lognormal&quot;, &quot;GB2&quot;), cex=0.8, # lty = c(4,2,3,1,1,1), #4 is &quot;longdash&quot; # col = c(&quot;black&quot;,&quot;red&quot;,&quot;blue&quot;,&quot;purple&quot;,&quot;lightblue&quot;,&quot;green&quot;)) 3.5.2 Maximum Likehood Estimators using Modified Data Pada subbab ini, saya ingin menjelaskan estimasi kemungkinan maksimal pada grup data(group data), sensor data(cencored), dan data terpotong(truncated data). 3.5.2.1 MLE fro Group Data Pada bagian ini, saya ingin memperoleh perkiraan kemungkinan maksimum parameter data grup data(dalam kategori rentang) dan setiap pengamatan terhadap fungsi likehood adalah probabbilitas jatuh dalam kelompok tertentu(interval). Biarkan \\(N_j\\) mewakili jumlah pengamatan dalam interval \\((C_{j-1},C_j]\\). Fungsi likehood untuk grup data dinyatakan sebagai berikut: \\[\\begin{equation} L\\left( \\theta \\right) = \\prod_{j = 1}^{k}\\left\\lbrack F_X\\left( \\left. \\ c_{j} \\right|\\theta \\right) - F_X\\left( \\left. \\ c_{j - 1} \\right|\\theta \\right) \\right\\rbrack^{n_{j}}, \\end{equation}\\] Dimana, \\(C_0\\) adalah kemungkinan pengamatan kecil dan \\(C_k\\) adalah pengamatan terbesar yang mungkin. 3.5.2.1.1 Contoh For a group of policies, you are given that losses follow the distribution function \\(F_X(x) = 1 - \\frac{\\theta}{x},\\)for \\(\\theta &lt; x &lt; \\infty\\). Further, a sample of 20 losses resulted in the following: \\[{\\small \\begin{matrix}\\hline \\text{Interval} &amp; \\text{Number of Losses} \\\\ \\hline (\\theta, 10] &amp; 9 \\\\ (10, 25] &amp; 6 \\\\ (25, \\infty) &amp; 5 \\\\ \\hline \\end{matrix} }\\] Calculate the maksimum likehood estimate of \\(\\theta\\) Jawab: Kontribusi dari setiap dari 9 pengamatan dalam interval pertama terhadap fungsi likelihood adalah probabilitas dari \\(X \\leq 10\\); yaitu, \\(\\Pr\\left( X \\leq 10 \\right) = F_X\\left( 10 \\right)\\). Demikian pula, kontribusi dari masing-masing 6 dan 5 pengamatan dalam interval kedua dan ketiga adalah \\(\\Pr\\left( 10 &lt; X \\leq 25 \\right) = F_X\\left( 25 \\right) - F_X(10\\) dan \\(P(X&gt;25)=1-F_X(25)\\), secara berturut-turut. Fungsi likelihood diberikan oleh: \\[\\begin{array}{ll} L\\left( \\theta \\right) &amp; = \\left\\lbrack F_X\\left( 10 \\right) \\right\\rbrack^{9}\\left\\lbrack F_X\\left( 25 \\right) - F_X(10) \\right\\rbrack^{6}\\left\\lbrack 1 - F_X(25) \\right\\rbrack^{5} \\\\ &amp; = {\\left( 1 - \\frac{\\theta}{10} \\right)}^{9}\\left( \\frac{\\theta}{10} - \\frac{\\theta}{25} \\right)^{6}\\left( \\frac{\\theta}{25} \\right)^{5} \\\\ &amp; = {\\left( \\frac{10 - \\theta}{10} \\right)}^{9}\\left( \\frac{15\\theta}{250} \\right)^{6}\\left( \\frac{\\theta}{25} \\right)^{5}. \\end{array}\\] Setelah itu, mencari likehood algoritma \\[\\begin{array}{ll} \\log L \\left( \\theta \\right) &amp;= 9\\log \\left( 10 - \\theta \\right) + 6\\log \\theta + 5\\log \\theta - 9\\log 10 + 6\\log 15 - 6\\log 250 - 5\\log 25 \\\\ &amp;= 9\\log \\left( 10 - \\theta \\right) + 11\\log \\theta + constant . \\end{array}\\] Constant disini maksudnya adalah angka yang tidak bergantung pada \\(\\theta\\). Kita turunkan dan mendapatkan, \\[\\begin{equation} \\frac{d \\log L \\left( \\theta \\right)}{d \\theta} = \\frac{- 9}{\\left( 10 - \\theta \\right)} + \\frac{11}{\\theta} . \\end{equation}\\] Estimator maksimum likelihood, \\(\\hat{\\theta}\\), adalah solusi dari persamaan: \\[\\begin{equation} \\frac{- 9}{\\left( 10 - \\hat{\\theta} \\right)} + \\frac{11}{\\hat{\\theta}} = 0 \\end{equation}\\] yang menghasilkan \\(\\hat{\\theta} = 5.5\\) 3.5.2.2 MLE for Cencored Data Kontribusi pengamatan tersensor terhadap fungsi kemungkinan adalah probabilitas variabel acak melebihi batas spesifik ini. Perhatikan bahwa kontribusi data lengkap dan tersensor berbagi fungsi bertahan hidup, untuk titik lengkap fungsi bertahan hidup ini dikalikan dengan fungsi hazard, tetapi untuk pengamatan tersensor tidak demikian. Fungsi kemungkinan untuk data yang disensor kemudian diberikan oleh \\[\\begin{equation} L(\\theta) = \\left[ \\prod_{i=1}^r f_X(x_i) \\right] \\left[ S_X(u) \\right]^m , \\end{equation}\\] Dimana \\(R\\) adalah jumlah kerugian yang diketahui dibawah batas limit \\(u\\) \\(m\\) adalah jumla loss yang lebih besar dari batas \\(u\\) 3.5.2.3 MLE for Truncated Data berkaitan dengan estimasi kemungkinan maksimum dari distribusi kontinu dari variabel acak \\(X\\) ketika data tidak lengkap karena truncated atau pemotongan. Kontribusi terhadap fungsi kemungkinan pengamatan X terpotong di D akan menjadi probabilitas bersyarat dan \\(F_X(x)\\) akan digantikan oleh \\(\\frac{F_X(x)}{S_X(d)}\\). Fungsi likehood untuk data truncated diberikan \\[\\begin{equation} L(\\theta) = \\prod_{i=1}^k \\frac{f_X(x_i)}{S_X(d)} , \\end{equation}\\] Dimana \\(k\\) adalah jumlah loss yang lebih besar dari yang dapat dikurangkan D "],["model-selection-and-estimation.html", "Bab 4 Model Selection and Estimation 4.1 Nonparametric Inference", " Bab 4 Model Selection and Estimation 4.1 Nonparametric Inference Di bagian ini, Anda mempelajari cara: Perkirakan momen, kuantil, dan distribusi tanpa mengacu pada distribusi parametrik Ringkas data secara grafis tanpa mengacu pada distribusi parametrik Tentukan ukuran yang meringkas penyimpangan parametrik dari kecocokan nonparametrik Gunakan estimator nonparametrik untuk memperkirakan parameter yang dapat digunakan untuk memulai prosedur estimasi parametrik 4.1.1 Estimasi Nonparametrik Pada bagian pembahasan sebelumnya telah mempelajari cara meringkas distribusi dengan cara menghitung, varians, kuantil/persentil, dan sebagainya. Untuk memperkirakan langkah-langkah ringkasan menggunakan kumpulan data, salah satu strateginya adalah: menganggap bentuk parametrik untuk distribusi, seperti binomial negatif untuk frekuensi atau distribusi gamma untuk tingkat keparahan, memperkirakan parameter distribusi itu, gunakan distribusi dengan estimasi parameter untuk menghitung ukuran ringkasan yang diinginkan. Ini adalah pendekatan parametrik . Strategi lain adalah memperkirakan ukuran ringkasan yang diinginkan langsung dari pengamatan tanpa mengacu pada model parametrik. Tidak mengherankan, ini dikenal sebagai pendekatan nonparametrik mempertimbangkan jenis skema pengambilan sampel yang paling dasar dan mengasumsikan bahwa observasi adalah realisasi dari serangkaian variabel acak \\(X_1, \\ldots, X_n\\) yang iid menarik dari distribusi populasi yang tidak diketahui \\(F( ⋅ )\\). Cara yang setara untuk mengatakan ini adalah itu \\(X_1, \\ldots, X_n\\), adalah sampel acak (dengan penggantian) dari F( ⋅) .Kemudian menjelaskan estimator nonparametrik dari banyak ukuran penting yang meringkas sebuah distribusi. 4.1.1.1 Estimator Momen Pada bagian 2.2.2. telah mendefinisikan momen untuk frekuensi dan pada bagian 3.1.1 untuk keparahan. Secara khusus, k -momen ke-, \\(\\mathrm{E~}[X^k] = \\mu^{\\prime}_k\\) , merangkum banyak aspek distribusi untuk berbagai pilihan k . Di Sini, μ′k kadang-kadang disebut k th momen populasi untuk membedakannya dari k momen sampel, \\[\\frac{1}{n} \\sum_{i=1}^n X_i^k ,\\] yang merupakan estimator nonparametrik yang sesuai. Dalam aplikasi tipikal, k adalah bilangan bulat positif, meskipun tidak perlu dalam teori. Kasus khusus yang penting adalah momen pertama di mana \\(k = 1\\) . Dalam hal ini, simbol prima ( \\(\\prime\\) ) dan 1 subskrip biasanya dijatuhkan dan satu digunakan \\(\\mu=\\mu^{\\prime}_1\\) untuk menunjukkan mean populasi, atau hanya mean . Estimator sampel yang sesuai untuk \\(μ\\) disebut rata-rata sampel , dilambangkan dengan bilah di atas variabel acak: \\[\\overline{X} =\\frac{1}{n} \\sum_{i=1}^n X_i .\\] Jenis ringkasan ukuran minat lainnya adalah k -momen pusat ke- , \\(\\mathrm{E~} [(X-\\mu)^k] = \\mu_k\\) . (Kadang-kadang, \\(\\mu^{\\prime}_k\\) disebut k -th momen mentah untuk membedakannya dari momen sentral μk .). Estimator nonparametrik, atau sampel, dari \\(\\mu_k\\) adalah \\[\\frac{1}{n} \\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^k .\\] Momen pusat kedua ( \\(k = 2\\) ) adalah kasus penting yang biasanya akan diberikan simbol baru, \\(\\sigma^2 = \\mathrm{E~} [(X-\\mu)^2]\\) , dikenal sebagai varians . Sifat penduga momen sampel dari varians seperti \\(n^{-1}\\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^2\\) telah dipelajari secara ekstensif tetapi bukan satu-satunya estimator yang mungkin. Versi yang paling banyak digunakan adalah versi di mana ukuran sampel efektif dikurangi satu, jadi kami mendefinisikannya \\[s^2 = \\frac{1}{n-1} \\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^2.\\] Membagi dengan \\(n − 1\\) alih-alih N masalah kecil ketika Anda memiliki ukuran sampel yang besar \\(N\\) seperti yang umum dalam aplikasi asuransi. Estimator varians sampel \\(s^2\\) tidak memihak dalam arti bahwa \\(\\mathrm{E~} [s^2] = \\sigma^2\\) , properti yang diinginkan terutama saat menginterpretasikan hasil analisis. 4.1.1.2 Fungsi Distribusi Empiris Kita telah melihat bagaimana menghitung estimator nonparametrik dari k saat ini \\(\\mathrm{E~} [X^k]\\) . Dengan cara yang sama, untuk fungsi apa pun yang diketahui g (⋅) , kita dapat memperkirakan \\(\\mathrm{E~} [\\mathrm{g}(X)]\\) menggunakan\\(n^{-1}\\sum_{i=1}^n \\mathrm{g}(X_i)\\) Sekarang perhatikan fungsinya \\(\\mathrm{g}(X) = I(X \\le x)\\) untuk tetap \\(X\\) . Di sini, notasi $I( ⋅ \\() adalah fungsi indikator ; itu mengembalikan 1 jika acara ( ⋅ ) benar dan 0 sebaliknya. Perhatikan bahwa sekarang variabel acak\\) g (X$) memiliki distribusi Bernoulli (distribusi binomial dengan \\(n = 1\\) ). Kita dapat menggunakan distribusi ini untuk dengan mudah menghitung jumlah seperti rata-rata dan varians. Misalnya, untuk pilihan ini \\(g (⋅)\\) , nilai harapannya adalah \\(\\mathrm{E~} [I(X \\le x)] = \\Pr(X \\le x) = F(x)\\) , fungsi distribusi dievaluasi pada \\(X\\) . Menggunakan prinsip analog , kami mendefinisikan estimator nonparametrik dari fungsi distribusi \\[ \\begin{aligned} F_n(x) &amp;= \\frac{1}{n} \\sum_{i=1}^n I\\left(X_i \\le x\\right) \\\\ &amp;= \\frac{\\text{number of observations less than or equal to }x}{n} . \\end{aligned} \\] Sebagai $F_N( ⋅ $) didasarkan hanya pada pengamatan dan tidak mengasumsikan keluarga parametrik untuk distribusi, itu nonparametrik dan juga dikenal sebagai fungsi distribusi empiris . Ia juga dikenal sebagai fungsi distribusi kumulatif empiris dan, dalam R, seseorang dapat menggunakan ecdf(.) fungsi tersebut untuk menghitungnya. Contoh 4.1.1. Kumpulan Data Mainan . Sebagai ilustrasi, pertimbangkan kumpulan data fiktif, atau “mainan”. \\(n = 10\\) observasi. Tentukan fungsi distribusi empiris. \\[ {\\small \\begin{array}{c|cccccccccc} \\hline i &amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;10 \\\\ X_i&amp; 10 &amp;15 &amp;15 &amp;15 &amp;20 &amp;23 &amp;23 &amp;23 &amp;23 &amp;30\\\\ \\hline \\end{array} }\\] Kemudian memeriksa bahwa rata-rata sampel adalah \\(\\overline{X} = 19.7\\) dan bahwa varians sampel adalah \\(S^2= 34,45556\\) . Fungsi distribusi empiris yang sesuai adalah \\[ \\begin{aligned} F_n(x) &amp;= \\left\\{ \\begin{array}{ll} 0 &amp; \\text{ for }\\ x&lt;10 \\\\ 0.1 &amp; \\text{ for }\\ 10 \\leq x&lt;15 \\\\ 0.4 &amp; \\text{ for }\\ 15 \\leq x&lt;20 \\\\ 0.5 &amp; \\text{ for }\\ 20 \\leq x&lt;23 \\\\ 0.9 &amp; \\text{ for }\\ 23 \\leq x&lt;30 \\\\ 1 &amp; \\text{ for }\\ x \\geq 30, \\end{array} \\right.\\end{aligned}\\] (xExample &lt;- c(10,rep(15,3),20,rep(23,4),30)) PercentilesxExample &lt;- ecdf(xExample) plot(PercentilesxExample, main=&quot;&quot;,xlab=&quot;x&quot;) 4.1.1.3 Quartiles, Percentiles and Quantiles Pada bagian 3.1.1 median , yaitu angka yang kira-kira setengah dari kumpulan data berada di bawah (atau di atasnya) . Kuartil pertama adalah angka yang kira-kira 25% datanya berada di bawahnya dan kuartil ketiga adalah angka yang kira-kira 75% datanya berada di bawahnya. 100 hal persentil adalah angka sehingga \\(100×p\\) persen dari data di bawahnya. Untuk menggeneralisasi konsep ini, pertimbangkan fungsi distribusi \\(F(⋅\\)) , yang mungkin kontinu atau tidak, dan biarkan Q menjadi pecahan sehingga \\(0 &lt; q&lt; 1\\) . Kami ingin mendefinisikan quantile , katakanlah \\(q_F\\) , menjadi bilangan sedemikian sehingga \\(F(q_F) \\approx q\\) . Perhatikan bahwa ketika \\(q=0.5\\) , \\(q_F\\) adalah median; Kapan \\(q=0.25\\) , \\(q_F\\) adalah kuartil pertama, dan seterusnya. Dengan cara yang sama, ketika \\(q = 0, 0.01, 0.02, \\ldots, 0.99, 1.00\\) , yang dihasilkan QF adalah persentil. Jadi, kuantil menggeneralisasikan konsep median, kuartil, dan persentil. Lebih tepatnya, untuk diberikan \\(0 &lt; q&lt; 1\\) , tentukan q kuantil \\(q_F\\) untuk menjadi nomor yang memenuhi: \\[ \\begin{equation} F(q_F-) \\le q \\le F(q_F) \\tag{4.1} \\end{equation}\\] Untuk mendapatkan pemahaman yang lebih baik tentang definisi ini, mari kita lihat beberapa kasus khusus. Pertama, pertimbangkan kasus di mana X adalah variabel acak kontinu sehingga fungsi distribusi \\(F(⋅)\\) tidak memiliki titik lompatan, seperti yang diilustrasikan pada Gambar 4.2 . Pada gambar ini, beberapa pecahan, Q1 , Q2 , Dan Q3 ditunjukkan dengan kuantil yang sesuai \\(q_{F,1} , q_{F,2} , dan q_{F,3}\\) . Dalam setiap kasus, dapat dilihat bahwa \\(F(q_F-)= F(q_F)\\) sehingga ada kuantil unik. Karena kita dapat menemukan invers unik dari fungsi distribusi di mana saja \\(0 &lt; q&lt; 1\\) , kita bisa menulis \\(q_F= F^{-1}(q)\\) Gambar 4.3 menunjukkan tiga kasus untuk fungsi distribusi. Panel kiri sesuai dengan kasus kontinu yang baru saja dibahas. Panel tengah menampilkan titik lompatan yang serupa dengan yang telah kita lihat dalam fungsi distribusi empiris Gambar 4.1 . Untuk nilai \\(q\\) ditampilkan di panel ini, kami masih memiliki nilai kuantil yang unik \\(q_F\\) . Meskipun ada banyak nilai Q seperti yang \\(F(q_F-) \\le q \\le F(q_F)\\) , untuk nilai tertentu dari \\(q\\) , hanya ada satu solusi untuk persamaan (4.1) . Panel kanan menggambarkan situasi di mana kuantil tidak dapat ditentukan secara unik untuk \\(q\\) ditampilkan karena ada berbagai \\(q_F\\) persamaan yang memuaskan (4.1) . Contoh 4.1.2. Kumpulan Data Mainan: Lanjutan. Tentukan kuantil yang sesuai dengan persentil ke-20, ke-50, dan ke-95. Solusi . Perhatikan Gambar 4.1 . Kasus \\(q=0.20\\) sesuai dengan panel tengah Gambar Gambar 4.3 , jadi persentil ke-20 adalah 15. Kasus \\(q=0.50\\) sesuai dengan panel kanan, jadi mediannya adalah angka antara 20 dan 23 inklusif. Banyak paket perangkat lunak menggunakan rata-rata 21,5 (misalnya R, seperti yang terlihat di bawah). Untuk persentil ke-95, solusinya adalah 30. Kita dapat melihat dari Gambar 4.1 bahwa 30 juga sesuai dengan persentil ke-99 dan ke-99,99. quantile(xExample, probs=c(0.2, 0.5, 0.95), type=6) Dengan mengambil rata-rata tertimbang antara pengamatan data, kuantil empiris yang dihaluskan dapat menangani kasus seperti panel kanan pada Gambar 4.3 . Itu Q kuantil empiris yang dihaluskan didefinisikan sebagai \\[\\hat{\\pi}_q = (1-h) X_{(j)} + h X_{(j+1)}\\] Di mana \\(j=\\lfloor(n+1)q\\rfloor\\) , Dan\\(X_{(1)}, \\ldots, X_{(n)}\\) adalah nilai yang diurutkan (dikenal sebagai statistik urutan ) yang sesuai dengan \\(X_1, \\ldots, X_n\\). (Ingat bahwa tanda kurung ⌊ ⋅ ⌋ adalah fungsi lantai yang menunjukkan nilai bilangan bulat terbesar.) Perhatikan bah wa \\(\\hat{\\pi}_q\\)$ hanyalah sebuah interpolasi linear antara \\(X_{( j )}\\) dan \\(X_{(j+1)}\\). Contoh 4.1.3. Kumpulan Data Mainan: Lanjutan. Tentukan persentil yang dihaluskan ke-50 dan ke-20. Solusi Ambil \\(n = 10\\) Dan \\(q= 0,5\\). Kemudian, \\(j=\\lfloor(11)(0.5) \\rfloor= \\lfloor 5.5 \\rfloor=5\\), . Maka kuantil empiris yang dihaluskan ke-0,5 adalah \\[\\hat{\\pi}_{0.5} = (1-0.5) X_{(5)} + (0.5) X_{(6)} = 0.5 (20) + (0.5)(23) = 21.5.\\] Sekarang ambil \\(n = 10\\) Dan \\(q= 0,2\\) . Pada kasus ini, \\(j=\\lfloor(11)(0.2)\\rfloor=\\lfloor 2.2 \\rfloor=2\\) . Maka kuantil empiris yang dihaluskan ke-0,2 adalah \\[\\hat{\\pi}_{0.2} = (1-0.2) X_{(2)} + (0.2) X_{(3)} = 0.8 (15) + (0.2)(15) = 15.\\] 4.1.1.4 Penduga Kepadatan Variabel Diskrit. Ketika variabel acak adalah diskrit, memperkirakan fungsi massa probabilitas \\(f(x) = \\Pr(X=x)\\) mudah. Kami hanya menggunakan rata-rata sampel, yang didefinisikan sebagai \\[f_n(x) = \\frac{1}{n} \\sum_{i=1}^n I(X_i = x),\\] yang merupakan proporsi sampel sama dengan X Variabel Berkelanjutan dalam Grup. Untuk variabel acak kontinu, pertimbangkan formulasi diskrit di mana domain dari F( ⋅ ) dipartisi oleh konstanta \\(\\{c_0 &lt; c_1 &lt; \\cdots &lt; c_k\\}\\) ke dalam interval bentuk \\([c_{j-1}, c_j)\\) , untuk \\(j=1, \\ldots, k\\) . Pengamatan data dengan demikian “dikelompokkan” berdasarkan interval di mana mereka jatuh. Kemudian, kita dapat menggunakan definisi dasar dari fungsi massa empiris, atau variasi seperti \\[f_n(x) = \\frac{n_j}{n \\times (c_j - c_{j-1})} \\ \\ \\ \\ \\ \\ c_{j-1} \\le x &lt; c_j,\\] Di mana \\(N_J\\) adalah jumlah pengamatan ( \\(X_i\\) ) yang termasuk dalam interval \\([c_{j-1}, c_j)\\). Variabel Berkelanjutan (tidak dikelompokkan). Memperluas gagasan ini ke contoh di mana kami mengamati data individual, perhatikan bahwa kami selalu dapat membuat pengelompokan arbitrer dan menggunakan rumus ini. Lebih formal, biarkan \\(b &gt; 0\\) menjadi konstanta positif kecil, yang dikenal sebagai bandwidth , dan menentukan penaksir kepadatan menjadi \\[\\begin{equation} f_n(x) = \\frac{1}{2nb} \\sum_{i=1}^n I(x-b &lt; X_i \\le x + b) \\tag{4.2} \\end{equation}\\] Secara lebih umum, tentukan penaksir kerapatan kernel dari pdf di X sebagai \\[\\begin{equation} f_n(x) = \\frac{1}{nb} \\sum_{i=1}^n w\\left(\\frac{x-X_i}{b}\\right) , \\tag{4.3} \\end{equation}\\] Di mana w adalah fungsi kerapatan probabilitas yang berpusat di sekitar 0. Perhatikan bahwa persamaan (4.2) adalah kasus khusus penduga kerapatan kernel di mana \\(w(x) = \\frac{1}{2}I(-1 &lt; x \\le 1)\\) , juga dikenal sebagai kernel seragam . Pilihan populer lainnya ditunjukkan pada Tabel 4.1 . \\[{\\small \\begin{matrix} \\begin{array}{l|cc} \\hline \\text{Kernel} &amp; w(x) \\\\ \\hline \\text{Uniform } &amp; \\frac{1}{2}I(-1 &lt; x \\le 1) \\\\ \\text{Triangle} &amp; (1-|x|)\\times I(|x| \\le 1) \\\\ \\text{Epanechnikov} &amp; \\frac{3}{4}(1-x^2) \\times I(|x| \\le 1) \\\\ \\text{Gaussian} &amp; \\phi(x) \\\\ \\hline \\end{array}\\end{matrix} }\\] Di Sini, \\(\\phi(\\cdot)\\) adalah fungsi kepadatan normal standar. Seperti yang akan kita lihat pada contoh berikut, pilihan bandwidth \\(B\\) hadir dengan tradeoff bias-varians antara mencocokkan fitur distribusi lokal dan mengurangi volatilitas. Contoh 4.1.4. Dana Properti. Gambar 4.4 menunjukkan histogram (dengan persegi panjang abu-abu yang diarsir) dari klaim properti logaritmik dari tahun 2010. Kurva tebal (biru) mewakili kerapatan kernel Gaussian di mana bandwidth dipilih secara otomatis menggunakan aturan ad hoc berdasarkan ukuran sampel dan volatilitas data ini . Untuk dataset ini, bandwidth ternyata b = 0,3255 . Sebagai perbandingan, kurva putus-putus (merah) menunjukkan penaksir densitas dengan lebar pita sama dengan 0,1 dan kurva halus berwarna hijau menggunakan lebar pita 1. Sebagaimana diantisipasi, lebar pita yang lebih kecil (0,1) menunjukkan mengambil rata-rata lokal dengan data yang lebih sedikit sehingga kita mendapatkan ide yang lebih baik dari rata-rata lokal, tetapi dengan harga volatilitas yang lebih tinggi. Sebaliknya, bandwidth yang lebih besar (1) memperhalus fluktuasi lokal, menghasilkan kurva yang lebih halus yang mungkin melewatkan gangguan pada rata-rata lokal. Untuk aplikasi aktuaria, kami terutama menggunakan estimator densitas kernel untuk mendapatkan kesan visual cepat dari data. Dari perspektif ini, Anda cukup menggunakan aturan ad hoc default untuk pemilihan bandwidth, mengetahui bahwa Anda memiliki kemampuan untuk mengubahnya tergantung pada situasi yang dihadapi. ClaimLev &lt;- read.csv(&quot;Data/CLAIMLEVEL.csv&quot;, header=TRUE); #nrow(ClaimLev); # 6258 ClaimData&lt;-subset(ClaimLev,Year==2010); #2010 subset #Density Comparison hist(log(ClaimData$Claim), main=&quot;&quot;, ylim=c(0,.35),xlab=&quot;Log Expenditures&quot;, freq=FALSE, col=&quot;lightgray&quot;) lines(density(log(ClaimData$Claim)), col=&quot;blue&quot;,lwd=2.5) lines(density(log(ClaimData$Claim), bw=1), col=&quot;green&quot;) lines(density(log(ClaimData$Claim), bw=.1), col=&quot;red&quot;, lty=3) legend(&quot;topright&quot;, c(&quot;b=0.3255 (default)&quot;, &quot;b=0.1&quot;, &quot;b=1.0&quot;), lty=c(1,3,1), lwd=c(2.5,1,1), col=c(&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;), cex=1) #density(log(ClaimData$Claim))$bw ##default bandwidth Estimator densitas nonparametrik, seperti estimator kernel, sering digunakan dalam praktik. Konsep ini juga dapat diperluas untuk memberikan versi halus dari fungsi distribusi empiris. Mengingat definisi penaksir densitas kernel, penaksir kernel dari fungsi distribusi dapat ditemukan sebagai \\[\\begin{aligned} \\tilde{F}_n(x) = \\frac{1}{n} \\sum_{i=1}^n W\\left(\\frac{x-X_i}{b}\\right).\\end{aligned}\\] Di mana \\(W\\) adalah fungsi distribusi yang terkait dengan densitas kernel \\(w\\) . Sebagai ilustrasi, untuk kernel yang seragam, kita punya \\(w(y) = \\frac{1}{2}I(-1 &lt; y \\le 1)\\) , Jadi \\[\\begin{aligned} W(y) = \\begin{cases} 0 &amp; y&lt;-1\\\\ \\frac{y+1}{2}&amp; -1 \\le y &lt; 1 \\\\ 1 &amp; y \\ge 1 \\\\ \\end{cases}\\end{aligned} .\\] Contoh 4.1.5. Soal Ujian Aktuaria. Anda mempelajari lima nyawa untuk memperkirakan waktu dari timbulnya penyakit hingga kematian. Waktu kematian adalah: \\[\\begin{array}{ccccc} 2 &amp; 3 &amp; 3 &amp; 3 &amp; 7 \\\\ \\end{array}\\] Menggunakan kernel segitiga dengan bandwidth 2 , hitung taksiran fungsi densitas pada 2,5. Solusi. Untuk perkiraan kepadatan kernel, kami punya \\[f_n(x) = \\frac{1}{nb} \\sum_{i=1}^n w\\left(\\frac{x-X_i}{b}\\right),\\] Di mana \\(n = 5\\) , \\(b = 2\\) , Dan \\(x = 2,5\\) . Untuk inti segitiga, \\(w(x) = (1-|x|)\\times I(|x| \\le 1)\\) . Dengan demikian, \\[\\begin{array}{c|c|c} \\hline X_i &amp; \\frac{x-X_i}{b} &amp; w\\left(\\frac{x-X_i}{b} \\right) \\\\ \\hline 2 &amp; \\frac{2.5-2}{2}=\\frac{1}{4} &amp; (1-\\frac{1}{4})(1) = \\frac{3}{4} \\\\ \\hline 3 &amp; &amp; \\\\ 3 &amp; \\frac{2.5-3}{2}=\\frac{-1}{4} &amp; \\left(1-\\left| \\frac{-1}{4} \\right| \\right)(1) = \\frac{3}{4} \\\\ 3 &amp; &amp; \\\\ \\hline 7 &amp; \\frac{2.5-7}{2}=-2.25 &amp; (1-|-2.25|)(0) = 0\\\\ \\hline \\end{array}\\] Kemudian perkiraan densitas kernel di \\(x = 2,5\\) adalah \\[f_n(2.5) = \\frac{1}{5(2)}\\left( \\frac{3}{4} + (3) \\frac{3}{4} + 0 \\right) = \\frac{3}{10}\\] 4.1.1.5 Prinsip Pengaya Salah satu cara untuk membuat penaksir nonparametrik dari beberapa kuantitas adalah dengan menggunakan prinsip analog atau plug-in di mana seseorang menggantikan cdf yang tidak diketahui \\(F\\) dengan estimasi yang diketahui seperti cdf empiris \\(F_N\\) . Jadi, jika kita mencoba memperkirakan \\(\\mathrm{E}~[\\mathrm{g}(X)]=\\mathrm{E}_F~[\\mathrm{g}(X)]\\) untuk fungsi generik g , maka kami mendefinisikan estimator nonparametrik menjadi \\(\\mathrm{E}_{F_n}~[\\mathrm{g}(X)]=n^{-1}\\sum_{i=1}^n \\mathrm{g}(X_i)\\). Untuk melihat cara kerjanya, sebagai kasus khusus dari g , kami menganggap kerugian per variabel acak pembayaran \\(Y = (X-d)_+\\) dan rasio eliminasi kerugian yang diperkenalkan di Bagian 3.4.1. Kita dapat mengungkapkan ini sebagai \\[LER(d) = \\frac{\\mathrm{E~}[X - (X-d)_+]}{\\mathrm{E~}[X]} =\\frac{\\mathrm{E~}[\\min(X,d)]}{\\mathrm{E~}[X]} ,\\] Contoh. 4.1.6. Klaim Cidera Tubuh dan Rasio Penghapusan Kerugian Kami menggunakan sampel 432 klaim mobil tertutup dari Boston dari Derrig, Ostaszewski, dan Rempala ( 2001 ) . Kerugian dicatat untuk pembayaran karena cedera tubuh dalam kecelakaan mobil. Kerugian tidak dapat dikurangkan tetapi dibatasi oleh berbagai jumlah pertanggungan maksimum yang juga tersedia dalam data. Ternyata hanya 17 dari 432 ( ≈ 4%) tunduk pada batasan kebijakan ini sehingga kami mengabaikan data ini untuk ilustrasi ini. Kerugian rata-rata yang dibayarkan adalah 6906 dalam dolar AS. Gambar 4.5 menunjukkan aspek lain dari distribusi. Secara khusus, panel sebelah kiri menunjukkan fungsi distribusi empiris, panel sebelah kanan memberikan plot kepadatan nonparametrik. Dampak kerugian cedera tubuh dapat dikurangi dengan pengenaan limit atau pembelian polis reasuransi (lihat Bagian 10.3). Untuk mengukur dampak dari alat mitigasi risiko ini, biasanya menghitung rasio eliminasi kerugian (LER) seperti yang diperkenalkan di Bagian 3.4.1. Fungsi distribusi tidak tersedia sehingga harus diestimasi dengan cara tertentu. Menggunakan prinsip plug-in, estimator nonparametrik dapat didefinisikan sebagai \\[LER_n(d) = \\frac{n^{-1} \\sum_{i=1}^n \\min(X_i,d)}{n^{-1} \\sum_{i=1}^n X_i} = \\frac{\\sum_{i=1}^n \\min(X_i,d)}{\\sum_{i=1}^n X_i} .\\] Gambar 4.6 menunjukkan estimator \\(LER_n(d)\\) untuk berbagai pilihan \\(d\\) . Misalnya, di \\(d= 1.000\\) dan punya \\(LER_n( 1000 ) ≈ 0,1442\\). Dengan demikian, memberlakukan batas 1.000 berarti ekspektasi klaim yang ditahan 14,42 persen lebih rendah bila dibandingkan dengan ekspektasi klaim dengan deductible nol. 4.1.2 Alat untuk Pemilihan Model dan Diagnostik Bagian sebelumnya memperkenalkan estimator nonparametrik di mana tidak ada bentuk parametrik yang diasumsikan tentang distribusi yang mendasarinya. Namun, dalam banyak aplikasi aktuaria, analis berusaha menggunakan kecocokan parametrik dari distribusi untuk kemudahan penjelasan dan kemampuan untuk memperluasnya ke situasi yang lebih kompleks seperti memasukkan variabel penjelas dalam pengaturan regresi. Saat memasang distribusi parametrik, seorang analis mungkin mencoba menggunakan distribusi gamma untuk mewakili sekumpulan data kerugian. Namun, analis lain mungkin lebih suka menggunakan distribusi Pareto. Bagaimana cara menentukan model mana yang akan dipilih? Alat nonparametrik dapat digunakan untuk menguatkan pemilihan model parametrik. Pada dasarnya, pendekatannya adalah untuk menghitung langkah-langkah ringkasan yang dipilih di bawah model parametrik yang dipasang dan membandingkannya dengan kuantitas yang sesuai di bawah model nonparametrik. Karena model nonparametrik tidak mengasumsikan distribusi tertentu dan hanya merupakan fungsi dari data, model ini digunakan sebagai tolok ukur untuk menilai seberapa baik distribusi/model parametrik mewakili data. Juga, ketika ukuran sampel meningkat, distribusi empiris hampir pasti menyatu dengan distribusi populasi yang mendasarinya (berdasarkan hukum jumlah besar yang kuat). Dengan demikian distribusi empiris adalah proksi yang baik untuk populasi. Perbandingan estimator parametrik dengan nonparametrik dapat mengingatkan analis akan kekurangan dalam model parametrik dan terkadang menunjukkan cara untuk meningkatkan spesifikasi parametrik. Prosedur diarahkan menilai validitas model yang dikenal sebagaidiagnostik model . 4.1.2.1 Perbandingan Grafik Distribusi Kita telah melihat teknik overlay grafik untuk tujuan perbandingan. Untuk memperkuat penerapan teknik ini, Gambar 4.7membandingkan distribusi empiris dengan dua distribusi pas parametrik. Panel kiri menunjukkan fungsi distribusi distribusi klaim. Titik-titik yang membentuk kurva “berbentuk S” mewakili fungsi distribusi empiris pada setiap pengamatan. Kurva biru tebal memberikan nilai yang sesuai untuk distribusi gamma yang pas dan ungu muda untuk distribusi Pareto yang pas. Karena Pareto lebih dekat dengan fungsi distribusi empiris daripada gamma, ini memberikan bukti bahwa Pareto adalah model yang lebih baik untuk kumpulan data ini. Panel kanan memberikan informasi serupa untuk fungsi kerapatan dan memberikan pesan yang konsisten. Berdasarkan (hanya) angka-angka ini, distribusi Pareto adalah pilihan yang jelas bagi analis. Untuk cara lain untuk membandingkan kesesuaian dua model yang cocok, pertimbangkan plot probabilitas-probabilitas (\\(pp\\)) . A \\[pp\\] plot membandingkan probabilitas kumulatif di bawah dua model. Untuk tujuan kami, kedua model ini adalah fungsi distribusi empiris nonparametrik dan model pas parametrik. Gambar 4.8 menunjukkan \\(pp\\) plot untuk data Dana Properti yang diperkenalkan di Bagian 1.3 . Gamma yang dipasang di sebelah kiri dan Pareto yang dipasang di sebelah kanan, dibandingkan dengan fungsi distribusi data empiris yang sama. Garis lurus mewakili kesetaraan antara dua distribusi yang dibandingkan, sehingga titik yang dekat dengan garis diinginkan. Seperti yang terlihat pada demonstrasi sebelumnya, Pareto jauh lebih dekat dengan distribusi empiris daripada gamma, memberikan bukti tambahan bahwa Pareto adalah model yang lebih baik. Itu QQ plot membandingkan dua model yang dipasang melalui kuantilnya. Seperti hal hal plot, kami membandingkan nonparametrik dengan model pas parametrik. Kuantil dapat dievaluasi pada setiap titik kumpulan data, atau pada kisi (misalnya, di 0 , 0,001 , 0,002 , … , 0,999 , 1,000 ), tergantung aplikasinya. Pada Gambar 4.9 , untuk setiap titik pada kisi tersebut, sumbu horizontal menampilkan kuantil empiris dan sumbu vertikal menampilkan kuantil parametrik yang sesuai (gamma untuk dua panel atas, Pareto untuk dua panel bawah). Kuantil diplot pada skala asli di panel kiri dan pada skala log di panel kanan untuk memungkinkan kita melihat di mana kekurangan distribusi yang pas. Garis lurus mewakili kesetaraan antara distribusi empiris dan distribusi pas. Dari plot ini, kita sekali lagi melihat bahwa Pareto secara keseluruhan lebih cocok daripada gamma. Selain itu, panel kanan bawah menunjukkan bahwa distribusi Pareto bekerja dengan baik dengan klaim besar, tetapi memberikan kecocokan yang lebih buruk untuk klaim kecil. Contoh 4.1.7. Soal Ujian Aktuaria. Grafik di bawah ini menunjukkan \\(pp\\) plot distribusi pas dibandingkan dengan sampel. Solusi. Ekor dari distribusi yang pas terlalu tebal di sebelah kiri, terlalu tipis di sebelah kanan, dan distribusi yang pas memiliki probabilitas yang lebih kecil di sekitar median daripada sampel. Untuk melihat ini, ingat bahwa hal hal plot grafik distribusi kumulatif dari dua distribusi pada sumbunya (empiris pada sumbu x dan dipasang pada sumbu y dalam kasus ini). Untuk nilai kecil dari X , model yang dipasang memberikan probabilitas yang lebih besar untuk berada di bawah nilai itu daripada yang terjadi dalam sampel (mis F( x ) &gt;FN( x ) ). Ini menunjukkan bahwa model memiliki ekor kiri yang lebih berat daripada datanya. Untuk nilai besar dari X , model kembali memberikan probabilitas yang lebih besar untuk berada di bawah nilai itu dan dengan demikian lebih kecil kemungkinannya untuk berada di atas nilai itu (mis S( x ) &lt;SN( x ) ). Hal ini menunjukkan bahwa model memiliki ekor kanan yang lebih ringan dari pada data. Selain itu, saat kita mulai dari 0,4 hingga 0,6 pada sumbu horizontal (dengan demikian melihat 20% tengah data), hal hal plot meningkat dari sekitar 0,3 menjadi 0,4. Ini menunjukkan bahwa model hanya menempatkan sekitar 10% dari probabilitas dalam kisaran ini. 4.1.2.2 Perbandingan Statistik Distribusi Saat memilih model, akan sangat membantu untuk menampilkan tampilan grafis. Namun, untuk melaporkan hasil, melengkapi tampilan grafis dengan statistik terpilih yang meringkas kebaikan kesesuaian model dapat efektif. Tabel 4.2 menyediakan tiga statistik kebaikan yang umum digunakan . Dalam tabel ini, \\(F_N\\) adalah distribusi empiris, \\(F\\) adalah distribusi pas atau hipotesis, dan \\(F_i^* = F(x_i)\\) . \\[{\\small \\begin{matrix} \\begin{array}{l|cc} \\hline \\text{Statistic} &amp; \\text{Definition} &amp; \\text{Computational Expression} \\\\ \\hline \\text{Kolmogorov-} &amp; \\max_x |F_n(x) - F(x)| &amp; \\max(D^+, D^-) \\text{ where } \\\\ ~~~\\text{Smirnov} &amp;&amp; D^+ = \\max_{i=1, \\ldots, n} \\left|\\frac{i}{n} - F_i^*\\right| \\\\ &amp;&amp; D^- = \\max_{i=1, \\ldots, n} \\left| F_i^* - \\frac{i-1}{n} \\right| \\\\ \\text{Cramer-von Mises} &amp; n \\int (F_n(x) - F(x))^2 f(x) dx &amp; \\frac{1}{12n} + \\sum_{i=1}^n \\left(F_i^* - (2i-1)/n\\right)^2 \\\\ \\text{Anderson-Darling} &amp; n \\int \\frac{(F_n(x) - F(x))^2}{F(x)(1-F(x))} f(x) dx &amp; -n-\\frac{1}{n} \\sum_{i=1}^n (2i-1) \\log\\left(F_i^*(1-F_{n+1-i})\\right)^2 \\\\ \\hline \\end{array} \\\\ \\end{matrix} }\\] Statistik Kolmogorov-Smirnov adalah perbedaan absolut maksimum antara fungsi distribusi yang dipasang dan fungsi distribusi empiris. Alih-alih membandingkan perbedaan antara titik tunggal, statistik Cramer-von Mises mengintegrasikan perbedaan antara fungsi distribusi empiris dan pas pada seluruh rentang nilai. Statistik Anderson-Darling juga mengintegrasikan perbedaan ini pada rentang nilai, meskipun diboboti oleh kebalikan dari varian. Oleh karena itu lebih menekankan pada ekor distribusi (yaitu kapan \\(F( x )\\) atau \\(1-F(x)=S(x)\\) kecil). Contoh 4.1.8. Soal Ujian Aktuaria (dimodifikasi). Contoh pembayaran klaim adalah: \\[\\begin{array}{ccccc} 29 &amp; 64 &amp; 90 &amp; 135 &amp; 182 \\\\ \\end{array}\\] Bandingkan distribusi klaim empiris dengan distribusi eksponensial dengan rata-rata 100 dengan menghitung nilai statistik uji Kolmogorov-Smirnov. Solusi. Untuk distribusi eksponensial dengan rata-rata 100 , fungsi distribusi kumulatif adalah \\(F(x)=1-e^{-x/100}\\) . Dengan demikian, \\[\\begin{array}{ccccc} \\hline x &amp; F(x) &amp; F_n(x) &amp; F_n(x-) &amp; \\max(|F(x)-F_n(x)|,|F(x)-F_n(x-)|) \\\\ \\hline 29 &amp; 0.2517 &amp; 0.2 &amp; 0 &amp; \\max(0.0517, 0.2517) = 0.2517 \\\\ 64 &amp; 0.4727 &amp; 0.4 &amp; 0.2 &amp; \\max(0.0727, 0.2727) = 0.2727 \\\\ 90 &amp; 0.5934 &amp; 0.6 &amp; 0.4 &amp; \\max(0.0066, 0.1934) = 0.1934 \\\\ 135 &amp; 0.7408 &amp; 0.8 &amp; 0.6 &amp; \\max(0.0592, 0.1408) = 0.1408 \\\\ 182 &amp; 0.8380 &amp; 1 &amp; 0.8 &amp; \\max(0.1620, 0.0380) = 0.1620 \\\\ \\hline \\end{array}\\] Oleh karena itu, statistik uji Kolmogorov-Smirnov adalah \\[KS = \\max(0.2517, 0.2727, 0.1934, 0.1408, 0.1620) = 0.2727 .\\] 4.1.3 Nilai Awal Metode pencocokan momen dan persentil merupakan metode estimasi nonparametrik yang memberikan alternatif kemungkinan maksimum. Umumnya, kemungkinan maksimum adalah teknik yang lebih disukai karena menggunakan data secara lebih efisien. (Lihat Lampiran Bab 17 untuk definisi efisiensi yang tepat.) Namun, metode pencocokan momen dan persentil berguna karena lebih mudah diinterpretasikan dan karena itu memungkinkan aktuaris atau analis untuk menjelaskan prosedur kepada orang lain. Selain itu, prosedur estimasi numerik (misalnya jika dilakukan di R) untuk kemungkinan maksimum adalah iteratif dan membutuhkan nilai awal untuk memulai proses rekursif. Meskipun banyak masalah yang kuat untuk pemilihan nilai awal, untuk beberapa situasi kompleks, penting untuk memiliki nilai awal yang mendekati nilai optimal (tidak diketahui). Metode momen dan pencocokan persentil adalah teknik yang dapat menghasilkan perkiraan yang diinginkan tanpa investasi komputasi yang serius dan dengan demikian dapat digunakan sebagai nilai awal untuk menghitung kemungkinan maksimum. 4.1.3.1 Metode Momen Di bawah metode momen , kami mengaproksimasi momen distribusi parametrik menggunakan momen empiris (nonparametrik) yang dijelaskan di Bagian 4.1.1.1 . Kami kemudian dapat memecahkan secara aljabar untuk estimasi parameter. Contoh 4.1.9. Dana Properti. Untuk dana properti 2010, ada \\(n = 1 , 377\\) klaim individu (dalam ribuan dolar) dengan \\[m_1 = \\frac{1}{n} \\sum_{i=1}^n X_i = 26.62259 \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ m_2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 = 136154.6 .\\] Sesuaikan parameter distribusi gamma dan Pareto menggunakan metode momen. Solusi. Agar sesuai dengan distribusi gamma, kami memiliki \\(\\mu_1 = \\alpha \\theta\\) Dan \\(\\mu_2^{\\prime} = \\alpha(\\alpha+1) \\theta^2\\) . Menyamakan keduanya menghasilkan metode penaksir momen, aljabar mudah menunjukkannya \\[\\alpha = \\frac{\\mu_1^2}{\\mu_2^{\\prime}-\\mu_1^2} \\ \\ \\ \\text{and} \\ \\ \\ \\theta = \\frac{\\mu_2^{\\prime}-\\mu_1^2}{\\mu_1}.\\] Jadi, metode penduga momen adalah \\[\\begin{aligned} \\hat{\\alpha} &amp;= \\frac{26.62259^2}{136154.6-26.62259^2} = 0.005232809 \\\\ \\hat{\\theta} &amp;= \\frac{136154.6-26.62259^2}{26.62259} = 5,087.629. \\end{aligned}\\] Sebagai perbandingan, nilai kemungkinan maksimum berubah menjadi \\(\\hat{\\alpha}_{MLE} = 0.2905959\\) Dan \\(\\hat{\\theta}_{MLE} = 91.61378\\) , jadi ada perbedaan besar antara dua prosedur estimasi. Ini adalah salah satu indikasi, seperti yang telah kita lihat sebelumnya, bahwa model gamma kurang cocok. Sebaliknya, sekarang asumsikan distribusi Pareto sehingga \\(\\mu_1 = \\theta/(\\alpha -1)\\) Dan \\(\\mu_2^{\\prime} = 2\\theta^2/((\\alpha-1)(\\alpha-2) )\\) . Perhatikan bahwa ungkapan ini untuk μ′2 hanya berlaku untuk α &gt; 2 . Pertunjukan aljabar yang mudah \\[\\alpha = 1+ \\frac{\\mu_2^{\\prime}}{\\mu_2^{\\prime}-\\mu_1^2} \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ \\ \\theta = (\\alpha-1)\\mu_1.\\] Jadi, metode penduga momen adalah \\[ \\begin{aligned} \\hat{\\alpha} &amp;= 1+ \\frac{136154.6}{136154.6-26,62259^2} = 2.005233 \\\\ \\hat{\\theta} &amp;= (2.005233-1) \\cdot 26.62259 = 26.7619 \\end{aligned}\\] Nilai kemungkinan maksimum berubah menjadi \\(\\hat{\\alpha}_{MLE} = 0.9990936\\) Dan \\(\\hat{\\theta}_{MLE} = 2.2821147\\) . Sangat menarik bahwa \\(\\hat{\\alpha}_{MLE}&lt;1\\) ; untuk distribusi Pareto, ingat itu \\(α &lt; 1\\) berarti rata-ratanya tak terhingga. Ini adalah indikasi lain bahwa kumpulan data klaim properti adalah distribusi ekor panjang. Seperti contoh di atas, ada fleksibilitas dengan metode momen. Misalnya, kita dapat mencocokkan momen kedua dan ketiga alih-alih yang pertama dan kedua, menghasilkan estimator yang berbeda. Selain itu, tidak ada jaminan bahwa solusi akan ada untuk setiap masalah. Untuk data yang disensor atau terpotong, momen pencocokan dimungkinkan untuk beberapa masalah, tetapi secara umum, ini adalah skenario yang lebih sulit. Terakhir, untuk distribusi di mana momen tidak ada atau tidak terbatas, metode momen tidak tersedia. Sebagai alternatif, seseorang dapat menggunakan teknik pencocokan persentil. 4.1.3.2 Pencocokan Persentil Di bawah pencocokan persentil , kami memperkirakan kuantil atau persentil dari distribusi parametrik menggunakan kuantil atau persentil empiris (nonparametrik) yang dijelaskan di Bagian 4.1.1.3 . Contoh 4.1.10. Dana Properti. Untuk dana properti 2010, kami mengilustrasikan pencocokan pada kuantil. Secara khusus, distribusi Pareto secara intuitif menyenangkan karena solusi bentuk tertutup untuk kuantil. Ingatlah bahwa fungsi distribusi untuk distribusi Pareto adalah \\[F(x) = 1 - \\left(\\frac{\\theta}{x+\\theta}\\right)^{\\alpha}.\\] Aljabar mudah menunjukkan bahwa kita dapat menyatakan kuantil sebagai \\[F^{-1}(q) = \\theta \\left( (1-q)^{-1/\\alpha} -1 \\right).\\] untuk sebagian kecil q , \\(0 &lt; q&lt; 1\\). Tentukan estimasi parameter distribusi Pareto menggunakan kuantil empiris ke-25 dan ke-95. Solusi. Persentil ke-25 (kuartil pertama) ternyata adalah 0,78853 dan persentil ke-95 adalah 50.98293 (keduanya dalam ribuan dolar). Dengan dua persamaan \\[0.78853 = \\theta \\left( 1- (1-.25)^{-1/\\alpha} \\right) \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ 50.98293 = \\theta \\left( 1- (1-.75)^{-1/\\alpha} \\right)\\] dan dua yang tidak diketahui, solusinya adalah \\[\\hat{\\alpha} = 0.9412076 \\ \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ \\hat{\\theta} = 2.205617 .\\] Sehingga kesimpulannya adalah rutin numerik diperlukan untuk solusi ini karena tidak ada solusi analitik yang tersedia. Selanjutnya, ingatlah perkiraan kemungkinan maksimumadalah α^ML E= 0,9990936 Dan θ^ML E= 2,2821147 , sehingga pencocokan persentil memberikan perkiraan yang lebih baik untuk distribusi Pareto daripada metode momen. Contoh 4.1.11. Soal Ujian Aktuaria. Anda diberikan: Kerugian mengikuti distribusi loglogistik dengan fungsi distribusi kumulatif: \\[F(x) = \\frac{\\left(x/\\theta\\right)^{\\gamma}}{1+\\left(x/\\theta\\right)^{\\gamma}}\\] Contoh kerugiannya adalah: \\[\\begin{array}{ccccccccccc} 10 &amp;35 &amp;80 &amp;86 &amp;90 &amp;120 &amp;158 &amp;180 &amp;200 &amp;210 &amp;1500 \\\\ \\end{array}\\] Hitung estimasi dari \\(θ\\) dengan pencocokan persentil, menggunakan perkiraan persentil ke-40 dan ke-80 yang dihaluskan secara empiris. Solusi. Dengan 11 pengamatan, kami memiliki \\(j=\\lfloor(n+1)q\\rfloor = \\lfloor 12(0.4) \\rfloor = \\lfloor 4.8\\rfloor=4\\). Dengan interpolasi, perkiraan persentil ke-40 yang dihaluskan secara empiris adalah \\(\\hat{\\pi}_{0.4} = (1-h) X_{(j)} + h X_{(j+1)} = 0.2(86)+0.8(90)=89.2\\). Demikian pula, untuk perkiraan persentil yang dihaluskan secara empiris ke-80, kami memiliki \\(12 ( 0,8 ) = 9,6\\) jadi perkiraannya \\(\\hat{\\pi}_{0.8} = 0.4(200)+0.6(210)=206\\). Dengan menggunakan distribusi kumulatif loglogistik, kita perlu menyelesaikan dua persamaan berikut untuk parameter \\({\\hat{\\theta}}\\) Dan \\({\\hat{\\gamma}}\\) : \\[0.4=\\frac{(89.2/{\\hat{\\theta}})^{\\hat{\\gamma}}}{1+(89.2/{\\hat{\\theta}})^{\\hat{\\gamma}}} \\ \\ \\ \\text{and} \\ \\ \\ \\ 0.8=\\frac{(206/{\\hat{\\theta}})^{\\hat{\\gamma}}}{1+(206/{\\hat{\\theta}})^{\\hat{\\gamma}}} .\\] Pemecahan untuk setiap ekspresi kurung memberi \\(\\frac{2}{3}=(89.2/\\theta)^{\\hat{\\gamma}}\\) Dan \\(4=(206/{\\hat{\\theta}})^{\\hat{\\gamma}}\\) . Mengambil rasio persamaan kedua dengan yang pertama memberi \\(6=(206/89.2)^{\\hat{\\gamma}}\\Rightarrow {\\hat{\\gamma}}=\\frac{\\log(6)}{\\log(206/89.2)} = 2.1407\\). Kemudian \\(4^{1/2.1407}=206/{\\hat{\\theta}} \\Rightarrow {\\hat{\\theta}}=107.8\\). Seperti metode momen, pencocokan persentil hampir terlalu fleksibel dalam arti bahwa estimator dapat bervariasi tergantung pada persentil berbeda yang dipilih. Misalnya, seorang aktuaris dapat menggunakan estimasi pada persentil ke-25 dan ke-95 sedangkan yang lain menggunakan persentil ke-20 dan ke-80. Secara umum estimasi parameter akan berbeda dan tidak ada alasan kuat untuk memilih salah satu dari yang lain. Seperti halnya metode momen, pencocokan persentil menarik karena memberikan teknik yang dapat diterapkan dengan mudah dalam situasi tertentu dan memiliki dasar intuitif. Meskipun sebagian besar aplikasi aktuaria menggunakan estimator kemungkinan maksimum, akan lebih mudah untuk memiliki pendekatan alternatif seperti metode momen dan pencocokan persentil yang tersedia. "],["aggregate-loss-models.html", "Bab 5 Aggregate Loss Models 5.1 Introduction 5.2 Moments and Distribution 5.3 5.4 Menghitung Distribusi Klaim Agregat", " Bab 5 Aggregate Loss Models 5.1 Introduction Sub bab ini membahas mengenai pembangunan model probabilitas untuk menggambarkan klaim agregat oleh sistem asuransi yang terjadi dalam periode waktu tertentu. Sistem asuransi dapat berupa polis tunggal, kontrak asuransi kelompok, lini bisnis , atau seluruh buku bisnis perusahaan asuransi. Dalam bab ini, klaim agregat mengacu pada jumlah klaim dari portofolio kontrak asuransi. Pertimbangkan portofolio asuransi dari \\(N\\) kontrak individu, dan \\(S\\) menunjukkan kerugian agregat portofolio dalam jangka waktu tertentu. Ada dua pendekatan untuk memodelkan kerugian agregat \\(S\\) , model risiko individu dan model risiko kolektif. Model risiko individu menekankan kerugian dari masing-masing kontrak individu dan mewakili kerugian agregat sebagai: \\[S_n=X_1 +X_2 +\\cdots+X_n,\\] Di mana \\(X_i~(i=1,\\ldots,n)\\) diinterpretasikan sebagai jumlah kerugian dari \\(X_i\\) kontrak. \\(N\\) menunjukkan jumlah kontrak dalam portofolio dan dengan demikian merupakan angka tetap daripada variabel acak. Untuk model risiko individu, biasanya diasumsikan \\(X_i\\) ini independen. Karena fitur kontrak yang berbeda seperti cakupan dan paparan , \\(X_i\\) belum tentu terdistribusi secara identik. Fitur penting dari distribusi masing-masing \\(X_i\\) adalah massa probabilitas pada nol yang sesuai dengan peristiwa tidak adanya klai Model risiko kolektif mewakili kerugian agregat dalam hal distribusi frekuensi dan distribusi keparahan: \\[S_N=X_1 +X_2 + \\cdots + X_N .\\] Sejumlah klaim acak \\(N\\) yang dapat mewakili baik jumlah kerugian atau jumlah pembayaran. Sebaliknya, dalam model risiko individual biasanya menggunakan sejumlah kontrak tetap \\(N\\).\\(X_1, X_2, \\ldots, X_N\\) sebagai representasi dari jumlah masing-masing kerugian. Setiap kerugian mungkin atau mungkin tidak sesuai dengan kontrak unik. Misalnya, mungkin ada banyak klaim yang timbul dari satu kontrak. Itu wajar untuk dipikirkan \\(X_i&gt;0\\) karena jika \\(X_i=0\\) maka tidak ada klaim yang terjadi. Biasanya kita menganggap bahwa kondisional pada \\(X_{1},X_{2},\\ldots ,X_{n}\\) adalah iid variabel acak. Distribusi dari N dikenal sebagai distribusi frekuensi , dan distribusi umum dari \\(X\\) dikenal sebagai distribusi keparahan . Dengan berasumsi \\(N\\) Dan \\(X\\) sendiri. Dengan model risiko kolektif, sehingga dapat menguraikan kerugian agregat menjadi frekuensi \\(( N )\\) proses dan tingkat keparahan \\(( X )\\) model. Fleksibilitas ini memungkinkan analis untuk mengomentari dua komponen terpisah ini. Misalnya, pertumbuhan penjualan karena standar penjaminan emisi yang lebih rendah dapat menyebabkan frekuensi kerugian yang lebih tinggi tetapi mungkin tidak memengaruhi keparahan. Demikian pula, inflasi atau kekuatan ekonomi lainnya dapat berdampak pada keparahan tetapi tidak pada frekuensi. 5.2 Moments and Distribution Jadi model risiko keleksif \\(SN=X_1+...+X_N\\) dan tidak bergantung pada N Misalkan \\(μ = E(X_i)\\) dan \\(σ^2=Var(X_i)\\) untuk semua \\(i\\) Dengan demikian, bersyarat pada N kita memiliki ekspektasi jumlah adalah jumlah ekspektasi dan varians. \\[ \\begin{aligned} {\\rm E}(S|N) &amp;= {\\rm E}(X_1 + \\cdots + X_N|N) = \\mu N \\\\ {\\rm Var}(S|N) &amp;= {\\rm Var}(X_1 + \\cdots + X_N|N) = \\sigma^2 N. \\end{aligned} \\] Dengan menggunakan hukum ekspektasi berulang,rata-rata kerugian agregat adalah \\({\\rm E}(S_N)={\\rm E}_N[{\\rm E}_S(S|N)] = {\\rm E}_N(N\\mu) = \\mu ~{\\rm E}(N).\\) Dengan menggunakan hukum varians total, varians dari kerugian agregat adalah \\[ \\begin{aligned} {\\rm Var}(S_N) &amp;= {\\rm E}_N[{\\rm Var}(S_N|N)] + {\\rm Var}_N[{\\rm E}(S_N|N)] \\\\ &amp;= \\mathrm{E}_N \\left[ \\sigma^2 N \\right] + \\mathrm{Var}_N\\left[ \\mu N \\right] \\\\ &amp;=\\sigma^2~{\\rm E}[N] + \\mu^2~ {\\rm Var}[N] . \\end{aligned} \\] Kasus Khusus: Frekuensi Berdistribusi Poisson. Jika \\(N∼Poi(λ)\\) maka \\[ \\begin{aligned} \\mathrm{E}(N) &amp;= \\mathrm{Var}(N) = \\lambda\\\\ \\mathrm{E}(S_N) &amp;= \\lambda ~\\mathrm{E}(X)\\\\ \\mathrm{Var}(S_N) &amp;= \\lambda (\\sigma^2 + \\mu^2) = \\lambda ~\\mathrm{E} (X^2). \\end{aligned} \\] 5.2.0.1 5.3.1 Actuarial Exam Question Jumlah kecelakaan mengikuti distribusi Poisson dengan rata-rata 12. Setiap kecelakaan menghasilkan 1, 2, atau 3 penuntut dengan probabilitas masing-masing 1/2, 1/3, dan 1/6. Hitunglah varians dalam jumlah total penuntut. JAWABAN \\[ \\begin{aligned} &amp; \\mathrm{E}(X^2) = 1^2 \\left( \\frac{1}{2}\\right) + 2^2\\left(\\frac{1}{3} \\right) + 3^2\\left(\\frac{1}{6}\\right) = \\frac{10}{3} \\\\ \\Rightarrow &amp;\\mathrm{Var}(S_N) = \\lambda \\ \\mathrm{E}(X^2) = 12\\left(\\frac{10}{3}\\right) = 40 . \\end{aligned} \\] Sebagai alternatif, Dapat menggunakan pendekatan umum, \\(\\mathrm{Var}(S_N) = \\sigma^2 \\mathrm{E}(N) + \\mu^2 \\mathrm{Var}(N)\\), Dimana \\[ \\begin{aligned} \\mathrm{E}(N) &amp;= \\mathrm{Var}(N) = 12 \\\\ \\mu &amp;= \\mathrm{E}(X) = 1\\left(\\frac{1}{2}\\right) + 2\\left(\\frac{1}{3}\\right) + 3\\left(\\frac{1}{6}\\right) = \\frac{5}{3} \\\\ \\sigma^2 &amp;= \\mathrm{E}(X^2) - [\\mathrm{E}(X)]^2 = \\frac{10}{3} - \\frac{25}{9} = \\frac{5}{9} \\\\ \\Rightarrow \\ \\mathrm{Var}(S_N) &amp;= \\left(\\frac{5}{9}\\right)\\left(12\\right) + \\left(\\frac{5}{3}\\right)^2\\left(12\\right) = 40 . \\end{aligned} \\] Secara umum, momen-momen SN dapat diturunkan dari fungsi pembangkit momen (mgf). Karena \\(X_i\\) adalah iid, dapat dinyatakan mgf dari X sebagai \\(M_{X(t)}= E(e^{tX})\\) . Dengan menggunakan hukum ekspektasi yang diiterasi, mgf dari \\(S_N\\) adalah \\[ \\begin{aligned} M_{S_N}(t) &amp;= \\mathrm{E}(e^{t S_N})=\\mathrm{E}_N[\\mathrm{E}(e^{tS_N}|N)]\\\\ &amp;= \\mathrm{E}_N \\left[ ~\\mathrm{E}\\left( e^{t(X_1+\\cdots+X_N)}\\right) ~\\right] = \\mathrm{E}_N \\left[ \\mathrm{E}(e^{tX_1})\\cdots\\mathrm{E}(e^{tX_N}) \\right] ~~ \\text{since } X_i \\text{&#39;s are independent} \\\\ &amp;= \\mathrm{E}N[~(M{X}(t))^N~] . \\end{aligned} \\] Lalu dapat melihat fungsi pembangkit probabilitas(pgf) dari N adalah \\(P_N(z)= E(Z^N)\\). Dengan menyatakan \\(M_X(t)=z\\), lalu mengganti ke dalam ekspresi untuk mgf dari SN di atas, maka diperoleh \\[ \\begin{aligned} M_{S_N}(t) = \\mathrm{E~}(z^N) = P_{N}(z) = P_{N}[M_{X}(t)]. \\end{aligned} \\] Demikian pula, jika \\(S_N\\) merupakan diskrit, dapat menunjukkan juga pgf dari \\(S_N\\) adalah \\[ \\begin{aligned} P_{S_N}(z) = P_{N}[P_{X}(z)] . \\end{aligned} \\] Untuk mendapatkan \\(E(S_N) = M′S_N(0)\\) dapat menggunakan aturan rantai: \\(M_{S_N}&#39;(t) = \\frac{\\partial}{\\partial t} P_{N}(M_{X}(t)) = P_{N}&#39;(M_{X}(t)) M_{X}&#39;(t)\\\\\\) Lalu Memanggil \\(M_{X}(0) = 1, M_{X}&#39;(0) = \\mathrm{E}(X) = \\mu, P_{N}&#39;(1) = \\mathrm{E}(N)\\) Jadi, \\(\\mathrm{E}(S_N) = M_{S_N}&#39;(0) = P_{N}&#39;(M_{X}(0)) M_{X}&#39;(0) = \\mu {\\rm E}(N) .\\) Demikian pula, dapat menggunakan relasi \\(E(S^2_N) = M′′_{S_N}(0)\\) untuk mendapatkan \\(\\mathrm{Var}(S_N) = \\sigma^2 \\mathrm{E}(N) + \\mu^2 \\mathrm{Var}(N).\\) Special Case. Poisson Frequency. Misalkan \\(N∼Poi(λ)\\) dengan demikian, pgf dari \\(N\\) adalah \\(P_N(z) = e^{λ(z-1)}\\) dan mgf dari \\(S_N\\) adalah \\[ \\begin{aligned} M_{S_N}(t) &amp;= P_N[M_X(t)] = e^{\\lambda(M_{X}(t) - 1)}. \\end{aligned} \\] Mengambil hasil turunan \\[ \\begin{aligned} M_{S_N}&#39;(t) &amp;= e^{\\lambda(M_{X}(t) - 1)}~ \\lambda~ M_{X}&#39;(t) = M_{S_N}(t) ~\\lambda ~M_{X}&#39;(t)\\\\ M_{S_N}&#39;&#39;(t) &amp;= M_{S_N}(t) ~\\lambda~ M_{X}&#39;&#39;(t) + [~M_{S_N}(t)~\\lambda~ M_{X}&#39;(t)~] ~\\lambda~ M_{X}&#39;(t) . \\end{aligned} \\] Mengevaluasi hal ini pada t = 0 menghasilkan \\[ \\begin{aligned} \\mathrm{E}(S_N) &amp;= M_{S_N}&#39;(0) = \\lambda \\mathrm{E}(X) = \\lambda \\mu \\end{aligned} \\] Lalu \\[ \\begin{aligned} M_{S_N}&#39;&#39;(0) &amp;= \\lambda \\mathrm{E}(X^2) + \\lambda^2 \\mu^2\\\\ \\Rightarrow \\mathrm{Var}(S_N) &amp;= \\lambda \\mathrm{E}(X^2) + \\lambda^2 \\mu^2 - (\\lambda \\mu)^2 = \\lambda~ \\mathrm{E}(X^2). \\end{aligned} \\] 5.2.0.2 Example 5.3.2. Actuarial Exam Question Dimisalkan produser acara kuis televisi yang memberikan hadiah uang tunai. Jumlah hadiah(N) dan jumlah hadiah(X) memiliki distribusi sebagai berikut: \\[{\\small \\begin{matrix} \\begin{array}{ccccc}\\hline n &amp; \\Pr(N=n) &amp; &amp; x &amp; \\Pr(X=x)\\\\ \\hline 1 &amp; 0.8 &amp; &amp; 0 &amp; 0.2 \\\\ 2 &amp; 0.2 &amp; &amp; 100 &amp; 0.7 \\\\ &amp; &amp; &amp; 1000 &amp; 0.1\\\\\\hline \\end{array} \\end{matrix} }\\] Sehingga Anggaran untuk hadiah sama dengan jumlah hadiah uang tunai yang diharapkan ditambah dengan deviasi standar dari jumlah hadiah uang tunai. Hitung anggaran! JAWABAN Diperlukan untuk menghitung rata-rata dan standar deviasi dari agregat (jumlah) hadiah uang tunai. Momen-momen dari distribusi frekuensi N adalah \\[ \\begin{aligned} \\mathrm{E}(N) &amp;= 1 (0.8) + 2 (0.2) =1.2\\\\ \\mathrm{E}(N^2) &amp;= 1^2 (0.8) + 2^2 (0.2) =1.6\\\\ \\mathrm{Var}(N) &amp;= \\mathrm{E}(N^2) - \\left[ \\mathrm{E}(N) \\right]^2= 0.16 . \\end{aligned} \\] Momen-momen dari distribusi tingkat keparahan X adalah \\[ \\begin{aligned} \\mathrm{E}(X) &amp;= 0 (0.2) + 100 (0.7) + 1000 (0.1) = 170 = \\mu\\\\ \\mathrm{E}(X^2) &amp;= 0^2 (0.2) + 100^2 (0.7) + 1000^2 (0.1) = 107,000\\\\ \\mathrm{Var}(X) &amp;= \\mathrm{E}(X^2) - \\left[ \\mathrm{E}(X) \\right]^2=78,100 = \\sigma^2 . \\end{aligned} \\] Dengan demikian, rata-rata dan varians dari keseluruhan hadiah uang tunai adalah \\[ \\begin{aligned} \\mathrm{E}(S_N) &amp;= \\mu \\mathrm{E}(N) = 170 (1.2) = 204 \\\\ \\mathrm{Var}(S_N) &amp;= \\sigma^2 \\mathrm{E}(N) + \\mu^2 \\mathrm{Var}(N)\\\\ &amp;= 78,100 (1.2) + 170^2 (0.16) = 98,344 . \\end{aligned} \\] Sehingga anggaran yang dibutuhkan sebagai berikut \\[ \\begin{aligned} Budget &amp;= \\mathrm{E}(S_N) + \\sqrt{\\mathrm{Var}(S_N)} \\\\ &amp;= 204 + \\sqrt{98,344} = 517.60 . \\end{aligned} \\] Distribusi \\(S_N\\) disebut distribusi majemuk, dan dapat diturunkan berdasarkan konvolusi \\(F_X\\) sebagai berikut: \\[ \\begin{aligned} F_{S_N}(s) &amp;= \\Pr \\left(X_1 + \\cdots + X_N \\le s \\right) \\\\ &amp;= \\mathrm{E} \\left[ \\Pr \\left(X_1 + \\cdots + X_N \\le s|N=n \\right) \\right]\\\\ &amp;= \\mathrm{E} \\left[ F_{X}^{\\ast N}(s) \\right] \\\\ &amp;= p_0 + \\sum_{n=1}^{\\infty }p_n F_{X}^{\\ast n}(s) . \\end{aligned} \\] Ketika \\(E(N)\\) dan \\(Var(N)\\) diketahui, kita juga dapat menggunakan suatu jenis teorema limit pusat untuk mengestimasi distribusi \\(S_N\\) seperti pada model risiko individu. Yaitu, \\(\\frac{S_N - \\mathrm{E}(S_N)}{\\sqrt{\\mathrm{Var}(S_N)}}\\) kira-kira mengikuti distribusi normal standar \\(N(0,1)\\) . Dari jenis teorema limit pusat ini, aproksimasi bekerja dengan baik jika \\(E[N]\\) cukup besar. 5.2.1 5.3.2 Stop-loss Insurance Modifikasi pertanggungan pada tingkat polis perorangan Pertanggungan atas kerugian agregat \\(S_N\\) , yang dikenakan sebuah deductible \\(d\\) disebut dengan . Nilai yang diharapkan dari jumlah kerugian agregat yang melebihi deductible, \\[ \\begin{eqnarray*} \\mathrm{E}[(S-d)_+] \\end{eqnarray*} \\] dikenal sebagai premi stop-loss bersih. Untuk menghitung premi stop-loss neto, kita memiliki \\[ \\begin{eqnarray*} \\mathrm{E}(S_N-d)_+ &amp;=&amp; \\left\\{\\begin{array}{ll} \\int_{d}^{\\infty}(s-d) f_{S_N}(s) ds&amp; \\text{for continuous } S_N\\\\ \\sum_{s&gt;d}(s-d) f_{S_N}(s) &amp; \\text{for discrete } S_N\\\\ \\end{array}\\right.\\\\ &amp;=&amp; \\mathrm{E}(S_N) - \\mathrm{E}(S_N\\wedge d)\\\\ \\end{eqnarray*} \\] ### * 5.3.6. Actuarial Exam Question* Dalam satu minggu tertentu, jumlah proyek yang mengharuskan Anda bekerja lembur memiliki distribusi geometris dengan \\(β = 2\\) . Untuk setiap proyek, distribusi jumlah jam lembur dalam seminggu, X adalah sebagai berikut \\[{\\small \\begin{matrix} \\begin{array}{ccc} \\hline x &amp; &amp; f(x)\\\\ \\hline 5 &amp; &amp; 0.2 \\\\ 10 &amp; &amp; 0.3 \\\\ 20 &amp; &amp; 0.5\\\\ \\hline \\end{array} \\end{matrix} } \\] Jumlah proyek dan jumlah jam lembur tidak bergantung. Anda akan dibayar untuk jam lembur yang melebihi 15 jam dalam seminggu. Hitunglah jumlah jam lembur yang akan diterima dalam seminggu. JAWABAN Jumlah proyek dalam seminggu yang membutuhkan kerja lembur memiliki distribusi \\(N∼Geo(β=2)\\) sedangkan jumlah jam kerja lembur per proyek memiliki distribusi \\(X\\) seperti yang telah dijelaskan di atas. Jumlah keseluruhan jam lembur dalam seminggu adalah SN dan oleh karena itu kita mencari \\(\\mathrm{E}(S_N-15)_+ = \\mathrm{E}(S_N) - \\mathrm{E}(S_N \\wedge 15).\\) Untuk mencari \\(\\mathrm{E}(S_N) = \\mathrm{E}(X) ~\\mathrm{E}(N)\\), maka akan didapat \\[ \\begin{aligned} &amp;\\mathrm{E}(X) = 5(0.2) + 10(0.3)+ 20(0.5)= 14 \\\\ &amp;\\mathrm{E}(N) = 2 \\\\ \\Rightarrow \\ &amp;\\mathrm{E}(S) = \\mathrm{E}(X) ~ \\mathrm{E}(N) = 14(2) = 28 . \\end{aligned} \\] Untuk mencari \\(\\mathrm{E} (S_N \\wedge 15) = 0 \\Pr (S_N=0) + 5 \\Pr(S_N=5) + 10 \\Pr(S_N=10) + 15 \\Pr(S_N \\geq 15))\\), maka akan didapat \\[ \\begin{aligned} \\Pr(S_N=0) &amp;= \\Pr(N=0) = \\frac{1}{1+\\beta} = \\frac{1}{3} \\\\ \\Pr(S_N=5) &amp;= \\Pr(X=5, \\ N=1) = 0.2 \\left(\\frac{2}{9} \\right)= \\frac{0.4}{9}\\\\ \\Pr(S_N=10) &amp;= \\Pr(X=10, \\ N=1) + \\Pr(X_1=X_2=5, N=2) \\\\ &amp;= 0.3 \\left(\\frac{2}{9} \\right) + (0.2)(0.2) \\left( \\frac{4}{27} \\right)= 0.0726 \\\\ \\Pr(S_N \\geq 15) &amp;= 1 - \\left(\\frac{1}{3} + \\frac{0.4}{9} + 0.0726 \\right) = 0.5496\\\\ \\Rightarrow \\mathrm{E}(S_N \\wedge 15) &amp;= 0 \\Pr (S_N=0) + 5 \\Pr(S_N=5) + 10 \\Pr(S_N=10) + 15 \\Pr(S_N \\geq 15) \\\\ &amp;= 0 \\left( \\frac{1}{3} \\right) + 5 \\left( \\frac{0.4}{9} \\right) + 10 (0.0726) + 15 (0.5496) = 9.193 .\\\\ \\end{aligned} \\] Oleh Karena itu: \\[ \\begin{aligned} \\mathrm{E}(S_N-15)_+ &amp;= \\mathrm{E}(S_N) - \\mathrm{E}(S_N \\wedge 15) \\\\ &amp;= 28 - 9.193 = 18.807 . \\end{aligned} \\] Recursive Net Stop-Loss Premium Calculation Untuk kasus diskrit, ini dapat dihitung secara rekursif sebagai \\[ \\begin{aligned} \\mathrm{E}\\left[ \\left( S_N-(j+1)h \\right) _{+} \\right]=\\mathrm{E}\\left[ ( S_N-jh )_{+} \\right] -h \\left( 1-F_{S_N}(jh) \\right) . \\end{aligned} \\] Ini mengasumsikan bahwa dukungan \\(S_N\\) tersebar secara merata di atas unit-unit h. Untuk menetapkan ini, kita mengasumsikan bahwa \\(h = 1\\) Kita memiliki \\[ \\begin{aligned} \\mathrm{E}\\left[ \\left( S_N-(j+1) \\right) _{+} \\right] &amp;=\\mathrm{E}(S_N) - \\mathrm{E}[S_N\\wedge (j+1)] \\ ,\\ \\text{ and } \\\\ \\mathrm{E}\\left[ \\left( S_N - j \\right)_+ \\right] &amp;=\\mathrm{E}(S_N) - \\mathrm{E}[S_N\\wedge j] \\end{aligned} \\] Jadi, \\[ \\begin{aligned} \\mathrm{E}\\left[ \\left(S_N-(j+1) \\right) _{+}\\right] - \\mathrm{E}\\left[ ( S_N-j )_{+} \\right] &amp;= \\left\\{\\mathrm{E}(S_N) - \\mathrm{E}(S_N\\wedge (j+1)) \\right\\} - \\left\\{\\mathrm{E}(S_N) - \\mathrm{E}(S_N\\wedge j) \\right\\} \\\\ &amp;= \\mathrm{E}\\left(S_N \\wedge j \\right) - \\mathrm{E}\\left[ S \\wedge (j+1) \\right] \\end{aligned} \\] Maka kita dapat menulis \\[ \\begin{aligned} \\mathrm{E}\\left[S_N\\wedge (j+1)\\right] &amp;= \\sum_{x=0}^{j}xf_{S_N}(x) + (j+1)~\\Pr(S_N \\ge j+1) \\\\ &amp;= \\sum_{x=0}^{j-1}x f_{S_N}(x) + j~\\Pr(S_N=j) + (j+1)~\\Pr(S_N \\ge j+1) \\end{aligned} \\] Demikian pula, \\[ \\begin{aligned} \\mathrm{E}(S_N\\wedge j) = \\sum_{x=0}^{j-1}xf_{S_N}(x) + j~\\Pr(S_N\\ge j) \\end{aligned} \\] Dengan ekspresi ini, kami memiliki \\[ \\begin{aligned} &amp; \\mathrm{E}\\left[ \\left( S_N-(j+1) \\right) _{+} \\right] - \\mathrm{E~}\\left[ ( S_N-j )_{+} \\right] \\\\ &amp;= \\mathrm{E}\\left(S_N \\wedge j \\right) - \\mathrm{E}\\left[ S \\wedge (j+1) \\right] \\\\ &amp;= \\left\\{ \\sum_{x=0}^{j-1}xf_{S_N}(x) + j~\\Pr(S_N\\ge j) \\right\\} - \\left\\{ \\sum_{x=0}^{j-1}x f_{S_N}(x) + j~\\Pr(S_N=j) + (j+1)~\\Pr(S_N \\ge j+1) \\right\\} \\\\ &amp;= j~\\left[\\Pr(S_N \\geq j) - \\Pr(S_N=j) \\right]- (j+1)~\\Pr(S_N \\ge j+1) \\\\ &amp;= j~\\Pr( S_N &gt; j) - (j+1)~\\Pr(S_N \\ge j+1) ~~~~ \\text{ (note } \\Pr(S_N &gt; j) = \\Pr(S_N \\geq j+1) \\text{)} \\\\ &amp;= -\\Pr(S_N\\ge j+1) = -\\left[1 - F_{S_N}(j)\\right], \\end{aligned} \\] sesuai kebutuhan. 5.2.2 5.3.7. Actuarial Exam Question - Continued Ingatlah bahwa tujuan dari pertanyaan ini adalah untuk menghitung \\(E(S_N-15)_+\\) . Perhatikan bahwa dukungan dari \\(S_N\\) berjarak sama dengan satuan 5, sehingga pertanyaan ini juga dapat dikerjakan secara rekursif, menggunakan ekspresi di atas dengan langkah-langkah \\(h=5\\) : Step 1: \\[ \\begin{aligned} \\mathrm{E~}(S_N-5)_+ &amp;= \\mathrm{E}(S_N) - 5 [1-\\Pr(S_N \\leq 0) ]\\\\ %\\Pr (S_N\\geq 5) \\\\ &amp;= 28 - 5 \\left(1 - \\frac{1}{3}\\right) = \\frac{74}{3}=24.6667 . \\end{aligned} \\] Step 2: \\[ \\begin{aligned} \\mathrm{E~}(S_N-10)_+ &amp;= \\mathrm{E~}(S_N-5)_+ - 5 [1-\\Pr(S_N \\leq 5)]\\\\ %\\Pr (S_N\\ge 10) \\\\ &amp;= \\frac{74}{3} - 5\\left( 1 - \\frac{1}{3} - \\frac{0.4}{9}\\right) = 21.555 . \\end{aligned} \\] Step 3: \\[ \\begin{aligned} \\mathrm{E~}(S_N-15)_+ &amp;= \\mathrm{E~}(S_N-10)_+ - 5 [1-\\Pr(S_N \\leq 10)] \\\\ %\\Pr (S_N\\ge 15) \\\\ &amp;= \\mathrm{E~}(S_N-10)_+ - 5\\Pr (S_N\\ge 15) \\\\ &amp;= 21.555 - 5 (0.5496) = 18.807 . \\end{aligned} \\] 5.2.3 5.3.4 Analytic Results Terdapat beberapa kombinasi distribusi frekuensi klaim dan tingkat keparahan yang menghasilkan distribusi yang mudah dihitung untuk kerugian agregat. Bagian ini memberikan beberapa contoh sederhana. Meskipun contoh-contoh ini mudah dihitung, namun pada umumnya terlalu sederhana untuk digunakan dalam praktik. 5.2.3.1 5.3.8 Example Salah satunya adalah ekspresi bentuk tertutup untuk distribusi kerugian agregat dengan mengasumsikan distribusi frekuensi geometris dan distribusi tingkat keparahan eksponensial. Asumsikan bahwa jumlah klaim \\(N\\) adalah geometrik dengan rata-rata \\(E(N)=β\\) dan jumlah klaim \\(X\\) adalah eksponensial dengan \\(E(X)=θ\\) .Dapat diingat bahwa pgf dari N dan pgf dari X adalah: \\[ \\begin{aligned} P_N (z) &amp;=\\frac{1}{1- \\beta (z-1)}\\\\ M_{X}(t) &amp;=\\frac{1}{1-\\theta t} . \\end{aligned} \\] Dengan demikian, mgf dari kerugian agregat \\(S_N\\) dapat dinyatakan dengan dua cara \\[ \\begin{eqnarray} M_{S_N}(t) &amp;=&amp; P_N [M_{X}(t)] = \\frac{1}{1 - \\beta \\left( \\frac{1}{1-\\theta t} - 1\\right)} \\nonumber\\\\ &amp;=&amp; 1+ \\frac{\\beta}{1+\\beta} \\left([1-\\theta(1+\\beta)t]^{-1}-1 \\right)\\\\ &amp;=&amp; \\frac{1}{1+\\beta}(1) +\\frac{\\beta}{1+\\beta} \\left( \\frac{1}{1-\\theta (1+\\beta)t}\\right) . \\end{eqnarray} \\] Sehingga, \\(S_N\\) ekuivalen dengan distribusi majemuk \\(S_N=X^∗_1+⋯+X^∗_N∗\\) dengan \\(N^∗\\) adalah Bernoulli dengan rata-rata \\(β/(1+β)\\) dan \\(X^∗\\) adalah eksponensial dengan mean \\(θ(1+β)\\). Untuk melihat hal ini, kita periksa mgf dari S : \\[ \\begin{aligned} M_{S_N}(t) = P_N [M_{X}(t)] = P_{N^{*}} [M_{X^{*}}(t)], \\end{aligned} \\] Dimana, \\[ \\begin{aligned} P_{N^*} (z) &amp;=1+ \\frac{\\beta}{1+ \\beta} (z-1),\\\\ M_{X^*} (t) &amp;=\\frac{1}{1- {{\\theta(1+\\beta)}} t}. \\end{aligned} \\] \\(S_N\\) juga ekuivalen dengan campuran dua titik dari 0 dan \\(X^∗\\). Secara khusus, \\[ \\begin{array}{cl} S_N &amp;= \\left\\{ \\begin{array}{cl} 0 &amp; {\\rm with~ probability ~Pr}(N^*=0) = 1/(1+\\beta) \\\\ Y^{*} &amp; {\\rm with~ probability ~Pr}(N^*=1) = \\beta/(1+\\beta) . \\end{array} \\right. \\end{array} \\] Fungsi distribusi \\(S_N\\) dalah \\[ \\begin{eqnarray*} \\Pr(S_N=0) &amp;=&amp; \\frac{1}{1+\\beta}\\\\ \\Pr(S_N&gt;s) &amp;=&amp; \\Pr(X^*&gt;s) =\\frac{\\beta}{1+\\beta} \\exp\\left( -\\frac{s}{ \\theta (1+\\beta)}\\right) \\end{eqnarray*} \\] dengan pdf untuk \\(s&gt;0\\) \\[ \\begin{eqnarray*} f_{S_N}(s) = \\frac{\\beta}{\\theta (1+\\beta)^2}\\exp\\left( -\\frac{s}{ \\theta (1+\\beta)}\\right) . \\end{eqnarray*} \\] 5.2.4 5.3.4 Tweedie Distribution Pada bagian ini, kita akan membahas distribusi gabungan tertentu di mana jumlah klaim memiliki distribusi Poisson dan jumlah klaim memiliki distribusi gamma. Spesifikasi ini menghasilkan apa yang dikenal sebagai distribusi Tweedie. Distribusi Tweedie memiliki probabilitas massa pada nol dan komponen kontinu untuk nilai positif. Karena fitur ini, distribusi ini banyak digunakan dalam pemodelan klaim asuransi, di mana massa nol ditafsirkan sebagai tidak ada klaim dan komponen positif sebagai jumlah klaim. Secara khusus, pertimbangkan model risiko kolektif \\(S_N = X_1 + ⋯ + X_N\\). Dengan menganggap bahwa N memiliki distribusi Poisson dengan mean \\(λ\\) dan masing-masing \\(X_i\\) memiliki distribusi gamma dengan parameter bentuk \\(α\\) dan parameter skala \\(γ\\) . Distribusi Tweedie diturunkan sebagai jumlah Poisson dari variabel gamma. Untuk memahami distribusi \\(S_N\\) pertama-tama kita akan melihat probabilitas massa pada nilai nol. Kerugian agregat adalah nol ketika tidak ada klaim yang terjadi, yaitu \\({\\rm Pr}(S_N=0)= {\\rm Pr}(N=0)=e^{-\\lambda}.\\) Selain itu, perhatikan bahwa \\(S_N\\) bersyarat pada N = n yang dinotasikan dengan \\(S_n = X_1 + ⋯ + X_n\\) mengikuti distribusi gamma dengan bentuk \\(nα\\) dan skala \\(γ\\) . Dengan demikian, untuk \\(s&gt;0\\) densitas dari distribusi Tweedie dapat dihitung sebagai \\[ \\begin{aligned} f_{S_N}(s)&amp;=\\sum_{n=1}^{\\infty} p_n f_{S_n}(s)\\\\ &amp;=\\sum_{n=1}^{\\infty}e^{-\\lambda}\\frac{(\\lambda)^n}{n!}\\frac{\\gamma^{na}}{\\Gamma(n\\alpha)}s^{n\\alpha-1}e^{-s\\gamma} . \\end{aligned} \\] Dengan demikian, distribusi Tweedie dapat dianggap sebagai campuran antara distribusi nol dan distribusi bernilai positif, yang membuatnya menjadi alat yang mudah digunakan untuk memodelkan klaim asuransi dan untuk menghitung premi murni. Rata-rata dan varians dari model Poisson gabungan Tweedie adalah: \\({\\rm E} (S_N)=\\lambda\\frac{\\alpha}{\\gamma}~~~~{\\rm and}~~~~{\\rm Var} (S_N)=\\lambda\\frac{\\alpha(1+\\alpha)}{\\gamma^2}.\\) Sebagai fitur penting lainnya, distribusi Tweedie adalah kasus khusus dari model dispersi eksponensial, sebuah kelas model yang digunakan untuk menggambarkan komponen acak dalam model linier umum. Untuk melihat hal ini, kami mempertimbangkan reparameterisasi berikut: \\[ \\begin{equation*} \\lambda=\\frac{\\mu^{2-p}}{\\phi(2-p)},~~~~\\alpha=\\frac{2-p}{p-1},~~~~\\frac{1}{\\gamma}=\\phi(p-1)\\mu^{p-1} . \\end{equation*} \\] Dengan hubungan di atas, kita dapat menunjukkan bahwa distribusi \\(S_N\\) adalah \\(f_{S_N}(s)=\\exp\\left[\\frac{1}{\\phi}\\left(\\frac{-s}{(p-1)\\mu^{p-1}}-\\frac{\\mu^{2-p}}{2-p}\\right)+C(s;\\phi)\\right]\\) Dimana \\[ C(s;\\phi)=\\left\\{\\begin{array}{ll} \\displaystyle 0 &amp; {\\rm if}~ s=0 \\\\ \\displaystyle \\log \\sum\\limits_{n \\ge 1} \\left\\{\\frac{(1/\\phi)^{1/(p-1)}s^{(2-p)/(p-1)}}{(2-p)(p-1)^{(2-p)/(p-1)}}\\right\\}^{n}\\frac{1}{n!~\\Gamma[n(2-p)/(p-1)]s} &amp; {\\rm if}~ s&gt;0 . \\end{array}\\right. \\] Oleh karena itu, distribusi \\(S_N\\) termasuk ke dalam keluarga eksponensial dengan parameter \\(μ\\) , \\(ϕ\\) , dan \\(1&lt;p&lt;2\\) , dan kita memiliki \\({\\rm E} (S_N)=\\mu~~~~{\\rm and}~~~~{\\rm Var} (S_N)=\\phi\\mu^{p}.\\) Hal ini memungkinkan kita untuk menggunakan distribusi Tweedie dengan model linear umum untuk memodelkan klaim. Perlu juga disebutkan dua kasus pembatas dari model Tweedie: \\(p→1\\) menghasilkan distribusi Poisson dan \\(p → 2\\) menghasilkan distribusi gamma. Dengan demikian, model Tweedie mengakomodasi situasi di antara distribusi gamma dan Poisson, yang secara intuitif masuk akal karena merupakan jumlah Poisson dari variabel acak gamma. ======= 5.3 5.4 Menghitung Distribusi Klaim Agregat Bagian ini membahas dua pendekatan praktis untuk menghitung distribusi kerugian agregat, yaitu metode rekursif dan simulasi. 5.3.1 metode rekursif penggunaan metode rekursif untuk membangun model majemuk dengan komponen frekuensi \\(N\\) yang termasuk dalam kelas \\((a,b,0)\\) atau \\((a,b,1)\\), dan komponen tingkat keparahan \\(X\\) yang memiliki distribusi diskrit. Namun, jika distribusi tingkat keparahan \\(X\\) kontinu, praktik yang umum dilakukan adalah mendiskritkan distribusinya terlebih dahulu agar metode rekursif dapat diterapkan. Dalam hal ini, diasumsikan bahwa N termasuk dalam kelas \\((a,b,1)\\), sehingga nilai probabilitas \\(N\\) pada waktu \\(k\\) dinyatakan sebagai \\(pk = (a + bk) pk-1\\). Selanjutnya, diasumsikan bahwa support (nilai yang mungkin) dari \\(X\\) terbatas pada \\({0,1,...,m}\\), dan distribusinya diskrit. Oleh karena itu, fungsi probabilitas dari \\(S_N\\) dapat dinyatakan dalam \\[\\begin{aligned} f_{S_N}(s)&amp;=\\Pr (S_N=s) \\\\ &amp;=\\frac{1}{1-af_{X}(0)}\\left\\{ \\left[ p_1 -(a+b)p_{0}\\right] f_X (s)+\\sum_{x=1}^{s\\wedge m}\\left( a+\\frac{bx}{s} \\right) f_X (x)f_{S_N}(s-x)\\right\\}. \\end{aligned}\\] Jika \\(N\\) berada dalam kelas \\((a,b,0)\\) maka \\(p1 = (a + b)p0\\) dan seterusnya \\[\\begin{align*} f_{S_N}(s)=\\frac{1}{1-af_X (0)}\\left\\{ \\sum_{x=1}^{s\\wedge m}\\left( a+\\frac{bx }{s}\\right) f_X (x)f_{S_N}(s-x)\\right\\}. \\end{align*}\\] karena model ARIMA yang digunakan berbeda. Persamaan tersebut hanya memperhitungkan faktor skala \\(a\\) dan \\(b\\) dan mengakumulasi probabilitas dari setiap nilai \\(x\\) dari \\(X\\) hingga mencapai nilai \\(s\\) yang diinginkan 5.3.1.1 contoh Jumlah klaim dalam suatu periode \\(N\\) memiliki distribusi geometrik dengan mean 4. Besarnya setiap klaim \\(X\\) mengikuti \\(Pr(X=x)=0.25\\), untuk \\(x=1,2,3,4\\). Jumlah klaim dan jumlah klaim bersifat independen. \\(S_N\\) adalah jumlah klaim keseluruhan pada periode tersebut. Hitunglah \\(F_{S_N}(3)\\). Solusi Distribusi tingkat keparahan \\(X\\) adalah sebagai berikut \\(f_X(x)=\\frac14\\), \\(x=1,2,3,4\\). Distribusi frekuensi \\(N\\) adalah geometris dengan rata-rata 4, yang merupakan anggota dari kelas \\((a,b,0)\\) dengan \\(b=0\\), \\(a=\\frac\\beta{1+\\beta}=\\frac45\\), dan \\(p0=\\frac1{1+\\beta}=\\frac15\\). nilai dari komponen tingkat keparahan \\(X\\) adalah \\({1,…,m=4}\\), yang bersifat diskrit dan terbatas. Dengan demikian, kita dapat menggunakan metode rekursif \\[\\begin{aligned} f_{S_N} (x) &amp;= 1 \\sum_{y=1}^{x\\wedge m} (a+0) f_X (y) f_{S_N} (x-y) \\\\ &amp;= \\frac{4}{5} \\sum_{y=1}^{x\\wedge m} f_X (y) f_{S_N} (x-y) . \\end{aligned}\\] Solusi ditemukan dengan menggunakan metode rekursif, di mana fungsi probabilitas \\(f_{S_N}(x)\\) untuk setiap nilai \\(x\\) dihitung menggunakan rumus \\(f_{S_N}(x) = \\sum_{y=1}^{x\\wedge m} (a+0) f_X(y) f_{S_N}(x-y)\\), di mana \\(m=4\\) adalah nilai maksimum dari distribusi nilai klaim \\(X\\), dan \\(a=\\frac{\\beta}{1+\\beta}=\\frac{4}{5}\\) dan \\(p_0=\\frac{1}{1+\\beta}=\\frac{1}{5}\\) adalah parameter dari distribusi frekuensi geometri yang diberikan. khususnya kita memiliki \\[\\begin{aligned} f_{S_N} (0) &amp;= \\Pr(N=0) = p_0=\\frac{1}{5}\\\\ f_{S_N} (1) &amp;= \\frac{4}{5}\\sum_{y=1}^{1} f_X (y) f_{S_N} (1-y) = \\frac{4}{5} f_X(1) f_{S_N}(0)\\\\ &amp;= \\frac{4}{5}\\left( \\frac{1}{4}\\right)\\left(\\frac{1}{5} \\right) = \\frac{1}{25}\\\\ f_{S_N} (2) &amp;= \\frac{4}{5}\\sum_{y=1}^{2} f_X (y) f_{S_N} (2-y) = \\frac{4}{5} \\left[ f_X(1)f_{S_N}(1) + f_X(2) f_{S_N}(0) \\right] \\\\ &amp;= \\frac{4}{5}\\left[ \\frac{1}{4} \\left( \\frac{1}{25} + \\frac{1}{5}\\right) \\right] = \\frac{4}{5}\\left( \\frac{6}{100}\\right) = \\frac{6}{125}\\\\ f_{S_N} (3) &amp;= \\frac{4}{5} \\left[ f_X(1) f_{S_N}(2) + f_X(2)f_{S_N}(1) + f_X(3) f_{S_N}(0) \\right]\\\\ &amp;= \\frac{4}{5}\\left[ \\frac{1}{4} \\left( \\frac{1}{25} + \\frac{1}{5} + \\frac{6}{125}\\right) \\right] = \\frac{1}{5}\\left( \\frac{5+25+6}{125}\\right) = 0.0576\\\\ \\Rightarrow \\ F_{S_N} (3) &amp;= f_{S_N} (0)+f_{S_N} (1)+f_{S_N} (2) +f_{S_N} (3) = 0.3456 . \\end{aligned}\\] Setelah menghitung nilai \\(f_{S_N}(0)\\), \\(f_{S_N}(1)\\), \\(f_{S_N}(2)\\), dan \\(f_{S_N}(3)\\), fungsi distribusi kumulatif \\(F_{S_N}(3)\\) diperoleh dengan menjumlahkan nilai-nilai tersebut. Hasil akhirnya adalah \\(F_{S_N}(3) = 0.3456\\). 5.3.2 simulasi Distribusi kerugian agregat dapat dievaluasi dengan menggunakan simulasi Monte Carlo. Untuk kerugian agregat, Simulasi Monte Carlo digunakan untuk menghasilkan sampel acak dari kerugian keseluruhan berdasarkan distribusi probabilitas yang dianggap sesuai untuk distribusi frekuensi dan tingkat keparahan klaim. gunanya adalah seseorang dapat menghitung distribusi empiris dari \\(S_N\\) dengan menggunakan sampel acak. Nilai ekspektasi dan varians dari kerugian agregat juga dapat diperkirakan dengan menggunakan rata-rata sampel dan varians sampel dari nilai simulasi. Sekarang kita rangkum prosedur simulasi untuk model kerugian agregat. Misalkan \\(m\\) adalah ukuran sampel acak yang dihasilkan dari kerugian agregat. Individual Risk Model: $S_n = X_1 + ⋯ + X_n $ misalkan \\(j=1,...,m\\) menjadi penghitung, dimulai dari \\(j = 1\\) Hitung setiap realisasi kerugian individu \\(x_{ij}\\) untuk \\(i=1,...,n\\) . Sebagai contoh, hal ini dapat dilakukan dengan menggunakan metode transformasi invers Hitung kerugian keseluruhan \\(s_j = x_{1j} + ⋯ + x_{nj}\\). terkahir Ulangi dua langkah di atas untuk \\(j=2,...,m\\) untuk mendapatkan sampel berukuran \\(m\\) dari \\(S_n\\), dengan kata lain \\({s_1,...,s_m}\\). Collective Risk Model : \\(S_n = X_1 + ... + X_n\\) misalkan \\(j=1,...,m\\) menjadi penghitung, dimulai dari \\(j = 1\\) Hitung jumlah klaim \\(n_j\\) dari distribusi frekuensi \\(N\\). Diberikan \\(n_j\\), hasilkan jumlah setiap klaim secara independen dari distribusi tingkat keparahan \\(X\\), dilambangkan dengan \\(x_{1j},...,x_{{n_j}j}\\). Hitung kerugian keseluruhan \\(s_j = x_{1j} + ⋯ + x_{{n_j}j}\\). Ulangi tiga langkah di atas untuk \\(j=2,...,m\\) untuk mendapatkan sampel berukuran \\(m\\) dari \\(S_N\\), dengan kata lain \\({s_1,...,s_m}\\) Dengan sampel acak \\(S\\), distribusi empiris dapat dihitung sebagai \\[\\begin{aligned} \\hat{F}_S(s)=\\frac{1}{m}\\sum_{i=1}^{m}I(s_i\\leq s), \\end{aligned}\\] Untuk individual risk model, kerugian keseluruhan dihitung sebagai jumlah kerugian individu yang acak, sedangkan untuk collective Risk Model, kerugian keseluruhan dihitung sebagai jumlah kerugian dari sejumlah klaim. Dalam kedua kasus, sampel acak dihasilkan dari distribusi probabilitas yang dianggap sesuai, dan kemudian distribusi empiris dari sampel tersebut dihitung untuk memperkirakan distribusi probabilitas dari kerugian agregat. dimana \\(I(\\cdot)\\) adalah fungsi indikator. distribusi empiris \\(\\hat{F}_S(s)\\) akan dikonvergen ke \\({F}_S(s)\\), dikarenakan ukuran sampel \\(m\\rightarrow \\infty\\) Dalam perhitungannya, asumsi-asumsi awal dibuat tentang distribusi probabilitas dan parameter-parameternya, kemudian model-model ini diestimasi menggunakan data yang tersedia dan kualitas kecocokan model dievaluasi menggunakan alat validasi model. Proses ini memberikan cara yang berguna untuk memperkirakan risiko yang terkait dengan kerugian agregat, dan dapat membantu perusahaan atau organisasi dalam merencanakan dan mengelola risiko mereka. Prosedur di atas mengasumsikan bahwa distribusi probabilitas, termasuk nilai parameter, dari distribusi frekuensi dan tingkat keparahan telah diketahui. Dalam praktiknya, kita perlu mengasumsikan distribusi-distribusi ini terlebih dahulu, mengestimasi parameter-parameternya dari data, dan kemudian menilai kualitas kecocokan model dengan menggunakan berbagai alat validasi model. Sebagai contoh, asumsi-asumsi dalam model risiko kolektif menyarankan estimasi dua tahap di mana satu model dikembangkan untuk jumlah klaim \\(N\\) dari data jumlah klaim, dan model lainnya dikembangkan untuk tingkat keparahan klaim \\(X\\) dari data jumlah klaim. "],["simulation-and-resampling.html", "Bab 6 Simulation and Resampling 6.1 Dasar-Dasar Simulasi 6.2 Bootstrap dan Resampling 6.3 Cross Validation 6.4 Importance Sampling 6.5 6.5.1 Metropolis Hastings 6.6 6.5.2 Gibbs Sampler", " Bab 6 Simulation and Resampling Bagian 6.1 memperkenalkan simulasi, alat komputasi luar biasa yang sangat berguna dalam pengaturan multivariat yang kompleks. Bagian 6.2 memperkenalkan resampling dalam konteks bootstrap untuk menentukan ketepatan estimator. Resampling merupakan proses simulasi untuk menggambar dari distribusi empiris. 6.1 Dasar-Dasar Simulasi Menghasilkan sekitar realisasi independen yang terdistribusi secara merata Ubah realisasi yang terdistribusi secara seragam menjadi pengamatan dari distribusi probabilitas yang menarik Hitung jumlah bunga dan tentukan ketepatan jumlah yang dihitung 6.1.1 Menghasilkan Pengamatan Seragam Independen Generator Kongruensi Linier. Linear Congruential Generators (LCG) adalah sebuah metode yang membangkitkan bilangan acak yang banyak dipergunakan dalam program komputer. Pada metode ini, dilakukan perulangan pada periode waktu tertentu atau setelah sekian kali pembangkitan.Untuk menghasilkan urutan angka acak, mulailah dengan \\(B_0\\) , nilai awal yang dikenal sebagai ‘seed’ . Nilai ini diperbarui menggunakan hubungan rekursif \\[B_{n+1} = (a B_n + c) \\text{ modulo }m, ~~ n=0, 1, 2, \\ldots .\\] Algoritma ini disebut \\(a\\). Kasus \\(c = 0\\) disebut generator kongruensial perkalian Untuk nilai ilustrasi dari \\(a\\) Dan $m4 , menggunakan Microsoft Visual Basic \\(m=2^{24}\\) , \\(a = 1 , 140 , 671 , 485\\) , Dan \\(c = 12 , 820 , 163\\). Ini adalah mesin yang mendasari pembuatan angka acak dalam program Microsoft Excel. Urutan yang digunakan oleh analis didefinisikan sebagai \\(U_n=B_n/m.\\). Analis dapat menginterpretasikan urutan \\(U_{i}\\) menjadi (kira-kira) identik dan independen terdistribusi secara seragam pada interval (0,1). Untuk mengilustrasikan algoritme, maka pertimbangkan hal berikut. Contoh 6.1.2. Menghasilkan Nomor Acak Seragam di R. Kode berikut menunjukkan cara menghasilkan tiga angka seragam (0,1) dalam R menggunakan perintah runif. Fungsi set.seed() di R digunakan untuk membuat hasil yang dapat direproduksi saat menulis kode yang melibatkan pembuatan variabel yang mengambil nilai acak. set.seed(2017) U &lt;- runif(3) knitr::kable(U, digits=5, align = &quot;c&quot;, col.names = &quot;Uniform&quot;) Uniform 0.92424 0.53718 0.46920 6.1.2 Metode Transformasi Invers Metode transformasi invers digunakan untuk membangkitkan data acak dari distribusi peluang kontinu yang diketahui bentuk fungsinya. Dengan urutan bilangan acak seragam, kemudian diubah menjadi distribution of interest (\\(F\\)). \\[X_i=F^{-1}\\left( U_i \\right) .\\] \\[F^{-1}(y) = \\inf_x ~ \\{ F(x) \\ge y \\}\\] inf singkatan dari *infimum atau batas bawah terbesar. Ini pada dasarnya adalah nilai \\(x\\) terkecil yang memenuhi pertidaksamaan \\(\\{F(x) \\ge y\\}\\). Hasilnya adalah urutan \\(X_{i}\\) kira-kira iid dengan fungsi distribusi \\(F\\) jika \\(U_{i}\\) adalah iid dengan fungsi distribusi seragam ( 0 , 1 ). Contoh 6.1.3. Menghasilkan Bilangan Acak Eksponensial. Misalkan ingin menghasilkan pengamatan dari distribusi eksponensial dengan parameter skala \\(θ\\) sehingga \\(F(x) = 1 - e^{-x/\\theta}\\). Untuk menghitung transformasi invers, maka dapat menggunakan langkah-langkah berikut: \\[\\begin{aligned} y = F(x) &amp;\\Leftrightarrow y = 1-e^{-x/\\theta} \\\\ &amp;\\Leftrightarrow -\\theta \\ln(1-y) = x = F^{-1}(y) . \\end{aligned}\\] Jadi, jika \\(U\\) memiliki distribusi seragam (0,1), maka \\(X = -\\theta \\ln(1-U)\\) memiliki distribusi eksponensial dengan parameter \\(θ\\). Seperti pada Contoh 6.1.2 kemudian mengubahnya menjadi variabel acak terdistribusi eksponensial independen dengan rata-rata \\(10\\). Sebagai alternatif, menggunakan fungsi rexp pada R digunakan untuk mensimulasikan sekumpulan bilangan acak yang diambil dari distribusi eksponensial. set.seed(2017) U &lt;- runif(3) X1 &lt;- -10*log(1-U) set.seed(2017) X2 &lt;- rexp(3, rate = 1/10) Contoh 6.1.4. Menghasilkan Angka Acak Pareto. Misalkan ingin menghasilkan pengamatan dari distribusi Pareto dengan parameter \\(α\\) dan \\(θ\\) sehingga \\(F(x) = 1 - \\left(\\frac{\\theta}{x+\\theta} \\right)^{\\alpha}\\). Untuk menghitung transformasi invers, maka dapat menggunakan langkah-langkah berikut: \\[\\begin{aligned} y = F(x) &amp;\\Leftrightarrow 1-y = \\left(\\frac{\\theta}{x+\\theta} \\right)^{\\alpha} \\\\ &amp;\\Leftrightarrow \\left(1-y\\right)^{-1/\\alpha} = \\frac{x+\\theta}{\\theta} = \\frac{x}{\\theta} +1 \\\\ &amp;\\Leftrightarrow \\theta \\left((1-y)^{-1/\\alpha} - 1\\right) = x = F^{-1}(y) .\\end{aligned}\\] Dengan demikian, \\(X = \\theta \\left((1-U)^{-1/\\alpha} - 1\\right)\\) memiliki distribusi Pareto dengan parameter \\(α\\) dan \\(θ\\) . Contoh 6.1.5. Menghasilkan Bilangan Acak Bernoulli. Misalkan ingin mensimulasikan variabel acak dari distribusi Bernoulli dengan parameter \\(Q= 0,85\\). Grafik fungsi distribusi kumulatif pada Gambar diatas menunjukkan bahwa fungsi kuantil dapat ditulis sebagai berikut. \\[\\begin{aligned} F^{-1}(y) = \\left\\{ \\begin{array}{cc} 0 &amp; 0&lt;y \\leq 0.85 \\\\ 1 &amp; 0.85 &lt; y \\leq 1.0 . \\end{array} \\right. \\end{aligned}\\] Jadi, dengan transformasi invers kita dapat mendefinisikan \\[\\begin{aligned} X = \\left\\{ \\begin{array}{cc} 0 &amp; 0&lt;U \\leq 0.85 \\\\ 1 &amp; 0.85 &lt; U \\leq 1.0 \\end{array} \\right. \\end{aligned}\\] Misalnya, ingin menghasilkan tiga angka acak untuk diperoleh set.seed(2017) U &lt;- runif(3) X &lt;- 1*(U &gt; 0.85) Contoh 6.1.6. Menghasilkan Angka Acak dari Distribusi Diskrit. Pertimbangkan waktu kegagalan mesin dalam lima tahun pertama. Distribusi waktu kegagalan diberikan sebagai: Dengan menggunakan grafik fungsi distribusi pada gambar diatas , dengan transformasi invers dapat definisikan \\[\\small{ \\begin{aligned} X = \\left\\{ \\begin{array}{cc} 1 &amp; 0&lt;U \\leq 0.1 \\\\ 2 &amp; 0.1 &lt; U \\leq 0.3\\\\ 3 &amp; 0.3 &lt; U \\leq 0.4\\\\ 4 &amp; 0.4 &lt; U \\leq 0.8 \\\\ 5 &amp; 0.8 &lt; U \\leq 1.0 . \\end{array} \\right. \\end{aligned} }\\] Untuk variabel acak diskrit umum mungkin tidak ada urutan hasil. Misalnya, seseorang dapat memiliki salah satu dari lima jenis produk asuransi jiwa dan dapat menggunakan algoritme berikut untuk menghasilkan hasil acak: \\[{\\small \\begin{aligned} X = \\left\\{ \\begin{array}{cc} \\textrm{whole life} &amp; 0&lt;U \\leq 0.1 \\\\ \\textrm{endowment} &amp; 0.1 &lt; U \\leq 0.3\\\\ \\textrm{term life} &amp; 0.3 &lt; U \\leq 0.4\\\\ \\textrm{universal life} &amp; 0.4 &lt; U \\leq 0.8 \\\\ \\textrm{variable life} &amp; 0.8 &lt; U \\leq 1.0 . \\end{array} \\right. \\end{aligned} }\\] Analis lain dapat menggunakan prosedur alternatif seperti: \\[{\\small \\begin{aligned} X = \\left\\{ \\begin{array}{cc} \\textrm{whole life} &amp; 0.9&lt;U&lt;1.0 \\\\ \\textrm{endowment} &amp; 0.7 \\leq U &lt; 0.9\\\\ \\textrm{term life} &amp; 0.6 \\leq U &lt; 0.7\\\\ \\textrm{universal life} &amp; 0.2 \\leq U &lt; 0.6 \\\\ \\textrm{variable life} &amp; 0 \\leq U &lt; 0.2 . \\end{array} \\right. \\end{aligned} }\\] Kedua algoritma menghasilkan (dalam jangka panjang) probabilitas yang sama, misalnya, \\(\\Pr(\\textrm{whole life})=0.1\\) , Dan seterusnya. Jadi, tidak ada yang salah ini menunjukkan bahwa ada lebih dari satu cara untuk mencapai suatu tujuan. Demikian pula, dapat menggunakan algoritme alternatif untuk hasil yang diurutkan (seperti waktu kegagalan 1, 2, 3, 4, atau 5, di atas). Contoh 6.1.7. Menghasilkan Angka Acak dari Distribusi Hybrid. Pertimbangkan variabel acak yaitu 0 dengan probabilitas 70% dan terdistribusi secara eksponensial dengan parameter \\(\\theta= 10,000\\) dengan probabilitas 30%. Dalam aplikasi asuransi, ini mungkin sesuai dengan peluang 70% tidak memiliki klaim asuransi dan peluang klaim 30% - jika klaim terjadi, maka itu didistribusikan secara eksponensial. Fungsi distribusi, digambarkan pada gambar dibawah ini , diberikan sebagai \\[\\begin{aligned} F(y) = \\left\\{ \\begin{array}{cc} 0 &amp; x&lt;0 \\\\ 1 - 0.3 \\exp(-x/10000) &amp; x \\ge 0 . \\end{array} \\right. \\end{aligned}\\] Dari Gambar diatas dapat dilihat bahwa transformasi invers untuk membangkitkan variabel acak dengan fungsi distribusi ini adalah \\[\\begin{aligned} X = F^{-1}(U) = \\left\\{ \\begin{array}{cc} 0 &amp; 0&lt; U \\leq 0.7 \\\\ -1000 \\ln (\\frac{1-U}{0.3}) &amp; 0.7 &lt; U &lt; 1 . \\end{array} \\right. \\end{aligned}\\] 6.1.3 (6.1.3) Presisi Simulasi Setelah mengetahui cara menghasilkan realisasi simulasi independen dari distribusi bunga, maka dapat menyusun distribusi empiris (distribusi empiris mengelompokkan data ke dalam suatu interval, di mana frekuensi data dalam setiap interval dapat digunakan untuk menentukan frekuensi relatifnya) dan memperkirakan distribusi yang diperlukan. Banyak dari aplikasi ini dapat direduksi menjadi masalah perkiraan \\(\\mathrm{E~}[h(X)]\\) , Di mana \\(h(\\cdot)\\) adalah beberapa fungsi yang diketahui. Berdasarkan simulasi R (replikasi), sehingga didapatkan \\(X_1,\\ldots,X_R\\). Dari sampel yang disimulasikan ini, dapat menghitung rata-rata sebagai berikut. \\[\\overline{h}_R=\\frac{1}{R}\\sum_{i=1}^{R} h(X_i)\\] sebagai perkiraan simulasi dari \\(\\mathrm{E~}[h(X)]\\). Untuk memperkirakan ketepatan perkiraan tersebut, maka menggunakan varians simulasi \\[s_{h,R}^2 = \\frac{1}{R-1} \\sum_{i=1}^{R}\\left( h(X_i) -\\overline{h}_R \\right) ^2.\\] Dari independensi, kesalahan standar estimasi adalah \\(s_{h,R}/\\sqrt{R}\\). Kesalahan standar estimasi dapat dibuat sekecil dengan meningkatkan jumlah replikasi \\(R\\). Contoh 6.1.8. Manajemen portofolio. Pada Bagian 3.4 telah mempelajari cara menghitung nilai ekspektasi polis dengan deductible. Sebagai contoh dari sesuatu yang tidak dapat dilakukan dengan ekspresi bentuk tertutup, kemudian akan mempertimbangkan dua risiko. (Ini adalah variasi dari contoh yang lebih kompleks yang akan dibahas sebagai Contoh 10.3.6). Dengan mempertimbangkan dua risiko properti dari perusahaan telekomunikasi: \\(X_1\\) - bangunan, dimodelkan menggunakan distribusi gamma dengan rata-rata 200 dan parameter skala 100. \\(X_2\\) - kendaraan bermotor, dimodelkan menggunakan distribusi gamma dengan mean 400 dan parameter skala 200. Nyatakan risiko total sebagai \\(X = X_1 + X_2\\). Untuk penyederhanaan, dapat diasumsikan bahwa risiko ini tidak bergantung. Untuk mengelola risiko maka diperlukan perlindungan atau penjamin asuransi dan bersedia mempertahankan jumlah bangunan dan kendaraan bermotor kecil secara internal, hingga \\(M\\). Jumlah acak lebih dari \\(M\\) akan memiliki pengaruh yang tidak terduga pada anggaran dan karenanya untuk jumlah ini dapat mencari perlindungan asuransi. Dinyatakan secara matematis, risiko yang dipertahankan adalah \\(Y_{retained}=\\min(X_1 + X_2,M)\\) dan bagian penanggung adalah \\(Y_{insurer} = X- Y_{retained}\\). Misalnya \\(M= 400\\) serta \\(R = 1000000\\). A. Dengan pengaturan tersebut, ingin menentukan perkiraan jumlah klaim dan standar deviasi terkait dari (i) yang ditahan, (ii) yang diterima oleh perusahaan asuransi, dan (iii) total jumlah keseluruhan. # Simulate the risks nSim &lt;- 1e6 #number of simulations set.seed(2017) #set seed to reproduce work X1 &lt;- rgamma(nSim ,alpha1,scale = theta1) X2 &lt;- rgamma(nSim ,alpha2,scale = theta2) # Portfolio Risks X &lt;- X1 + X2 Yretained &lt;- pmin(X, M) Yinsurer &lt;- X - Yretained Kemudian jumlah klaim yang diharapkan adalah # Expected Claim Amounts ExpVec &lt;- t(as.matrix(c(mean(Yretained),mean(Yinsurer),mean(X)))) sdVec &lt;- t(as.matrix(c(sd(Yretained),sd(Yinsurer),sd(X)))) outMat &lt;- rbind(ExpVec, sdVec) colnames(outMat) &lt;- c(&quot;Retained&quot;, &quot;Insurer&quot;,&quot;Total&quot;) row.names(outMat) &lt;- c(&quot;Mean&quot;,&quot;Standard Deviation&quot;) round(outMat,digits=2) B. Untuk klaim yang diasuransikan, kesalahan standar perkiraan simulasi adalah \\(s_{h,R}/\\sqrt{1000000} =/\\sqrt{1000000} =0.281\\). Untuk contoh ini, simulasi cepat dan nilai yang besar seperti 1000000 adalah pilihan yang mudah. Namun, untuk masalah yang lebih kompleks, ukuran simulasi mungkin menjadi masalah. Yinsurefct &lt;- function(numSim){ X1 &lt;- rgamma(numSim,alpha1,scale = theta1) X2 &lt;- rgamma(numSim,alpha2,scale = theta2) # Portfolio Risks X &lt;- X1 + X2 Yinsurer &lt;- X - pmin(X, M) return(Yinsurer) } R &lt;- 1e3 nPath &lt;- 20 set.seed(2017) simU &lt;- matrix(Yinsurefct(R*nPath),R,nPath) sumP2 &lt;- apply(simU, 2, cumsum)/(1:R) matplot(1:R,sumP2[,1:20],type=&quot;l&quot;,col=rgb(1,0,0,.2), ylim=c(100, 400), xlab=expression(paste(&quot;Number of Simulations (&quot;, italic(&#39;R&#39;), &quot;)&quot;)), ylab=&quot;Expected Insurer Claims&quot;) abline(h=mean(Yinsurer),lty=2) bonds &lt;- cbind(1.96*sd(Yinsurer)*sqrt(1/(1:R)),-1.96*sd(Yinsurer)*sqrt(1/(1:R))) matlines(1:R,bonds+mean(Yinsurer),col=&quot;red&quot;,lty=1) Dari grafik diatas dapat dilihat, semakin banyak jumlah simulasi R maka semakin sedikit jumlah klaim yang diharapkan. Penentuan Jumlah Simulasi Misalkan ingin berada dalam 1% dari rata-rata dengan kepastian 95%. Artinya, \\(\\Pr \\left( |\\overline{h}_R - \\mathrm{E~}[h(X)]| \\le 0.01 \\mathrm{E~}[h(X)] \\right) \\ge 0.95\\). Menurut teorema limit pusat, perkiraan harus terdistribusi secara normal dan mengharapkan R cukup besar untuk \\(0.01 \\mathrm{E~}[h(X)]/\\sqrt{\\mathrm{Var~}[h(X)]/R}) \\ge 1.96\\) . (Ingat bahwa 1,96 adalah persentil ke-97,5 dari distribusi normal standar.) Mengganti \\(\\mathrm{E~}[h(X)]\\) Dan \\(\\mathrm{Var~}[h(X)]\\) dengan estimasi,sehingga \\[\\frac{.01\\overline{h}_R}{s_{h,R}/\\sqrt{R}}\\geq 1.96\\] \\[\\begin{equation} R \\geq 38,416\\frac{s_{h,R}^2}{\\overline{h}_R^2}. \\tag{6.1} \\end{equation}\\] Contoh 6.1.9. Pilihan Perkiraan. Sebuah aplikasi penting dari simulasi adalah pendekatan dari \\(\\mathrm{E~}[h(X)]\\). Dalam contoh ini, kami menunjukkan bahwa pilihan dari \\(h(\\cdot)\\) fungsi dan distribusi \\(X\\) dapat berperan. Pertimbangkan pertanyaan berikut: apa itu \\(\\Pr[X&gt;2]\\). Kapan \\(X\\) mempunyai sebuah distribusi Cauchy (distribusi probabilitas kontinu), dengan fungsi kepadatan \\(f(x) =\\left(\\pi(1+x^2)\\right)^{-1}\\), pada garis sebenarnya? Nilai sebenarnya adalah \\[\\Pr\\left[X&gt;2\\right] = \\int_2^\\infty \\frac{dx}{\\pi(1+x^2)} .\\] true_value &lt;- integrate(function(x) 1/(pi*(1+x^2)),lower=2,upper=Inf)$value true_value ## [1] 0.1475836 Perkiraan 1. Sebagai alternatif, seseorang dapat menggunakan teknik simulasi untuk memperkirakan besaran tersebut. Dari kalkulus, dapat memeriksa bahwa fungsi kuantil dari distribusi Cauchy adalah \\(F^{-1}(y) = \\tan \\left( \\pi(y-0.5) \\right)\\) . Kemudian, dengan variasi seragam (0,1) yang disimulasikan, \\(U_1, \\ldots, U_R\\), sehingga dapat membangun estimator Q &lt;- function(u) tan(pi*(u-.5)) R &lt;- 1e6 set.seed(1) X &lt;- Q(runif(R)) p1 &lt;- mean(X&gt;2) se.p1 &lt;- sd(X&gt;2)/sqrt(R) p1 ## [1] 0.147439 se.p1 ## [1] 0.0003545432 Dengan satu juta simulasi, diperoleh estimasi sebesar 0,14744 dengan standard error 0,355 (dibagi 1000). Dapat dibuktikan bahwa varian dari \\(P_1\\) teratur \\(0.127/R\\). Perkiraan 2. Dengan pilihan lain dari \\(h(\\cdot)\\) Dan \\(f(\\cdot)\\) adalah mungkin untuk mengurangi ketidakpastian bahkan dengan menggunakan jumlah simulasi yang sama \\(R\\) . Untuk memulai, seseorang dapat menggunakan simetri distribusi Cauchy untuk menulis \\(\\Pr[X&gt;2]=0.5\\cdot\\Pr[|X|&gt;2]\\) . Dengan ini, dapat membuat estimator baru \\[p_2 = \\frac{1}{2R}\\sum_{i=1}^R \\mathrm{I}(|F^{-1}(U_i)|&gt;2) .\\] Dengan satu juta simulasi, diperoleh estimasi sebesar 0,14748 dengan standard error 0,228 (dibagi 1000). Dapat dibuktikan bahwa varian dari \\(P_2\\) teratur \\(0.052/R\\). Perkiraan 3. Integral tak wajar dapat ditulis dengan sifat simetri sederhana (karena fungsinya simetris dan integral pada garis real sama dengan 1 ). \\[\\int_2^\\infty \\frac{dx}{\\pi(1+x^2)}=\\frac{1}{2}-\\int_0^2\\frac{dx}{\\pi(1+x^2)} .\\] \\[p_3 = \\frac{1}{2}-\\frac{1}{R}\\sum_{i=1}^R h_3(2U_i), ~~~~~~\\text{where}~h_3(x)=\\frac{2}{\\pi(1+x^2)} .\\] Dengan satu juta simulasi, diperoleh estimasi sebesar 0,14756 dengan standard error 0,169 (dibagi 1000). Dapat dibuktikan bahwa varian dari \\(P_3\\) teratur \\(0,0285 / R\\). Perkiraan 4. Akhirnya, seseorang juga dapat mempertimbangkan beberapa perubahan variabel dalam integral. \\[\\int_2^\\infty \\frac{dx}{\\pi(1+x^2)}=\\int_0^{1/2}\\frac{y^{-2}dy}{\\pi(1-y^{-2})} .\\] \\[p_4 = \\frac{1}{R}\\sum_{i=1}^R h_4(U_i/2),~~~~~\\text{where}~h_4(x)=\\frac{1}{2\\pi(1+x^2)} .\\] Dengan satu juta simulasi, diperoleh estimasi sebesar 0,14759 dengan standard error 0,01 (dibagi 1000). Dapat dibuktikan bahwa varian dari \\(P_4\\) teratur \\(0,00009 / R\\) , yang jauh lebih kecil dari yang lainnya. Tabel berikut merupakan rangkuman dari empat pilihan \\(h(\\cdot)\\) Dan \\(f(\\cdot)\\)) untuk memperkirakan \\(\\Pr[X&gt;2] = 0,14758\\). Kesalahan standar bervariasi. Jadi, jika memiliki tingkat akurasi yang diinginkan, maka jumlah simulasi sangat bergantung pada bagaimana menulis integral yang akan diaproksimasi. 6.1.4 Simulasi dan Inferensi Statistik Simulasi tidak hanya membantu dalam memperkirakan nilai yang diharapkan tetapi juga berguna dalam menghitung aspek lain dari fungsi distribusi. Secara khusus, ini sangat berguna ketika distribusi statistik uji terlalu rumit untuk diturunkan. Dalam hal ini, seseorang dapat menggunakan simulasi untuk memperkirakan distribusi referensi. Contoh 6.1.10. Uji Distribusi Kolmogorov-Smirnov. Misalkan terdapata \\(n = 100\\) observasi \\(\\{x_1,\\cdots,x_n\\}\\) yang, tidak diketahui oleh analis, dihasilkan dari distribusi gamma dengan parameter \\(\\alpha = 6\\) Dan \\(\\theta=2\\) . Analis percaya bahwa data berasal dari distribusi lognormal dengan parameter 1 dan 0,4 dan ingin menguji asumsi ini. set.seed(1) n &lt;- 100 x &lt;- rgamma(n, 6, 2) u=seq(0,7,by=.01) vx = c(0,sort(x)) vy = (0:n)/n par(mfrow=c(1,2)) hist(x,probability = TRUE,main=&quot;Histogram&quot;, col=&quot;light blue&quot;, border=&quot;white&quot;,xlim=c(0,7),ylim=c(0,.4)) lines(u,dlnorm(u,1,.4),col=&quot;red&quot;,lty=2) plot(vx,vy,type=&quot;l&quot;,xlab=&quot;x&quot;,ylab=&quot;Cumulative Distribution&quot;,main=&quot;Empirical cdf&quot;) lines(u,plnorm(u,1,.4),col=&quot;red&quot;,lty=2) Dari grafik diatas dapat dilihat bahwa garis putus-putus merah tersebut sesuai dengan distribusi lognormal yang dihipotesiskan. Perlu digaris bawahi bahwa statistik Kolmogorov-Smirnov sama dengan perbedaan terbesar antara distribusi empiris dan hipotesis. Ini \\(\\max_x |F_n(x)-F_0(x)|\\), Di mana \\(F_0\\) adalah distribusi lognormal yang dihipotesiskan, sehingga # test statistic D &lt;- function(data, F0){ F &lt;- Vectorize(function(x) mean((data&lt;=x))) n &lt;- length(data) x &lt;- sort(data) d1=abs(F(x+1e-6)-F0(x+1e-6)) d2=abs(F(x-1e-6)-F0(x-1e-6)) return(max(c(d1,d2))) } D(x,function(x) plnorm(x,1,.4)) ## [1] 0.09703627 ks.test(x, plnorm, mean=1, sd=0.4) ## ## Asymptotic one-sample Kolmogorov-Smirnov test ## ## data: x ## D = 0.097037, p-value = 0.3031 ## alternative hypothesis: two-sided Secara khusus, untuk menghitung P-value, maka hasilkan ribuan sampel acak dari \\(LN(1,0.4)\\) distribusi (dengan ukuran yang sama), dan menghitung secara empiris distribusi statistik, ns &lt;- 1e4 d_KS &lt;- rep(NA,ns) # compute the test statistics for a large (ns) number of simulated samples for(s in 1:ns) d_KS[s] &lt;- D(rlnorm(n,1,.4),function(x) plnorm(x,1,.4)) mean(d_KS&gt;D(x,function(x) plnorm(x,1,.4))) ## [1] 0.2843 hist(d_KS,probability = TRUE,col=&quot;light blue&quot;,border=&quot;white&quot;,xlab=&quot;Test Statistic&quot;,main=&quot;&quot;) lines(density(d_KS),col=&quot;red&quot;) abline(v=D(x,function(x) plnorm(x,1,.4)),lty=2,col=&quot;red&quot;) Distribusi yang disimulasikan berdasarkan 10.000 sampel acak dirangkum grafik diatas. Di sini, statistik melebihi nilai empiris (0,09704) dalam 28,43%, sedangkan P-value adalah 0,3031. Baik untuk simulasi maupun teoretis P-value, kesimpulannya adalah data tidak memberikan bukti yang cukup untuk menolak hipotesis distribusi lognormal. Meskipun hanya perkiraan, pendekatan simulasi bekerja dalam berbagai distribusi dan uji statistik tanpa perlu mengembangkan nuansa teori yang mendasari untuk setiap situasi. Berikut ringkasan prosedur untuk mengembangkan distribusi simulasi dan p-value sebagai berikut: Gambarlah sampel berukuran n , katakanlah, \\(X_1, \\ldots, X_n\\), dari fungsi distribusi yang diketahui \\(F\\). Hitung statistik minat, dilambangkan sebagai \\(\\hat{\\theta}(X_1, \\ldots, X_n)\\). Panggil ini \\(\\hat{\\theta}^r\\) untuk replikasi ke -r . Ulangi ini \\(r=1, \\ldots, R\\) kali untuk mendapatkan sampel statistik, \\(\\hat{\\theta}^1, \\ldots,\\hat{\\theta}^R\\). Dari sampel statistik pada Langkah 2, \\(\\{\\hat{\\theta}^1, \\ldots,\\hat{\\theta}^R\\}\\), hitung ukuran ringkasan minat, seperti p-value. 6.2 Bootstrap dan Resampling Subbab ini akan mempelajari : Hasilkan distribusi bootstrap nonparametrik untuk statistik minat Gunakan distribusi bootstrap untuk menghasilkan estimasi presisi untuk statistik yang diminati, termasuk bias, standar deviasi, dan interval kepercayaan Lakukan analisis bootstrap untuk distribusi parametrik 6.2.1 Dasar-dasar Bootstrap Metode bootstrap adalah metode berbasis resampling data sampel dengan syarat pengembalian pada datanya dalam menyelesaikan statistik ukuran suatu sampel dengan harapan sampel tersebut mewakili data populai sebenarnya, biasanya ukuran resampling diambil secara ribuan kali agar dapat mewakili data populasinya. Algoritma resamplign umum dengan \\(\\{X_1, \\ldots, X_n\\}\\) untuk menunjukkan sampel asli dan \\(\\{X_1^*, \\ldots, X_n^*\\}\\) menunjukkan undian yang disimulasikan. Untuk setiap sampel, \\(n\\) merupakan undian simulasi, jumlah yang sama dengan ukuran sampel asli. Untuk membedakan prosedur ini dari simulasi, biasanya digunakan \\(B\\) (untuk bootstrap) sebagai jumlah sampel yang disimulasikan. Sehingga dapat dituliskan \\(\\{X_1^{(b)}, \\ldots, X_n^{(b)}\\}\\). Ada dua metode resampling dasar, model-free dan model-based , masing-masing sebagai nonparametrik dan parametrik . Pengundian yang disimulasikan berasal dari fungsi distribusi empiris \\(F_n(\\cdot)\\) , jadi setiap undian berasal \\(\\{X_1, \\ldots, X_n\\}\\) dengan probabilitas \\(1/n\\). Bootstrap Nonparametrik Gagasan bootstrap nonparametrik adalah menggunakan metode transformasi terbalik \\(F_N\\) , fungsi distribusi kumulatif empiris, digambarkan pada grafik dibawah ini. Karena \\(F_N\\) adalah step-function, \\(F_n^{-1}\\) subtitusi nilai-nilai \\(\\{x_1,\\cdots,x_n\\}\\) sehingga jika \\(y\\in(0,1/n)\\) (dengan probabilitas \\(1 / n\\) ) dengan menggambar nilai terkecil ( \\(\\min\\{x_i\\}\\) ) jika \\(y\\in(1/n,2/n)\\) (dengan probabilitas \\(1 / n\\) ) dengan menggambar nilai terkecil kedua, … jika \\(y\\in((n-1)/n,1)\\) (dengan probabilitas \\(1 / n\\) ) kami menggambar nilai terbesar ( \\(\\max\\{x_i\\}\\) ) Menggunakan metode transformasi terbalik dengan \\(F_N\\) berarti pengambilan sampel dari \\(\\{x_1,\\cdots,x_n\\}\\), dengan probabilitas \\(1 / n\\) . Menghasilkan sampel ukuran bootstrap \\(B\\) berarti pengambilan sampel dari \\(\\{x_1,\\cdots,x_n\\}\\) , dengan probabilitas \\(1 / n\\) , dengan penggantian. set.seed(1) n &lt;- 10 x &lt;- rexp(n, 1/6) m &lt;- 8 bootvalues &lt;- sample(x, size=m, replace=TRUE) 6.2.2 Presisi Bootstrap: Bias, Standar Deviasi, dan Mean Square Error Berikut adalah rangkuman prosedur bootstrap nonparametrik sebagai berikut: Dari sampel \\(\\{X_1, \\ldots, X_n\\}\\), gambar sampel berukuran n (dengan penggantian), katakanlah, \\(X_1^*, \\ldots, X_n^*\\) . Dari undian yang disimulasikan, hitung statistik minat, dilambangkan sebagai \\(\\hat{\\theta}(X_1^*, \\ldots, X_n^*)\\) . Panggil ini \\(\\hat{\\theta}_b^*\\) untuk ulangan ke-b . Ulangi ini \\(b=1, \\ldots, B\\) kali untuk mendapatkan sampel statistik \\(\\hat{\\theta}_1^*, \\ldots,\\hat{\\theta}_B^*\\). Dari sampel statistik pada Langkah 2,\\(\\{\\hat{\\theta}_1^*, \\ldots, \\hat{\\theta}_B^*\\}\\) hitung ukuran ringkasan minat. Pada bagian ini, ada tiga langkah ringkasan yaitu bias, standar deviasi, dan mean square error ( MSE ). Tabel dibawah ini merangkum ketiga ukuran. Di Sini, \\(\\overline{\\hat{\\theta^*}}\\) adalah rata-rata dari \\(\\{\\hat{\\theta}_1^*, \\ldots,\\hat{\\theta}_B^*\\}\\). # Example from Derrig et al BIData &lt;- read.csv(&quot;Data/DerrigResampling.csv&quot;, header =T) BIData$Censored &lt;- 1*(BIData$AmountPaid &gt;= BIData$PolicyLimit) BIDataUncensored &lt;- subset(BIData, Censored == 0) LER.boot &lt;- function(ded, data, indices){ resample.data &lt;- data[indices,] sumClaims &lt;- sum(resample.data$AmountPaid) sumClaims_d &lt;- sum(pmin(resample.data$AmountPaid,ded)) LER &lt;- sumClaims_d/sumClaims return(LER) } ##Derrig et al set.seed(2019) dVec2 &lt;- c(4000, 5000, 10500, 11500, 14000, 18500) OutBoot &lt;- matrix(0,length(dVec2),6) for (i in 1:length(dVec2)) { OutBoot[i,1] &lt;- dVec2[i] results &lt;- boot(data=BIDataUncensored, statistic=LER.boot, R=1000, ded=dVec2[i]) OutBoot[i,2] &lt;- results$t0 biasboot &lt;- mean(results$t)-results$t0 -&gt; OutBoot[i,3] sdboot &lt;- sd(results$t) -&gt; OutBoot[i,4] temp &lt;- boot.ci(results) OutBoot[i,5] &lt;- temp$normal[2] OutBoot[i,6] &lt;- temp$normal[3] } Berdasarkan tabel diatas hasil estimasi bootstrap. Misalnya, di D= 14000 , estimasi nonparametrik LER adalah 0,97678. Ini memiliki perkiraan bias 0,00018 dengan standar deviasi 0,00701. Untuk beberapa aplikasi, mungkin ingin menerapkan estimasi bias ke estimasi asli untuk memberikan estimator yang dikoreksi bias. Untuk ilustrasi ini, biasnya kecil sehingga koreksi semacam itu tidak relevan. Standar deviasi bootstrap memberikan ukuran presisi. Untuk satu penerapan standar deviasi dapat menggunakan pendekatan normal untuk membuat selang kepercayaan. Misalnya, pada R fungsi boot.ci menghasilkan interval kepercayaan normal sebesar 95%. Ini dihasilkan dengan membuat interval dua kali panjang standar deviasi bootstrap 1,95994, berpusat di sekitar estimator yang dikoreksi bias (1,95994 adalah kuantil ke-97,5 dari distribusi normal). Misalnya, CI 95% normal yang lebih rendah di \\(D= 14000\\) adalah \\((0.97678-0.00018)- 1.95994*0.00701\\). Contoh 6.2.2. Memperkirakan \\(\\exp(\\mu)\\) . Bootstrap dapat digunakan untuk mengukur bias estimator, misalnya. Pertimbangkan di sini sampel \\(\\mathbf{x}=\\{x_1,\\cdots,x_n\\}\\) adalah rata-rata μ . sample_x &lt;- c(2.46,2.80,3.28,3.86,2.85,3.67,3.37,3.40,5.22,2.55, 2.79,4.50,3.37,2.88,1.44,2.56,2.00,2.07,2.19,1.77) Misalkan kuantitas bunga adalah \\(\\theta=\\exp(\\mu)\\). Penaksir alami akan menjadi \\(\\widehat{\\theta}_1=\\exp(\\overline{x})\\). Estimator ini bias (karena ketidaksetaraan Jensen) tetapi tidak bias secara asimtotik. Untuk sampel, perkiraannya adalah sebagai berikuT (theta_1 &lt;- exp(mean(sample_x))) ## [1] 19.13463 Seseorang dapat menggunakan teorema limit pusat untuk mendapatkan koreksi menggunakan \\[\\overline{X}\\approx\\mathcal{N}\\left(\\mu,\\frac{\\sigma^2}{n}\\right)\\text{ where }\\sigma^2=\\text{Var}[X_i] ,\\] sehingga dengan fungsi pembangkit momen normal didapatkan \\[\\mathrm{E}~\\left[\\exp(\\overline{X})\\right] \\approx \\exp\\left(\\mu+\\frac{\\sigma^2}{2n}\\right) .\\] Oleh karena itu, seseorang dapat mempertimbangkan secara alami \\[\\widehat{\\theta}_2=\\exp\\left(\\overline{x}-\\frac{\\widehat{\\sigma}^2}{2n}\\right) .\\] n &lt;- length(sample_x) (theta_2 &lt;- exp(mean(sample_x)-var(sample_x)/(2*n))) ## [1] 18.73334 Sebagai strategi lain, seseorang juga dapat menggunakan pendekatan Taylor untuk mendapatkan penaksir yang lebih akurat (seperti dalam metode delta) \\[g(\\overline{x})=g(\\mu)+(\\overline{x}-\\mu)g&#39;(\\mu)+(\\overline{x}-\\mu)^2\\frac{g&#39;&#39;(\\mu)}{2}+\\cdots\\] Alternatif selanjutnya adalah menggunakan strategi bootstrap dengan sampel bootstrap \\(\\mathbf{x}^{\\ast}_{b}\\) sehingga \\(\\overline{x}^{\\ast}_{b}\\). \\[\\widehat{\\theta}_3=\\frac{1}{B}\\sum_{b=1}^B\\exp(\\overline{x}^{\\ast}_{b}) .\\] library(boot) results &lt;- boot(data=sample_x, statistic=function(y,indices) exp(mean(y[indices])), R=1000) theta_3 &lt;- mean(results$t) Ini menghasilkan tiga estimator, estimator mentah \\(\\widehat{\\theta}_1=19.135\\), koreksi urutan kedua \\(\\widehat{\\theta}_2= 18.733\\), dan estimator bootstrap \\(\\widehat{\\theta}_3= 19.388\\). Bagaimana cara kerjanya dengan ukuran sampel yang berbeda? Diasumsikan bahwa \\(X_i\\) dihasilkan dari distribusi lognormal \\(LN(0,1)\\) , sehingga \\(\\mu = \\exp(0 + 1/2) = 1.648721\\) Dan \\(\\theta = \\exp(1.648721)= 5,200326\\). Dengan menggunakan simulasi untuk menggambar ukuran sampel. param &lt;- function(x){ n &lt;- length(x) theta_1 &lt;- exp(mean(x)) theta_2 &lt;- exp(mean(x)-var(x)/(2*n)) results &lt;- boot(data=x, statistic=function(y,indices) exp(mean(y[indices])), R=999) theta_3 &lt;- mean(results$t) return(c(theta_1,theta_2,theta_3)) } set.seed(2074) ns&lt;- 200 est &lt;- function(n){ call_param &lt;- function(i) param(rlnorm(n,0,1)) V &lt;- Vectorize(call_param)(1:ns) apply(V,1,median) } VN=seq(15,100,by=5) Est &lt;- Vectorize(est)(VN) matplot(VN,t(Est),type=&quot;l&quot;, col=2:4, lty=2:4, ylim=exp(exp(1/2))+c(-1,1), xlab=&quot;sample size (n)&quot;, ylab=&quot;estimator&quot;) abline(h=exp(exp(1/2)),lty=1, col=1) legend(&quot;topleft&quot;, c(&quot;raw estimator&quot;, &quot;second order correction&quot;, &quot;bootstrap&quot;), col=2:4,lty=2:4, bty=&quot;n&quot;) Hasil perbandingan dirangkum dalam gambar diatas menunjukkan bahwa estimator bootstrap mendekati nilai parameter sebenarnya untuk hampir semua ukuran sampel. Bias dari ketiga estimator berkurang dengan meningkatnya ukuran sampel. 6.2.3 Interval Keyakinan Prosedur bootstrap menghasilkan \\(B\\) bentuk ulang dari \\(\\hat{\\theta}_1^*, \\ldots,\\hat{\\theta}_B^*\\) dari penaksir \\(\\hat{\\theta}\\) . Dalam Contoh 6.2.1, dapat dilihat bagaimana menggunakan pendekatan normal standar untuk membuat interval kepercayaan untuk parameter yang diinginkan. Namun, mengingat poin utamanya adalah menggunakan bootstrapping untuk menghindari ketergantungan pada asumsi perkiraan normalitas, tidak mengherankan jika tersedia interval kepercayaan alternatif. Untuk estimator \\(\\hat{\\theta}\\) , interval kepercayaan bootstrap dasar adalah \\[\\begin{equation} \\left(2 \\hat{\\theta} - q_U, 2 \\hat{\\theta} - q_L \\right) , \\tag{6.2} \\end{equation}\\] Di mana \\(q_L\\) Dan \\(q_U\\) adalah kuantil 2,5% bawah dan atas dari sampel bootstrap \\(\\hat{\\theta}_1^*, \\ldots,\\hat{\\theta}_B^*\\) Untuk melihat dari mana asalnya, mula-mula \\((q_L, q_U)\\) menyediakan interval 95% untuk \\(\\hat{\\theta}_1^*, \\ldots,\\hat{\\theta}_B^*\\) . Jadi, untuk acak \\(\\hat{\\theta}_b^*\\), ada kemungkinan 95% itu \\(q_L \\le \\hat{\\theta}_b^* \\le q_U\\). Membalikkan pertidaksamaan dan menjumlahkan \\(\\hat{\\theta}\\) ke setiap sisi memberikan interval 95% \\[\\hat{\\theta} -q_U \\le \\hat{\\theta} - \\hat{\\theta}_b^* \\le \\hat{\\theta} -q_L .\\] Jadi, \\(\\left( \\hat{\\theta}-q_U, \\hat{\\theta} -q_L\\right)\\) adalah interval 95% untuk \\(\\hat{\\theta} - \\hat{\\theta}_b^*\\). Ide perkiraan bootstrap mengatakan bahwa ini juga merupakan interval 95% untuk \\(\\theta - \\hat{\\theta}\\). Dengan menambahkan \\(\\hat{\\theta}\\) ke setiap sisi memberikan interval 95% dalam persamaan diatas. Banyak alternatif interval bootstrap yang tersedia. Yang paling mudah dijelaskan adalah interval bootstrap persentil yang didefinisikan sebagai \\((q_L,q_U)\\). Contoh 6.2.3. Klaim Cidera Tubuh dan Tindakan Risiko. Untuk melihat bagaimana interval kepercayaan bootstrap bekerja, dengan kembali ke klaim otomatis cedera tubuh yang dipertimbangkan dalam Contoh 6.2.1 . Alih-alih rasio eliminasi kerugian, misalkan ingin memperkirakan persentil ke-95 \\(F^{-1}(0.95)\\) dan ukuran didefinisikan sebagai \\[TVaR_{0.95}[X] = \\mathrm{E}[X | X &gt; F^{-1}(0.95)] .\\] Pengukuran ini disebut dengan ekor nilai berisiko; itu adalah nilai yang diharapkan dari X bersyarat X melebihi persentil ke-95. Bagian 10.2 menjelaskan bagaimana quantiles dan tail value-at-risk adalah dua contoh paling penting dari apa yang disebut sebagai ukuran risiko . Untuk saat ini, hanya akan menganggap ini sebagai ukuran yang ingin diperkirakan. Untuk persentil, dengan menggunakan estimator nonparametrik \\(F^{-1}_n(0.95)\\) didefinisikan dalam Bagian 4.1.1.3 . Untuk tail value-at-risk, menggunakan prinsip plug-in untuk menentukan estimator nonparametrik \\[TVaR_{n,0.95}[X] = \\frac{\\sum_{i=1}^n X_i I(X_i &gt; F^{-1}_n(0.95))}{\\sum_{i=1}^n I(X_i &gt; F^{-1}_n(0.95))} ~.\\] Dalam ungkapan ini, penyebut menghitung jumlah pengamatan yang melebihi persentil ke-95 \\(F^{-1}_n(0.95)\\) . Pembilang menjumlahkan kerugian untuk pengamatan yang melebihi \\(F^{-1}_n(0.95)\\) . Tabel dibawah ini merangkum penaksir untuk pecahan terpilih. # Example from Derrig et al #BIData &lt;- read.csv(&quot;Data/DerrigResampling.csv&quot;, header =T) BIData$Censored &lt;- 1*(BIData$AmountPaid &gt;= BIData$PolicyLimit) BIDataUncensored &lt;- subset(BIData, Censored == 0) set.seed(2017) PercentVec &lt;- c(0.50, 0.80, 0.90, 0.95, 0.98) OutBoot1 &lt;- matrix(0,5,10) for (i in 1:length(PercentVec)) { OutBoot1[i,1] &lt;- PercentVec[i] results &lt;- boot(data=BIDataUncensored$AmountPaid, statistic=function(X,indices) quantile(X[indices],PercentVec[i]), R=1000) if (i==1){bootreal &lt;- results$t} OutBoot1[i,2] &lt;- results$t0 OutBoot1[i,3] &lt;- mean(results$t)-results$t0 OutBoot1[i,4] &lt;- sd(results$t) temp &lt;- boot.ci(results, type = c(&quot;norm&quot;, &quot;basic&quot;, &quot;perc&quot;)) OutBoot1[i,5] &lt;- temp$normal[2] OutBoot1[i,6] &lt;- temp$normal[3] OutBoot1[i,7] &lt;- temp$basic[4] OutBoot1[i,8] &lt;- temp$basic[5] OutBoot1[i,9] &lt;- temp$percent[4] OutBoot1[i,10] &lt;- temp$percent[5] } Misalnya, ketika pecahannya adalah 0,50, dapat melihat bahwa kuantil 2,5 bawah dan atas dari simulasi bootstrap adalah \\(q_L= 6000\\) dan \\(q_U= 6700\\). Ini membentuk interval kepercayaan bootstrap persentil. Dengan estimator nonparametrik \\(6500\\), ini menghasilkan batas bawah dan atas interval kepercayaan dasar masing-masing \\(6300\\) dan \\(7000\\). CTE.boot &lt;- function(data, indices, RiskLevel){ resample.data &lt;- data[indices,] X &lt;- resample.data$AmountPaid cutoff &lt;- quantile(X, RiskLevel) CTE &lt;- sum(X*(X &gt; cutoff))/sum(X &gt; cutoff) return(CTE) } set.seed(2017) PercentVec &lt;- c(0.50, 0.80, 0.90, 0.95, 0.98) OutBoot1 &lt;- matrix(0,5,10) for (i in 1:length(PercentVec)) { OutBoot1[i,1] &lt;- PercentVec[i] results &lt;- boot(data=BIDataUncensored, statistic=CTE.boot, R=1000, RiskLevel=PercentVec[i]) OutBoot1[i,2] &lt;- results$t0 OutBoot1[i,3] &lt;- mean(results$t)-results$t0 OutBoot1[i,4] &lt;- sd(results$t) temp &lt;- boot.ci(results, type = c(&quot;norm&quot;, &quot;basic&quot;, &quot;perc&quot;)) OutBoot1[i,5] &lt;- temp$normal[2] OutBoot1[i,6] &lt;- temp$normal[3] OutBoot1[i,7] &lt;- temp$basic[4] OutBoot1[i,8] &lt;- temp$basic[5] OutBoot1[i,9] &lt;- temp$percent[4] OutBoot1[i,10] &lt;- temp$percent[5] } Tabel di atas menunjukkan kalkulasi serupa untuk tail value-at-risk. Dalam setiap kasus, dapat melihat bahwa deviasi standar bootstrap meningkat seiring dengan peningkatan fraksi. Hal ini karena ada lebih sedikit pengamatan untuk memperkirakan kuantil seiring meningkatnya fraksi, yang menyebabkan ketidaktepatan yang lebih besar. Interval kepercayaan juga menjadi lebih lebar. Menariknya, tampaknya tidak ada pola yang sama dalam estimasi bias tersebut. 6.2.4 Bootstrap Parametrik Gagasan dari bootstrap nonparametrik adalah untuk mengambil sampel ulang dengan menggambar variabel independen dari fungsi distribusi kumulatif empiris \\(F_n\\). Sebaliknya, dengan bootstrap parametrik, kami menarik variabel independen dari \\(F_{\\widehat{\\theta}}\\) di mana distribusi yang mendasarinya diasumsikan dalam keluarga parametrik \\(\\mathcal{F}=\\{F_{\\theta},\\theta\\in\\Theta\\}\\) . Biasanya, parameter dari distribusi ini diperkirakan berdasarkan sampel dan dinotasikan sebagai \\(\\hat{\\theta}\\). contoh 6.2.4. distribusi lognormal. Pertimbangkan lagi kumpulan datanya sample_x &lt;- c(2.46,2.80,3.28,3.86,2.85,3.67,3.37,3.40, 5.22,2.55,2.79,4.50,3.37,2.88,1.44,2.56,2.00,2.07,2.19,1.77) Bootstrap klasik (nonparametrik) didasarkan pada contoh berikut. x &lt;- sample(sample_x,replace=TRUE) Sebagai gantinya, untuk bootstrap parametrik harus mengasumsikan bahwa distribusi dari \\(x_i\\) adalah dari kelompok tertentu. Sebagai contoh, kode berikut menggunakan distribusi lognormal. library(MASS) fit &lt;- fitdistr(sample_x, dlnorm, list(meanlog = 1, sdlog = 1)) fit x &lt;- rlnorm(length(sample_x), meanlog=fit$estimate[1], sdlog=fit$estimate[2]) set.seed(2074) CV &lt;- matrix(NA,1e5,2) for(s in 1:nrow(CV)){ x1 &lt;- sample(sample_x,replace=TRUE) x2 &lt;- rlnorm(length(sample_x), meanlog=fit$estimate[1], sdlog=fit$estimate[2]) CV[s,] &lt;- c(sd(x1)/mean(x1),sd(x2)/mean(x2)) } plot(density(CV[,1]),col=&quot;red&quot;,main=&quot;&quot;,xlab=&quot;Coefficient of Variation&quot;, lty=1) lines(density(CV[,2]),col=&quot;blue&quot;,lty=2) abline(v=sd(sample_x)/mean(sample_x),lty=3) legend(&quot;topright&quot;,c(&quot;nonparametric&quot;,&quot;parametric(LN)&quot;), col=c(&quot;red&quot;,&quot;blue&quot;),lty=1:2,bty=&quot;n&quot; Grafik di atas membandingkan distribusi bootstrap untuk koefisien variasi, yang satu berdasarkan pendekatan nonparametrik dan yang lainnya berdasarkan pendekatan parametrik, dengan asumsi distribusi lognormal. Contoh 6.2.5. Pengamatan yang Disensor Bootstrap. Bootstrap parametrik menarik realisasi simulasi dari perkiraan parametrik dari fungsi distribusi. Dengan cara yang sama, sehingga dapat menggambar realisasi simulasi dari estimasi fungsi distribusi. Sebagai salah satu contoh, dengan mengambil dari estimasi yang dihaluskan dari fungsi distribusi yang diperkenalkan di Bagian 4.1.1.4 . Kasus khusus lainnya, yang dipertimbangkan di sini adalah menggambar estimasi dari estimator Kaplan-Meier yang dibahas di Bagian 4.3.2.2. Dengan cara ini, dapat ditangani pengamatan yang disensor. Secara khusus, kembali ke data cedera tubuh pada Contoh 6.2.1 dan 6.2.3 tetapi sekarang menyertakan 17 klaim yang disensor oleh batasan kebijakan. Dalam Contoh 4.3.6 menggunakan kumpulan data lengkap ini untuk mengestimasi estimator Kaplan-Meier dari fungsi survival yang diperkenalkan di Bagian 4.3.2.2 . Tabel 6.6 menyajikan estimasi bootstrap kuantil dari estimator fungsi survival Kaplan-Meier. Ini termasuk perkiraan presisi bootstrap, bias dan standar deviasi, serta interval kepercayaan dasar 95%. # Example from Derrig et al library(survival) # for Surv(), survfit() BIData$UnCensored &lt;- 1*(BIData$AmountPaid &lt; BIData$PolicyLimit) ## KM estimate KM0 &lt;- survfit(Surv(AmountPaid, UnCensored) ~ 1, type=&quot;kaplan-meier&quot;, data=BIData) set.seed(2019) PercentVec &lt;- c(0.50, 0.80, 0.90, 0.95, 0.98) OutBoot1 &lt;- matrix(NA,5,6) KM.survobj &lt;- Surv(BIData$AmountPaid, BIData$UnCensored) for (i in 1:length(PercentVec)) { OutBoot1[i,1] &lt;- PercentVec[i] results &lt;- bootkm(KM.survobj, q=1-PercentVec[i], B=1000, pr = FALSE) if (i==1){bootreal &lt;- results} OutBoot1[i,2] &lt;- quantile(KM0, PercentVec[i])$quantile OutBoot1[i,3] &lt;- mean(results)-OutBoot1[i,2] OutBoot1[i,4] &lt;- sd(results) # temp &lt;- boot.ci(results, type = c(&quot;norm&quot;, &quot;basic&quot;,&quot;perc&quot;)) OutBoot1[i,5] &lt;- 2*OutBoot1[i,2]-quantile(results,.975, type=6) OutBoot1[i,6] &lt;- 2*OutBoot1[i,2]-quantile(results,.025, type=6) } Hasil pada tabel di atas konsisten dengan hasil untuk subsampel tanpa sensor pada Tabel 6.4 . Pada tabel di atas tercatat kesulitan dalam memperkirakan kuantil pada pecahan besar karena penyensoran. Namun, untuk fraksi berukuran sedang (0,50, 0,80, dan 0,90), estimasi nonparametrik Kaplan-Meier (KM NP) dari kuantil konsisten dengan Tabel 6.4 . Standar Deviasi bootstrap lebih kecil pada 0,50 (sesuai dengan median) tetapi lebih besar pada level 0,80 dan 0,90. Analisis data tersensor yang dirangkum dalam tabel di atas menggunakan lebih banyak data daripada analisis subsampel tanpa sensor pada Tabel 6.4 , tetapi juga mengalami kesulitan dalam mengekstraksi informasi untuk kuantil besar. 6.3 Cross Validation Dalam bagian ini, kita akan mempelajari caranya: Membandingkan dan membedakan validasi silang dengan teknik simulasi dan metode bootstrap. Menggunakan teknik validasi silang untuk pemilihan model Menjelaskan metode jackknife sebagai kasus khusus validasi silang dan menghitung estimasi bias dan kesalahan standar jackknife Validasi silang, yang diperkenalkan secara singkat pada Bagian 4.2.4, adalah teknik yang didasarkan pada hasil simulasi. Sekarang kita akan membandingkan dan membedakan validasi silang dengan teknik simulasi lain yang telah diperkenalkan dalam bab ini.” Simulasi, atau Monte-Carlo, yang diperkenalkan pada Bagian 6.1, memungkinkan kita untuk menghitung nilai ekspektasi dan rangkuman distribusi statistik lainnya, seperti nilai-p, dengan mudah. Bootstrap, dan metode resampling lainnya yang diperkenalkan pada Bagian 6.2, menyediakan estimator presisi, atau variabilitas, statistik. Validasi silang penting ketika menilai seberapa akurat model prediktif akan bekerja dalam praktiknya. Tumpang tindih memang ada, namun tetap saja akan sangat membantu untuk memikirkan tujuan luas yang terkait dengan setiap metode statistik. Untuk membahas validasi silang, mari kita ingat kembali dari Bagian 4.2 beberapa ide kunci dari validasi model. Ketika menilai, atau memvalidasi, sebuah model, kita melihat kinerja yang diukur pada data baru, atau setidaknya bukan data yang digunakan untuk mencocokkan model. Pendekatan klasik, yang dijelaskan di Bagian 4.2.3, adalah membagi sampel menjadi dua: satu bagian (dataset pelatihan) digunakan untuk menyesuaikan model dan bagian lainnya (dataset pengujian) digunakan untuk memvalidasi. Namun, keterbatasan dari pendekatan ini adalah bahwa hasilnya bergantung pada pembagian; meskipun keseluruhan sampel tetap, pembagian antara sub-sampel pelatihan dan pengujian bervariasi secara acak. Sampel pelatihan yang berbeda berarti parameter estimasi model akan berbeda. Parameter model yang berbeda dan sampel uji yang berbeda berarti statistik validasi akan berbeda. Dua orang analis dapat menggunakan data yang sama dan model yang sama, namun mencapai kesimpulan yang berbeda tentang kelayakan suatu model (berdasarkan pembagian acak yang berbeda), sebuah situasi yang membuat frustasi. 6.3.1 k-Fold Cross-Validation Untuk mengurangi kesulitan ini, biasanya digunakan pendekatan validasi silang seperti yang diperkenalkan di Bagian 4.2.4. Ide utamanya adalah meniru pendekatan pengujian/pelatihan dasar untuk validasi model dengan mengulanginya berkali-kali melalui rata-rata dari beberapa bagian data yang berbeda. Keuntungan utamanya adalah bahwa statistik validasi tidak terikat pada model parametrik (atau nonparametrik) tertentu - seseorang dapat menggunakan statistik nonparametrik atau statistik yang memiliki interpretasi ekonomi - sehingga dapat digunakan untuk membandingkan model yang tidak bersarang (tidak seperti prosedur rasio kemungkinan). Contoh 6.3.1. Dana Properti Wisconsin. Untuk data dana properti 2010 yang diperkenalkan pada Bagian 1.3, kami mencocokkan distribusi gamma dan Pareto dengan 1.377 data klaim. Untuk rincian kecocokan terkait, lihat Lampiran Bagian 15.4.4. Sekarang kita mempertimbangkan statistik Kolmogorov-Smirnov yang diperkenalkan di Bagian 4.1.2.2. Ketika seluruh dataset telah sesuai, statistik kecocokan Kolmogorov-Smirnov untuk distribusi gamma adalah 0,2639 dan untuk distribusi Pareto adalah 0,0478. Nilai yang lebih rendah untuk distribusi Pareto menunjukkan bahwa distribusi ini lebih cocok daripada gamma. Untuk melihat bagaimana validasi silang k-lipatan bekerja, kami membagi data secara acak menjadi \\(k=8\\) kelompok, atau lipatan, yang masing-masing memiliki sekitar \\(1377/8≈172\\) pengamatan. Kemudian, kami mencocokkan model gamma dan Pareto pada set data dengan tujuh lipatan pertama (sekitar $172⋅7 = 120$4 pengamatan), menentukan estimasi parameter, dan kemudian menggunakan model-model yang cocok dengan data yang ditahan untuk menentukan statistik Kolmogorov-Smirnov. library(VGAM) ## Loading required package: stats4 ## Loading required package: splines ## ## Attaching package: &#39;VGAM&#39; ## The following objects are masked from &#39;package:boot&#39;: ## ## logit, simplex library(MASS) claim_lev &lt;- read.csv(&quot;data/CLAIMLEVEL.csv&quot;, header = TRUE) claim_data &lt;- subset(claim_lev, Year == 2010); # Randomly re-order the data - &quot;shuffle it&quot; n &lt;- nrow(claim_data) set.seed(12347) cvdata &lt;- claim_data[sample(n), ] # Number of folds k &lt;- 8 cvalvec &lt;- matrix(0,2,k) for (i in 1:k) { indices &lt;- (((i-1) * round((1/k)*nrow(cvdata))) + 1):((i*round((1/k) * nrow(cvdata)))) # Pareto fit.pareto &lt;- vglm(Claim ~ 1, paretoII, loc = 0, data = cvdata[-indices,]) ksResultPareto &lt;- ks.test(cvdata[indices,]$Claim, &quot;pparetoII&quot;, loc = 0, shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1])) cvalvec[1,i] &lt;- ksResultPareto$statistic # Gamma fit.gamma &lt;- glm(Claim ~ 1, data = cvdata[-indices,], family = Gamma(link = log)) gamma_theta &lt;- exp(coef(fit.gamma)) * gamma.dispersion(fit.gamma) alpha &lt;- 1 / gamma.dispersion(fit.gamma) ksResultGamma &lt;- ks.test(cvdata[indices,]$Claim, &quot;pgamma&quot;, shape = alpha, scale = gamma_theta) cvalvec[2,i] &lt;- ksResultGamma$statistic } KScv &lt;- rowSums(cvalvec)/k Hasilnya tampak pada Gambar 6.12 di mana sumbu horizontal adalah Fold=1. Proses ini diulangi untuk tujuh lipatan lainnya. Hasil yang dirangkum dalam Gambar 6.12 menunjukkan bahwa Pareto secara konsisten memberikan distribusi prediktif yang lebih dapat diandalkan daripada gamma. # Plot the statistics matplot(1:k,t(cvalvec),type=&quot;b&quot;, col=c(1,3), lty=1:2, ylim=c(0,0.4), pch = 0, xlab=&quot;Fold&quot;, ylab=&quot;KS Statistic&quot;) legend(&quot;left&quot;, c(&quot;Pareto&quot;, &quot;Gamma&quot;), col=c(1,3),lty=1:2, bty=&quot;n&quot;) “Figure 6.2:” Statistik Kolmogorov-Smirnov (KS) yang telah divalidasi silang untuk Data Klaim Dana Asuransi. Garis hitam solid untuk distribusi Pareto, garis putus-putus hijau untuk distribusi gamma. Statistik KS mengukur deviasi terbesar antara distribusi yang sesuai dengan distribusi empiris untuk masing-masing dari 8 kelompok, atau lipatan, data yang dipilih secara acak. 6.3.2 6.3.2 Leave-One-Out Cross-Validation Kasus khusus di mana \\(k=n\\) dikenal sebagai validasi silang tinggalkan-satu-keluar. Kasus ini secara historis sangat menonjol dan terkait erat dengan jackknifestatistik yang merupakan pendahulu dari teknik bootstrap. Meskipun kita menyajikannya sebagai kasus khusus validasi silang, akan sangat membantu jika kami memberikan definisi eksplisit. Pertimbangkan sebuah statistik umum \\(θˆ = t(x)\\) yang merupakan penaksir untuk sebuah parameter yang diminati \\(θ\\). Ide dari jackknife adalah menghitung n nilai \\(θˆ_{-i} = t(x-i)\\), di mana \\(x-i\\) adalah subsampel dari \\(x\\) dengan nilai \\(ke-i\\) dihilangkan. Rata-rata dari nilai-nilai ini dilambangkan sebagai \\[\\overline{\\widehat{\\theta}}_{(\\cdot)}=\\frac{1}{n}\\sum_{i=1}^n \\widehat{\\theta}_{-i} .\\] Nilai-nilai ini dapat digunakan untuk membuat estimasi bias dari statistik \\(\\hatθ\\) \\[\\begin{equation} Bias_{jack} = (n-1) \\left(\\overline{\\widehat{\\theta}}_{(\\cdot)} - \\widehat{\\theta}\\right) \\tag{6.3} \\end{equation}\\] serta estimasi standar deviasi \\[\\begin{equation} s_{jack} =\\sqrt{\\frac{n-1}{n}\\sum_{i=1}^n \\left(\\widehat{\\theta}_{-i} -\\overline{\\widehat{\\theta}}_{(\\cdot)}\\right)^2} ~. \\tag{6.4} \\end{equation}\\] Contoh 6.3.2. Koefisien Variasi. Sebagai ilustrasi, pertimbangkan sebuah sampel fiktif kecil \\(x = {x_1,...,x_n}\\) dengan realisasi sample_x &lt;- c(2.46,2.80,3.28,3.86,2.85,3.67,3.37,3.40, 5.22,2.55,2.79,4.50,3.37,2.88,1.44,2.56,2.00,2.07,2.19,1.77) Misalkan kita tertarik dengan \\(\\theta = CV = \\sqrt{\\mathrm{Var~}[X]}/\\mathrm{E~}[X]\\) Dengan dataset ini, estimator koefisien variasi menjadi 0,31196. Namun, seberapa handalkah estimasi tersebut? Untuk menjawab pertanyaan ini, kita dapat menghitung estimator pisau lipat dari bias dan deviasi standarnya. Kode berikut ini menunjukkan bahwa penaksir jackknife untuk bias adalah \\(Bias_{jack} = -0,00627\\) dan standar deviasi jackknife adalah \\(s_{jack} = 0,01293\\). CVar &lt;- function(x) sqrt(var(x))/mean(x) JackCVar &lt;- function(i) sqrt(var(sample_x[-i]))/mean(sample_x[-i]) JackTheta &lt;- Vectorize(JackCVar)(1:length(sample_x)) BiasJack &lt;- (length(sample_x)-1)*(mean(JackTheta) - CVar(sample_x)) sd(JackTheta) ## [1] 0.01293001 Contoh 6.3.3. Klaim Cidera Badan dan Rasio Eliminasi Kerugian. Pada Contoh 6.2.1, kita telah menunjukkan bagaimana menghitung estimasi bootstrap dari bias dan deviasi standar untuk rasio eliminasi kerugian dengan menggunakan data klaim cedera badan pada Contoh 4.1.11. Sekarang kita menindaklanjuti dengan memberikan jumlah yang sebanding dengan menggunakan statistik jackknife. Tabel 6.7 merangkum hasil estimasi jackknife. Tabel ini menunjukkan bahwa estimasi jackknife terhadap bias dan deviasi standar dari rasio eliminasi kerugian \\(E [min (X, d)]/E [X]\\) sebagian besar konsisten dengan metodologi bootstrap. Selain itu, kita dapat menggunakan standar deviasi untuk membangun interval kepercayaan berbasis normal, yang berpusat di sekitar penaksir yang dikoreksi bias. Sebagai contoh, pada \\(d = 14000\\), kita melihat pada Contoh 4.1.11 bahwa estimasi nonparametrik dari \\(LER\\) adalah 0.97678. Estimasi ini memiliki bias sebesar 0,00010, sehingga menghasilkan estimator terkoreksi-bias sebesar 0,97688. Interval kepercayaan 95% dihasilkan dengan membuat interval dua kali panjang 1,96 deviasi standar jackknife, yang berpusat pada estimator terkoreksi bias (1,96 adalah perkiraan kuantil ke-97,5 dari distribusi normal standar). library(boot) # Example from Derrig et al BIData &lt;- read.csv(&quot;data/DerrigResampling.csv&quot;, header =T) BIData$Censored &lt;- 1*(BIData$AmountPaid &gt;= BIData$PolicyLimit) BIDataUncensored &lt;- subset(BIData, Censored == 0) LER.boot &lt;- function(ded, data, indices){ resample.data &lt;- data[indices,] sumClaims &lt;- sum(resample.data$AmountPaid) sumClaims_d &lt;- sum(pmin(resample.data$AmountPaid,ded)) LER &lt;- sumClaims_d/sumClaims return(LER) } x &lt;- BIDataUncensored$AmountPaid LER.jack&lt;- function(ded,i){ LER &lt;- sum(pmin(x[-i],ded))/sum(x[-i]) return(LER) } LER &lt;- function(ded) sum(pmin(x,ded))/sum(x) ##Derrig et al set.seed(2019) dVec2 &lt;- c(4000, 5000, 10500, 11500, 14000, 18500) OutJack &lt;- matrix(0,length(dVec2),8) for (j in 1:length(dVec2)) { OutJack[j,1] &lt;- dVec2[j] results &lt;- boot(data=BIDataUncensored, statistic=LER.boot, R=1000, ded=dVec2[j]) OutJack[j,2] &lt;- results$t0 biasboot &lt;- mean(results$t)-results$t0 -&gt; OutJack[j,3] sdboot &lt;- sd(results$t) -&gt; OutJack[j,4] temp &lt;- boot.ci(results) LER.jack.ded&lt;- function(i) LER.jack(ded=dVec2[j],i) JackTheta.ded &lt;- Vectorize(LER.jack.ded)(1:length(x)) OutJack[j,5] &lt;- BiasJack.ded &lt;- (length(x)-1)*(mean(JackTheta.ded) - LER(ded=dVec2[j])) OutJack[j,6] &lt;- sd(JackTheta.ded) OutJack[j,7:8] &lt;- mean(JackTheta.ded)+qt(c(0.025,0.975),length(x)-1)*OutJack[j,6] } Table 6.7. Estimasi Jackknife dari LER pada Deductible yang Dipilih d NP Estimate Bootstrap Bias Bootstrap SD Jackknife Bias Jackknife SD Lower Jackknife 95% CI Upper Jackknife 95% CI 4000 0.54113 0.00011 0.01237 0.00031 0.00061 0.53993 0.54233 5000 0.64960 0.00027 0.01412 0.00033 0.00068 0.64825 0.65094 10500 0.93563 0.00004 0.01017 0.00019 0.00053 0.93460 0.93667 11500 0.95281 -0.00003 0.00941 0.00016 0.00047 0.95189 0.95373 14000 0.97678 0.00016 0.00687 0.00010 0.00034 0.97612 0.97745 18500 0.99382 0.00014 0.00331 0.00003 0.00017 0.99350 0.99415 Diskusi. Salah satu dari banyak hal menarik tentang kasus khusus leave-one-out adalah kemampuan untuk mereplikasi estimasi dengan tepat. Artinya, ketika ukuran lipatan hanya satu, maka tidak ada ketidakpastian tambahan yang disebabkan oleh validasi silang. Ini berarti bahwa para analis dapat mereplikasi pekerjaan satu sama lain dengan tepat, sebuah pertimbangan yang penting. Statistik Jackknife dikembangkan untuk memahami ketepatan estimator, menghasilkan estimator bias dan deviasi standar pada persamaan (6.3) dan (6.4). Hal ini sesuai dengan tujuan yang telah kita kaitkan dengan teknik bootstrap, bukan metode validasi silang. Hal ini menunjukkan bagaimana teknik statistik dapat digunakan untuk mencapai tujuan yang berbeda. 6.3.3 Cross-Validation and Bootstrap Bootstrap berguna untuk memberikan estimator presisi, atau variabilitas, dari statistik. Hal ini juga berguna untuk validasi model. Pendekatan bootstrap untuk validasi model mirip dengan prosedur validasi leave-one-out dan k-fold: Buat sampel bootstrap dengan mengambil sampel ulang (dengan penggantian) \\(n\\) indeks dalam \\({1, ⋯, n}\\). Ini akan menjadi sampel pelatihan kita. Perkirakan model yang sedang dipertimbangkan berdasarkan sampel ini. Uji, atau sampel validasi, terdiri dari pengamatan yang tidak dipilih untuk pelatihan. Mengevaluasi model yang cocok (berdasarkan data pelatihan) dengan menggunakan data uji. Ulangi proses ini beberapa kali (katakanlah \\(B\\)). Ambil rata-rata dari hasil-hasilnya dan pilih model berdasarkan statistik evaluasi rata-rata. Contoh 6.3.4. Dana Properti Wisconsin. Kembali ke Contoh 6.3.1 di mana kita menyelidiki kecocokan distribusi gamma dan Pareto pada data dana properti. Kita kembali membandingkan kinerja prediksi menggunakan statistik Kolmogorov-Smirnov (KS), namun kali ini menggunakan prosedur bootstrap untuk membagi data antara sampel pelatihan dan pengujian. Berikut ini adalah kode ilustrasinya. library(goftest) n &lt;- nrow(claim_data) set.seed(12347) indices &lt;- 1:n # Number of Bootstrap Samples B &lt;- 100 cvalvec &lt;- matrix(0,2,B) for (i in 1:B) { bootindex &lt;- unique(sample(indices, size=n, replace= TRUE)) traindata &lt;- claim_data[bootindex,] testdata &lt;- claim_data[-bootindex,] # Pareto fit.pareto &lt;- vglm(Claim ~ 1, paretoII, loc = 0, data = traindata) ksResultPareto &lt;- ks.test(testdata$Claim, &quot;pparetoII&quot;, loc = 0, shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1])) cvalvec[1,i] &lt;- ksResultPareto$statistic # Gamma fit.gamma &lt;- glm(Claim ~ 1, data = traindata, family = Gamma(link = log)) gamma_theta &lt;- exp(coef(fit.gamma)) * gamma.dispersion(fit.gamma) alpha &lt;- 1 / gamma.dispersion(fit.gamma) ksResultGamma &lt;- ks.test(testdata$Claim, &quot;pgamma&quot;, shape = alpha, scale = gamma_theta) cvalvec[2,i] &lt;- ksResultGamma$statistic } KSBoot &lt;- rowSums(cvalvec)/B Kami melakukan pengambilan sampel dengan menggunakan B= 100 ulangan. Statistik KS rata-rata untuk distribusi Pareto adalah 0,058 dibandingkan dengan rata-rata untuk distribusi gamma, 0,262. Hal ini konsisten dengan hasil sebelumnya dan memberikan bukti lain bahwa Pareto adalah model yang lebih baik untuk data ini dibandingkan dengan gamma. 6.4 Importance Sampling Bagian 6.1 memperkenalkan teknik Monte Carlo dengan menggunakan teknik inversi: untuk membangkitkan sebuah variabel acak \\(X\\) dengan distribusi \\(F\\), terapkan \\(F^{-1}\\) pada pemanggilan sebuah generator acak (seragam pada interval satuan). Bagaimana jika kita ingin menggambar sesuai dengan \\(X\\), dengan syarat \\(X∈[a,b]\\)? Seseorang dapat menggunakan mekanisme terima-tolak: menarik \\(x\\) dari distribusi \\(F\\) jika \\(x\\in[a,b]\\): simpan (“terima”) jika \\(x\\notin[a,b]\\): gambar yang lain (“tolak”) Amati bahwa dari n nilai yang awalnya dihasilkan, kita simpan di sini hanya \\([F(b)-F(a)] ⋅ n\\) hasil imbang, rata-rata. Contoh 6.4.1. Penarikan dari Distribusi Normal. Misalkan kita menggambar dari distribusi normal dengan rata-rata 2,5 dan varians 1, \\(N(2,5,1)\\), tetapi hanya tertarik pada gambar yang lebih besar dari \\(a≥2\\) dan kurang dari \\(b≤4\\). Artinya, kita hanya dapat menggunakan \\(F(4)-F(2)=Φ(4-2.5)-Φ(2-2.5) = 0.9332 - 0.3085 = 0.6247\\) proporsi undian. Gambar 6.13 menunjukkan bahwa beberapa hasil undian berada di dalam interval \\((2,4)\\) dan beberapa di luarnya. mu = 2.5 sigma = 1 a = 2 b = 4 Fa = pnorm(a,mu,sigma) Fb = pnorm(b,mu,sigma) pic_ani = function(){ u=seq(0,5,by=.01) plot(u,pnorm(u,mu,sigma),col=&quot;white&quot;,ylab=&quot;&quot;,xlab=&quot;&quot;) rect(-1,-1,6,2,col=rgb(1,0,0,.2),border=NA) rect(a,Fa,b,Fb,col=&quot;white&quot;,border=NA) lines(u,pnorm(u,mu,sigma),lwd=2) abline(v=c(a,b),lty=2,col=&quot;red&quot;) ru &lt;- runif(1) clr &lt;- &quot;red&quot; if((qnorm(ru,mu,sigma)&gt;=a)&amp;(qnorm(ru,mu,sigma)&lt;=b)) clr &lt;- &quot;blue&quot; segments(-1,ru,qnorm(ru,mu,sigma),ru,col=clr,lwd=2) arrows(qnorm(ru,mu,sigma),ru,qnorm(ru,mu,sigma),0,col=clr,lwd=2,length = .1) } for (i in 1:numAnimation) {pic_ani()} Sebagai gantinya, seseorang dapat menggambar menurut distribusi bersyarat \\(F^⋆\\) yang didefinisikan sebagai \\[F^{\\star}(x) = \\Pr(X \\le x | a &lt; X \\le b) =\\frac{F(x)-F(a)}{F(b)-F(a)}, \\ \\ \\ \\text{for } a &lt; x \\le b .\\] Dengan menggunakan metode inverse transform pada Bagian 6.1.2, kita mendapatkan hasil imbang \\[X^\\star=F^{\\star-1}\\left( U \\right) = F^{-1}\\left(F(a)+U\\cdot[F(b)-F(a)]\\right)\\] memiliki distribusi \\(F⋆^\\). Dinyatakan dengan cara lain, definisikan \\[\\tilde{U} = (1-U)\\cdot F(a)+U\\cdot F(b)\\] dan kemudian gunakan \\(F^{-1}(\\tilde{U})\\). Dengan pendekatan ini, setiap undian dihitung. Hal ini dapat dikaitkan dengan mekanisme pengambilan sampel kepentingan: kita menarik lebih sering di wilayah yang kita harapkan memiliki kuantitas yang memiliki kepentingan. Transformasi ini dapat dianggap sebagai “perubahan ukuran.” pic_ani = function(){ u=seq(0,5,by=.01) plot(u,pnorm(u,mu,sigma),col=&quot;white&quot;,ylab=&quot;&quot;,xlab=&quot;&quot;) rect(-1,-1,6,2,col=rgb(1,0,0,.2),border=NA) rect(a,Fa,b,Fb,col=&quot;white&quot;,border=NA) lines(u,pnorm(u,mu,sigma),lwd=2) abline(h=pnorm(c(a,b),mu,sigma),lty=2,col=&quot;red&quot;) ru &lt;- runif(1) rutilde &lt;- (1-ru)*Fa+ru*Fb segments(-1,rutilde,qnorm(rutilde,mu,sigma),rutilde,col=&quot;blue&quot;,lwd=2) arrows(qnorm(rutilde,mu,sigma),rutilde,qnorm(rutilde,mu,sigma),0,col=&quot;blue&quot;,lwd=2,length = .1) } for (i in 1:numAnimation) {pic_ani()} Pada Contoh 6.4.1., kebalikan dari distribusi normal sudah tersedia (dalam R, fungsinya adalah qnorm). Namun, untuk aplikasi lain, hal ini tidak terjadi. Kemudian, kita cukup menggunakan metode numerik untuk menentukan \\(X^⋆\\) sebagai solusi dari persamaan \\(F(X^\\star) =\\tilde{U}\\) di mana \\(\\tilde{U}=(1-U)\\cdot F(a)+U\\cdot F(b)\\)). Lihat kode ilustrasi berikut ini. pic_ani = function(){ u=seq(0,5,by=.01) plot(u,pnorm(u,mu,sigma),col=&quot;white&quot;,ylab=&quot;&quot;,xlab=&quot;&quot;) rect(-1,-1,6,2,col=rgb(1,0,0,.2),border=NA) rect(2,-1,4,2,col=&quot;white&quot;,border=NA) lines(u,pnorm(u,mu,sigma),lty=2) pnormstar &lt;- Vectorize(function(x){ y=(pnorm(x,mu,sigma)-Fa)/(Fb-Fa) if(x&lt;=a) y &lt;- 0 if(x&gt;=b) y &lt;- 1 return(y) }) qnormstar &lt;- function(u) as.numeric(uniroot((function (x) pnormstar(x) - u), lower = 2, upper = 4)[1]) lines(u,pnormstar(u),lwd=2) abline(v=c(2,4),lty=2,col=&quot;red&quot;) ru &lt;- runif(1) segments(-1,ru,qnormstar(ru),ru,col=&quot;blue&quot;,lwd=2) arrows(qnormstar(ru),ru,qnormstar(ru),0,col=&quot;blue&quot;,lwd=2,length = .1) } for (i in 1:numAnimation) {pic_ani()} Sebenarnya materi yang dari web untuk 6.5 itu masih sedang dalam penulisan dan belum selesai dalam pengeditan. Jadi apa yang ditulis disini hanya memberikan gambaran besarnya saja. Ide dari teknik Monte Carlo bergantung pada hukum bilangan besar (yang menjamin konvergensi rata-rata terhadap integral) dan teorema limit pusat (yang digunakan untuk mengukur ketidakpastian dalam perhitungan). Perlu diingat kembali jika (\\(X_i\\)) adalah urutan ke-i dari variabel acak dengan distribusi F, maka \\[ \\frac{1}{\\sqrt{n}}\\left(\\sum_{i=1}^n h(X_i)-\\int h(x)dF(x)\\right)\\overset{\\mathcal{L}}{\\rightarrow }\\mathcal{N}(0,\\sigma^2),\\text{ as }n\\rightarrow\\infty , \\] atau beberapa varian \\(σ^2&gt;0\\) . Namun sebenarnya, teorema ergodik dapat digunakan untuk melemahkan hasil sebelumnya, karena independensi variabel tidak diperlukan. Lebih tepatnya, jika (\\(X_i\\)) adalah Proses Markov dengan ukuran invarian \\(μ\\) , di bawah beberapa asumsi teknis tambahan, maka dapat diperoleh \\[ \\frac{1}{\\sqrt{n}}\\left(\\sum_{i=1}^n h(X_i)-\\int h(x)d\\mu(x)\\right)\\overset{\\mathcal{L}}{\\rightarrow }\\mathcal{N}(0,\\sigma_\\star^2),\\text{ as }n\\rightarrow\\infty. \\] untuk beberapa varian \\(σ^2_⋆&gt;0\\) . Oleh karena itu, dari sifat ini, dapat melihat bahwa tidak selalu mungkin untuk menghasilkan nilai-nilai independen dari F , tetapi untuk menghasilkan proses Markov dengan ukuran invarian F , dan untuk mempertimbangkan rata-rata dari proses (tidak harus independen). Dengan mempertimbangkan kasus vektor Gaussian terkendala: kami ingin menghasilkan pasangan acak dari vektor acak \\(X\\) , tetapi kami hanya tertarik pada kasus di mana jumlah komposisinya cukup besar, yang dapat ditulis \\(X^T1&gt;m\\) untuk nilai nyata \\(m\\) . Tentu saja, dimungkinkan untuk menggunakan algoritme terima-tolak, tetapi kami telah melihat bahwa ini mungkin sangat tidak efisien. Satu dapat menggunakan Metropolis Hastingsand Gibbs sampler untuk menghasilkan proses Markov dengan ukuran invarian tersebut. 6.5 6.5.1 Metropolis Hastings Algoritma agak sederhana untuk dihasilkan dari \\(f\\) : dapat dimulai dengan nilai layak \\(x_1\\) . Kemudian, pada langkah \\(t\\) , kita perlu menentukan kernel transisi : diberikan \\(x_t\\) , kita memerlukan distribusi bersyarat untuk \\(X_{t+1}\\) diberikan \\(x_t\\) . Algoritme akan bekerja dengan baik jika distribusi bersyarat itu dapat dengan mudah disimulasikan. dengan \\(π(⋅|xt)\\) menunjukkan probabilitas itu. Gambarkan nilai potensial \\(x^⋆_{t+1}\\) , dan \\(u\\) , dari distribusi seragam. Selanjutnya Menghitung \\(R= \\frac{f(x_{t+1}^\\star)}{f(x_t)}\\) jika \\(u&lt;r\\) , lalu atur \\(x_{t+1}=x^⋆_t\\) jika \\(u≤r\\) , maka atur \\(x_{t+1}=x_t\\) Di sini r disebut rasio penerimaan selanjutnya menerima nilai baru dengan probabilitas r (atau sebenarnya yang terkecil antara 1 dan r karena r dapat melebihi 1 ). Misalnya, asumsikan bahwa \\(f(⋅|xt)\\) seragam pada \\([x_t−ε,x_t+ε]\\) untuk beberapa \\(ε&gt;0\\) , dan di mana$ $f (distribusi target kita) adalah \\(N(0,1)\\) . Kami tidak akan pernah menarik dari \\(f\\) , tetapi kami akan menggunakannya untuk menghitung rasio penerimaan kami di setiap langkah. metrop1 &lt;- function(n=1000,eps=0.5){ vec &lt;- matrix(NA, n, 3) x=0 vec[1] &lt;- x for (i in 2:n) { innov &lt;- runif(1,-eps,eps) mov &lt;- x+innov R &lt;- min(1,dnorm(mov)/dnorm(x)) u &lt;- runif(1) if (u &lt; R) x &lt;- mov vec[i,] &lt;- c(x,mov,R) } return(vec)} #install.packages(&#39;gifski&#39;) #if (packageVersion(&#39;knitr&#39;) &lt; &#39;1.20.14&#39;) { # remotes::install_github(&#39;yihui/knitr&#39;) #} vec &lt;- metrop1(25) u=seq(-3,3,by=.01) pic_ani = function(k){ plot(1:k,vec[1:k,1],pch=19,xlim=c(0,25),ylim=c(-2,2),xlab=&quot;&quot;,ylab=&quot;&quot;) if(vec[k+1,1]==vec[k+1,2]) points(k+1,vec[k+1,1],col=&quot;blue&quot;,pch=19) if(vec[k+1,1]!=vec[k+1,2]) points(k+1,vec[k+1,1],col=&quot;red&quot;,pch=19) points(k+1,vec[k+1,2],cex=1.5) arrows(k+1,vec[k,1]-.5,k+1,vec[k,1]+.5,col=&quot;green&quot;,angle=90,code = 3,length=.1) polygon(c(k+dnorm(u)*10,rep(k,length(u))),c(u,rev(u)),col=rgb(0,1,0,.3), border=NA) segments(k,vec[k,1],k+dnorm(vec[k,1])*10,vec[k,1]) segments(k,vec[k+1,2],k+dnorm(vec[k+1,2])*10,vec[k+1,2]) text(k,2,round(vec[k+1,3],digits=3)) } for (k in 2:23) {pic_ani(k)} Selanjutnya dapat menggunakan simulasi, maka didapat vec &lt;- metrop1(10000) simx &lt;- vec[1000:10000,1] par(mfrow=c(1,4)) plot(simx,type=&quot;l&quot;) hist(simx,probability = TRUE,col=&quot;light blue&quot;,border=&quot;white&quot;) lines(u,dnorm(u),col=&quot;red&quot;) qqnorm(simx) acf(simx,lag=100,lwd=2,col=&quot;light blue&quot;) 6.6 6.5.2 Gibbs Sampler Dapat mempertimbangkan beberapa vektor \\(X=(X_1,⋯,X_d)\\) dengan komponen independen, \\(X_i∼E(λ_i)\\) . Selanjutnya mengambil sampel untuk sampel dari \\(X\\) yang diberikan \\(X^T1&gt;s\\) untuk beberapa ambang batas \\(s&gt;0\\) . beberapa titik awal x0 , Mengambil secara acak \\(i∈{1,⋯,d}\\) \\(X_i\\) mengingat \\(X_i&gt;s−x^T_{(−i)}1\\) berdistribusi Eksponensial \\(E(λ_i)\\) Menggambar \\(Y∼E(λ_i)\\) dan atur \\(x_i=y+(s−x^T_{(−i)}1)_+\\) hingga \\(x^T_{(−i)}1+x_i&gt;s\\) sim &lt;- NULL lambda &lt;- c(1,2) X &lt;- c(3,3) s &lt;- 5 for(k in 1:1000){ i &lt;- sample(1:2,1) X[i] &lt;- rexp(1,lambda[i])+max(0,s-sum(X[-i])) while(sum(X)&lt;s){ X[i] &lt;- rexp(1,lambda[i])+max(0,s-sum(X[-i])) } sim &lt;- rbind(sim,X) } plot(sim,xlim=c(1,11),ylim=c(0,4.3)) polygon(c(-1,-1,6),c(-1,6,-1),col=&quot;red&quot;,density=15,border=NA) abline(5,-1,col=&quot;red&quot;) Konstruksi urutan (algoritma MCMC bersifat iteratif) dapat divisualisasikan di bawah ini lambda &lt;- c(1,2) X &lt;- c(3,3) sim &lt;- X s &lt;- 5 for(k in 1:100){ set.seed(k) i &lt;- sample(1:2,1) X[i] &lt;- rexp(1,lambda[i])+max(0,s-sum(X[-i])) while(sum(X)&lt;s){ X[i] &lt;- rexp(1,lambda[i])+max(0,s-sum(X[-i])) } sim &lt;- rbind(sim,X) } pic_ani = function(n){ plot(sim[1:n,],xlim=c(1,11),ylim=c(0,5),xlab=&quot;&quot;,ylab=&quot;&quot;) i=which(apply(sim[(n-1):n,],2,diff)==0) if(i==1) abline(v=sim[n,1],col=&quot;grey&quot;) if(i==2) abline(h=sim[n,2],col=&quot;grey&quot;) if(n&gt;=1) points(sim[n,1],sim[n,2],pch=19,col=&quot;blue&quot;,cex=1.4) if(n&gt;=2) points(sim[n-1,1],sim[n-1,2],pch=19,col=&quot;red&quot;,cex=1.4) polygon(c(-1,-1,6),c(-1,6,-1),col=&quot;red&quot;,density=15,border=NA) abline(5,-1,col=&quot;red&quot;) } for (i in 2:100) {pic_ani(i)} "],["premium-foundations.html", "Bab 7 Premium Foundations 7.1 Pengenalan Ratemaking 7.2 Metode Penentuan Tarif Gabungan", " Bab 7 Premium Foundations 7.1 Pengenalan Ratemaking Pada bagian ini, Anda akan belajar cara: Menggambarkan ekspektasi sebagai metode dasar untuk menentukan premi asuransi Menganalisis persamaan akuntansi untuk menghubungkan premi dengan kerugian, biaya, dan keuntungan Merangkum strategi untuk memperluas penetapan harga untuk mencakup risiko heterogen dan tren dari waktu ke waktu. Bab ini menjelaskan bagaimana menentukan harga yang tepat untuk produk asuransi, yang dikenal sebagai premi. Premi adalah jumlah uang yang dibebankan untuk perlindungan asuransi terhadap kejadian yang tidak pasti. Dalam asuransi, harga/premi ini dikenal sebagai tarif karena dinyatakan dalam unit standar, misalnya harga per seribu dolar pertanggungan atas rumah atau manfaat jika terjadi kematian. Namun, keunikan asuransi adalah bahwa biaya perlindungan asuransi tidak diketahui pada saat penjualan kontrak. Biaya mungkin tidak terungkap selama berbulan-bulan atau bertahun-tahun, tergantung pada kejadian yang diasuransikan. Oleh karena itu, penetapan harga asuransi berbeda dengan pendekatan ekonomi pada umumnya. Dalam pendekatan penetapan harga aktuaria tradisional, harga ditentukan sebagai fungsi dari biaya asuransi. Premi dianggap sebagai sumber pendapatan yang menyediakan pembayaran klaim, biaya kontrak, dan margin operasi, yang dapat dirumuskan dalam persamaan akuntansi: \\[\\begin{equation} \\small{ \\text{Premium = Loss + Expense + UW Profit} . } \\end{equation}\\] Namun, ada pasar asuransi di mana harga aktuaria hanya memberikan masukan untuk harga pasar umum. Untuk memperkuat perbedaan ini, premi berbasis biaya aktuaria kadang-kadang dikenal sebagai harga teknis. Oleh karena itu, keputusan perusahaan seperti penetapan harga harus dievaluasi dengan mengacu pada dampaknya terhadap nilai pasar perusahaan. Tujuan ini lebih komprehensif daripada gagasan statis tentang maksimalisasi laba. Istilah Biaya dapat dibagi menjadi biaya yang bervariasi berdasarkan premi, seperti komisi penjualan, dan yang tidak, seperti biaya bangunan dan gaji karyawan. Istilah Keuntungan UW adalah singkatan dari keuntungan underwriting dan dapat mencakup biaya modal. Persamaan ini berlaku untuk jumlah banyak kontrak, atau portofolio, dan digunakan untuk membantu menetapkan premi, seperti dengan menetapkan tujuan laba. Istilah kerugian dalam persamaan tersebut didasarkan pada biaya yang diharapkan, karena sulit untuk memprediksi kerugian yang tepat untuk masing-masing kontrak. Namun, teks tersebut mengakui bahwa pendekatan ini mengasumsikan adanya ketidakpastian dan memperkenalkan prinsip-prinsip premi alternatif yang memasukkan ketidakpastian ke dalam penetapan harga. Bab ini juga memperluas pertimbangan penetapan harga ke kumpulan risiko yang heterogen dan membahas perkembangan dan tren pengalaman kerugian untuk mengembangkan tingkat suku bunga ke depan. Terakhir, bab ini memperkenalkan metode untuk memilih premi dengan membandingkan metode pemeringkatan premi dengan kerugian dari portofolio yang ditahan dan memilih metode yang menghasilkan kecocokan terbaik dengan data yang ditahan. Bab ini juga mencakup suplemen teknis mengenai peraturan pemerintah tentang tarif asuransi. 7.2 Metode Penentuan Tarif Gabungan Pada bagian ini, Anda akan belajar tentang: Definisi pure premium sebagai biaya kerugian serta dalam hal frekuensi dan keparahan. Menghitung tarif yang diindikasikan menggunakan pure premiums, biaya, dan beban keuntungan. Definisi rasio kerugian. Menghitung perubahan tarif yang diindikasikan menggunakan rasio kerugian. Membandingkan metode pure premium dan rasio kerugian untuk menentukan premi. Dalam kasus ini, diasumsikan terdapat \\(n\\) kontrak asuransi dengan kerugian (losses) \\(X1,...,Xn\\). Kontrak-kontrak tersebut memiliki distribusi kerugian yang sama dan dianggap sebagai portofolio homogen yang terdiri dari kontrak-kontrak yang sama. Hal ini dapat diterapkan pada asuransi pribadi seperti asuransi mobil atau asuransi rumah di mana perusahaan asuransi menulis banyak kontrak pada risiko yang sangat mirip. Selain itu, asumsi tentang distribusi yang identik tidak terlalu membatasi karena dalam bagian selanjutnya akan diperkenalkan variabel paparan yang memungkinkan pengalaman dapat diskalakan agar dapat dibandingkan. Dalam kasus ini, diasumsikan bahwa \\(X1,...,Xn\\) adalah iid (independen dan identik terdistribusi). 7.2.1 Metode Penghitungan Premi Murni Dalam metode ini, diperoleh estimasi kerugian yang diharapkan dengan menghitung rata-rata dari kerugian yang terjadi pada seluruh polis dalam suatu kumpulan (n polis). \\[\\begin{equation} \\small{ \\mathrm{E}(X) \\approx \\frac{\\sum_{i=1}^n X_i}{n} = \\frac{\\text{Kerugian}}{\\text{Eksposur}} = \\text{Premi Murni}. } \\end{equation}\\] Dalam kasus risiko homogen, di mana semua polis dianggap sama, jumlah polis n dapat digunakan sebagai ukuran eksposur. Namun, pada Bagian 7.4.1, konsep eksposur diperluas ketika polis tidak memiliki karakteristik yang sama. Untuk mendapatkan premi murni, kita juga dapat menggunakan pendekatan frekuensi-keparahan. Dalam hal ini, premi murni dihitung sebagai hasil kali antara frekuensi klaim dan besar kerugian. \\[\\begin{equation} \\small{ \\text{Premi Murni} = \\frac{\\text{jumlah klaim}}{\\text{Eksposur}} \\times \\frac{\\text{Kerugian}}{\\text{jumlah klaim}} = \\text{frekuensi} \\times \\text{keparahan}. } \\end{equation}\\] Ketika menggunakan metode premi murni, dapat digunakan baik rata-rata kerugian (biaya kerugian) maupun pendekatan frekuensi-keparahan untuk menentukan premi. Untuk lebih mendekatkan diri pada aplikasi dalam praktik, sekarang kita kembali ke persamaan (7.1) yang menyertakan biaya. Persamaan (7.1) juga mengacu pada Laba UW untuk laba underwriting. Ketika diskalakan dengan premi, ini dikenal sebagai pembebanan laba. Karena klaim tidak pasti, perusahaan asuransi harus memiliki modal untuk memastikan bahwa semua klaim dibayar. Memegang modal ekstra ini adalah biaya menjalankan bisnis, investor di perusahaan perlu dikompensasi untuk ini, dengan demikian pemuatan ekstra. Sekarang kita menguraikan Beban menjadi beban yang bervariasi berdasarkan premi, Variabel, beban yang tidak bervariasi,dan Premi Tetap, sehingga Beban = Variabel + Premi Tetap. Dengan menganggap biaya variabel dan laba sebagai bagian dari premi, kita mendefinisikan \\[\\begin{equation} \\small{ V = \\frac{\\text{Variable}}{\\text{Premium}} ~~~ \\text{and}~~~ Q = \\frac{\\text{UW Profit}}{\\text{Premium}} ~. } \\end{equation}\\] Dengan definisi dan persamaan (7.1) ini, kita dapat menulis \\[\\begin{equation} \\small{ \\begin{matrix} \\begin{array}{ll} \\text{Premium} &amp;= \\text{Losses + Fixed} + \\text{Premium} \\times \\frac{\\text{Variable + UW Profit}}{\\text{Premium}} \\\\ &amp; = \\text{Losses + Fixed} + \\text{Premium} \\times (V+Q) . \\end{array} \\end{matrix} } \\end{equation}\\] Penyelesaian untuk hasil premi \\[\\begin{equation} \\small{ \\text{Premium} = \\frac{\\text{Losses + Fixed}}{1-V-Q} . } \\end{equation}\\] Dibagi dengan eksposur, tarif dapat dihitung sebagai \\[\\begin{equation} \\begin{matrix} \\begin{array}{ll} \\text{Rate} &amp;= \\frac{\\text{Premium}}{\\text{Exposure}} = \\frac{\\text{Losses/Exposure + Fixed/Exposure}}{1-V-Q} \\\\ &amp;= \\frac{\\text{Pure Premium + Fixed/Exposure}}{1-V-Q} ~. \\end{array} \\end{matrix} \\end{equation}\\] Dengan kata lain, ini adalah \\[\\begin{equation} \\small{ \\text{Rate} =\\frac{\\text{pure premium + fixed expense per exposure}}{\\text{1 - variable expense factor - profit and contingencies factor}} . } \\end{equation}\\] 7.2.2 Metode Rasio Kerugian Rasio kerugian adalah rasio jumlah kerugian terhadap premi \\[\\begin{equation} \\small{ \\text{Loss Ratio} = \\frac{\\text{Loss}}{\\text{Premium}} . } \\end{equation}\\] Ketika menentukan premi, agak berlawanan dengan intuisi untuk menekankan rasio ini karena komponen premi dimasukkan ke dalam penyebut. Seperti yang akan kita lihat, metode rasio kerugian mengembangkan perubahan tingkat daripada tingkat; kita dapat menggunakan perubahan tingkat untuk memperbarui pengalaman masa lalu untuk mendapatkan tingkat saat ini. Untuk melakukan hal ini, perubahan tingkat terdiri dari rasio rasio kerugian pengalaman terhadap rasio kerugian target. Faktor penyesuaian ini kemudian diterapkan pada rate saat ini untuk mendapatkan rate yang baru. Untuk melihat cara kerjanya dalam konteks yang sederhana, mari kita kembali ke persamaan (7.1) tetapi sekarang abaikan biaya untuk mendapatkan $ Premi = Kerugian + Keuntungan UW $. Membagi dengan premi menghasilkan \\[\\begin{equation} \\small{ \\frac{\\text{UW Profit}}{\\text{Premium}} = 1 - LR = 1 - \\frac{\\text{Loss}}{\\text{Premium}} . } \\end{equation}\\] Misalkan kita memiliki pemuatan laba “target” baru, katakanlah \\(Q_{target}\\) . Dengan asumsi bahwa kerugian, eksposur, dan hal-hal lain mengenai kontrak tetap sama, maka untuk mencapai target pemuatan laba yang baru, kita akan menyesuaikan premi. Gunakan ICF untuk faktor perubahan yang ditunjukkan yang didefinisikan melalui ekspresi \\[\\begin{equation} \\small{ \\frac{\\text{New UW Profit}}{\\text{Premium}} = Q_{target} = 1 - \\frac{\\text{Loss}}{ICF \\times \\text{Premium}}. } \\end{equation}\\] Menyelesaikan untuk \\(ICF\\), kita mendapatkan }\\[\\begin{equation} \\small{ ICF = \\frac{\\text{Loss}}{\\text{Premium} \\times (1-Q_{target})} = \\frac{LR}{1-Q_{target}}. } \\end{equation}\\] Jadi, sebagai contoh, jika kita memiliki rasio kerugian saat ini = 85% dan target keuntungan \\(Q_{target} = 0,20\\), maka \\(ICF = 0,85/0,80 = 1,0625\\), yang berarti kita meningkatkan premi sebesar 6,25%. Sekarang mari kita lihat bagaimana hal ini bekerja dengan biaya dalam persamaan (7.1). Kita dapat menggunakan pengembangan yang sama seperti pada Bagian 7.2.1 dan mulai dengan persamaan (7.2), selesaikan pembebanan laba untuk mendapatkan \\[\\begin{equation} \\small{ Q = 1 - \\frac{\\text{Loss+Fixed}}{\\text{Premium}} - V . } \\end{equation}\\] Kita menginterpretasikan kuantitas \\(Rugi + Premi Tetap + V\\) sebagai “rasio biaya operasional”. Sekarang, tetapkan persentase keuntungan Q pada target dan sesuaikan premi melalui “faktor perubahan yang ditunjukkan” $ ICF \\[\\begin{equation} \\small{ Q_{target} = 1 -\\frac{\\text{Loss + Fixed}}{\\text{Premium}\\times ICF} - V . } \\end{equation}\\] Menyelesaikan untuk hasil $ ICF$ \\[\\begin{equation} {\\small \\begin{array}{ll} ICF &amp;= \\frac{\\text{Loss + Fixed}}{\\text{Premium} \\times (1 - V - Q_{target})} \\\\ &amp;= \\frac{\\text{Loss Ratio + Fixed Expense Ratio}}{1 - V - Q_{target}} . \\end{array} } \\end{equation}\\] (edit sendiri kalo ga pas, kalimatnya di ganti juga gapapa) tertanda -Garry "],["risk-classification.html", "Bab 8 Risk Classification", " Bab 8 Risk Classification "],["experience-rating-using-credibility-theory.html", "Bab 9 Experience Rating Using Credibility Theory 9.1 Pengantar Aplikasi Teori Kredibilitas 9.2 Limited Fluctuation Credibility 9.3 Bühlmann Credibility 9.4 Bühlmann-Straub Credibility", " Bab 9 Experience Rating Using Credibility Theory 9.1 Pengantar Aplikasi Teori Kredibilitas Berapa premi yang harus dibebankan untuk menyediakan asuransi? Jawabannya tergantung pada eksposur risiko kerugian. Metode yang umum digunakan untuk menghitung premi asuransi adalah dengan menilai tertanggung menggunakan rencana peringkat klasifikasi. Rencana klasifikasi digunakan untuk memilih tarif asuransi berdasarkan karakteristik peringkat tertanggung seperti wilayah geografis, usia, dll. Semua rencana pemeringkatan klasifikasi menggunakan seperangkat kriteria terbatas untuk mengelompokkan tertanggung ke dalam “kelas” dan akan ada variasi risiko kerugian di antara tertanggung di dalam kelas tersebut. Rencana pemeringkatan pengalaman mencoba untuk menangkap beberapa variasi dalam risiko kerugian di antara tertanggung dalam kelas pemeringkatan dengan menggunakan pengalaman kerugian tertanggung sendiri untuk melengkapi tingkat dari rencana pemeringkatan klasifikasi. Salah satu cara untuk melakukan hal ini adalah dengan menggunakan bobot kredibilitas \\(Z\\) dengan \\(0\\leq Z \\leq 1\\) untuk menghitung \\[\\hat{R}=Z\\bar{X}+(1-Z)M,\\] \\[\\begin{eqnarray*} \\hat{R}&amp;=&amp;\\textrm{tingkat bobot kredibilitas untuk risiko,}\\\\ \\bar{X}&amp;=&amp;\\textrm{kerugian rata-rata untuk risiko selama periode waktu tertentu,}\\\\ M&amp;=&amp;\\textrm{tingkat untuk kelompok klasifikasi, sering disebut tingkat manual.}\\\\ \\end{eqnarray*}\\] Untuk risiko yang pengalaman kerugiannya stabil dari tahun ke tahun, \\(Z\\) mungkin mendekati \\(1\\). Untuk risiko yang kerugiannya sangat bervariasi dari tahun ke tahun, \\(Z\\) mungkin mendekati \\(0\\). Teori kredibilitas juga digunakan untuk menghitung tingkat untuk masing-masing kelas dalam rencana peringkat klasifikasi. Ketika tingkat rencana klasifikasi sedang ditentukan, beberapa atau banyak kelompok mungkin tidak memiliki data yang cukup untuk menghasilkan tingkat yang stabil dan dapat diandalkan. Pengalaman kerugian aktual untuk suatu kelompok akan diberi bobot kredibilitas \\(Z\\) dan komplemen kredibilitas \\(1-Z\\) dapat diberikan pada pengalaman rata-rata untuk risiko di seluruh kelas. Atau, jika rencana pemeringkatan kelas sedang diperbarui, komplemen kredibilitas dapat diberikan pada tingkat kelas saat ini. Teori kredibilitas juga dapat diterapkan pada perhitungan frekuensi dan tingkat keparahan yang diharapkan. Menghitung nilai numerik untuk \\(Z\\) membutuhkan analisis dan pemahaman data. Apa saja varians dalam jumlah kerugian dan ukuran kerugian untuk risiko? Berapa varians antara nilai yang diharapkan di seluruh risiko? 9.2 Limited Fluctuation Credibility Di bagian ini, kita akan mempelajari cara: Hitung standar kredibilitas penuh untuk jumlah klaim, ukuran rata-rata klaim, dan kerugian agregat. Pelajari bagaimana hubungan antara sarana dan varians distribusi yang mendasari mempengaruhi standar kredibilitas penuh. Menentukan bobot kredibilitas \\(Z\\) menggunakan rumus kredibilitas parsial akar kuadrat. Kredibilitas fluktuasi terbatas, juga disebut “kredibilitas klasik” dan “kredibilitas Amerika”, diberi nama ini karena metode ini secara eksplisit mencoba untuk membatasi fluktuasi dalam estimasi frekuensi klaim, tingkat keparahan, atau kerugian. Sebagai contoh, anggaplah Anda ingin memperkirakan jumlah klaim yang diharapkan sebanyak \\(N\\) untuk sekelompok risiko dalam suatu kelas peringkat asuransi. Berapa banyak risiko yang diperlukan dalam kelas tersebut untuk memastikan bahwa tingkat akurasi tertentu dapat dicapai dalam estimasi? Pertama, pertanyaan ini akan dipertimbangkan dari perspektif berapa banyak klaim yang dibutuhkan. 9.2.1 Kredibilitas Penuh untuk Frekuensi Klaim Misalkan N adalah variabel acak yang mewakili jumlah klaim untuk sekelompok risiko, misalnya, risiko dalam klasifikasi peringkat tertentu. Jumlah klaim yang teramati akan digunakan untuk mengestimasi \\(\\mu_N=\\mathrm{E}[N]\\), jumlah klaim yang diharapkan. Seberapa besar \\(μ_N\\) yang dibutuhkan untuk mendapatkan estimasi yang baik? Salah satu cara untuk mengukur keakuratan estimasi adalah dengan pernyataan seperti: “Nilai \\(N\\) yang teramati harus berada dalam rentang 5% dari μN setidaknya 90% dari waktu.” Menuliskan ini sebagai ekspresi matematis akan menghasilkan \\(\\Pr[0.95 \\mu_N \\leq N \\leq 1.05 \\mu_N] \\geq 0.90\\). Menggeneralisasi pernyataan ini dengan membiarkan parameter rentang k menggantikan 5% dan tingkat probabilitas \\(p\\) menggantikan 0,90 memberikan persamaan \\[\\begin{equation} \\Pr[(1-k) \\mu_N \\leq N \\leq (1+k) \\mu_N] \\geq p . \\tag{9.1} \\end{equation}\\] Jumlah klaim yang diharapkan yang diperlukan agar probabilitas di sisi kiri (9.1) sama dengan \\(p\\) disebut standar kredibilitas penuh. Jika jumlah klaim yang diharapkan lebih besar atau sama dengan standar kredibilitas penuh maka kredibilitas penuh dapat diberikan pada data sehingga \\(Z = 1\\) . Biasanya nilai yang diharapkan \\(μ_N\\) tidak diketahui sehingga kredibilitas penuh akan diberikan pada data jika jumlah klaim aktual yang diamati \\(n\\) lebih besar atau sama dengan standar kredibilitas penuh. Nilai \\(k\\) dan \\(p\\) harus dipilih dan aktuaris dapat mengandalkan pengalaman, penilaian, dan faktor-faktor lain dalam membuat pilihan. Mengurangkan \\(μ_N\\) dari setiap suku dalam (9.1) dan membaginya dengan deviasi standar \\(σ_N\\) dari \\(N\\) memberikan \\[\\begin{equation} \\Pr\\left[\\frac{-k\\mu_N}{\\sigma_N}\\leq \\frac{N-\\mu_N}{\\sigma_N} \\leq \\frac{k\\mu_N}{\\sigma_N}\\right] \\geq p. \\tag{9.2} \\end{equation}\\] Dalam kredibilitas fluktuasi terbatas, distribusi normal standar digunakan untuk mendekati distribusi \\((N-\\mu_N)/\\sigma_N\\) . Jika \\(N\\) adalah jumlah dari banyak klaim dari sekelompok besar risiko yang sama dan klaim-klaim tersebut independen, maka perkiraannya mungkin masuk akal. Biarkan \\(y_p\\) adalah nilai yang sedemikian rupa sehingga \\[\\Pr[-y_p\\leq \\frac{N-\\mu_N}{\\sigma_N} \\leq y_p]=\\Phi(y_p)-\\Phi(-y_p)=p\\] di mana \\(Φ()\\) adalah fungsi distribusi kumulatif dari normal standar. Karena \\(\\Phi(-y_p)=1-\\Phi(y_p)\\) persamaan tersebut dapat ditulis ulang sebagai \\(2\\Phi(y_p)-1=p\\) . Penyelesaian untuk \\(y_p\\) memberikan \\(y_p=\\Phi^{-1}((p+1)/2)\\) dimana \\(\\Phi^{-1}( )\\) adalah kebalikan dari \\(Φ()\\) . Persamaan (9.2) akan terpenuhi jika \\(k\\mu_N/\\sigma_N \\geq y_p\\) dengan mengasumsikan aproksimasi normal. Pertama, kita akan mempertimbangkan ketidaksamaan ini untuk kasus ketika \\(N\\) memiliki distribusi Poisson: \\(\\Pr[N=n] = \\lambda^n\\textrm{e}^{-\\lambda}/n!\\) . Karena \\(\\lambda=\\mu_N=\\sigma_N^2\\) untuk Poisson, mengambil akar kuadrat menghasilkan \\(\\mu_N^{1/2}=\\sigma_N\\) . Jadi, \\(k\\mu_N/\\mu_N^{1/2} \\geq y_p\\) yang setara dengan \\(\\mu_N \\geq (y_p/k)^2\\) . Mari kita definisikan \\(\\lambda_{kp}\\) sebagai nilai dari \\(μ_N\\) yang mana kesetaraan berlaku. Maka standar kredibilitas penuh untuk distribusi Poisson adalah \\[\\begin{equation} \\lambda_{kp} = \\left(\\frac{y_p}{k}\\right)^2 \\textrm{with } y_p=\\Phi^{-1}((p+1)/2). \\tag{9.3} \\end{equation}\\] Jika jumlah klaim yang diharapkan \\(μ_N\\) lebih besar atau sama dengan \\(\\lambda_{kp}\\) maka persamaan (9.1) diasumsikan berlaku dan kredibilitas penuh dapat diberikan pada data. Sebagaimana disebutkan sebelumnya, karena \\(μ_N\\) biasanya tidak diketahui, kredibilitas penuh diberikan jika jumlah klaim yang diamati \\(n\\) memenuhi \\(n≥\\lambda_{kp}\\). Contoh 9.2.1. Standar kredibilitas penuh ditetapkan sehingga jumlah klaim yang teramati berada dalam kisaran 5% dari nilai yang diharapkan dengan probabilitas \\(p = 0.95\\) . Jika jumlah klaim berdistribusi Poisson, tentukan jumlah klaim yang dibutuhkan untuk kredibilitas penuh. Solusi. Mengacu pada tabel distribusi normal standar, \\(y_p=\\Phi^{-1}((p+1)/2)=\\Phi^{-1}((0.95+1)/2)\\)\\(\\Phi^{-1}(0.975)=1.960\\). Dengan menggunakan nilai ini dan \\(k=.05\\) lalu \\(\\lambda_{kp} = (y_p/k)^{2}=(1.960/0.05)^{2}=1,536.64\\). Setelah dibulatkan, standar kredibilitas penuhnya adalah 1.537. 9.2.2 Kredibilitas Penuh untuk Kerugian Agregat dan Premi Murni Kerugian agregat adalah total dari semua jumlah kerugian untuk risiko atau kelompok risiko. Membiarkan \\(S\\) mewakili kerugian agregat \\[S=X_1+X_2+\\cdots+X_N.\\] Variabel acak \\(N\\) mewakili jumlah kerugian dan variabel acak \\(X_1, X_2,\\ldots,X_N\\) adalah jumlah kerugian individu. Pada bagian ini diasumsikan bahwa \\(N\\) tidak bergantung pada jumlah kerugian dan bahwa \\(X_1, X_2,\\ldots,X_N\\) adalah Independen dan berdistribusi identik. Rata-rata dan varians dari \\(S\\) adalah \\[\\mu_S=\\mathrm{E}(S)=\\mathrm{E}(N)\\mathrm{E}(X)=\\mu_N\\mu_X\\] dan \\[\\sigma^{2}_S=\\mathrm{Var}(S)=\\mathrm{E}(N)\\mathrm{Var}(X)+[\\mathrm{E}(X)]^{2}\\mathrm{Var}(N)=\\mu_N\\sigma^{2}_X+\\mu^{2}_X\\sigma^{2}_N ,\\] dimana \\(X\\) adalah jumlah kerugian tunggal. Lihat diskusi tentang model risiko kolektif Kerugian yang teramati \\(S\\) akan digunakan untuk mengestimasi kerugian yang diharapkan \\(μ_S = E(S)\\) . Seperti halnya model frekuensi pada bagian sebelumnya, kerugian yang teramati harus mendekati kerugian yang diharapkan seperti yang dikuantifikasikan dalam persamaan \\[\\Pr[(1-k)\\mu_S\\leq S \\leq(1+k)\\mu_S] \\geq p.\\] Setelah mengurangi rata-rata dan membaginya dengan deviasi standar, \\[\\Pr\\left[\\frac{-k\\mu_S}{\\sigma_S}\\leq (S-\\mu_S)/\\sigma_S \\leq \\frac{k\\mu_S}{\\sigma_S}\\right] \\geq p .\\] Seperti yang dilakukan pada bagian sebelumnya, diasumsikan bahwa distribusi \\((S-\\mu_S)/\\sigma_S\\) adalah standar normal dan \\(k\\mu_S/\\sigma_S=y_p=\\Phi^{-1}((p+1)/2)\\). Persamaan ini dapat ditulis ulang sebagai \\(\\mu_S^2=(y_p/k)^2\\sigma_S^2\\). Dengan menggunakan rumus sebelumnya untuk \\(μ_S\\) dan \\(\\sigma_{S}^2\\), maka didapatkan \\((\\mu_N\\mu_X)^2=(y_p/k)^2(\\mu_N\\sigma^{2}_X+\\mu^{2}_X\\sigma^{2}_N)\\). Dengan membagi kedua sisi dengan \\(\\mu_N\\mu_X^2\\) dan mengurutkan sisi kanan, maka didapatkan standar kredibilitas penuh \\(n_S\\) untuk kerugian agregat. \\[\\begin{equation} n_S=\\left(\\frac{y_p}{k}\\right)^2\\left[\\left(\\frac{\\sigma_N^2}{\\mu_N}\\right)+\\left(\\frac{\\sigma_X}{\\mu_X}\\right)^2\\right]=\\lambda_{kp}\\left[\\left(\\frac{\\sigma_N^2}{\\mu_N}\\right)+\\left(\\frac{\\sigma_X}{\\mu_X}\\right)^2\\right]. \\tag{9.5} \\end{equation}\\] Contoh 9.2.5. Jumlah klaim memiliki distribusi Poisson. Jumlah kerugian individu didistribusikan secara independen dan identik dengan distribusi Pareto \\(F(x)=1-[\\theta/(x+\\theta)]^{\\alpha}\\). Jumlah klaim dan jumlah kerugian adalah independen. Jika kerugian agregat yang diamati harus berada dalam 5% dari nilai yang diharapkan dengan probabilitas \\(p=0.95\\), berapa banyak kerugian yang diperlukan untuk kredibilitas penuh? Solusi. Karena jumlah klaim berdistribusi Poisson, maka \\((\\sigma_N^2/\\mu_N)=1\\). Rata-rata dari distribusi Pareto adalah \\(\\mu_X=\\theta/(\\alpha-1)\\) dan variansinya adalah \\(\\sigma_X^2=\\theta^{2}\\alpha/[(\\alpha-1)^{2}(\\alpha-2)]\\), sehingga \\((\\sigma_X/\\mu_X)^2=\\alpha/(\\alpha-2)\\). Menggabungkan istilah frekuensi dan severity memberikan \\([(\\sigma_N^2/\\mu_N)+(\\sigma_X/\\mu_X)^2]=2(\\alpha-1)/(\\alpha-2)\\). Dari tabel distribusi normal standar, didapatkan \\(y_p=\\Phi^{-1}((0.95+1)/2)=1.960\\). Standar kredibilitas penuh adalah \\(n_S=(1.96/0.05)^{2}[2(\\alpha-1)/(\\alpha-2)]=3,073.28(\\alpha-1)/(\\alpha-2)\\). Jika \\(α=3\\) maka \\(n_S=6,146.56\\) untuk standar kredibilitas penuh sebesar 6.147. Perlu diperhatikan bahwa jumlah klaim yang jauh lebih banyak diperlukan untuk kredibilitas penuh untuk kerugian agregat dibandingkan dengan frekuensi saja. 9.2.3 Kredibilitas Penuh untuk Tingkat Keparahan Misalkan X adalah variabel acak yang merepresentasikan besarnya satu klaim. Severity klaim adalah \\(\\mu_X=\\mathrm{E}(X)\\). Anggaplah \\({X_1,X_2, \\ldots, X_n}\\) adalah sampel acak dari n klaim yang akan digunakan untuk mengestimasi severity klaim \\(μ_X\\). Klaim-klaim tersebut diasumsikan iid. Nilai rata-rata dari sampel adalah \\[\\bar{X}=\\frac{1}{n}\\left(X_1+X_2+\\cdots+X_n\\right).\\] Seberapa besar nilai n yang diperlukan untuk mendapatkan estimasi yang baik? Perhatikan bahwa n bukanlah variabel acak sedangkan di model kerugian agregat ia adalah variabel acak. Pada Bagian 9.2.1, akurasi sebuah estimator untuk frekuensi didefinisikan dengan menentukan agar jumlah klaim berada di dalam interval tertentu sekitar rata-rata jumlah klaim dengan probabilitas tertentu. Untuk severity, persyaratan ini adalah \\[\\Pr[(1-k)\\mu_X\\leq \\bar{X} \\leq(1+k)\\mu_X ]\\geq p ,\\] dimana \\(k\\) dan \\(p\\) harus ditentukan. Dengan mengikuti langkah-langkah pada Bagian 9.2.1, rata-rata severity klaim \\(μ_X\\) dikurangi dari setiap termin dan simpangan baku estimator severity klaim \\(\\sigma_{\\bar{X}}\\) dibagi ke dalam setiap termin sehingga diperoleh \\[\\Pr\\left[\\frac{-k~\\mu_X}{\\sigma_{\\bar{X}}}\\leq (\\bar{X}-\\mu_X)/\\sigma_{\\bar{X}} \\leq \\frac{k~\\mu_X}{\\sigma_{\\bar{X}}}\\right] \\geq p .\\] Seperti pada bagian sebelumnya, diasumsikan bahwa \\((\\bar{X}-\\mu_X)/\\sigma_{\\bar{X}}\\) secara kasar terdistribusi normal dan persamaan sebelumnya terpenuhi jika \\(k\\mu_X/\\sigma_{\\bar{X}}\\geq y_p\\) dengan \\(y_p=\\Phi^{-1}((p+1)/2)\\). Karena \\(\\bar{X}\\) adalah rata-rata klaim individual \\(X_1, X_2,\\dots, X_n\\), simpangan baku X¯ sama dengan simpangan baku klaim individual dibagi \\(\\sigma_{\\bar{X}}=\\sigma_X/\\sqrt{n}\\). Sehingga, \\(k\\mu_X/(\\sigma_X/\\sqrt{n})\\geq y_p\\) dan dengan sedikit aljabar, persamaan ini dapat dituliskan ulang sebagai \\(n \\geq (y_p/k)^2(\\sigma_X/\\mu_X)^2\\). Standar kredibilitas penuh untuk keparahan adalah \\[\\begin{equation} n_X=\\left(\\frac{y_p}{k}\\right)^2\\left(\\frac{\\sigma_X}{\\mu_X}\\right)^2=\\lambda_{kp}\\left(\\frac{\\sigma_X}{\\mu_X}\\right)^2. \\tag{9.6} \\end{equation}\\] Perhatikan bahwa istilah \\(\\sigma_X/\\mu_X\\) adalah koefisien variasi untuk klaim individual. Meskipun \\(\\lambda_{kp}\\) adalah standar kredibilitas penuh untuk frekuensi dengan diasumsikan distribusi Poisson, tidak ada asumsi tentang distribusi untuk jumlah klaim. Contoh 9.2.6. Besaran klaim individual didistribusikan secara independen dan identik dengan distribusi Pareto Tipe \\(F(x)=1-[\\theta/(x+\\theta)]^{\\alpha}\\). Berapa banyak klaim yang dibutuhkan agar rata-rata keparahan klaim yang diamati berada dalam 5% dari nilai harapan dengan probabilitas \\(p=0.95\\)? Solusi. Rata-rata Pareto adalah \\(\\mu_X=\\theta/(\\alpha-1)\\) dan variansnya adalah \\(\\sigma_X^2=\\theta^{2}\\alpha/[(\\alpha-1)^{2}(\\alpha-2)]\\) sehingga \\((\\sigma_X/\\mu_X)^2=\\alpha/(\\alpha-2)\\). Dari tabel distribusi normal standar, kita dapat menggunakan \\(y_p=\\Phi^{-1}((0.95+1)/2)=1.960\\). Standar kredibilitas penuh adalah \\(n_X=(1.96/0.05)^{2}[\\alpha/(\\alpha-2)]=1,536.64\\alpha/(\\alpha-2)\\). Misalkan \\(α=3\\) maka \\(n_X=4,609.92\\) untuk standar kredibilitas penuh sebesar 4.610. 9.2.4 Kredibilitas parsial Pada bagian sebelumnya, standar kredibilitas penuh dihitung untuk memperkirakan frekuensi (\\(n_f\\)), premi murni (\\(n_{PP}\\)), dan tingkat keparahan (\\(n_X\\)) - pada bagian ini, standar kredibilitas penuh ini akan ditandai dengan \\(n_0\\). Dalam setiap kasus, standar kredibilitas penuh adalah jumlah klaim yang diharapkan untuk mencapai tingkat akurasi tertentu saat menggunakan data empiris untuk memperkirakan nilai yang diharapkan. Jika jumlah klaim yang diamati lebih besar atau sama dengan standar kredibilitas penuh, maka bobot kredibilitas penuh \\(Z = 1\\) diberikan pada data. Dalam kredibilitas fluktuasi terbatas, bobot kredibilitas \\(Z\\) yang ditugaskan pada data adalah: \\[Z= \\left\\{ \\begin{array}{ll} \\sqrt{n /n_{0}} &amp;\\textrm{if } n &lt; n_{0} \\\\ 1 &amp; \\textrm{if } n \\ge n_{0} , \\end{array} \\right.\\] Di mana \\(n_0\\) merupakan standar kredibilitas penuh. Jumlah klaim \\(n\\) merupakan jumlah klaim untuk data yang digunakan untuk memperkirakan frekuensi yang diharapkan, tingkat keparahan, atau premi murni. Contoh 9.2.7. Jumlah klaim memiliki distribusi Poisson. Jumlah kerugian individu didistribusikan secara independen dan identik dengan distribusi Pareto Tipe II \\(F(x)=1-[\\theta/(x+\\theta)]^{\\alpha}\\). Dalam hal ini, \\(α=3\\). Jumlah klaim dan jumlah kerugian adalah independen. Standar kredibilitas penuh adalah bahwa premi murni yang diamati harus berada dalam 5% dari nilai yang diharapkan dengan probabilitas \\(p=0.95\\). Berapa kredibilitas \\(Z\\) yang diberikan untuk premi murni yang dihitung dari 1.000 klaim? Solusi. Karena jumlah klaim adalah Poisson, \\[\\frac{\\mathrm{E}(X^2)}{[\\mathrm{E}~(X)]^2} =\\frac{\\sigma_N^2}{\\mu_N}+\\left(\\frac{\\sigma_X}{\\mu_X}\\right)^2.\\] Rata-rata dari Pareto adalah \\(μX=θ/(α−1)\\) dan momen kedua adalah \\(\\mathrm{E}(X^2)=2\\theta^{2}/[(\\alpha-1)(\\alpha-2)]\\) sehingga \\(\\mathrm{E}(X^2)/[\\mathrm{E}~(X)]^2=2(\\alpha-1)/(\\alpha-2)\\). Dari tabel distribusi normal standar, \\(y_p=\\Phi^{-1}((0.95+1)/2)=1.960\\). Standar kredibilitas penuh adalah \\[n_{PP}=(1.96/0.05)^{2}[2(\\alpha-1)/(\\alpha-2)]=3,073.28(\\alpha-1)/(\\alpha-2)\\] dan jika \\(α=3\\), maka \\(n_0=n_{PP}=6,146.56\\) atau 6.147 jika dibulatkan ke atas. Kredibilitas yang diberikan untuk 1.000 klaim adalah \\(Z=(1,000/6,147)^{1/2}=0.40\\). Kredibilitas fluktuasi terbatas menggunakan rumus \\(Z=\\sqrt{n/n_0}\\) untuk membatasi fluktuasi dalam perkiraan yang diboboti kredibilitas untuk sesuai dengan fluktuasi yang diizinkan untuk data dengan jumlah klaim yang diharapkan pada standar kredibilitas penuh. Varians atau simpangan baku digunakan sebagai ukuran fluktuasi. Selanjutnya, kami akan menunjukkan contoh untuk menjelaskan mengapa rumus akar kuadrat digunakan. Misalkan tingkat keparahan klaim rata-rata sedang diestimasi dari sampel ukuran \\(n\\) yang lebih kecil dari standar kredibilitas penuh \\(n_0=n_X\\). Dengan menerapkan teori kredibilitas, perkiraan \\(\\hat{\\mu}_X\\) akan menjadi: \\[\\hat{\\mu}_X=Z\\bar{X}+(1-Z)M_X ,\\] dengan \\(\\bar{X}=(X_1+X_2+\\cdots+X_n)/n\\) dan variabel acak iid \\(X_i\\) yang mewakili ukuran klaim individu. Kredibilitas komplementer diterapkan pada \\(M_X\\) yang bisa menjadi perkiraan tingkat keparahan rata-rata tahun lalu yang disesuaikan dengan inflasi, rata-rata tingkat keparahan untuk kumpulan risiko yang jauh lebih besar, atau kuantitas relevan lainnya yang dipilih oleh aktuaris. Diasumsikan bahwa varians dari \\(M_X\\) adalah nol atau bisa diabaikan. Dengan asumsi ini, \\[\\mathrm{Var}(\\hat{\\mu}_X)=\\mathrm{Var}(Z\\bar{X})=Z^2\\mathrm{Var}(\\bar{X})=\\frac{n}{n_0}\\mathrm{Var}(\\bar{X}).\\] Karena \\(\\bar{X}=(X_1+X_2+\\cdots+X_n)/n\\) maka berlaku bahwa \\(\\mathrm{Var}(\\bar{X})=\\mathrm{Var}(X_i)/n\\) di mana variabel acak \\(X_i\\) adalah satu klaim. Oleh karena itu, \\[\\mathrm{Var}(\\hat{\\mu}_X)=\\frac{n}{n_0}\\mathrm{Var}(\\bar{X})=\\frac{n}{n_0}\\frac{\\mathrm{Var}(X_i)}{n}=\\frac{\\mathrm{Var}(X_i)}{n_0}.\\] Term terakhir adalah varians tepat dari rata-rata sampel \\(\\bar{X}\\) ketika ukuran sampel sama dengan standar kredibilitas penuh \\(n_0=n_X\\). 9.3 Bühlmann Credibility Dalam bagian ini, kita akan mempelajari: Menghitung perkiraan yang ditimbang kredibilitas untuk kerugian yang diharapkan untuk suatu risiko atau kelompok risiko. Menentukan kredibilitas \\(Z\\) yang diberikan kepada pengamatan. Menghitung nilai yang diperlukan dalam kredibilitas Bühlmann, termasuk Nilai Harapan Variansi Proses \\(( EPV )\\), Variansi Rata-rata Hipotetis \\(( VHM )\\) dan rata-rata kolektif \\(μ\\) . Mengenali situasi di mana model Bühlmann sesuai. Rencana peringkat klasifikasi mengelompokkan pemegang polis ke dalam kelas berdasarkan karakteristik risiko. Meskipun pemegang polis dalam satu kelas memiliki kesamaan, mereka tidak identik dan kerugian yang diharapkan tidak akan sama persis. Rencana peringkat pengalaman dapat melengkapi rencana peringkat kelas dengan menimbang kredibilitas pengalaman kerugian individu pemegang polis dengan tarif kelas untuk menghasilkan tarif yang lebih akurat bagi pemegang polis. Dalam penyajian kredibilitas Bühlmann, disarankan untuk menetapkan parameter risiko \\(θ\\) untuk setiap pemegang polis. Kerugian \\(X\\) untuk pemegang polis akan memiliki fungsi distribusi yang umum \\(Fθ(x)\\) dengan rata-rata \\(μ(θ)=E(X|θ)\\) dan varians \\(σ2(θ)=Var(X|θ)\\). Kerugian \\(X\\) dapat mewakili premi murni, kerugian agregat, jumlah klaim, keparahan klaim, atau ukuran kerugian lainnya untuk periode waktu, seringkali selama satu tahun. Parameter risiko \\(θ\\) dapat bersifat kontinu atau diskrit dan dapat multivariat tergantung pada model yang digunakan. Jika seorang pemegang polis dengan parameter risiko \\(θ\\) mengalami kerugian \\(X1,...,Xn\\) selama \\(n\\) periode waktu, maka tujuannya adalah untuk menemukan \\(E(μ(θ)|X1,...,Xn)\\), yaitu ekspektasi bersyarat dari \\(μ(θ)\\) yang diberikan \\(X1,...,Xn\\). Perkiraan yang ditimbang kredibilitas Bühlmann untuk \\(E(μ(θ)|X1,...,Xn)\\) untuk pemegang polis adalah sebagai berikut: \\[\\begin{equation}\\hat{\\mu}(\\theta)=Z\\bar{X}+(1-Z)\\mu \\tag{9.7}\\end{equation}\\] Dengan : \\[\\begin{eqnarray*} \\theta&amp;=&amp;\\textrm{a risk parameter that identifies a policyholder&#39;s risk level}\\\\ \\hat{\\mu}(\\theta)&amp;=&amp;\\textrm{estimated expected loss for a policyholder with parameter }\\theta\\\\ &amp; &amp; \\textrm{and loss experience } \\bar{X}\\\\ \\bar{X}&amp;=&amp;(X_1+\\cdots+X_n)/n \\textrm{ is the average of $n$ observations of the policyholder } \\\\ Z&amp;=&amp;\\textrm{credibility assigned to $n$ observations } \\\\ \\mu&amp;=&amp;\\textrm{the expected loss for a randomly chosen policyholder in the class.}\\\\ \\end{eqnarray*}\\] Untuk pemegang polis yang dipilih, asumsi variabel acak \\(Xj\\) dianggap iid untuk \\(j=1,...,n\\) karena diasumsi8kan bahwa paparan pemegang polis terhadap kerugian tidak berubah dari waktu ke waktu. Kuantitas \\(\\bar{X}\\) adalah rata-rata dari \\(n\\) pengamatan dan \\(E(\\bar{X}|θ)=E(Xj|θ)=μ(θ)\\). Jika seorang pemegang polis dipilih secara acak dari kelas dan tidak ada informasi kerugian tentang risiko, maka kerugian yang diharapkan adalah \\(μ=E(μ(θ))\\) di mana harapan diambil dari semua \\(θ\\) dalam kelas. Dalam situasi ini, \\(Z=0\\) dan kerugian yang diharapkan adalah \\(μ^(θ)=μ\\) untuk risiko tersebut. Kuantitas \\(μ\\) juga dapat ditulis sebagai \\(μ=E(Xj)\\) atau \\(μ=E(\\bar{X})\\) dan sering disebut sebagai mean keseluruhan atau collective mean. Perhatikan bahwa \\(E(Xj)\\) dievaluasi dengan hukum total ekspektasi: \\(E(Xj)=E(E[Xj|θ])\\). Example 9.3.1 Jumlah klaim \\(X\\) untuk seorang tertanggung dalam suatu kelas memiliki distribusi Poisson dengan mean \\(θ&gt;0\\). Parameter risiko \\(θ\\) didistribusikan secara eksponensial di dalam kelas dengan pdf \\(f(θ)=e−θ\\). Berapakah jumlah klaim yang diharapkan untuk seorang tertanggung yang dipilih secara acak dari kelas tersebut? Solusi Variabel acak \\(X\\) memiliki distribusi Poisson dengan parameter \\(θ\\) dan \\(E(X|θ)=θ\\). Jumlah klaim yang diharapkan untuk seorang tertanggung yang dipilih secara acak adalah \\(μ=E(μ(θ))=E(E(X|θ))=E(θ)=∫∞0θe−θdθ=1\\). Pada contoh di atas, parameter risiko \\(θ\\) adalah variabel acak dengan distribusi eksponensial. Pada contoh berikutnya, terdapat tiga jenis risiko dan parameter risiko memiliki distribusi diskrit. Example 9.3.2 Untuk setiap risiko (pemegang polis) dalam populasi, jumlah kerugian \\(N\\) dalam setahun memiliki distribusi Poisson dengan parameter \\(λ\\). Jumlah kerugian individu \\(Xi\\) untuk sebuah risiko independen dari \\(N\\) dan identik dan memiliki distribusi Pareto Tipe II dengan \\(F(x) = 1 - [θ / (x + θ)]α\\). Ada tiga jenis risiko dalam populasi sebagai berikut: \\[\\small{ \\begin{array}{|c|c|c|c|} \\hline \\text{Risk } &amp; \\text{Percentage} &amp; \\text{Poisson} &amp; \\text{Pareto} \\\\ \\text{Type} &amp; \\text{of Population} &amp; \\text{Parameter} &amp; \\text{Parameters} \\\\ \\hline A &amp; 50\\% &amp; \\lambda=0.5 &amp; \\theta=1000, \\alpha=2.0 \\\\ B &amp; 30\\% &amp; \\lambda=1.0 &amp; \\theta=1500, \\alpha=2.0 \\\\ C &amp; 20\\% &amp; \\lambda=2.0 &amp; \\theta=2000, \\alpha=2.0 \\\\ \\hline \\end{array} }\\] Jika sebuah risiko dipilih secara acak dari populasi tersebut, berapa kerugian total yang diharapkan dalam setahun? Solusi Untuk suatu risiko, jumlah klaim yang diharapkan adalah \\(E(N|λ) = λ\\). Nilai harapan dari suatu variabel acak yang didistribusikan Pareto adalah \\(E(X|θ,α) = θ/(α-1)\\). Nilai harapan dari variabel acak kehilangan agregat \\(S = X1 +⋯+XN\\) untuk risiko dengan parameter \\(λ\\), \\(α\\), dan \\(θ\\) adalah \\(E(S) = E(N)E(X) = λθ/(α-1)\\). Kerugian agregat yang diharapkan untuk suatu risiko jenis A adalah \\(E(SA) = (0,5)(1000)/(2-1) = 500\\). Kerugian agregat yang diharapkan untuk suatu risiko yang dipilih secara acak dari populasi adalah \\(E(S) = 0,5[(0,5)(1000)]+0,3[(1,0)(1500)]+0,2[(2,0)(2000)] = 1500\\). Berapa parameter risiko untuk suatu risiko (pemegang polis) pada contoh sebelumnya? Dapat dikatakan bahwa parameter risiko memiliki tiga komponen \\((λ, θ, α)\\) dengan nilai mungkin (0,5, 1000, 2,0), (1,0, 1500, 2,0), dan (2,0, 2000, 2,0) tergantung pada jenis risiko. Perlu diperhatikan bahwa pada kedua contoh tersebut, parameter risiko adalah kuantitas acak dengan distribusi probabilitasnya sendiri. Kita tidak tahu nilai parameter risiko untuk risiko yang dipilih secara acak. Meskipun formula (9.7) diperkenalkan dengan menggunakan rating pengalaman sebagai contoh, model kredibilitas Bühlmann memiliki aplikasi yang lebih luas. Misalkan ada rencana rating dengan beberapa kelas. Formula kredibilitas (9.7) dapat digunakan untuk menentukan tarif kelas individu. Rata-rata keseluruhan \\(μ\\) akan menjadi rata-rata kerugian untuk semua kelas yang digabungkan, \\(\\bar{X}\\) akan menjadi pengalaman untuk kelas individu, dan \\(μ^(θ)\\) akan menjadi perkiraan kerugian untuk kelas tersebut. 9.3.1 Credibility Z, EPV, and VHM Ketika menghitung estimasi kredibilitas \\(μ^(θ)=Z\\bar{X}+(1−Z)μ\\), berapa bobot \\(Z\\) yang harus diberikan pada pengalaman \\(\\bar{X}\\) dan berapa bobot \\((1−Z)\\) pada rata-rata keseluruhan \\(μ\\)? Dalam kredibilitas Bühlmann, terdapat tiga faktor yang perlu dipertimbangkan: Berapa variasi dalam satu pengamatan \\(Xj\\) untuk risiko yang dipilih? Dengan \\(\\bar{X} = (X1 + ⋯ + Xn) / n\\) dan dengan asumsi bahwa pengamatan adalah iid kondisional pada \\(θ\\), maka mengikuti bahwa \\(Var(\\bar{X}|θ) = Var(Xj|θ) / n\\). Untuk \\(Var(\\bar{X}|θ)\\) yang lebih besar, bobot kredibilitas \\(Z\\) yang lebih kecil harus diberikan pada pengalaman \\(\\bar{X}\\). Nilai Harapan dari Varians Proses, disingkat \\(EPV\\), adalah nilai harapan dari \\(Var(Xj|θ)\\) di seluruh risiko: \\(EPV = \\mathrm{E}(\\mathrm{Var}(X_j|\\theta)).\\) karena \\(Var(\\bar{X}|θ) = Var(Xj|θ)/n)\\) maka berlaku bahwa \\(E(Var(\\bar{X}|θ))=EPV/n\\). Seberapa homogen populasi risiko yang pengalaman kerugiannya digabungkan untuk menghitung rata-rata keseluruhan \\(μ\\)? Jika semua risiko memiliki potensi kerugian yang serupa, maka bobot yang lebih besar \\((1-Z)\\) diberikan pada rata-rata keseluruhan \\(μ\\) karena \\(μ\\) adalah rata-rata untuk sekelompok risiko yang serupa dan rata-rata \\(μ(θ)\\) tidak terlalu jauh. Homogenitas atau heterogenitas populasi diukur dengan Variance of the Hypothetical Means dengan singkatan \\(VHM\\): \\[VHM=\\mathrm{Var}(\\mathrm{E}(X_j|\\theta))=\\mathrm{Var}(\\mathrm{E}(\\bar{X}|\\theta)).\\] Perhatikan bahwa kita menggunakan \\(E(\\bar{X}|θ)=E(Xj|θ)\\) untuk kesamaan kedua. Berapa banyak pengamatan n yang digunakan untuk menghitung \\(\\bar{X}\\)? Sampel yang lebih besar akan menghasilkan \\(Z\\) yang lebih besar. Example 9.3.3 Jumlah klaim \\(N\\) dalam setahun untuk suatu risiko dalam populasi memiliki distribusi Poisson dengan mean \\(λ&gt;0\\). Parameter risiko \\(λ\\) didistribusikan secara seragam di selang (0,2). Hitung \\(EPV\\) dan \\(VHM\\) untuk populasi. Solusi Variabel acak N berdistribusi Poisson dengan parameter λ sehingga Var(N|λ)=λ . Nilai harapan dari varian proses adalah EPV=E(Var(N|λ)) = E(λ)=∫20λ12dλ=1 . Varians dari rata-rata hipotetis adalah \\(VHM=Var(E(N|λ)) = Var(λ)=E(λ^2)−(E(λ))^2 = \\int_{0}^{2}\\lambda^2 \\frac{1}{2} d\\lambda-(1)^2=\\frac{1}{3}\\) Formula kepercayaan Bühlmann meliputi nilai untuk \\(n\\), \\(EPV\\), dan \\(VHM\\): \\[\\begin{equation} Z=\\frac{n}{n+K} \\quad , \\quad K =\\frac{EPV}{VHM}. \\tag{9.8} \\end{equation}\\] Jika \\(VHM\\) meningkat maka \\(Z\\) juga meningkat. Jika \\(EPV\\) meningkat maka \\(Z\\) menjadi lebih kecil. Berbeda dengan kredibilitas fluktuasi terbatas di mana \\(Z=1\\) ketika jumlah klaim yang diharapkan lebih besar dari standar kredibilitas penuh, \\(Z\\) dapat mendekati tetapi tidak sama dengan 1 ketika jumlah pengamatan n mendekati tak hingga. Jika Anda mengalikan pembilang dan penyebut rumus \\(Z\\) dengan \\(( VHM/n )\\), maka \\(Z\\) dapat ditulis kembali sebagai: \\[Z=\\frac{VHM}{VHM+(EPV/n)} .\\] Jumlah pengamatan \\(n\\) tertangkap dalam istilah \\((EPV/n)\\). Seperti yang ditunjukkan di bullet (1) di awal bagian, \\(E(Var(\\bar{X}|θ)) = EPV/n\\). Seiring dengan bertambahnya jumlah pengamatan, varians yang diharapkan dari \\(\\bar{X}\\) menjadi lebih kecil dan kredibilitas \\(Z\\) meningkat sehingga lebih banyak bobot diberikan pada \\(\\bar{X}\\) dalam perkiraan yang dibobotkan kredibilitas \\(μ^(θ)\\). 9.4 Bühlmann-Straub Credibility Di bagian ini, Anda akan belajar cara: Menghitung perkiraan bobot kredibilitas untuk kerugian yang diharapkan untuk risiko atau kelompok risiko menggunakan model Bühlmann-Straub. Menentukan kredibilitas \\(Z\\) yang diberikan kepada pengamatan. Menghitung nilai yang dibutuhkan termasuk Expected Value of the Process Variance \\((EPV)\\), Variance of the Hypothetical Means \\((VHM)\\), dan mean kolektif \\(μ\\). Mengenali situasi di mana model Bühlmann-Straub sesuai. Dengan kredibilitas Bühlmann standar atau least-squares seperti yang dijelaskan pada bagian sebelumnya, kerugian \\(X1,…,Xn\\) yang timbul dari pemegang polis yang dipilih diasumsikan sebagai iid. Jika subskrip menunjukkan tahun 1, tahun 2, dan seterusnya hingga tahun \\(n\\), maka asumsi iid berarti bahwa pemegang polis memiliki paparan kerugian yang sama setiap tahun. Misalkan ada pemegang polis komersial yang menggunakan armada kendaraan dalam bisnisnya. Pada tahun 1, ada \\(m1\\) kendaraan dalam armada, \\(m2\\) kendaraan pada tahun 2, .., dan \\(mn\\) kendaraan pada tahun n. Paparan kerugian dari kepemilikan dan penggunaan armada ini akan tidak konstan dari tahun ke tahun. Kerugian tahunan untuk armada tersebut tidak dapat didefinisikan sebagai iid. Maka untuk permisalan tersebut \\(Yjk\\) didefinisikan sebagai kerugian untuk kendaraan ke-k dalam armada untuk tahun ke-j. Kemudian, total kerugian untuk armada pada tahun ke-j adalah \\(Yj1+⋯+Yjmj\\), di mana kita menambahkan kerugian untuk masing-masing dari \\(mj\\) kendaraan. Sedangkan dalam model Bühlmann-Straub, diasumsikan bahwa variabel acak \\(Yjk\\), iid di semua kendaraan dan tahun untuk pemegang polis. Dengan asumsi ini, rata-rata \\(E(Yjk|θ)=μ(θ)\\) dan variansi \\(Var(Yjk|θ)=σ2(θ)\\) sama untuk semua kendaraan dan tahun. Jumlah \\(μ(θ)\\) adalah kerugian yang diharapkan dan \\(σ2(θ)\\) adalah varians pada kerugian untuk satu tahun untuk satu kendaraan untuk pemegang polis dengan parameter risiko \\(θ\\). Jika \\(Xj\\) adalah kerugian rata-rata per unit paparan pada tahun ke-j, \\(Xj=(Yj1+⋯+Yjmj)/mj\\), maka \\(E(Xj|θ)=μ(θ)\\) dan \\(Var(Xj|θ)=σ^2(θ)/mj\\) untuk pemegang polis dengan parameter risiko \\(θ\\). Kerugian rata-rata per kendaraan untuk seluruh periode \\(n\\) tahun adalah: \\[\\begin{equation*} \\bar{X}= \\frac{1}{m} \\sum_{j=1}^{n} m_j X_{j} \\quad , \\quad m=\\sum_{j=1}^{n} m_j. \\end{equation*}\\] Maka berikutnya \\(E (\\bar{X}|θ)=μ(θ)\\) dan \\(Var(\\bar{X}|θ)=σ2(θ)/m\\) di mana \\(μ(θ)\\) dan \\(σ^2(θ)\\) adalah rata-rata dan varians untuk satu kendaraan selama satu tahun untuk pemegang polis. Example 9.4.1 Prove that \\(Var(\\bar{X}|θ)=σ^2(θ)/m\\) for a risk with risk parameter \\(θ\\). Solusi \\[\\begin{eqnarray*} \\mathrm{Var}(\\bar{X}|\\theta)&amp;=&amp;\\mathrm{Var}\\left(\\frac{1}{m} \\sum_{j=1}^{n} m_j X_j|\\theta \\right)\\\\ &amp;=&amp;\\frac{1}{m^2}\\sum_{j=1}^{n} \\mathrm{Var}(m_j X_{j}|\\theta)=\\frac{1}{m^2}\\sum_{j=1}^{n} m_j^2 \\mathrm{Var}(X_j|\\theta)\\\\ &amp;=&amp;\\frac{1}{m^2}\\sum_{j=1}^{n} m_j^2 (\\sigma^2(\\theta)/m_j)=\\frac{\\sigma^2(\\theta)}{m^2}\\sum_{j=1}^{n} m_j=\\sigma^2(\\theta)/m.\\\\ \\end{eqnarray*}\\] Dimana Buhlmann-Straub credibility adalah: \\[\\begin{equation}\\hat{\\mu}(\\theta)=Z\\bar{X}+(1-Z)\\mu \\tag{9.9} \\end{equation}\\] Dengan : \\[\\begin{eqnarray*} \\theta&amp;=&amp;\\textrm{a risk parameter that identifies a policyholder&#39;s risk level}\\\\ \\hat{\\mu}(\\theta)&amp;=&amp;\\textrm{estimated expected loss for one exposure for the policyholder}\\\\ &amp; &amp; \\textrm{with loss experience } \\bar{X}\\\\ \\bar{X}&amp;=&amp; \\frac{1}{m} \\sum_{j=1}^{n} m_j X_j \\textrm{ is the average loss per exposure for $m$ exposures.}\\\\ &amp; &amp; \\textrm{$X_j$ is the average loss per exposure and $m_j$ is the number of exposures in year $j$.} \\\\ Z&amp;=&amp;\\textrm{credibility assigned to $m$ exposures } \\\\ \\mu&amp;=&amp;\\textrm{expected loss for one exposure for randomly chosen}\\\\ &amp; &amp; \\textrm{ policyholder from population.}\\\\ \\end{eqnarray*}\\] Perlu diperhatikan bahwa \\(μ^(θ)\\) merupakan estimator untuk kerugian yang diharapkan untuk satu paparan. Jika pemegang polis memiliki mj paparan maka kerugian yang diharapkan adalah \\(mjμ^(θ)\\). "],["insurance-portfolio-management-including-reinsurance.html", "Bab 10 Insurance Portfolio Management including Reinsurance 10.1 Introduction to Insurance Portfolios 10.2 Tails of Distributions 10.3 Risk Measures 10.4 Reinsurance", " Bab 10 Insurance Portfolio Management including Reinsurance Portofolio asuransi adalah kumpulan kontrak asuransi. Untuk membantu mengelola ketidakpastian portofolio, bab ini akan membahas mengenai: Menghitung kewajiban yang luar biasa besar dengan memeriksa bagian ekor dari distribusi, Menghitung risiko secara keseluruhan dengan memperkenalkan ringkasan yang dikenal sebagai ukuran risiko, dan Membahas opsi-opsi penyebaran risiko portofolio melalui reasuransi, yaitu pembelian proteksi asuransi oleh perusahaan asuransi. 10.1 Introduction to Insurance Portfolios Kontrak merupakan perjanjian antara pemegang polis dan perusahaan asuransi. Penanggung, dan mengelola, portofolio yang merupakan kumpulan kontrak. Seperti di bidang keuangan lainnya, ada pilihan pengambilan keputusan manajemen yang hanya terjadi di tingkat portofolio. Misalnya, pengambilan keputusan strategis tidak terjadi di tingkat kontrak. Itu terjadi di ruang konferensi, di mana manajemen meninjau data yang tersedia dan mungkin mengarahkan arah baru. Dari perspektif portofolio, perusahaan asuransi ingin melakukan perencanaan kapasitas, menetapkan kebijakan manajemen, dan menyeimbangkan bauran produk yang dipesan untuk meningkatkan pendapatan sambil mengendalikan volatilitas. Secara konseptual bahwa perusahaan asuransi tidak lebih dari sebuah kumpulan atau portofolio, kontrak asuransi. Pada Bab 5 telah mempelajari tentang pemodelan portofolio asuransi sebagai jumlah kontrak individu berdasarkan asumsi independensi antar kontrak. Karena pentingnya hal tersebut, bab ini berfokus langsung pada distribusi portofolio. Portofolio asuransi (Kumpulan, atau agregasi, kontrak asuransi) mewakili kewajiban perusahaan asuransi dengan membahas probabilitas hasil yang besar dengan menggunakan gagasan distribusi heavy-tail di Bagian 10.2. Portofolio asuransi mewakili kewajiban perusahaan sehingga perusahaan asuransi menyimpan aset dalam jumlah yang setara untuk memenuhi kewajiban tersebut. Ukuran risiko yang diperkenalkan pada Bagian 10.3, meringkas distribusi portofolio asuransi dan ukuran ringkasan ini digunakan untuk mengukur jumlah aset yang perlu dipertahankan oleh perusahaan asuransi untuk memenuhi kewajiban. Pada Bagian 3.4 mempelajari mekanisme yang digunakan pemegang polis untuk menyebarkan risiko seperti deductible dan batasan polis. Dengan cara yang sama, perusahaan asuransi menggunakan mekanisme yang sama untuk menyebarkan risiko portofolio. Mereka membeli perlindungan risiko dari reasuradur, sebuah perusahaan asuransi untuk perusahaan asuransi.4. 10.2 Tails of Distributions Pada subab ini akan membahas mengenai: Menggambarkan distribusi ekor berat secara intuitif. Mengklasifikasikan berat ekor distribusi berdasarkan momen. Membandingkan ekor dari dua distribusi. Pada tahun 1998, hujan es turun di Ontario bagian timur, barat daya Quebec dan berlangsung selama enam hari. Peristiwa ini merupakan dua kali lipat dari curah hujan yang pernah terjadi pada badai es sebelumnya dan mengakibatkan bencana yang menghasilkan lebih dari 840.000 klaim asuransi. Jumlah ini adalah 20 lebih banyak daripada klaim yang disebabkan oleh Badai Andrew. Bencana ini menyebabkan sekitar 1,44 miliar dolar Kanada dalam penyelesaian asuransi yang merupakan beban kerugian tertinggi dalam sejarah Kanada. Ini bukan contoh yang terisolasi dengan peristiwa bencana serupa yang menyebabkan kerugian asuransi yang ekstrim adalah Badai Harvey, Superstorm Sandy, gempa bumi dan tsunami Jepang tahun 2011, dan lain sebagainya. Dalam konteks asuransi, beberapa kerugian besar yang menimpa portofolio dan kemudian dikonversi menjadi klaim biasanya mewakili bagian terbesar dari ganti rugi yang dibayarkan oleh perusahaan asuransi. Kerugian juga disebut ‘ekstrem’, dimodelkan secara kuantitatif oleh ekor dari distribusi probabilitas terkait. Misalnya, periode tekanan pada keuangan dapat muncul dengan frekuensi yang lebih tinggi dari yang diharapkan, dan kerugian asuransi dapat terjadi dengan tingkat keparahan yang lebih buruk. Oleh karena itu, studi tentang perilaku probabilistik pada bagian ekor model aktuaria sangat penting dalam kerangka kerja modern manajemen risiko kuantitatif. Untuk alasan ini, bagian ini dikhususkan untuk pengenalan beberapa gagasan matematika yang mencirikan bobot ekor variabel acak. Secara formal, definisikan X sebagai kewajiban (acak) yang muncul dari kumpulan (portofolio) kontrak asuransi. (Pada bab-bab sebelumnya telah menggunakan S untuk kerugian agregat). Pada bagian ini mempelajari ekor kanan dari distribusi X yang merepresentasikan terjadinya kerugian besar. Secara informal, sebuah variabel acak dikatakan berekor berat jika probabilitas tinggi diberikan pada nilai yang besar. Perhatikan bahwa ini tidak berarti bahwa densitas probabilitas/fungsi massa meningkat ketika nilai X menuju tak terhingga. Memang untuk variabel acak bernilai riil, pdf/pmf harus berkurang hingga tak terhingga untuk menjamin probabilitas total sama dengan satu. Namun, yang menjadi perhatian adalah laju peluruhan pdf/pmf. Hasil yang tidak diinginkan lebih mungkin terjadi pada portofolio asuransi yang digambarkan oleh variabel acak kerugian yang memiliki ekor yang lebih berat (kanan). Bobot ekor dapat berupa konsep absolut atau relatif. Khususnya, untuk yang pertamadapat menganggap variabel acak memiliki ekor yang berat jika sifat matematis tertentu dari distribusi probabilitas terpenuhi. Maka dapat dikatakan ekor dari satu distribusi lebih berat/ringan dari yang lain jika beberapa ukuran ekor lebih besar/kecil. Beberapa pendekatan kuantitatif telah diusulkan untuk mengklasifikasikan dan membandingkan bobot ekor. Di antara sebagian besar pendekatan ini, fungsi kelangsungan hidup berfungsi sebagai blok bangunan. Berikut ini merupakan memperkenalkan dua metode klasifikasi ekor yang sederhana namun berguna, yang keduanya didasarkan pada perilaku fungsi kelangsungan hidup X. 10.2.1 Classification Based on Moments Salah satu cara untuk mengklasifikasikan bobot ekor dari suatu distribusi adalah dengan menilai keberadaan momen-momen sesaae. Karena tujuan utama terletak pada ekor kanan distribusi, maka mengasumsikan variabel acak kewajiban atau kerugian \\(X\\) bernilai positif. Pada awalnya, momen sesaat ke-k dari peubah acak kontinu \\(X\\) yang diperkenalkan pada Bagian 3.1, dapat dihitung sebagai berikut. \\[\\mu_k&#39; = \\int_0^{\\infty} x^k f(x) ~dx = k \\int_0^{\\infty} x^{k-1} S(x) ~dx, \\\\\\] di mana \\(S(\\cdot)\\) menyatakan fungsi survival dari \\(X\\) . Ungkapan ini menekankan bahwa keberadaan momen mentah bergantung pada perilaku asimtotik dari fungsi survival di tak terhingga. Yakni, semakin cepat fungsi survival meluruh ke nol, semakin tinggi orde momen berhingga \\((k)\\) yang dimiliki oleh variabel acak terkait. Anda dapat menafsirkan \\(k^{\\ast}\\) sebagai nilai terbesar dari \\(k\\) sehingga momennya terbatas. Secara formal, definisikan \\(k^{\\ast}=\\sup\\{k &gt; 0:\\mu_k&#39;&lt;\\infty \\}\\) , dimana sup mewakili supremum. Definisi 10.1. Pertimbangkan variabel acak kerugian non-negatif \\(X\\) . Jika semua momen baku positif ada, yaitu orde maksimal dari momen berhingga \\(k^{\\ast}=\\infty\\) , maka \\(X\\) dikatakan berekor ringan berdasarkan metode momen. Jika \\(k^{\\ast} &lt; \\infty\\), maka \\(X\\) dikatakan berekor berat (dikatakan berekor berat jika probabilitas tinggi diberikan pada nilai yang besar) berdasarkan metode momen. Selain itu, untuk dua variabel acak rugi positif \\(X_1\\) dan \\(X_2\\) dengan orde maksimal momen masing-masing \\(k^{\\ast}_1\\) dan \\(k^{\\ast}_1\\), dengan mengatakan \\(X_1\\) memiliki ekor (kanan) yang lebih berat daripada \\(X_2\\) jika \\(k^{\\ast}_1\\leq k^{\\ast}_2\\). bagian pertama dari Definisi 10.1 adalah konsep absolut dari bobot ekor, sedangkan bagian kedua adalah konsep relatif dari bobot ekor yang membandingkan ekor (kanan) di antara dua distribusi. Selanjutnya, kami menyajikan beberapa contoh yang mengilustrasikan aplikasi metode berbasis momen untuk membandingkan bobot ekor. contoh 10.2.1. Sifat ekor ringan dari distribusi gamma. Misalkan \\(X\\sim gamma(\\alpha,\\theta)\\), dengan \\(\\alpha&gt;0\\) dan \\(\\theta&gt;0\\) , maka untuk semua \\(k&gt;0\\) , tunjukkan bahwa \\(\\mu_k&#39; &lt; \\infty\\). \\[\\begin{eqnarray*} \\mu_k&#39; &amp;=&amp; \\int_0^{\\infty} x^k \\frac{x^{\\alpha-1} e^{-x/\\theta}}{\\Gamma(\\alpha) \\theta^{\\alpha}} dx \\\\ &amp;=&amp; \\int_0^{\\infty} (y\\theta)^k \\frac{(y\\theta)^{\\alpha-1} e^{-y}}{\\Gamma(\\alpha) \\theta^{\\alpha}} \\theta dy \\\\ &amp;=&amp; \\frac{\\theta^k}{\\Gamma(\\alpha)} \\Gamma(\\alpha+k) &lt; \\infty. \\end{eqnarray*}\\] karena semua momen positif ada, yaitu \\(k^{\\ast}=\\infty\\), sesuai dengan metode klasifikasi berbasis momen pada Definisi 10.1, maka distribusi gamma berekor ringan Contoh 10.2.2. Sifat ekor ringan dari distribusi Weibull. Misalkan \\(X\\sim Weibull(\\theta,\\tau)\\), dengan \\(\\theta&gt;0\\) dan \\(\\tau&gt;0\\) , maka untuk semua \\(k&gt;0\\) , tunjukkan bahwa \\(\\mu_k&#39; &lt; \\infty\\). \\[\\begin{eqnarray*} \\mu_k&#39; &amp;=&amp; \\int_0^{\\infty} x^k \\frac{\\tau x^{\\tau-1} }{\\theta^{\\tau}} e^{-(x/\\theta)^{\\tau}}dx \\\\ &amp;=&amp; \\int_0^{\\infty} \\frac{ y^{k/\\tau} }{\\theta^{\\tau}} e^{-y/\\theta^{\\tau}}dy \\\\ &amp;=&amp; \\theta^{k} \\Gamma(1+k/\\tau) &lt; \\infty. \\end{eqnarray*}\\] Sekali lagi, karena adanya semua momen positif, distribusi Weibull berekor ringan. distribusi gamma dan Weibull digunakan secara luas dalam praktik aktuaria. Aplikasi dari kedua distribusi ini sangat luas, termasuk, namun tidak terbatas pada, pemodelan tingkat keparahan klaim asuransi, penilaian solvabilitas, pencadangan kerugian, perkiraan risiko agregat, rekayasa keandalan, dan analisis kegagalan. Sejauh ini kami telah melihat dua contoh penggunaan metode berbasis momen untuk menganalisis distribusi ekor ringan. Kami mendokumentasikan contoh distribusi ekor berat sebagai berikut. Contoh 10.2.3. Sifat ekor yang berat dari distribusi Pareto. Misalkan \\(X\\sim Pareto(\\alpha,\\theta)\\) , dengan \\(\\alpha&gt;0\\) dan \\(\\theta&gt;0\\) , maka untuk \\(k&gt;0\\) \\[\\begin{eqnarray*} \\mu_k^{&#39;} &amp;=&amp; \\int_0^{\\infty} x^k \\frac{\\alpha \\theta^{\\alpha}}{(x+\\theta)^{\\alpha+1}} dx \\\\ &amp;=&amp; \\alpha \\theta^{\\alpha} \\int_{\\theta}^{\\infty} (y-\\theta)^k {y^{-(\\alpha+1)}} dy. \\end{eqnarray*}\\] mempertimbangkan integrasi serupa: \\[\\begin{eqnarray*} g_k=\\int_{\\theta}^{\\infty} {y^{k-\\alpha-1}} dy=\\left\\{ \\begin{array}{ll} &lt;\\infty, &amp; \\hbox{for } k&lt;\\alpha;\\\\ =\\infty, &amp; \\hbox{for } k\\geq \\alpha. \\end{array} \\right. \\end{eqnarray*}\\] \\[\\lim_{y\\rightarrow \\infty} \\frac{(y-\\theta)^k {y^{-(\\alpha+1)}}}{y^{k-\\alpha-1}}=\\lim_{y\\rightarrow \\infty} (1-\\theta/y)^{k}=1.\\] Penerapan teorema perbandingan limit untuk integral tak tentu menghasilkan μ′k terbatas jika dan hanya jika gk terbatas. Oleh karena itu, kita dapat menyimpulkan bahwa momen mentah dari variabel acak Pareto hanya ada sampai k &lt; α , yaitu, k∗ = α , dan dengan demikian distribusinya berekor berat. Terlebih lagi, orde maksimal dari momen berhingga hanya bergantung pada parameter bentuk α dan merupakan fungsi yang meningkat dari α . Dengan kata lain, berdasarkan metode momen, bobot ekor dari variabel acak Pareto hanya dimanipulasi oleh α - semakin kecil nilai α , semakin berat bobot ekornya. Karena k∗&lt;∞ , ekor dari distribusi Pareto lebih berat dibandingkan dengan distribusi gamma dan Weibull. kami menyimpulkan bagian ini dengan diskusi terbuka tentang keterbatasan metode berbasis momen. Meskipun implementasinya sederhana dan interpretasi intuitif, ada beberapa keadaan tertentu di mana penerapan metode berbasis momen tidak cocok. Pertama, untuk model probabilistik yang lebih rumit, momen mentah ke-k mungkin tidak mudah untuk diperoleh, dan dengan demikian identifikasi urutan maksimal dari momen hingga dapat menjadi tantangan. Kedua, metode berbasis momen tidak sesuai dengan bagian utama dari teori heavy tail yang sudah mapan dalam literatur. Secara khusus, keberadaan fungsi pembangkit momen merupakan metode yang paling populer untuk mengklasifikasikan heavy tail versus light tail di dalam komunitas aktuaris akademis. Namun, untuk beberapa variabel acak seperti variabel acak lognormal, fungsi pembangkit momennya tidak ada bahkan semua momen positifnya terbatas. Dalam kasus ini, penerapan metode berbasis momen dapat menghasilkan penilaian bobot ekor yang berbeda. Ketiga, ketika kita perlu membandingkan bobot ekor antara dua distribusi berekor ringan yang memiliki semua momen positif, metode berbasis momen tidak lagi informatif (lihat, misalnya, Contoh 10.2.1 dan 10.2.2). 10.2.2 Comparison Based on Limiting Tail Behavior Untuk mengatasi masalah-masalah yang disebutkan di atas pada metode klasifikasi berbasis momen, sebuah pendekatan alternatif untuk membandingkan bobot ekor adalah dengan secara langsung mempelajari perilaku pembatas dari fungsi-fungsi survival. Definisi 10.2. Untuk dua variabel acak \\(X\\) dan \\(Y\\) , misalkan \\[\\gamma=\\lim_{t\\rightarrow \\infty}\\frac{S_X(t)}{S_Y(t)}.\\] Dengan : \\(X\\) memiliki ekor kanan yang lebih berat daripada \\(Y\\) jika \\(\\gamma=\\infty\\); \\(X\\) dan \\(Y\\) secara proporsional ekuivalen pada ekor kanan jika \\(\\gamma =c \\in (0, \\infty)\\); \\(X\\) memiliki ekor kanan yang lebih ringan daripada \\(Y\\) jika \\(\\gamma=0\\). Contoh 10.2.4. Perbandingan distribusi Pareto dan distribusi Weibull. Misalkan \\(X\\sim Pareto(\\alpha, \\theta)\\) dan \\(Y\\sim Weibull(\\tau, \\theta)\\), untuk \\(\\alpha&gt;0\\), \\(\\tau&gt;0\\), dan \\(\\theta&gt;0\\). Tunjukkan bahwa Pareto memiliki ekor kanan yang lebih berat daripada Weibull. \\[\\begin{eqnarray*} \\lim_{t\\rightarrow \\infty}\\frac{S_X(t)}{S_Y(t)} &amp;=&amp; \\lim_{t\\rightarrow \\infty}\\frac{(1+t/\\theta)^{-\\alpha}}{\\exp\\{-(t/\\theta)^{\\tau}\\}} \\\\ &amp;=&amp; \\lim_{t\\rightarrow \\infty}\\frac{\\exp\\{t/\\theta^{\\tau} \\}}{(1+t^{1/\\tau}/\\theta)^{\\alpha}} \\\\ &amp;=&amp; \\lim_{t\\rightarrow \\infty}\\frac{\\sum_{i=0}^{\\infty}\\left(\\frac{t}{\\theta^{\\tau}}\\right)^{i}/i!}{(1+t^{1/\\tau}/\\theta)^{\\alpha}}\\\\ &amp;=&amp; \\lim_{t\\rightarrow \\infty} \\sum_{i=0}^{\\infty} \\left(t^{-i/\\alpha}+\\frac{t^{(1/\\tau-i/\\alpha)}}{\\theta} \\right)^{-\\alpha}/\\theta^{\\tau i}i!\\\\ &amp;=&amp; \\infty. \\end{eqnarray*}\\] Oleh karena itu, distribusi Pareto memiliki ekor yang lebih berat daripada distribusi Weibull. Kita juga dapat menyadari bahwa eksponensial mencapai tak terhingga lebih cepat daripada polinomial, oleh karena itu, batas yang disebutkan di atas haruslah tak terhingga. untuk beberapa distribusi yang fungsi-fungsi kelangsungan hidupnya tidak dapat diekspresikan secara eksplisit, kita dapat menggunakan rumus alternatif berikut ini: \\[\\begin{eqnarray*} \\lim_{t\\to \\infty} \\frac{S_X(t)}{S_Y(t)} &amp;=&amp; \\lim_{t \\to \\infty} \\frac{S_X^{&#39;}(t)}{S_Y^{&#39;}(t)} \\\\ &amp;=&amp; \\lim_{t \\to \\infty} \\frac{-f_X(t)}{-f_Y(t)}\\\\ &amp;=&amp; \\lim_{t\\to \\infty} \\frac{f_X(t)}{f_Y(t)}. \\end{eqnarray*}\\] mengingat bahwa fungsi kepadatannya ada. Ini adalah aplikasi dari Aturan L’Hôpital dari kalkulus Contoh 10.2.5. Perbandingan distribusi Pareto dengan distribusi gamma. Misalkan \\(X\\sim Pareto(\\alpha, \\theta)\\) dan \\(Y\\sim gamma(\\alpha, \\theta)\\), untuk \\(\\alpha&gt;0\\) dan \\(\\theta&gt;0\\) . Tunjukkan bahwa Pareto memiliki ekor kanan yang lebih berat daripada gamma. \\[\\begin{eqnarray*} \\lim_{t\\to \\infty} \\frac{f_{X}(t)}{f_{Y}(t)} &amp;=&amp; \\lim_{t \\to \\infty} \\frac{\\alpha \\theta^{\\alpha} (t+ \\theta)^{-\\alpha-1}}{t^{\\tau-1} e^{-t/\\lambda} \\lambda^{-\\tau} \\Gamma(\\tau)^{-1}} \\\\ &amp;\\propto&amp; \\lim_{t\\to \\infty} \\frac{e^{t/\\lambda}}{(t+\\theta)^{\\alpha+1} t^{\\tau-1}} \\\\ &amp;=&amp; \\infty, \\end{eqnarray*}\\] karena eksponensial menuju tak terhingga lebih cepat daripada polinomial. 10.3 Risk Measures Materi ini akan mempelajari : Mendefinisikan ide koherensi dan menentukan apakah suatu ukuran risiko koheren atau tidak. Mendefinisikan nilai-at-risiko dan menghitung kuantitas ini untuk distribusi tertentu. Mendefinisikan nilai-at-risiko ekor dan menghitung besaran ini untuk distribusi tertentu. Dapat menyatakan bahwa risiko yang terkait dengan satu distribusi lebih berbahaya (secara asimtotik) dibandingkan distribusi lainnya jika ekornya lebih berat. Namun, mengetahui bahwa satu risiko lebih berbahaya (secara asimtotik) daripada risiko lainnya mungkin tidak memberikan informasi yang cukup untuk tujuan manajemen risiko yang canggih, dan sebagai tambahan, kita juga tertarik untuk mengukur seberapa besar risiko tersebut. Faktanya, besarnya risiko yang terkait dengan distribusi kerugian yang diberikan merupakan input penting untuk banyak aplikasi asuransi, seperti penentuan harga aktuaria, pemesanan, lindung nilai, pengawasan peraturan asuransi, dan sebagainya. 10.3.1 Coherent Risk Measures Untuk membandingkan besarnya risiko dengan cara yang praktis, kami mencari fungsi yang memetakan variabel acak kerugian yang diminati ke nilai numerik yang menunjukkan tingkat risiko, yang disebut ukuran risiko. Secara matematis, ukuran risiko secara sederhana meringkas fungsi distribusi variabel acak sebagai satu angka. Dua ukuran risiko sederhana adalah rata-rata \\(\\mathrm{E}[X]\\) dan deviasi standar \\(\\mathrm{SD}(X)=\\sqrt{\\mathrm{Var}(X)}\\). Contoh klasik lain dari ukuran risiko termasuk prinsip deviasi standar \\[ \\begin{equation} H_{\\mathrm{SD}}(X)=\\mathrm{E}[X]+\\alpha \\mathrm{SD}(X),\\text{ for } \\alpha\\geq 0, \\tag{10.1} \\end{equation} \\] dan prinsip varians \\[H_{\\mathrm{Var}}(X)=\\mathrm{E}[X]+\\alpha \\mathrm{Var}(X),\\text{ for } \\alpha\\geq 0.\\] Untuk memeriksa bahwa semua fungsi yang disebutkan di atas dapat menggunakan ukuran risiko di mana dengan memasukkan variabel acak kerugian dan fungsi-fungsi tersebut menghasilkan nilai numerik. Dengan catatan yang berbeda, fungsi \\(H^{\\ast}(X)=\\alpha X^{\\beta}\\) untuk setiap \\(\\alpha,\\beta\\neq 0\\) yang bernilai riil, β≠0 bukan merupakan ukuran risiko karena \\(H^{\\ast}\\) menghasilkan variabel acak lain dan bukan nilai numerik tunggal. Karena ukuran risiko adalah ukuran skalar yang bertujuan untuk menggunakan nilai numerik tunggal untuk menggambarkan sifat stokastik dari variabel acak kerugian, maka tidak mengherankan jika tidak ada ukuran risiko yang dapat menangkap semua informasi risiko dari variabel acak yang terkait. Oleh karena itu, ketika mencari ukuran risiko yang berguna, dalam mengingat bahwa ukuran tersebut setidaknya harus dapat ditafsirkan secara praktis; dapat dihitung dengan mudah; dan mampu merefleksikan informasi risiko yang paling penting yang mendasari distribusi kerugian. Beberapa ukuran risiko telah dikembangkan dalam literatur. Namun, tidak ada ukuran risiko terbaik yang dapat mengungguli yang lain, dan pemilihan ukuran risiko yang tepat sangat bergantung pada pertanyaan aplikasi yang dihadapi. Dalam hal ini, sangat penting untuk menekankan bahwa risiko adalah konsep yang subyektif, dan dengan demikian, bahkan dengan masalah yang sama, ada berbagai pendekatan untuk menilai risiko. Namun, untuk banyak aplikasi manajemen risiko, ada kesepakatan luas bahwa ukuran risiko yang masuk akal secara ekonomi harus memenuhi empat aksioma utama yang akan kami jelaskan secara rinci selanjutnya. Ukuran risiko yang memenuhi aksioma-aksioma ini disebut sebagai ukuran risiko yang koheren. Pertimbangkan sebuah ukuran risiko \\(H(\\cdot)\\). Ukuran ini dikatakan sebagai ukuran risiko yang koheren untuk dua variabel acak \\(X\\) dan \\(Y\\) jika aksioma-aksioma berikut ini terpenuhi. Aksioma 1. Subaditifitas: \\(H(X+Y)\\leq H(X)+H(Y)\\) Implikasi ekonomi dari aksioma ini adalah bahwa manfaat diversifikasi ada jika risiko-risiko yang berbeda digabungkan. Aksioma 2. Monotonisitas: jika \\(Pr[X≤Y]=1\\). maka H(X)≤H(Y). Ingat bahwa X dan Y adalah variabel acak yang mewakili kerugian, implikasi ekonomi yang mendasarinya adalah bahwa kerugian yang lebih tinggi pada dasarnya mengarah ke tingkat risiko yang lebih tinggi. Aksioma 3. Homogenitas positif: \\(H(cX) = cH(X)\\) untuk setiap konstanta positif c. Implikasi ekonomi yang potensial dari aksioma ini adalah bahwa ukuran risiko harus independen dari unit moneter yang digunakan untuk mengukur risiko. Sebagai contoh, misalkan c adalah nilai tukar mata uang antara dolar AS dan dolar Kanada, maka risiko kerugian acak yang diukur dalam satuan dolar AS (yaitu, X) dan dolar Kanada (yaitu, \\(cX\\)) seharusnya hanya berbeda sampai dengan nilai tukar c (yaitu, \\(cH(x)=H(cX)\\)). Aksioma 4. Ketidakvariasian terjemahan: \\(H(X + c) = H(X) + c\\) untuk setiap konstanta positif c. Jika konstanta c diinterpretasikan sebagai uang tunai bebas risiko dan X adalah portofolio asuransi, maka penambahan uang tunai ke dalam portofolio hanya meningkatkan risiko portofolio sebesar jumlah uang tunai. Memverifikasi sifat koheren untuk beberapa ukuran risiko bisa sangat mudah, tetapi terkadang sangat menantang. Sebagai contoh, adalah hal yang mudah untuk memeriksa apakah rata-rata adalah ukuran risiko yang koheren. SPECIAL CASE Rata-rata adalah ukuran risiko yang koheren. Untuk setiap pasangan variabel acak X dan Y yang memiliki mean berhingga dan konstanta \\(c&gt;0\\) validasi subaditifitas: \\(E[X+Y]=E[X]+E[Y]\\) validasi monotonitas: jika \\(Pr[X≤Y]=1\\) maka \\(E[X]≤E[Y]\\) validasi homogenitas positif: \\(E[cX]=cE[X]\\) validasi invariansi penerjemahan: \\(E[X+c]=E[X]+c\\) Untuk melihat bahwa deviasi standar bukanlah ukuran risiko yang koheren, mulailah dengan memeriksa apakah deviasi standar memenuhi Verification of the Special Case Untuk melihat bahwa deviasi standar bukanlah ukuran risiko yang koheren, mulailah dengan memeriksa apakah deviasi standar memenuhi validasi subaditifitas: \\[ \\begin{eqnarray*} \\mathrm{SD}[X+Y]&amp;=&amp;\\sqrt{\\mathrm{Var}(X)+\\mathrm{Var}(Y)+2\\mathrm{Cov}(X,Y)}\\\\ &amp;\\leq&amp; \\sqrt{\\mathrm{SD}(X)^2+\\mathrm{SD}(Y)^2+2\\mathrm{SD}(X)\\mathrm{SD}(Y)}\\\\ &amp;=&amp; \\mathrm{SD}(X)+\\mathrm{SD}(Y); \\end{eqnarray*} \\] validasi homogenitas positif: \\(\\mathrm{SD}[cX]=c~\\mathrm{SD}[X]\\) Namun, deviasi standar tidak memenuhi sifat invariansi terjemahan karena untuk setiap konstanta positif c \\(\\mathrm{SD}(X+c)=\\mathrm{SD}(X)&lt;\\mathrm{SD}(X)+c.\\) Selain itu, deviasi standar juga tidak memenuhi sifat monotonitas. Untuk melihat hal ini, pertimbangkan dua variabel acak berikut: \\[ \\begin{eqnarray} X=\\left\\{ \\begin{array}{ll} 0, &amp; \\hbox{with probability $0.25$;} \\\\ 4, &amp; \\hbox{with probability $0.75$,} \\end{array} \\right. \\tag{10.2} \\end{eqnarray} \\] dan Y adalah variabel acak yang merosot sedemikian sehingga \\[ \\begin{eqnarray} \\Pr[Y = 4] = 1. \\tag{10.3} \\end{eqnarray} \\] Dapat Memeriksa \\(\\Pr[X\\leq Y]=1\\), tapi \\[ \\mathrm{SD}(X)=\\sqrt{4^2\\cdot 0.25\\cdot 0.75}=\\sqrt{3}&gt;\\mathrm{SD}(Y)=0 \\] Special Case. The Standard Deviation Principle (10.1) is a Coherent Risk Measure. Untuk tujuan ini, untuk sebuah \\(α&gt;0\\) dapat meriksa empat aksioma untuk \\(H_{SD}(X+Y)\\) satu per satu: validasi subaditifitas : \\[ \\begin{eqnarray*} H_{\\mathrm{SD}}(X+Y) &amp;=&amp; \\mathrm{E}[X+Y]+\\alpha \\mathrm{SD}(X+Y) \\\\ &amp;\\leq&amp; \\mathrm{E}[X]+\\mathrm{E}[Y]+\\alpha [\\mathrm{SD}(X) +\\mathrm{SD}(Y)]\\\\ &amp;=&amp; H_{\\mathrm{SD}}(X)+ H_{\\mathrm{SD}}(Y); \\end{eqnarray*} \\] - validasi homogenitas positif: \\(H_{\\mathrm{SD}}(cX)=c\\mathrm{E}[X]+c\\alpha\\mathrm{SD}(X)=cH_{\\mathrm{SD}}(X);\\) validasi invariansi terjemahan: \\(H_{\\mathrm{SD}}(X+c)=\\mathrm{E}[X]+c+\\alpha\\mathrm{SD}(X)=H_{\\mathrm{SD}}(X)+c.\\) Hanya untuk memverifikasi properti monotonitas, yang mungkin terpenuhi atau tidak, tergantung pada nilai α. Untuk melihat hal ini, mempertimbangkan rumus diatas di mana \\(Pr[X≤Y]=1\\). Dengan memisalkan \\(\\alpha=0.1\\cdot \\sqrt{3}\\) maka \\(H_{\\mathrm{SD}}(X)=3+0.3=3.3&lt; H_{\\mathrm{SD}}(Y)=4\\) dan kondisi monotonitas terpenuhi. Di sisi lain, misalkan \\(\\alpha=\\sqrt{3}\\). maka \\(H_{\\mathrm{SD}}(X)=3+3=6&gt; H_{\\mathrm{SD}}(Y)=4\\) dan kondisi monotonitas tidak terpenuhi. Lebih tepatnya, dengan menetapkan \\[ H_{\\mathrm{SD}}(X) = 3+\\alpha\\sqrt{3} \\leq4= H_{\\mathrm{SD}}(Y) \\] Dapt menemukan bahwa kondisi monotonitas hanya terpenuhi untuk \\(0\\leq\\alpha\\leq 1/\\sqrt{3}\\) dan dengan demikian prinsip deviasi standar \\(H_{SD}\\) adalah koheren. Hasil ini tampak sangat intuitif bagi kami karena prinsip deviasi standar \\(H_{SD}\\) adalah kombinasi linier dari dua ukuran risiko yang satu koheren dan yang lainnya tidak koheren. Jika \\(\\alpha\\leq 1/\\sqrt{3}\\) maka ukuran yang koheren mendominasi ukuran yang tidak koheren, sehingga ukuran yang dihasilkan \\(H_{SD}\\) yang dihasilkan adalah koheren dan sebaliknya. Perlu dicatat bahwa kesimpulan di atas tidak dapat digeneralisasi untuk setiap pasangan variabel acak X dan Y. Literatur mengenai ukuran risiko telah berkembang pesat dalam hal popularitas dan kepentingannya. Dalam dua subbab berikutnya, kami memperkenalkan dua indeks yang baru-baru ini mendapatkan perhatian yang belum pernah terjadi sebelumnya di antara para ahli teori, praktisi, dan regulator. Kedua indeks tersebut adalah Value-at-Risk (\\(VaR\\)) dan Tail Value-at-Risk (\\(TVaR\\)). Alasan ekonomi di balik dua ukuran risiko populer ini mirip dengan metode klasifikasi ekor yang diperkenalkan pada bagian sebelumnya, yang dengannya kami berharap dapat menangkap risiko kerugian ekstrem yang diwakili oleh ekor distribusi. 10.3.2 Value-at-Risk Dengan mempertimbangkan variabel acak kerugian asuransi \\(X\\) . Ukuran nilai-at-risiko dari \\(X\\) dengan tingkat kepercayaan \\(q∈(0,1)\\) dirumuskan sebagai \\[ \\begin{eqnarray} VaR_q[X]=\\inf\\{x:F_X(x)\\geq q\\}. \\tag{10.4} \\end{eqnarray} \\] Di sini, \\(inf\\) adalah operator infimum sehingga ukuran \\(VaR\\) menghasilkan nilai terkecil dari \\(X\\) sedemikian rupa sehingga cdf yang terkait pertama kali melebihi atau sama dengan q . Selanjutnya dapat menginterpretasikan VaR dalam konteks aplikasi aktuarial. VaR adalah ukuran dari ‘kerugian maksimal’ yang mungkin terjadi pada produk asuransi/portfolio atau investasi berisiko, terjadi sebesar q × 100% waktu, selama periode waktu tertentu (biasanya satu tahun). Misalnya, jika X adalah variabel acak kerugian tahunan dari produk asuransi, VaR0.95 [X] = 100 juta berarti tidak lebih dari 5% peluang bahwa kerugian akan melebihi 100 juta selama satu tahun tertentu. Karena interpretasi yang bermakna ini, VaR telah menjadi standar industri untuk mengukur risiko keuangan dan asuransi sejak tahun 1990-an. Konglomerasi keuangan, regulator, dan akademisi sering menggunakan VaR untuk mengukur modal risiko, memastikan kepatuhan dengan aturan regulasi, dan mengungkapkan posisi keuangan. 10.3.2.1 Example 10.3.1. VaR for the exponential distribution Dengan mempertimbangkan variabel acak kerugian asuransi X dengan distribusi eksponensial yang memiliki parameter \\(θ\\) untuk \\(θ&gt;0\\), maka cdf dari \\(X\\) diberikan oleh \\[ F_X(x)=1-e^{-x/\\theta}, \\text{ for } x&gt;0. \\] Mencari ekspresi bentuk tertutup untuk VaR JAWAB Karena distribusi eksponensial adalah distribusi kontinu, nilai terkecil di mana cdf pertama kali melebihi atau sama dengan \\(q ∈ (0,1)\\) harus berada pada titik xq yang memenuhi. \\(q=F_X(x_q)=1-\\exp\\{-x_q/\\theta \\}.\\) Maka \\(VaR_q[X]=F_X^{-1}(q)=-\\theta[\\log(1-q)].\\) Hasil yang didapat pada rumus diatas dapat digeneralisasikan untuk variabel acak kontinu apa pun yang memiliki cdf yang ketat meningkat. Secara khusus, \\(VaR\\) dari variabel acak kontinu mana pun adalah kebalikan dari cdf yang sesuai. Mari kita pertimbangkan contoh lain dari variabel acak kontinu yang memiliki dukungan dari negatif tak terhingga hingga positif tak terhingga. 10.3.3 Example 10.3.2. VaR for the normal distribution. Dengan mempertimbangkan variabel acak kerugian asuransi \\(X\\sim Normal(\\mu,\\sigma^2)\\) dengan \\(σ&gt;0\\). Dalam kasus ini, seseorang dapat menginterpretasikan nilai negatif dari X sebagai keuntungan atau pendapatan. Berikan ekspresi bentuk tertutup untuk VaR. Karena distribusi normal adalah distribusi kontinu, maka VaR dari X harus memenuhi \\[ \\begin{eqnarray*} q &amp;=&amp; F_X(VaR_q[X])\\\\ &amp;=&amp;\\Pr\\left[(X-\\mu)/\\sigma\\leq (VaR_q[X]-\\mu)/\\sigma\\right]\\\\ &amp;=&amp;\\Phi((VaR_q[X]-\\mu)/\\sigma). \\end{eqnarray*} \\] Maka didapat \\[ VaR_q[X]=\\Phi^{-1}(q)\\ \\sigma+\\mu. \\] Dalam banyak aplikasi asuransi, kita harus menangani transformasi dari variabel acak. Misalnya, pada Contoh 10.3.2, variabel acak kerugian \\(X\\sim Normal(\\mu, \\sigma^2)\\) dapat dilihat sebagai transformasi linier dari variabel acak normal standar \\(Z\\sim Normal(0,1)\\), yaitu \\(X=Z\\sigma+\\mu\\). Dengan mengatur \\(μ = 0\\) dan \\(σ = 1\\), Dapar mempermudah untuk memeriksa \\(VaR_q[Z]=\\Phi^{-1}(q).\\). Transformasi linier dari variabel acak normal setara dengan transformasi linier dari VaR dari variabel acak asli. Temuan ini dapat digeneralisasikan lebih lanjut ke variabel acak mana pun selama transformasinya ketat meningkat. 10.3.3.1 Example 10.3.3. VaR for transformed variables. Pertimbangkan variabel acak kerugian asuransi Y dengan distribusi lognormal dengan parameter \\(μ∈R\\) dan \\(σ^2&gt;0\\) Mencari ekspresi \\(VaR\\) dari Y dalam hal invers cdf normal standar. Dengan memperhatikan bahwa \\(\\log Y\\sim Normal(\\mu,\\sigma^2)\\), atau setara dengan membiarkan \\(X\\sim Normal(\\mu,\\sigma^2)\\), maka \\(Y\\overset{d}{=}e^{X}\\) yang merupakan transformasi ketat meningkat. Di sini, notasi \\(\\overset{d}{=}\\) berarti kesamaan dalam distribusi. \\(VaR\\) dari\\(Y\\) diberikan oleh transformasi eksponensial dari VaR dari X. Secara tepat, untuk \\(q∈(0,1)\\), \\(VaR_{q}[Y]= e^{VaR_q[X]}=\\exp\\{\\Phi^{-1}(q)\\ \\sigma+\\mu\\}.\\) Sejauh ini telah melihat beberapa contoh tentang VaR untuk variabel acak kontinu, selanjutnya dengan mempertimbangkan contoh mengenai VaR untuk variabel acak diskrit. 10.3.3.2 Example 10.3.4. VaR for a discrete random variable Dengan mempertimbangkan variabel acak kerugian asuransi dengan distribusi probabilitas sebagai berikut: \\[ {\\small \\Pr[X=x] = \\left\\{ \\begin{array}{ll} 0.75, &amp; \\text{for }x=1 \\\\ 0.20, &amp; \\text{for }x=3 \\\\ 0.05, &amp; \\text{for }x=4. \\end{array} \\right. } \\] Menemukan \\(VaR\\) pada \\(q = 0.6, 0.9, 0.95, 0.95001\\) JAWAB Nilai cdf yang sesuai dari X adalah \\[ F_X(x)=\\left\\{ \\begin{array}{ll} 0, &amp; \\hbox{ $x&lt;1$;} \\\\ 0.75, &amp; \\hbox{ $1\\leq x&lt;3$;} \\\\ 0.95, &amp; \\hbox{ $3\\leq x&lt;4$;} \\\\ 1, &amp; \\hbox{ $4\\leq x$.} \\end{array} \\right. \\] Berdasarkan definisi VaR dengan demikian, maka memiliki \\(VaR_{0.6}[X]=1\\) \\(VaR_{0.9}[X]=3\\) \\(VaR_{0.95}[X]=3\\) \\(VaR_{0.950001}[X]=4\\) Selanjutnya adalah ringkasan dari bagian tentang ukuran VaR. Beberapa keuntungan dalam menggunakan VaR antara lain: memiliki interpretasi yang bermakna secara praktis; relatif mudah untuk dihitung untuk banyak distribusi dengan fungsi distribusi tertutup; tidak ada asumsi tambahan yang diperlukan untuk penghitungan VaR. Namun, ada beberapa keterbatasan dalam penggunaan VaR dalam praktik manajemen risiko, di antaranya adalah: pemilihan tingkat kepercayaan \\(q∈(0,1)\\) sangat subjektif, sementara \\(VaR\\) dapat sangat sensitif terhadap pilihan q skenario/informasi kerugian yang berada di atas \\((1-q)\\times 100\\%\\) peristiwa terburuk, sepenuhnya diabaikan; VaR bukanlah ukuran risiko yang koheren (terutama, ukuran VaR tidak memenuhi aksioma subadditivitas, artinya manfaat diversifikasi mungkin tidak sepenuhnya tercermin). 10.3.4 Tail Value-at-Risk Dengan mengingat bahwa VaR mewakili kerugian maksimal dengan peluang \\((1-q)\\times 100\\%\\). Seperti yang disebutkan pada bagian sebelumnya, satu kelemahan utama dari pengukuran VaR adalah tidak mencerminkan kerugian ekstrem yang terjadi di luar skenario terburuk dengan peluang \\((1-q)\\times 100\\%\\). Untuk tujuan ilustrasi, mari kita pertimbangkan contoh yang sedikit tidak realistis namun inspiratif berikut. 10.3.4.1 Example 10.3.5 Dengan mempertimbangkan dua variabel kerugian, \\(X\\sim Uniform [0,100]\\), dan \\(Y\\) dengan distribusi eksponensial yang memiliki parameter \\(θ = 31,71\\). Kami menggunakan \\(VaR\\) pada tingkat kepercayaan \\(95%\\) untuk mengukur tingkat risiko dari \\(X\\) dan \\(Y\\). Perhitungan sederhana memberikan \\[ VaR_{0.95}[X]=VaR_{0.95}[Y]=95, \\] dan dengan demikian kedua distribusi kerugian ini memiliki tingkat risiko yang sama menurut \\(VaR_{0.95}\\). Namun, Y lebih berisiko daripada X jika kerugian ekstrem menjadi masalah utama karena X dibatasi di atas sedangkan Y tidak dibatasi. Hanya memperkirakan risiko dengan menggunakan VaR pada tingkat kepercayaan tertentu bisa menyesatkan dan mungkin tidak mencerminkan sifat sebenarnya dari risiko. Sebagai solusinya, Tail Value-at-Risk (\\(TVaR\\)) diusulkan untuk mengukur kerugian ekstrem yang berada di atas suatu tingkat \\(VaR\\) tertentu sebagai rata-rata. Kami mendokumentasikan definisi TVaR dalam apa yang mengikuti. Untuk kesederhanaan, kami akan membatasi diri pada variabel acak positif kontinu saja, yang lebih sering digunakan dalam konteks manajemen risiko asuransi. Kami merujuk pembaca yang tertarik ke Hardy (2006) untuk diskusi yang lebih komprehensif tentang TVaR untuk variabel acak diskrit dan kontinu. Menetapkan \\(q ∈ (0,1)\\), nilai Tail Value-at-Risk dari variabel acak (kontinu) X dirumuskan sebagai \\[ \\begin{eqnarray*} TVaR_q[X] &amp;=&amp; \\mathrm{E}[X|X&gt;VaR_q[X]], \\end{eqnarray*} \\] yang diasumsikan harapan eksistensi. Perhitungan \\(TVaR\\) biasanya terdiri dari dua komponen utama - \\(VaR\\) dan rata-rata kerugian yang berada di atas \\(VaR\\). \\(TVaR\\) dapat dihitung melalui sejumlah formula. Pertimbangkan variabel acak positif kontinu X, untuk kenyamanan notional, mari kita tulis \\(\\pi_q=VaR_q[X]\\). Sesuai definisi, \\(TVaR\\) dapat dihitung melalui \\[ \\begin{eqnarray} TVaR_{q}[X]=\\frac{1}{(1-q)}\\int_{\\pi_q}^{\\infty}xf_X(x)dx. \\tag{10.5} \\end{eqnarray} \\] 10.3.4.2 Example 10.3.6. TVaR for a normal distribution Dengan mempertimbangkan variabel acak kerugian asuransi \\(X\\sim Normal (\\mu,\\sigma^2)\\) dengan μ ∈ R dan \\(σ &gt; 0\\). Mencari ekspresi \\(TVaR\\) Biarkan Z adalah variabel acak normal standar. Untuk \\(q∈(0,1)\\) , maka \\(TVaR\\) dari \\(X\\) dapat dihitung melalui \\[ \\begin{eqnarray*} TVaR_q[X] &amp;=&amp; \\mathrm{E}[X|X&gt;VaR_q[X]]\\\\ &amp;=&amp;\\mathrm{E}[\\sigma Z+\\mu|\\sigma Z+\\mu&gt;VaR_q[X]]\\\\ &amp;=&amp; \\sigma\\mathrm{E}[Z|Z&gt;(VaR_q[X]-\\mu)/\\sigma]+\\mu\\\\ &amp;\\overset{(1)}{=}&amp; \\sigma\\mathrm{E}[Z|Z&gt;VaR_q[Z]]+\\mu, \\end{eqnarray*} \\] dimana \\(\\overset{(1)}{=}\\) berlaku karena hasil yang dilaporkan pada contoh diatas. Selanjutnya, kita beralih untuk mempelajari \\(TVaR_q[Z]=\\mathrm{E}[Z|Z&gt;VaR_q[Z]]\\) dengan \\(\\omega(q)=(\\Phi^{-1}(q))^2/2\\), maka kami dapat \\[ \\begin{eqnarray*} (1-q)\\ TVaR_q[Z] &amp;=&amp; \\int_{\\Phi^{-1}(q)}^{\\infty} z \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}dz\\\\ &amp;=&amp; \\int_{\\omega(q)}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-x}dx\\\\ &amp;=&amp; \\frac{1}{\\sqrt{2\\pi}} e^{-\\omega(q)}\\\\ &amp;=&amp; \\phi(\\Phi^{-1}(q)). \\end{eqnarray*} \\] Maka \\(TVaR_q[X]=\\sigma\\frac{\\phi(\\Phi^{-1}(q))}{1-q}+\\mu.\\) Sebelumya telah diseebutkan pada subseksi sebelumnya bahwa VaR dari fungsi acak yang ketat meningkat sama dengan fungsi VaR dari variabel acak asli.Seseorang dapat menunjukkan bahwa TVaR dari transformasi linier variabel acak yang ketat meningkat sama dengan fungsi VaR dari variabel acak asli. Hal ini disebabkan oleh sifat linearitas dari harapan. Namun, temuan tersebut tidak dapat diperluas ke fungsi non-linear. Contoh variabel acak lognormal berikut menjadi contoh yang berlawanan. 10.3.4.3 Example 10.3.7. TVaR of a lognormal distribution Dengan memertimbangkan sebuah variabel acak kerugian asuransi \\(X\\) dengan distribusi lognormal dengan parameter \\(μ∈R\\) dan \\(σ&gt;0\\). dapat menunjukkan \\(TVaR_q[X] = \\frac{e^{\\mu+\\sigma^2/2}}{(1-q)} \\Phi(\\Phi^{-1}(q)-\\sigma).\\) JAWAB Dengan mengingat bahwa pdf dari distribusi lognormal dirumuskan sebagai \\[ f_X(x)=\\frac{1}{\\sigma\\sqrt{2\\pi} x}\\exp\\{-(\\log x-\\mu )^2/2\\sigma^2 \\}, \\text{ for } x&gt;0. \\] Menetapkan \\(q∈(0,1)\\), maka \\(TVaR\\) dari \\(X\\) dapat dihitung melalui \\[ \\begin{eqnarray} TVaR_q[X] &amp;=&amp; \\frac{1}{(1-q)} \\int_{\\pi_q}^{\\infty} x f_X(x)dx \\nonumber\\\\ &amp;=&amp;\\frac{1}{(1-q)} \\int_{\\pi_q}^{\\infty} \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left\\{ -\\frac{(\\log x-\\mu)^2}{2\\sigma^2} \\right\\}dx\\nonumber\\\\ &amp;\\overset{(1)}{=}&amp;\\frac{1}{(1-q)} \\int_{\\omega(q)}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{ -\\frac{1}{2}w^2+\\sigma w+\\mu}dw\\nonumber\\\\ &amp;=&amp;\\frac{e^{\\mu+\\sigma^2/2}}{(1-q)} \\int_{\\omega(q)}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{ -\\frac{1}{2}(w-\\sigma)^2}dw\\nonumber\\\\ &amp;=&amp;\\frac{e^{\\mu+\\sigma^2/2}}{(1-q)} \\Phi(\\omega(q)-\\sigma), \\tag{10.6} \\end{eqnarray} \\] Di sini, \\(\\overset{(1)}{=}\\) terpenuhi dengan menerapkan perubahan variabel \\(w=(logx−μ)/σ\\), dan \\(ω(q)=(logπq−μ)/σ\\). Dengan memanggil rumus \\(VaR\\) untuk variabel acak lognormal dapat menyederhanakan menjadi \\[ \\begin{eqnarray*} TVaR_q[X] &amp;=&amp; \\frac{e^{\\mu+\\sigma^2/2}}{(1-q)} \\Phi(\\Phi^{-1}(q)-\\sigma). \\end{eqnarray*} \\] \\(TVaR\\) dari variabel acak lognormal bukanlah eksponensial dari \\(TVaR\\) dari variabel acak normal. Untuk distribusi di mana fungsi distribusi kelangsungan hidupnya lebih mudah untuk dikerjakan, dapat menerapkan teknik integrasi dengan bagian (asumsikan rata-ratanya terbatas) untuk menulis ulang persamaan (10.5) sebagai \\[ \\begin{eqnarray*} TVaR_{q}[X]&amp;=&amp;\\left[-x S_X(x)\\big |_{\\pi_q}^{\\infty}+\\int_{\\pi_q}^{\\infty}S_X(x)dx\\right]\\frac{1}{(1-q)}\\\\ &amp;=&amp; \\pi_q +\\frac{1}{(1-q)}\\int_{\\pi_q}^{\\infty}S_X(x)dx. \\end{eqnarray*} \\] 10.3.4.4 Example 10.3.8. TVaR of an exponential distribution Pertimbangkan sebuah variabel acak kerugian asuransi X dengan distribusi eksponensial yang memiliki parameter θ untuk \\(θ&gt;0\\). Mencari suatu ekspresi untuk TVaR. JAWAB Dari subseksi sebelumnya, telah melihat bahwa \\(\\pi_q=-\\theta[\\log(1-q)].\\) Dengan mempertimbangkan \\(TVaR\\): \\[ \\begin{eqnarray*} TVaR_q[X] &amp;=&amp; \\pi_q+\\int_{\\pi_q}^{\\infty} e^{-x/\\theta}dx/(1-q)\\\\ &amp;=&amp; \\pi_q+\\theta e^{-\\pi_q/\\theta}/(1-q)\\\\ &amp;=&amp; \\pi_q+\\theta. \\end{eqnarray*} \\] Pengukuran berikut erat kaitannya dengan TVaR Juga dapat membantu untuk menyatakan \\(TVaR\\) dalam bentuk nilai harapan terbatas. Secara khusus, kita memiliki \\[ \\begin{eqnarray} TVaR_q[X] &amp;=&amp; \\int_{\\pi_q}^{\\infty} (x-\\pi_q+\\pi_q)f_X(x)dx/(1-q) \\nonumber\\\\ &amp;=&amp; \\pi_q+\\frac{1}{(1-q)}\\int_{\\pi_q}^{\\infty} (x-\\pi_q)f_X(x)dx\\nonumber\\\\ &amp;=&amp; \\pi_q+e_X(\\pi_q)\\nonumber\\\\ &amp;=&amp; \\pi_q +\\frac{\\left({\\mathrm{E}[X]-\\mathrm{E}[X\\wedge\\pi_q]}\\right)}{(1-q)}, \\tag{10.7} \\end{eqnarray} \\] Dimana \\(e_X(d)=\\mathrm{E}[X-d|X&gt;d]\\) untuk \\(d&gt;0\\) menyatakan fungsi kerugian berlebih rata-rata. Untuk banyak distribusi parametrik yang umum digunakan, rumus-rumus untuk menghitung \\(E[X\\)] dan \\(E[X∧π_q]\\) dapat ditemukan dalam tabel distribusi. 10.3.4.5 Example 10.3.9. TVaR of a Pareto distribution Pertimbangkan sebuah variabel acak kerugian \\(X\\sim Pareto(\\theta,\\alpha)\\) dengan \\(θ&gt;0\\) dan \\(α&gt;0\\). Fungsi distribusi kumulatif (cdf) dari X diberikan oleh \\(F_X(x)=1-\\left(\\frac{\\theta}{\\theta+x} \\right)^{\\alpha}, \\text{ for } x&gt;0 .\\) menetapkan \\(q∈(0,1)\\) dan atur \\(F_X(π_q)=q\\) , maka kita dapat dengan mudah memperoleh \\[ \\begin{eqnarray} \\pi_q=\\theta\\left[(1-q)^{-1/\\alpha}-1 \\right]. \\tag{10.8} \\end{eqnarray} \\] Sebelumnya diketahui bahwa \\[ \\mathrm{E}[X]=\\frac{\\theta}{\\alpha-1}, \\] dan \\[ \\mathrm{E}[X\\wedge \\pi_q]=\\frac{\\theta}{\\alpha-1}\\left[ 1-\\left(\\frac{\\theta}{\\theta+\\pi_q}\\right)^{\\alpha-1} \\right]. \\] Dengan memanfaatkan persamaan sebelumnya menghasilkan \\[ \\begin{eqnarray*} TVaR_q[X] &amp;=&amp; \\pi_q+\\frac{\\theta}{\\alpha-1} \\frac{(\\theta/(\\theta+\\pi_q))^{\\alpha-1}} {(\\theta/(\\theta+\\pi_q))^{\\alpha}}\\\\ &amp;=&amp;\\pi_q +\\frac{\\theta}{\\alpha-1}\\left( \\frac{\\pi_q+\\theta}{\\theta} \\right)\\\\ &amp;=&amp; \\pi_q+\\frac{\\pi_q+\\theta}{\\alpha-1}, \\end{eqnarray*} \\] Menyesuaikan \\(q∈(0,1)\\), nilai risiko bersyarat untuk sebuah variabel acak X diformulasikan sebagai \\[CVaR_q[X] = \\frac{1}{1-q}\\int_{q}^{1} VaR_{\\alpha}[X]\\ d\\alpha .\\] Nilai risiko bersyarat juga dikenal sebagai rata-rata nilai risiko (\\(AVaR\\)) dan kegagalan yang diharapkan (\\(ES\\)). Dapat ditunjukkan bahwa \\(CVaR_q[X] = TVaR_q[X]\\) ketika \\(\\Pr(X=VaR_q[X])=0\\), yang berlaku untuk variabel acak kontinu. Artinya, jika X kontinu, maka melalui perubahan variabel, kita dapat menulis ulang persamaan sebagai \\[ \\begin{eqnarray} TVaR_{q}[X] &amp;=&amp; \\frac{1}{1-q}\\int_{q}^{1} VaR_{\\alpha}[X]\\ d\\alpha. \\tag{10.9} \\end{eqnarray} \\] Formula alternatif (10.9) ini memberitahu bahwa \\(TVaR\\) adalah rata-rata dari \\(VaR_α[X]\\) dengan tingkat kepercayaan yang bervariasi di atas \\(α∈[q,1]\\). Oleh karena itu, \\(TVaR\\) secara efektif menyelesaikan sebagian besar keterbatasan dari VaR yang diuraikan pada subbab sebelumnya. Pertama, karena efek rata-rata, TVaR mungkin kurang sensitif terhadap perubahan tingkat kepercayaan dibandingkan dengan VaR. Kedua, semua kerugian ekstrem yang berada di atas peristiwa terburuk \\((1-q)\\times 100\\%\\) yang paling mungkin dihitung. Dalam hal ini, seseorang dapat melihat bahwa untuk setiap \\(q∈(0,1)\\) \\(TVaR_q[X]\\geq VaR_q[X].\\) Ketiga dan mungkin yang paling penting, TVaR adalah ukuran risiko koheren dan dengan demikian mampu menangkap efek diversifikasi dari portofolio asuransi dengan lebih akurat. 10.4 Reinsurance reasuransi adalah asuransi yang dibeli oleh perusahaan asuransi. Berbeda dengan asuransi yang dibeli oleh individu, reasuransi biasanya dirancang khusus untuk pembeli dan memiliki fleksibilitas kontrak yang lebih besar. Ada dua jenis reasuransi, yaitu reasuransi proporsional dan non-proporsional. Reasuransi proporsional melibatkan persentase tertentu dari kerugian dan premi yang diambil oleh perusahaan reasuransi. Sedangkan reasuransi non-proporsional mencakup kontrak stop-loss dan excess of loss. semua jenis kontrak reasuransi membagi risiko total menjadi dua bagian, yaitu risiko yang ditanggung oleh perusahaan reasuransi dan risiko yang ditahan oleh perusahaan asuransi. Dalam hal ini, \\(X\\) adalah risiko total, \\(Y_{reinsurer}\\) adalah risiko yang ditanggung oleh perusahaan reasuransi, dan \\(Y_{insurer}\\) adalah risiko yang ditahan oleh perusahaan asuransi. Dinyatakan dalam \\(X = Y_{insurer}+Y_{reinsurer}\\) struktur matematika dasar dari sebuah perjanjian reasuransi sama dengan modifikasi cakupan dalam asuransi personal yang diperkenalkan dalam Bab 3. Dalam reasuransi proporsional, transformasi \\(Y_{insurer} = cX\\) identik dengan penyesuaian co-insurance dalam asuransi personal. Dalam reasuransi stop-loss, transformasi \\(Y_{reinsurer} = max(0, X-M)\\) sama dengan pembayaran asuransi dengan nilai retensi (deductible) \\(M\\), sedangkan \\(Y_{insurer} = min(X, M)\\) setara dengan apa yang dibayarkan oleh pemegang polis dengan nilai retensi \\(M\\). Namun, dalam aplikasi praktis matematika, fokus dalam asuransi personal umumnya pada harapan sebagai bahan utama yang digunakan dalam penetapan harga. Sedangkan dalam reasuransi, fokusnya adalah pada seluruh distribusi risiko, karena peristiwa ekstrim menjadi perhatian utama untuk stabilitas keuangan perusahaan asuransi dan reasuransi. 10.4.1 Proportional Reinsurance Jumlah yang dibayarkan oleh perusahaan asuransi utama dan perusahaan reasuransi dituliskan sebagai \\[\\begin{equation} Y_{insurer} = c X \\ \\ \\text{and} \\ \\ \\ Y_{reinsurer} = (1-c) X, \\end{equation}\\] dimana \\(c\\in (0,1)\\) menunjukkan proporsi yang disimpan oleh perusahaan asuransi. Perhatikan bahwa \\(Y_{insurer}+Y_{reinsurer}=X\\) contoh 10.4.1 akan menunjukkan bagaimana perjanjian quota-share mempengaruhi distribusi kerugian melalui demonstrasi \\(r\\) singkat menggunakan simulasi. Gambar yang disertakan memberikan bentuk relatif dari distribusi kerugian total, bagian yang ditahan oleh perusahaan asuransi, dan bagian yang ditanggung oleh perusahaan reasuransi. set.seed(2018) theta = 1000 alpha = 3 nSim = 10000 library(actuar) ## ## Attaching package: &#39;actuar&#39; ## The following objects are masked from &#39;package:VGAM&#39;: ## ## dgumbel, dlgamma, dpareto, pgumbel, plgamma, ppareto, qgumbel, ## qlgamma, qpareto, rgumbel, rlgamma, rpareto ## The following objects are masked from &#39;package:stats&#39;: ## ## sd, var ## The following object is masked from &#39;package:grDevices&#39;: ## ## cm X &lt;- rpareto(nSim, shape = alpha, scale = theta) par(mfrow=c(1,3)) plot(density(X), xlim=c(0,3*theta), ylim=c(0,0.008), main=&quot;Total Loss&quot;, xlab=&quot;Losses&quot;) plot(density(0.75*X), xlim=c(0,3*theta), ylim=c(0,0.008), main=&quot;Insurer (75%)&quot;, xlab=&quot;Losses&quot;) plot(density(0.25*X), xlim=c(0,3*theta), ylim=c(0,0.008), main=&quot;Reinsurer (25%)&quot;, xlab=&quot;Losses&quot;) 10.4.1.1 Quota Share is Desirable for Reinsurers Kontrak bagian kuota (quota share) sangat diinginkan bagi reinsurer. Untuk melihat ini, asumsikan bahwa perusahaan asuransi dan reinsurer ingin memasuki kontrak untuk berbagi total kerugian \\(X\\) \\[\\begin{equation} Y_{insurer}=g(X) \\ \\ \\ \\text{and} \\ \\ \\ \\ Y_{reinsurer}=X-g(X), \\end{equation}\\] Dalam kontrak quota share, diasumsikan ada sebuah fungsi generik \\(g(\\cdot)\\) (dikenal sebagai fungsi retensi) yang digunakan untuk membagi kerugian antara perusahaan asuransi dan reinsurer. Fungsi retensi tersebut harus memastikan bahwa perusahaan asuransi tidak mempertahankan lebih banyak kerugian daripada yang sebenarnya terjadi, sehingga hanya fungsi yang memenuhi \\(g(x) ≤ x\\) yang dianggap. Selanjutnya, diasumsikan bahwa perusahaan asuransi hanya peduli dengan variabilitas klaim yang dipertahankan dan tidak memperdulikan pilihan fungsi \\(g\\) selama \\(variansi (Y_{insurer})\\) tetap sama dan sama dengan \\(Q\\), sebagai contoh. Kemudian, hasil berikut menunjukkan bahwa kontrak reasuransi quota share meminimalkan ketidakpastian reinsurer sebagaimana diukur dengan \\(Var(Y_{reinsurer})\\). Proposisi. Misalkan \\(Var(Y_{insurer}) =Q\\) maka \\(Var((1-c)X) \\le Var (g(X))\\) untuk semua \\(g(\\cdot)\\) sehingga \\(E[g(X)]=K\\) dimana \\(c=Q/Var(X)\\) Proposisi ini memiliki daya tarik secara intuitif - dengan asuransi quota share, reinsurer membagi tanggung jawab untuk klaim yang sangat besar pada ekor distribusi. Ini berbeda dengan perjanjian non-proportional di mana reinsurer bertanggung jawab atas klaim yang sangat besar. 10.4.1.2 Optimizing Quota Share Agreements for Insurers Dalam kasus di mana ada \\(n\\) risiko dalam suatu portofolio, di mana setiap risiko dinyatakan oleh \\(X_i\\), kita dapat mempertimbangkan variasi dari kesepakatan kuota bersama di mana jumlah yang ditahan oleh perusahaan asuransi dapat bervariasi dengan setiap risiko, disebut ci. Dalam hal ini, bagian perusahaan asuransi dari risiko portofolio adalah \\(Y_{insurer} = \\sum_{i=1}^n c_i X_i\\). Kami mencari nilai-nilai \\(c_i\\) yang meminimalkan \\(Var(Y_{insurer})\\) dengan batasan bahwa \\(E(Y_{insurer}) = K\\). Ini berarti bahwa perusahaan asuransi ingin mempertahankan pendapatan setidaknya sebesar konstanta \\(K\\) dan sejalan dengan batasan ini, ingin meminimalkan ketidakpastian risiko yang ditahan dalam hal varians. Kasus ini dapat diterapkan pada berbagai aplikasi di mana risiko individu dapat didefinisikan sebagai risiko kebijakan atau klaim individu atau subportofolio, tergantung pada aplikasi spesifik. Sebagai contoh, perusahaan asuransi dapat membagi portofolionya menjadi subportofolio yang terdiri dari beberapa jenis bisnis, seperti (1) mobil pribadi, (2) mobil komersial, (3) pemilik rumah, (4) kompensasi pekerja, dan lain-lain. Dari hasil perhitungan matematika, diketahui bahwa konstanta untuk risiko ke-\\(i\\), \\(c_i\\), berkorelasi dengan rasio \\(\\frac{E(X_i)}{Var(X_i)}\\). Secara intuitif, jika \\(E(X_i)\\) lebih besar, maka nilai \\(c_i\\) akan semakin besar pula, dan sebaliknya, jika \\(Var(X_i)\\) semakin besar, maka nilai \\(c_i\\) akan semakin kecil. Faktor pengali proporsional ditentukan oleh persyaratan pendapatan \\(E(Y_{insurer})=K\\). Contoh yang diberikan membantu memahami hubungan ini. Contohnya, terdapat tiga risiko yang masing-masing memiliki distribusi Pareto dengan parameter yang berbeda. \\(\\alpha_1 =3\\), _1=1000 untuk resiko pertama \\(X_1\\), \\(\\alpha_2 =3\\), _2=2000 untuk resiko kedua \\(X_2\\), dan \\(\\alpha_3 =4\\), _3=3000 untuk resiko ketiga \\(X_3\\). Grafik disediakan untuk menunjukkan nilai \\(c_1\\), \\(c_2\\), dan \\(c_3\\) untuk pendapatan yang dibutuhkan sebesar \\(K\\). Perlu diperhatikan bahwa nilai-nilai ini meningkat secara linear dengan \\(K\\). solusi: theta1 = 1000; theta2 = 2000; theta3 = 3000; alpha1 = 3; alpha2 = 3; alpha3 = 4; library(actuar) propnfct &lt;- function(alpha,theta){ mu &lt;- mpareto(shape=alpha, scale=theta, order=1) var &lt;- mpareto(shape=alpha, scale=theta, order=2) - mu^2 mu/var } temp &lt;- propnfct(alpha1, theta1)*mpareto(shape=alpha1, scale=theta1, order=1)+ propnfct(alpha2, theta2)*mpareto(shape=alpha2, scale=theta2, order=1)+ propnfct(alpha3, theta3)*mpareto(shape=alpha3, scale=theta3, order=1) KVec &lt;- seq(100, 2500, length.out=20) Lambdavec &lt;- 2*KVec/temp c1 &lt;- propnfct(alpha1, theta1) c2 &lt;- propnfct(alpha2, theta2) c3 &lt;- propnfct(alpha3, theta3) c1Vec &lt;- c2Vec &lt;- c3Vec &lt;- 0*KVec for (j in 1:20) { c1Vec[j] &lt;- (Lambdavec[j]/2) * propnfct(alpha1, theta1) c2Vec[j] &lt;- (Lambdavec[j]/2) * propnfct(alpha2, theta2) c3Vec[j] &lt;- (Lambdavec[j]/2) * propnfct(alpha3, theta3) } plot(KVec, c1Vec, type=&quot;l&quot;, ylab=&quot;proportion&quot;, xlab=&quot;required revenue (K)&quot;, ylim=c(0,1)) lines(KVec, c2Vec) lines(KVec, c3Vec) text(1200,0.80, expression(c[1])) text(2000,0.75, expression(c[2])) text(1500,0.30, expression(c[3])) 10.4.2 Non-Proportional Reinsurance 10.4.2.1 The Optimality of Stop-Loss Insurance Dalam sebuah perjanjian stop-loss, asuransi menetapkan level retensi \\(M (&gt; 0)\\) dan membayar seluruh klaim untuk nilai \\(X ≤ M\\). Selanjutnya, untuk klaim dengan nilai \\(X &gt; M\\), asuransi primer membayar \\(M\\) dan reasuransi membayar sisanya, yaitu \\(X - M\\). Oleh karena itu, asuransi menanggung risiko sebesar \\(M\\). Singkatnya, jumlah yang dibayar oleh asuransi primer dan reasuransi adalah sebagai berikut: \\[\\begin{equation} Y_{insurer} = \\begin{cases} X &amp; \\text{for } X \\le M\\\\ M &amp; \\text{for } X &gt;M \\\\ \\end{cases} \\ \\ \\ \\ = \\min(X,M) = X \\wedge M \\end{equation}\\] dan \\[\\begin{equation} Y_{reinsurer} = \\begin{cases} 0 &amp; \\text{for } X \\le M\\\\ X- M &amp; \\text{for } X &gt;M \\\\ \\end{cases} \\ \\ \\ \\ = \\max(0,X-M) \\end{equation}\\] sama seperti sebelumnya \\(Y_{insurer}+Y_{reinsurer}=X\\) Kontrak tipe stop-loss sangat diinginkan oleh perusahaan asuransi. Dalam hal ini, perusahaan asuransi dan reasuransi ingin memasuki kontrak sehingga \\(Y_{insurer} = g(X)\\) dan \\(Y_{reinsurer} = X - g(X)\\) untuk beberapa fungsi retensi generik \\(g(\\cdot)\\). Dengan asumsi bahwa perusahaan asuransi hanya peduli tentang variabilitas klaim yang disimpan dan tidak peduli dengan pilihan \\(g\\) selama \\(Var(Y_{insurer})\\) dapat diminimalkan. Kembali, kita mengenakan batasan bahwa \\(E(Y_{insurer}) = K\\) ; perusahaan asuransi perlu mempertahankan pendapatan \\(K\\). Dalam rangka untuk memenuhi batasan ini, perusahaan asuransi ingin meminimalkan ketidakpastian risiko yang disimpan (sebagaimana diukur oleh varians). Kemudian, hasil berikut menunjukkan bahwa perjanjian reasuransi stop-loss meminimalkan ketidakpastian reinsurer sebagaimana diukur oleh \\(Var(Y_{reinsurer})\\). Proposisi. Anggaplah \\(E(Y_{insurer})=K\\). Maka, \\(Var(X\\wedge M)\\le Var(g(X))\\) untuk semua \\(g(\\cdot)\\), di mana \\(M\\) adalah nilai sedemikian rupa sehingga \\(E(X\\wedge M)=K\\). 10.4.2.2 Excess of Loss Dalam reasuransi non-proporsional, terdapat jenis polis excess of loss. Dalam polis ini, risiko total \\(X\\) diasumsikan terdiri dari \\(n\\) risiko terpisah \\(X_1,...,X_n\\), dimana masing-masing risiko tersebut memiliki batas atas, misalnya \\(M_i\\). Pada polis ini, perusahaan asuransi menahan risiko \\[\\begin{equation} Y_{insurer} = \\sum_{i=1}^n Y_{i,insurer}, \\ \\ \\ \\ \\text{dengan} \\ \\ \\ \\ \\ Y_{i,insurer} = X_i \\wedge M_i. \\end{equation}\\] Sedangkan, untuk bagian risiko yang melebihi batas tersebut, reinsurer bertanggung jawab menanggungnya, yaitu \\(Y_{reinsurer}=X−Y_{insurer}\\). Batas retensi dapat bervariasi untuk setiap risiko atau dapat sama untuk semua risiko, yaitu \\(M_i=M\\) untuk semua \\(i\\). 10.4.2.3 Optimal Choice for Excess of Loss Retention Limits apa pilihan terbaik dari batas retensi excess of loss \\(M_i\\)? Untuk menjawab pertanyaan ini, kita perlu mencari nilai-nilai \\(M_i\\) yang meminimalkan \\(Var(Y_{insurer})\\) dengan mempertahankan konstrain bahwa \\(E(Y_{insurer})=K\\). Dalam rangka mempertahankan konstrain pendapatan ini, perusahaan asuransi ingin meminimalkan ketidakpastian risiko yang dipertahankan (yang diukur dengan variansinya). Dari perhitungan matematika, terungkap bahwa selisih antara batas retensi dan klaim rata-rata yang diharapkan oleh insurer, \\(M_i−E(X_i\\wedge M_i)\\), adalah sama untuk semua risiko. Hal ini secara intuitif menarik. Contoh 10.4.3. Excess of loss untuk tiga risiko Pareto. Pertimbangkan tiga risiko yang memiliki distribusi Pareto, masing-masing memiliki set parameter yang berbeda (sehingga mereka independen tetapi tidak identik). Gunakan set parameter yang sama seperti pada Contoh 10.4.2. Untuk contoh ini: A. Tunjukkan secara numerik bahwa batas retensi optimal \\(M_1\\), \\(M_2\\), dan \\(M_3\\) (dikurangi klaim rata-rata yang diharapkan oleh insurer, \\(M_i−E(X_i\\wedge M_i))\\) adalah sama untuk semua risiko, seperti yang kita turunkan secara teoritis. B. Selanjutnya, bandingkan secara grafis distribusi total risiko dengan risiko yang disimpan oleh insurer dan oleh reinsurer. solusi: A. mengoptimasi Lagrangian menggunakan paket R alabama untuk Algoritma Minimasi Barrier Adaptif Lagrangian. theta1 = 1000;theta2 = 2000;theta3 = 3000; alpha1 = 3; alpha2 = 3; alpha3 = 4; Pmin &lt;- 2000 library(actuar) VarFct &lt;- function(M){ M1=M[1];M2=M[2];M3=M[3] mu1 &lt;- levpareto(limit=M1,shape=alpha1, scale=theta1, order=1) var1 &lt;- levpareto(limit=M1,shape=alpha1, scale=theta1, order=2)-mu1^2 mu2 &lt;- levpareto(limit=M2,shape=alpha2, scale=theta2, order=1) var2 &lt;- levpareto(limit=M2,shape=alpha2, scale=theta2, order=2)-mu2^2 mu3 &lt;- levpareto(limit=M3,shape=alpha3, scale=theta3, order=1) var3 &lt;- levpareto(limit=M3,shape=alpha3, scale=theta3, order=2)-mu3^2 varFct &lt;- var1 +var2+var3 meanFct &lt;- mu1+mu2+mu3 c(meanFct,varFct) } f &lt;- function(M){VarFct(M)[2]} h &lt;- function(M){VarFct(M)[1] - Pmin} library(alabama) ## Loading required package: numDeriv par0=rep(1000,3) op &lt;- auglag(par=par0,fn=f,hin=h,control.outer=list(trace=FALSE)) Batas retensi optimal \\(M_1\\), \\(M_2\\), dan \\(M_3\\) yang menghasilkan batas retensi dikurangi klaim yang diharapkan dari perusahaan asuransi, \\(M_i-E(X_i\\wedge M_i)\\), sama untuk semua risiko, seperti yang kita dapatkan secara teoritis. M1star = op$par[1];M2star = op$par[2];M3star = op$par[3] M1star -levpareto(M1star,shape=alpha1, scale=theta1,order=1) ## [1] 1344.135 M2star -levpareto(M2star,shape=alpha2, scale=theta2,order=1) ## [1] 1344.133 M3star -levpareto(M3star,shape=alpha3, scale=theta3,order=1) ## [1] 1344.133 B. membandingkan secara grafis distribusi risiko total dengan yang dipertahankan oleh perusahaan asuransi dan perusahaan reasuransi. set.seed(2018) nSim = 10000 library(actuar) Y1 &lt;- rpareto(nSim, shape = alpha1, scale = theta1) Y2 &lt;- rpareto(nSim, shape = alpha2, scale = theta2) Y3 &lt;- rpareto(nSim, shape = alpha3, scale = theta3) YTotal &lt;- Y1 + Y2 + Y3 Yinsur &lt;- pmin(Y1,M1star)+pmin(Y2,M2star)+pmin(Y3,M3star) Yreinsur &lt;- YTotal - Yinsur par(mfrow=c(1,3)) plot(density(YTotal), xlim=c(0,10000), main=&quot;Total Loss&quot;, xlab=&quot;Losses&quot;) plot(density(Yinsur), xlim=c(0,10000), main=&quot;Insurer&quot;, xlab=&quot;Losses&quot;) plot(density(Yreinsur), xlim=c(0,10000), main=&quot;Reinsurer&quot;, xlab=&quot;Losses&quot;) 10.4.3 Additional Reinsurance Treaties 10.4.3.1 Surplus Share Proportional Treaty Jenis perjanjian reasuransi proposional lainnya adalah surplus share, yang umum digunakan dalam asuransi properti komersial. Perjanjian surplus share memungkinkan reinsured untuk membatasi eksposurnya pada risiko dengan jumlah tertentu (retained line). Reinsurer mengambil bagian dari risiko secara proporsional terhadap jumlah nilai yang diasuransikan melebihi retained line, hingga batas tertentu (dinyatakan sebagai kelipatan dari retained line, atau jumlah line). Sebagai contoh, jika retained line adalah 100.000 dan batas yang diberikan adalah 4 line (400.000), maka jika \\(X\\) adalah kerugian, bagian dari reinsurer adalah \\(min(400000,(X−100000)_+)\\). 10.4.3.2 Layers of Coverage Dalam reasuransi non-proporsional stop-loss, kita dapat memperluas kontrak dengan menambahkan pihak lain ke dalam kontrak. Sebagai contoh, selain hanya ada perusahaan asuransi dan reasuransi atau perusahaan asuransi dan pemegang polis, kita bisa mempertimbangkan situasi di mana ada tiga pihak, yaitu pemegang polis, perusahaan asuransi, dan reasuransi, yang sepakat untuk berbagi risiko. Secara umum, kita dapat mempertimbangkan keberadaan \\(k\\) pihak. Jika \\(k = 3\\), maka bisa jadi perusahaan asuransi dan dua reasuransi yang berbeda. Contoh 10.4.4. Lapisan perlindungan untuk tiga pihak. Misalkan ada \\(k = 3\\) pihak. Pihak pertama bertanggung jawab atas klaim pertama senilai 100, pihak kedua bertanggung jawab atas klaim dari 100 hingga 3000, dan pihak ketiga bertanggung jawab atas klaim di atas 3000. Jika ada empat klaim masing-masing senilai 50, 600, 1800, dan 4000, maka klaim tersebut akan dialokasikan ke pihak-pihak sebagai berikut: library(kableExtra) tabel &lt;- data.frame( Buyer = c(&quot;0-100&quot;, &quot;100-3000&quot;, &quot;3000-∞&quot;, &quot;total&quot;), claim_1 = c(50, 0, 0, 50), claim_2 = c(100, 500, 0, 600), claim_3 = c(100, 1700, 0, 1800), claim_4 = c(100, 2900, 1000,4000), total = c(350, 5100, 1000, 6450) ) kable(tabel, align = &quot;c&quot;, caption = &quot;Tabel Klaim&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 10.1: Tabel Klaim Buyer claim_1 claim_2 claim_3 claim_4 total 0-100 50 100 100 100 350 100-3000 0 500 1700 2900 5100 3000-∞ 0 0 0 1000 1000 total 50 600 1800 4000 6450 Untuk menangani situasi umum dengan \\(k\\) kelompok, partisi garis bilangan real positif menjadi \\(k\\) interval menggunakan titik potong \\[\\begin{equation} 0 = M_0 &lt; M_1 &lt; \\cdots &lt; M_{k-1} &lt; M_k = \\infty. \\end{equation}\\] Perhatikan bahwa interval ke-\\(j\\) adalah \\((M_{j−1},M_j]\\). Sekarang biarkan \\(Y_j\\) menjadi jumlah risiko yang dibagi oleh pihak ke-\\(j\\). Untuk mengilustrasikan, jika kerugian \\(x\\) adalah sehingga \\(M_{j−1}&lt;x≤M_j\\), maka \\[\\begin{equation} \\left(\\begin{array}{c} Y_1\\\\ Y_2 \\\\ \\vdots \\\\ Y_j \\\\Y_{j+1} \\\\ \\vdots \\\\Y_k \\end{array}\\right) =\\left(\\begin{array}{c} M_1-M_0 \\\\ M_2-M_1 \\\\ \\vdots \\\\ x-M_{j-1} \\\\ 0 \\\\ \\vdots \\\\0 \\end{array}\\right) \\end{equation}\\] kita dapat menuliskannya sebagai \\[\\begin{equation} Y_j = \\min(X,M_j) - \\min(X,M_{j-1}) . \\end{equation}\\] Dengan ungkapan \\(Y_j=min(X,M_j)−min(X,M_{j−1})\\), kita dapat melihat bahwa pihak ke-\\(j\\) bertanggung jawab atas klaim dalam interval \\((M_{j−1},M_j]\\). Dengan ini, Anda dapat memeriksa bahwa \\(X=Y_1+Y_2+⋯+Y_k\\). Seperti yang ditekankan dalam contoh berikut, kami juga mencatat bahwa pihak-pihak yang terlibat tidak harus berbeda. 10.4.3.3 Portfolio Management Example Banyak variasi kontrak yang mendasar mungkin terjadi. Untuk satu ilustrasi lagi, pertimbangkan yang berikut ini. Contoh 10.4.6. Manajemen Portofolio. Anda adalah Kepala Petugas Risiko dari sebuah perusahaan telekomunikasi. Perusahaan Anda memiliki beberapa risiko properti dan tanggung jawab. Kami akan mempertimbangkan: \\(X_1\\) - gedung, dimodelkan menggunakan distribusi gamma dengan rata-rata 200 dan parameter skala 100. \\(X_2\\) - kendaraan bermotor, dimodelkan menggunakan distribusi gamma dengan rata-rata 400 dan parameter skala 200. \\(X_3\\) - risiko direktur dan pejabat eksekutif, dimodelkan menggunakan distribusi Pareto dengan rata-rata 1000 dan parameter skala 1000. \\(X_4\\) - risiko siber, dimodelkan menggunakan distribusi Pareto dengan rata-rata 1000 dan parameter skala 2000. Sebutkan total risiko sebagai \\(X = X_1 + X_2 + X_3 + X_4\\). Untuk kesederhanaan, Anda mengasumsikan bahwa risiko ini independen. (Nanti, di Bagian 14.6, kami akan mempertimbangkan kasus yang lebih kompleks dari ketergantungan.) Untuk mengelola risiko, Anda mencari perlindungan asuransi. Anda ingin mengelola jumlah bangunan dan kendaraan bermotor yang kecil secara internal, hingga M1 dan M2, masing-masing. Anda mencari asuransi untuk menutupi semua risiko lain. Secara khusus, bagian dari perusahaan asuransi adalah \\[\\begin{equation} Y_{insurer} = (X_1 - M_1)_+ + (X_2 - M_2)_+ + X_3 + X_4 , \\end{equation}\\] sehingga risiko yang Anda tanggung adalah \\(Y_{retained} = X-Y_{insurer} = min(X_1,M_1)+min(X_2,M_2)\\). Menggunakan deductible \\(M_1=100\\) dan \\(M_2=200\\): A. Tentukan jumlah klaim yang diharapkan dari (i) yang ditanggung, (ii) yang diterima oleh asuransi, dan (iii) jumlah keseluruhan. B. Tentukan persentil ke-80, ke-90, ke-95, dan ke-99 untuk (i) yang ditanggung, (ii) yang diterima oleh asuransi, dan (iii) jumlah keseluruhan. C. Bandingkan distribusi dengan memplotting densitas untuk (i) yang ditanggung, (ii) yang diterima oleh asuransi, dan (iii) jumlah keseluruhan. Solusi menyiapkan parameter # For the gamma distributions, use alpha1 &lt;- 2; theta1 &lt;- 100 alpha2 &lt;- 2; theta2 &lt;- 200 # For the Pareto distributions, use alpha3 &lt;- 2; theta3 &lt;- 1000 alpha4 &lt;- 3; theta4 &lt;- 2000 # Limits M1 &lt;- 100 M2 &lt;- 200 dengan parameter ini, selanjutnya mensimulasikan realisasi dari risiko-risiko dalam portofolio # Simulate the risks nSim &lt;- 10000 #number of simulations set.seed(2017) #set seed to reproduce work X1 &lt;- rgamma(nSim,alpha1,scale = theta1) X2 &lt;- rgamma(nSim,alpha2,scale = theta2) # For the Pareto Distribution, use library(actuar) X3 &lt;- rpareto(nSim,scale=theta3,shape=alpha3) X4 &lt;- rpareto(nSim,scale=theta4,shape=alpha4) # Portfolio Risks X &lt;- X1 + X2 + X3 + X4 Yretained &lt;- pmin(X1,M1) + pmin(X2,M2) Yinsurer &lt;- X - Yretained A. hasil dari jumlah ekspektasi klaim # Expected Claim Amounts ExpVec &lt;- t(as.matrix(c(mean(Yretained),mean(Yinsurer),mean(X)))) colnames(ExpVec) &lt;- c(&quot;Retained&quot;, &quot;Insurer&quot;,&quot;Total&quot;) round(ExpVec,digits=2) ## Retained Insurer Total ## [1,] 269.05 2274.41 2543.46 B. hasil untuk kuantilnya # Quantiles quantMat &lt;- rbind( quantile(Yretained, probs=c(0.80, 0.90, 0.95, 0.99)), quantile(Yinsurer, probs=c(0.80, 0.90, 0.95, 0.99)), quantile(X , probs=c(0.80, 0.90, 0.95, 0.99))) rownames(quantMat) &lt;- c(&quot;Retained&quot;, &quot;Insurer&quot;,&quot;Total&quot;) round(quantMat,digits=2) ## 80% 90% 95% 99% ## Retained 300.00 300.00 300.00 300.00 ## Insurer 3075.67 4399.80 6172.69 11859.02 ## Total 3351.35 4675.04 6464.20 12159.02 C. Berikut adalah hasil plot densitas risiko yang dipertahankan, diasuransikan, dan total portofolio. par(mfrow=c(1,3)) plot(density(Yretained), xlim=c(0,500), main=&quot;Retained Portfolio Risk&quot;, xlab=&quot;Loss (Note the different horizontal scale)&quot;, ylab = &quot;Density (Note different vertical scale)&quot;) plot(density(Yinsurer), xlim=c(0,15000), main=&quot;Insurer Portfolio Risk&quot;, xlab=&quot;Loss&quot;) plot(density(X), xlim=c(0,15000), main=&quot;Total Portfolio Risk&quot;, xlab=&quot;Loss&quot;) "],["loss-reserving.html", "Bab 11 Loss Reserving 11.1 Motivation 11.2 Loss Reserve Data 11.3 The Chain-Ladder Method 11.4 GLMs and Bootstrap for Loss Reserves", " Bab 11 Loss Reserving Pratinjau Bab. Bab ini memperkenalkan pereservasi kerugian (juga dikenal sebagai pereservasi klaim) untuk produk asuransi harta benda dan kecelakaan (P&amp;C, atau umum, non-hidup). Secara khusus, bab ini menggambarkan beberapa alat analitik dasar, meskipun penting, untuk menilai cadangan pada portofolio produk asuransi P&amp;C. Pertama, Bagian 11.1 memberikan motivasi tentang perlunya pereservasi kerugian, kemudian Bagian 11.2 mempelajari sumber data yang tersedia dan memperkenalkan beberapa notasi formal untuk menangani pereservasi kerugian sebagai tantangan prediksi. Selanjutnya, Bagian 11.3 membahas metode tangga rantai (chain-ladder method) dan model tangga rantai tanpa distribusi Mack. Bagian 11.4 kemudian mengembangkan pendekatan yang sepenuhnya stokastik untuk menentukan cadangan yang belum diselesaikan dengan menggunakan model linear generalisasi (GLM), termasuk teknik bootstrapping untuk mendapatkan distribusi prediktif cadangan yang belum diselesaikan melalui simulasi. 11.1 Motivation Titik awal adalah masa hidup klaim asuransi P&amp;C. Gambar 11.1 menggambarkan perkembangan klaim tersebut dari waktu ke waktu dan mengidentifikasi peristiwa yang menarik perhatian: Kejadian yang diasuransikan atau kecelakaan terjadi pada waktu tocc. Kejadian ini dilaporkan kepada perusahaan asuransi pada waktu trep, setelah beberapa keterlambatan. Jika klaim yang diajukan diterima oleh perusahaan asuransi, pembayaran akan dilakukan untuk mengganti kerugian keuangan pemegang polis. Dalam contoh ini, perusahaan asuransi mengganti kerugian yang terjadi dengan pembayaran kerugian pada waktu t1, t2, dan t3. Pada akhirnya, klaim diselesaikan atau ditutup pada waktu tset. Seringkali klaim tidak langsung diselesaikan karena adanya keterlambatan dalam pelaporan klaim, keterlambatan dalam proses penyelesaian, atau keduanya. Keterlambatan pelaporan adalah waktu yang berlalu antara terjadinya kejadian yang diasuransikan dan pelaporan kejadian ini kepada perusahaan asuransi. Waktu antara pelaporan dan penyelesaian klaim dikenal sebagai keterlambatan penyelesaian. Misalnya, secara intuitif dapat dipahami bahwa klaim kerusakan material atau properti diselesaikan lebih cepat daripada klaim cedera tubuh yang melibatkan jenis cedera yang kompleks. Klaim yang ditutup juga dapat dibuka kembali karena perkembangan baru, misalnya cedera yang membutuhkan perawatan tambahan. Secara keseluruhan, perkembangan klaim biasanya membutuhkan waktu. Adanya keterlambatan ini dalam penyelesaian klaim membutuhkan perusahaan asuransi untuk memiliki modal guna menyelesaikan klaim-kilaim ini di masa depan. 11.1.1 Closed, IBNR, and RBNS Claims Berdasarkan status run-off klaim, kami membedakan tiga jenis klaim dalam catatan perusahaan asuransi. Jenis pertama adalah klaim yang ditutup. Untuk klaim-klaim ini, perkembangan lengkap telah diamati. Dengan garis merah pada Gambar 11.2 menunjukkan saat ini, semua peristiwa dalam perkembangan klaim terjadi sebelum saat ini. Oleh karena itu, peristiwa-peristiwa ini diamati pada saat ini. Untuk kenyamanan, kami akan mengasumsikan bahwa klaim yang ditutup tidak dapat dibuka kembali. Klaim RBNS adalah klaim yang Sudah Dilaporkan, Tetapi Belum sepenuhnya Diselesaikan pada saat ini atau saat evaluasi (tanggal penilaian), yaitu saat di mana cadangan harus dihitung dan dicatat oleh perusahaan asuransi. Kejadian, pelaporan, dan mungkin beberapa pembayaran kerugian terjadi sebelum saat ini, tetapi penyelesaian klaim terjadi di masa depan, setelah saat ini. Klaim IBNR adalah klaim yang Telah Terjadi di masa lalu, Tetapi Belum Dilaporkan. Untuk klaim seperti ini, kejadian yang diasuransikan telah terjadi, tetapi perusahaan asuransi belum menyadari adanya klaim terkait. Klaim ini akan dilaporkan di masa depan dan perkembangannya yang lengkap (dari pelaporan hingga penyelesaian) terjadi di masa depan. Perusahaan asuransi akan melakukan reservasi modal untuk memenuhi kewajiban masa depan mereka terkait dengan klaim RBNS dan juga klaim IBNR. Perkembangan masa depan dari klaim-klaim tersebut tidak pasti, dan teknik pemodelan prediktif akan digunakan untuk menghitung cadangan yang tepat, berdasarkan data perkembangan historis yang diamati pada klaim-klaim serupa. 11.1.2 Why Reserving? Siklus produksi terbalik dalam pasar asuransi dan dinamika klaim yang digambarkan dalam Bagian 11.1.1 menjadi motivasi untuk perlunya pereservasi dan perancangan alat pemodelan prediktif untuk mengestimasi cadangan. Dalam asuransi, pendapatan premi mendahului biaya. Seorang perusahaan asuransi akan mengenakan premi kepada klien sebelum sebenarnya mengetahui seberapa mahal kebijakan atau kontrak asuransi tersebut akan menjadi. Hal ini berbeda dengan industri manufaktur yang biasanya seorang produsen mengetahui - sebelum menjual produk - berapa biaya produksi produk tersebut. Pada saat evaluasi yang ditentukan τ, perusahaan asuransi akan memprediksi kewajiban yang belum diselesaikan terkait dengan kontrak yang terjual di masa lalu. Ini adalah cadangan klaim atau cadangan kerugian; ini adalah modal yang diperlukan untuk menyelesaikan klaim terbuka dari paparan masa lalu. Ini adalah elemen yang sangat penting dalam neraca perusahaan asuransi, lebih khususnya pada sisi kewajiban neraca perusahaan. 11.2 Loss Reserve Data 11.2.1 From Micro to Macro Sekarang kami akan menjelaskan data yang tersedia untuk mengestimasi cadangan yang belum diselesaikan untuk portofolio kontrak asuransi P&amp;C. Perusahaan asuransi biasanya mencatat data mengenai perkembangan klaim individual seperti yang digambarkan dalam garis waktu di sebelah kiri Gambar 11.5. Kami menyebut data yang tercatat pada tingkat ini sebagai data granular atau mikro-level. Biasanya, seorang aktuaris menggabungkan informasi yang tercatat mengenai perkembangan klaim individual dari seluruh klaim dalam sebuah portofolio. Penggabungan ini menghasilkan data yang terstruktur dalam format segitiga seperti yang ditunjukkan di sebelah kanan Gambar 11.5. Data seperti ini disebut data agregat atau makro-level karena setiap sel di segitiga menampilkan informasi yang diperoleh dengan menggabungkan perkembangan dari beberapa klaim. Tampilan segitiga yang digunakan dalam pereservasi kerugian disebut segitiga run-off atau segitiga perkembangan. Pada sumbu vertikal, segitiga tersebut mencantumkan tahun kejadian atau kecelakaan yang diikuti oleh sebuah portofolio. Pembayaran kerugian yang tercatat untuk klaim tertentu terhubung dengan tahun dimana kejadian yang diasuransikan terjadi. Sumbu horizontal mengindikasikan keterlambatan pembayaran sejak terjadinya kejadian yang diasuransikan. 11.2.2 Run-off Triangles Contoh pertama dari segitiga run-off dengan pembayaran bertambah ditampilkan dalam Gambar 11.6 (diambil dari Wüthrich dan Merz (2008), Tabel 2.2, juga digunakan dalam Wüthrich dan Merz (2015), Tabel 1.4). Tahun kecelakaan (atau tahun kejadian) ditampilkan pada sumbu vertikal dan mulai dari tahun 2004 hingga 2013. Ini merujuk pada tahun dimana kejadian yang diasuransikan terjadi. Sumbu horizontal mengindikasikan keterlambatan pembayaran dalam tahun sejak terjadinya kejadian yang diasuransikan. Keterlambatan 0 digunakan untuk pembayaran yang dilakukan pada tahun terjadinya kecelakaan atau kejadian yang diasuransikan. Keterlambatan satu tahun digunakan untuk pembayaran yang dilakukan pada tahun setelah terjadinya kecelakaan. Sementara segitiga pada Gambar 11.6 menampilkan data pembayaran bertambah, Gambar 11.7 menunjukkan informasi yang sama dalam format akumulatif. Sekarang, sel (2004,1) menampilkan total jumlah klaim yang dibayarkan hingga keterlambatan pembayaran 1 untuk semua klaim yang terjadi pada tahun 2004. Oleh karena itu, itu merupakan jumlah dari jumlah yang dibayarkan pada tahun 2004 dan jumlah yang dibayarkan pada tahun 2005 atas kecelakaan yang terjadi pada tahun 2004. Berbagai informasi dapat disimpan dalam segitiga run-off seperti yang ditunjukkan dalam Gambar 11.6 dan Gambar 11.7. Tergantung pada jenis data yang disimpan, segitiga akan digunakan untuk mengestimasi jumlah yang berbeda. Sebagai contoh, dalam format bertambah, sebuah sel dapat menampilkan: Pembayaran klaim, seperti yang dijelaskan sebelumnya. Jumlah klaim yang terjadi pada tahun tertentu dan dilaporkan dengan keterlambatan tertentu, ketika tujuannya adalah untuk mengestimasi jumlah klaim IBNR. Perubahan jumlah yang terjadi, di mana jumlah klaim yang terjadi adalah jumlah dari klaim yang dibayarkan secara akumulatif dan estimasi kasus. Estimasi kasus adalah perkiraan ahli penangan klaim mengenai jumlah yang belum diselesaikan dalam suatu klaim. Dalam format akumulatif, sebuah sel dapat menampilkan: Jumlah pembayaran akumulatif, seperti yang dijelaskan sebelumnya. Total jumlah klaim dari tahun kejadian, dilaporkan hingga keterlambatan tertentu. Jumlah klaim yang terjadi. Sumber informasi lainnya juga mungkin tersedia, misalnya kovariat (seperti jenis klaim), informasi eksternal (seperti inflasi, perubahan dalam regulasi). Sebagian besar metode pereservasi klaim yang dirancang untuk segitiga run-off didasarkan pada satu sumber informasi, meskipun kontribusi terbaru fokus pada penggunaan data yang lebih terperinci untuk pereservasi kerugian. 11.2.3 Loss Reserve Notation Run-off Triangles Untuk memformalkan tampilan yang ditunjukkan dalam Gambar 11.6 dan 11.7, kita akan menggunakan notasi sebagai berikut. Kita mengasosiasikan i dengan tahun kejadian atau tahun kecelakaan, yaitu tahun di mana kejadian yang diasuransikan terjadi. Dalam notasi kita, tahun kecelakaan pertama yang dipertimbangkan dalam portofolio ditandai dengan 1, dan tahun kecelakaan terakhir yang paling baru ditandai dengan I . Selanjutnya, j merujuk pada keterlambatan pembayaran atau tahun perkembangan, di mana keterlambatan yang sama dengan 0 sesuai dengan tahun kecelakaan itu sendiri. Gambar 11.8 menunjukkan sebuah segitiga di mana jumlah tahun yang sama dipertimbangkan baik pada sumbu vertikal maupun sumbu horizontal, oleh karena itu j berjalan dari 0 hingga \\(J=I−1\\) . Variabel acak \\(X_{ij}\\) menunjukkan klaim bertambah yang dibayarkan dalam periode perkembangan \\(j\\) untuk klaim dari tahun kecelakaan \\(i\\) . Dengan demikian, \\(X_{ij}\\) adalah total jumlah yang dibayarkan pada tahun perkembangan \\(j\\) untuk semua klaim yang terjadi pada tahun kejadian \\(i\\) . Pembayaran ini sebenarnya dilakukan dalam tahun akuntansi atau tahun kalender \\(i+j\\) . Dalam perspektif akumulatif, \\(C_{ij}\\) adalah jumlah kumulatif yang dibayarkan hingga (dan termasuk) tahun perkembangan \\(j\\) untuk kecelakaan yang terjadi pada tahun \\(i\\) . Pada akhirnya, jumlah total \\(C_{ij}\\) dibayarkan pada tahun perkembangan akhir \\(J\\) untuk klaim yang terjadi pada tahun kecelakaan \\(i\\) . Dalam bab ini, waktu diungkapkan dalam tahun, meskipun unit waktu lainnya juga dapat digunakan, misalnya periode enam bulan atau kuartal. The Loss Reserve Pada saat evaluasi \\(\\tau\\) , data pada segitiga atas telah diamati, sedangkan segitiga bawah harus diprediksi. Di sini, saat evaluasi adalah akhir tahun kecelakaan \\(I\\) , yang berarti bahwa sel \\((i,j)\\) dengan \\(i+j \\leq I\\) diamati, dan sel \\((i,j)\\) dengan \\(i+j &gt; I\\) termasuk masa depan dan harus diprediksi. Oleh karena itu, untuk segitiga run-off kumulatif, tujuan metode pereservasi kerugian adalah memprediksi \\(C_i,I−1\\) , jumlah klaim akhir untuk tahun kejadian \\(i\\) , yang sesuai dengan periode perkembangan akhir \\(I−1\\) dalam Gambar 11.7. Kami mengasumsikan bahwa - setelah periode ini tidak akan ada pembayaran lebih lanjut, meskipun asumsi ini bisa dikendurkan. Karena \\(C_{i,I-1}\\) bersifat kumulatif, itu mencakup bagian yang diamati serta bagian yang harus diprediksi. Oleh karena itu, kewajiban yang belum diselesaikan atau cadangan kerugian untuk tahun kecelakaan \\(i\\) adalah \\[\\begin{eqnarray*} \\mathcal{R}^{(0)}_{i} = \\sum_{\\ell=I-i+1}^{I-1} X_{i\\ell} = C_{i,I}-C_{i,I-i}. \\end{eqnarray*}\\] Kami menyatakan cadangan baik sebagai jumlah data bertambah, \\(X_{i\\ell}\\) , maupun sebagai selisih antara angka kumulatif. Dalam kasus terakhir, jumlah yang belum diselesaikan adalah jumlah kumulatif akhir \\(C_{i,I}\\) dikurangi dengan jumlah kumulatif yang diamati paling baru \\(C_{i,I-i}\\) . Mengikuti Wüthrich dan Merz (2015), notasi \\(\\mathcal{R}^{(0)}_{i}\\) mengacu pada cadangan untuk tahun kejadian \\(i\\) di mana \\(i=1,\\ldots,I\\) . Superskrip (0) mengacu pada evaluasi cadangan pada saat ini, katakanlah \\(\\tau = 0\\) . Kami memahami \\(\\tau = 0\\) pada akhir tahun kejadian \\(I\\) , tahun kalender paling baru di mana data diamati dan terdaftar. 11.2.4 R Code to Summarize Loss Reserve Data Kami menggunakan paket ChainLadder (Gesmann et al. 2019) untuk mengimpor segitiga run-off di R dan untuk menjelajahi tren yang ada dalam segitiga tersebut. Vignette paket ini dengan baik mendokumentasikan fungsi-fungsi untuk bekerja dengan data segitiga. Pertama, kami menjelajahi dua cara untuk mengimpor sebuah segitiga. Long Format Data Dataset triangle_W_M_long.txt menyimpan segitiga run-off kumulatif dari Wüthrich dan Merz (2008) (Tabel 2.2) dalam format panjang. Artinya: setiap sel dalam segitiga merupakan satu baris dalam dataset ini, dan tiga fitur disimpan: ukuran pembayaran (kumulatif, dalam contoh ini), tahun kejadian ( \\(i\\) ) dan penundaan pembayaran ( \\(j\\) ). Kami mengimpor file .txt tersebut dan menyimpan data frame hasilnya sebagai my_triangle_long: my_triangle_long &lt;- read.table(&quot;Data/triangle_W_M_long.txt&quot;, header = TRUE) head(my_triangle_long) Kami menggunakan fungsi as.triangle dari paket ChainLadder untuk mengubah data frame menjadi tampilan segitiga. Objek hasilnya, my_triangle,` sekarang merupakan tipe segitiga. my_triangle &lt;- as.triangle(my_triangle_long, origin = &quot;origin&quot;, dev = &quot;dev&quot;, value = &quot;payment&quot;) str(my_triangle) Kami menampilkan segitiga dan mengenali angka-angka (dalam ribuan) dari Gambar 11.7. Sel-sel di segitiga bagian bawah ditandai sebagai tidak tersedia, NA. round(my_triangle/1000, digits = 0) Triangular Format Data Sebagai alternatif, segitiga dapat disimpan dalam file .csv dengan tahun kejadian di baris dan tahun perkembangan di sel-sel kolom. Kami mengimpor file .csv ini dan mengubah my_triangle_csv hasilnya menjadi matriks. my_triangle_csv &lt;- read.csv2(&quot;Data/triangle_W_M.csv&quot;, header = FALSE) my_triangle_matrix &lt;- as.matrix(my_triangle_csv) dimnames(my_triangle_matrix) &lt;- list(origin = 2004 : 2013, dev = 0:(ncol(my_triangle_matrix)-1)) my_triangle &lt;- as.triangle(my_triangle_matrix) round(my_triangle/1000, digits = 0) From Cumulative to Incremental, and vice versa Fungsi R cum2incr() dan incr2cum() memungkinkan kita untuk beralih dengan mudah antara tampilan kumulatif dan tampilan incremental, serta sebaliknya. plot(my_triangle) Sebagai alternatif, argumen lattice menciptakan satu plot per tahun kejadian. plot(my_triangle, lattice = TRUE) Daripada memplot segitiga kumulatif yang disimpan dalam my_triangle, kita dapat memplot segitiga run-off incremental. plot(my_triangle_incr) plot(my_triangle_incr, lattice = TRUE) 11.3 The Chain-Ladder Method Metode yang paling banyak digunakan untuk memperkirakan cadangan kerugian yang belum diselesaikan adalah metode chain-ladder. Asal-usul metode ini tidak jelas tetapi telah terbukti efektif dalam aplikasi praktis pada awal tahun 1970-an, Taylor (1986). Seperti yang akan kita lihat, nama ini merujuk pada penggabungan serangkaian faktor perkembangan (dari tahun ke tahun) menjadi tangga faktor; kerugian yang belum matang naik menuju kedewasaan ketika dikalikan dengan rangkaian rasio ini, maka muncullah deskriptor yang sesuai, yaitu metode chain-ladder. Kita akan memulai dengan menjelajahi metode chain-ladder dalam versi deterministik atau algoritma, tanpa membuat asumsi stokastik apa pun. Kemudian kita akan menggambarkan model chain-ladder distribusi bebas risiko dari Mack. 11.3.1 The Deterministic Chain-Ladder Metode chain-ladder deterministik berfokus pada run-off triangle dalam bentuk kumulatif. Ingatlah bahwa sel \\((i,j)\\) dalam segitiga ini menampilkan jumlah kumulatif yang dibayarkan hingga periode pengembangan \\(j\\) untuk klaim yang terjadi pada tahun \\(i\\). Metode chain-ladder mengasumsikan bahwa faktor pengembangan \\(f_j\\) (juga disebut faktor usia-ke-usia, rasio tautan, atau faktor chain-ladder) ada sehingga: \\[C_{i,j+1} = f_j \\times C_{i,j}.\\] Maka, faktor pengembangan memberitahu Anda bagaimana jumlah kumulatif dalam tahun pengembangan \\(j\\) tumbuh menjadi jumlah kumulatif dalam tahun \\(j+1\\). Kami menyoroti jumlah kumulatif dalam periode 0 dengan warna biru dan jumlah kumulatif dalam periode 1 dengan warna merah pada Gambar 11.10 yang diambil dari Wüthrich dan Merz (2008) (Tabel 2.2, juga digunakan dalam Wüthrich dan Merz (2015), Tabel 1.4). Metode chain-ladder kemudian menyajikan resep yang intuitif untuk memperkirakan atau menghitung faktor pengembangan ini. Karena faktor pengembangan pertama \\(f_0\\) menggambarkan perkembangan jumlah klaim kumulatif dari periode pengembangan 0 hingga periode pengembangan 1, faktor ini dapat diestimasi sebagai rasio dari jumlah kumulatif yang ditandai dengan warna merah dan jumlah kumulatif yang ditandai dengan warna biru, seperti yang terlihat pada Gambar 11.10. Dengan demikian, kita memperoleh perkiraan berikut untuk faktor pengembangan pertama \\(\\hat{f}_0^{CL}\\), dengan observasi \\(\\mathcal{D}_I\\). \\[\\hat{f}^{CL}_{\\color{magenta}{0}} = \\frac{\\sum_{i=1}^{10-\\color{magenta}{0}-1} \\color{red}{C_{i,\\color{magenta}{0}+1}}}{\\sum_{i=1}^{10-\\color{magenta}{0}-1} \\color{blue}{C_{i\\color{magenta}{0}}}}= 1.4925.\\] Perhatikan bahwa indeks i yang digunakan dalam penjumlahan pada pembilang dan penyebut berjalan dari periode kejadian pertama (1) hingga periode kejadian terakhir (9) di mana kedua periode pengembangan 0 dan 1 diamati. Sebagai hasilnya, faktor pengembangan ini mengukur bagaimana data yang ditandai dengan warna biru berkembang menjadi data yang ditandai dengan warna merah, dengan rata-rata di seluruh periode kejadian di mana kedua periode diamati. Metode chain-ladder kemudian menggunakan estimasi faktor pengembangan ini untuk memprediksi jumlah kumulatif \\(C_{10,1}\\) (yaitu jumlah kumulatif yang dibayarkan hingga dan termasuk tahun pengembangan 1 untuk kecelakaan yang terjadi pada tahun ke-10). Prediksi ini diperoleh dengan mengalikan jumlah klaim kumulatif terakhir yang diamati untuk periode kejadian 10 (yaitu \\(C_{10,0}\\) dengan periode pengembangan 0) dengan estimasi faktor pengembangan \\(\\hat{f}^{CL}_0\\). \\[\\hat{C}_{10, 1} = C_{10,0} \\cdot \\hat{f}^{CL}_0 = 5,676\\cdot 1.4925=8,471.\\] Melanjutkan pemikiran ini, faktor pengembangan berikutnya, \\(f_1\\), dapat diestimasi. Karena \\(f_1\\) mencerminkan perkembangan dari periode 1 ke periode 2, dapat diestimasi sebagai rasio angka-angka yang ditandai dengan warna merah dan biru seperti yang ditunjukkan dalam Gambar 11.11. Notasi matematis untuk perkiraan \\(\\hat{f}_1^{CL}\\) dari faktor pengembangan berikutnya \\(f_1\\), dengan mengingat observasi DI, adalah sebagai berikut: \\[\\hat{f}^{CL}_{\\color{magenta}{1}} = \\frac{\\sum_{i=1}^{10-\\color{magenta}{1}-1} \\color{red}{C_{i,\\color{magenta}{1}+1}}}{\\sum_{i=1}^{10-\\color{magenta}{1}-1} \\color{blue}{C_{i\\color{magenta}{1}}}}=1.0778.\\] Dengan demikian, faktor ini mengukur pertumbuhan jumlah yang dibayarkan secara kumulatif pada periode pengembangan 1 menjadi periode 2, dihitung rata-rata untuk semua periode kejadian di mana kedua periode tersebut diamati. Indeks i sekarang berjalan dari periode 1 hingga 8, karena ini adalah periode kejadian di mana kedua periode pengembangan 1 dan 2 diamati. Perkiraan untuk faktor pengembangan kedua ini kemudian digunakan untuk memprediksi sel-sel yang hilang dan tidak teramati pada periode pengembangan 2: \\[\\begin{array}{rl} \\hat{C}_{10,2} &amp;= C_{10,0} \\cdot \\hat{f}^{CL}_0 \\cdot \\hat{f}_1^{CL} = \\hat{C}_{10,1} \\cdot \\hat{f}_1^{CL} = 8,471 \\cdot 1.0778 = 9,130 \\\\ \\hat{C}_{9,2} &amp;= C_{9,1} \\cdot \\hat{f}^{CL}_1 = 7,649 \\cdot 1.0778 = 8,244. \\end{array}\\] Perlu diperhatikan bahwa untuk \\(\\hat{C}_{10,2}\\), Anda sebenarnya menggunakan perkiraan \\(\\hat{C}_{10,1}\\) dan mengalikannya dengan perkiraan faktor pengembangan \\(\\hat{f}_1^{CL}\\). Kita melanjutkannya secara analog dan mendapatkan prediksi berikut, dicetak miring pada Gambar 11.12: Pada akhirnya, kita perlu memperkirakan nilai-nilai pada kolom terakhir. Faktor perkembangan terakhir, f8 , mengukur pertumbuhan dari periode perkembangan 8 ke periode perkembangan 9 dalam segitiga. Karena hanya baris pertama dalam segitiga yang memiliki kedua sel yang diamati, faktor terakhir ini diperkirakan sebagai rasio antara nilai yang berwarna merah dan nilai yang berwarna biru pada Gambar 11.13. Diberikan observasi \\(\\mathcal{D}_I\\), perkiraan faktor ini \\(\\hat{f}^{CL}_{8}\\) adalah sama dengan: \\[\\hat{f}^{CL}_{\\color{magenta}{8}} = \\frac{\\sum_{i=1}^{10-\\color{magenta}{8}-1} \\color{red}{C_{i,\\color{magenta}{8}+1}}}{\\sum_{i=1}^{10-\\color{magenta}{8}-1} \\color{blue}{C_{i\\color{magenta}{8}}}}=1.001.\\] Biasanya, faktor perkembangan terakhir ini mendekati 1 dan oleh karena itu arus kas yang dibayarkan dalam periode pengembangan terakhir relatif kecil. Dengan menggunakan perkiraan faktor pengembangan ini, kita sekarang dapat memperkirakan jumlah klaim kumulatif yang tersisa dalam kolom dengan mengalikan nilai-nilai untuk tahun pengembangan 8 dengan faktor ini. Notasi matematika umum untuk prediksi tangga rantai untuk segitiga bawah \\(( i+j&gt;I )\\) adalah sebagai berikut: \\[\\begin{array}{rl} \\hat{C}_{ij}^{CL} &amp;= C_{i,I-i} \\cdot \\prod_{l=I-i}^{j-1} \\hat{f}_l^{CL} \\\\ \\hat{f}_j^{CL} &amp;= \\frac{\\sum_{i=1}^{I-j-1} C_{i,j+1}}{\\sum_{i=1}^{I-j-1} C_{ij}}, \\end{array}\\] di mana \\(C_{i,I-i}\\) adalah pada diagonal terakhir yang diamati. Jelas bahwa asumsi penting dari metode chain-ladder adalah bahwa perkembangan proporsional klaim dari satu periode pengembangan ke periode berikutnya serupa untuk semua tahun kejadian. Ini menghasilkan Figure 11.14 berikut: angka-angka dalam kolom terakhir menunjukkan perkiraan jumlah klaim akhir. Estimasi jumlah klaim yang tertunda \\(\\hat{\\mathcal{R}}_i^{CL}\\) untuk periode kejadian tertentu \\(i=I-J+1,\\ldots, I\\) kemudian diberikan oleh selisih antara jumlah klaim akhir dan jumlah kumulatif yang diamati pada diagonal terbaru: \\[\\hat{\\mathcal{R}}_i^{CL} =\\hat{C}_{iJ}^{CL}-C_{i,I-i}.\\] Ini adalah estimasi chain-ladder untuk cadangan yang diperlukan untuk memenuhi kewajiban di masa depan terkait klaim yang terjadi dalam periode kejadian ini. Cadangan per periode kejadian dan total yang dijumlahkan dari semua periode kejadian dirangkum dalam Figure 11.15. 11.3.2 Mack’s Distribution-Free Chain-Ladder Model Pada tahap ini, metode chain-ladder tradisional memberikan estimasi titik \\(\\hat{C}^{CL}_{iJ}\\) untuk ramalan \\(C_{iJ}\\) , menggunakan informasi \\(\\mathcal{D}_I\\) . Karena metode chain-ladder adalah algoritma yang sepenuhnya deterministik dan intuitif untuk melengkapi segitiga run-off, kita tidak dapat menentukan seberapa dapat diandalkan estimasi titik tersebut atau memodelkan variasi pembayaran di masa depan. Untuk menjawab pertanyaan-pertanyaan tersebut, diperlukan sebuah model stokastik yang mendasari yang dapat mereproduksi estimasi cadangan chain-ladder. Dalam bagian ini, kami akan fokus pada model chain-ladder bebas distribusi sebagai model stokastik yang mendasar, diperkenalkan dalam Mack (1993). Metode ini memungkinkan kita untuk mengestimasi kesalahan standar dari prediksi chain-ladder. Pada Bagian berikutnya, yaitu Bagian 11.4, model linear umum digunakan untuk mengembangkan pendekatan yang sepenuhnya stokastik untuk memprediksi cadangan yang belum terbayar. Dalam pendekatan Mack, berlaku kondisi-kondisi berikut (tanpa mengasumsikan distribusi): Klaim Kumulatif \\((C_{ij})_{j=0,\\ldots,J}\\) adalah independen di antara periode kejadian yang berbeda i . Terdapat konstanta tetap \\(f_0, \\ldots, f_{J-1}\\) dan \\(\\sigma^2_0,\\ldots, \\sigma^2_{J-1}\\) yang memenuhi untuk semua \\(i=1,\\ldots, I\\) dan \\(j=0,\\ldots,J-1\\): \\[\\begin{array}{rl} E[C_{i,j+1}|C_{i0},\\ldots,C_{ij}] &amp;= f_j \\cdot C_{ij} \\\\ \\text{Var}(C_{i,j+1}|C_{ij}) &amp;= \\sigma^2_j \\cdot C_{ij}. \\end{array}\\] Ini berarti klaim kumulatif \\((C_{ij})_{j=0,\\ldots,J}\\) adalah proses Markov (di periode pengembangan j) dan oleh karena itu masa depan hanya bergantung pada masa sekarang. Dengan asumsi ini, nilai harapan dari jumlah klaim akhir \\(C_{i,J}\\), dengan data yang tersedia di segitiga atas, adalah jumlah kumulatif pada diagonal terbaru \\(C_{i, I-1}\\) dikali dengan faktor pengembangan yang sesuai \\(f_j\\) . Dalam notasi matematika, kita mendapatkan dengan faktor pengembangan yang diketahui \\(f_j\\)dan observasi \\(\\mathcal{D}_I\\) : \\[E[C_{iJ}|\\mathcal{D}_I] = C_{i,I-i} \\prod_{j=I-i}^{J-1} f_j\\] Ini persis apa yang dilakukan metode chain-ladder deterministik, seperti yang dijelaskan di Bagian 11.3.1. Dalam praktiknya, faktor pengembangan tidak diketahui dan perlu diestimasi dari data yang tersedia di segitiga atas. Dalam pendekatan Mack, kita mendapatkan persis ekspresi yang sama untuk mengestimasi faktor pengembangan \\(f_j\\) pada saat \\(I\\) seperti dalam algoritma chain-ladder deterministik: \\[\\hat{f}_j^{CL} =\\frac{\\sum_{j=1}^{I-j-1} C_{i,j+1}}{\\sum_{i=1}^{I-j-1} C_{ij}}.\\] Prediksi untuk sel-sel dalam segitiga bawah (yaitu sel-sel \\(C_{i,j}\\) dimana \\(i+j&gt;I\\)) kemudian diperoleh dengan menggantikan faktor-faktor yang tidak diketahui \\(f_j\\) dengan perkiraan mereka yang sesuai \\(\\hat{f}_j^{CL}\\) : \\[\\hat{C}^{CL}_{ij} = C_{i,I-i}\\prod_{l=I-i}^{j-1} \\hat{f}_l^{CL}.\\] Untuk mengkuantifikasi kesalahan prediksi yang muncul dengan prediksi chain-ladder, Mack juga memperkenalkan parameter-varian \\(\\sigma^2_j\\). Untuk mendapatkan wawasan dalam estimasi parameter-varian ini, diperkenalkan faktor-faktor perkembangan individu \\(f_{i,j}\\) (yang spesifik untuk periode kejadian i ) \\[f_{i,j} = \\frac{C_{i,j+1}}{C_{ij}}.\\] Faktor-faktor perkembangan individu ini juga menggambarkan bagaimana jumlah akumulasi tumbuh dari periode $j $ ke periode \\(j+1\\) , tetapi mereka hanya mempertimbangkan rasio dua sel (daripada mengambil rasio dua jumlah selama semua periode kejadian yang tersedia). Perhatikan bahwa faktor-faktor perkembangan dapat ditulis sebagai rata-rata tertimbang dari faktor-faktor perkembangan individu: \\[\\hat{f}_j^{CL} = \\sum_{i=1}^{I-j-1} \\frac{C_{ij}}{\\sum_{i=1}^{I-j-1} C_{ij}} f_{i,j},\\] Mari kita sekarang mengestimasi parameter-varian \\(\\sigma^2\\) dengan menulis asumsi varians Mack dalam bentuk yang setara. Pertama, varians dari rasio \\(C_{i,j+1}\\) dan \\(c_{i,j}\\) yang bersyarat pada \\(C_{i,0},\\ldots, C_{i,j}\\) berbanding terbalik dengan \\(C_{i,j}\\): \\[\\text{Var}[C_{i,j+1}/C_{ij}|C_{i0},\\ldots,C_{ij}] ~ \\propto ~ \\frac{1}{C_{ij}}.\\] Ini mengingatkan kita pada pengaturan kuadrat terkecil berbobot yang khas di mana bobotnya adalah kebalikan dari variabilitas respons. Oleh karena itu, variabel respons yang lebih volatil atau tidak presisi akan diberi bobot lebih rendah. \\(C_{i,j}\\) berperan sebagai bobotnya. Dengan menggunakan parameter-varian yang tidak diketahui \\(\\sigma^2_j\\) , asumsi varians ini dapat ditulis sebagai: \\[\\text{Var}[C_{i,j+1}|C_{i0},\\ldots,C_{ij}] = \\sigma^2_j \\cdot C_{ij},\\] Koneksi dengan kuadrat terkecil berbobot kemudian secara langsung menghasilkan estimasi tak bias untuk parameter-varian yang tidak diketahui \\(\\sigma^2_j\\) dalam bentuk jumlah kuadrat residu yang diboboti: \\[\\hat{\\sigma}^2_j = \\frac{1}{I-j-2}\\sum_{i=1}^{I-j-1} C_{ij}\\left(\\frac{C_{i,j+1}}{C_{ij}}-\\hat{f}_j^{CL}\\right)^2.\\] 11.3.3 R code for Chain-Ladder Predictions Kami menggunakan objek my_triangle dengan tipe triangle yang dibuat pada Bagian 11.2.4. Model chain-ladder bebas distribusi dari Mack (1993) diimplementasikan dalam paket ChainLadder (Gesmann et al. 2019) (sebagai bentuk khusus dari kuadrat terkecil berbobot) dan dapat diterapkan pada data my_triangle untuk memprediksi jumlah klaim yang belum diselesaikan dan mengestimasi kesalahan standar di sekitar ramalan tersebut. CL &lt;- MackChainLadder(my_triangle) CL round(CL$f,digits = 4) Kita juga dapat mencetak seluruh run-off triangle (termasuk prediksi). MSEP (Mean Squared Error of Prediction) untuk total cadangan melintasi semua periode kejadian diberikan oleh: CL$Total.Mack.S.E^2 Disarankan untuk memvalidasi asumsi Mack dengan memeriksa bahwa tidak ada tren dalam plot residu. Empat plot terakhir yang kita peroleh dengan perintah berikut menunjukkan masing-masing residu standar terhadap nilai yang cocok, periode asal, periode kalender, dan periode pengembangan. plot(CL) Plot bagian kiri atas adalah grafik batang posisi klaim terbaru ditambah IBNR dan kesalahan standar Mack berdasarkan periode kejadian. Plot bagian kanan atas menunjukkan pola perkembangan yang diprediksi untuk semua periode kejadian (dimulai dari 1 untuk periode kejadian tertua). Ketika mengatur argumen lattice=TRUE, kita akan mendapatkan plot perkembangan, termasuk prediksi dan perkiraan kesalahan standar berdasarkan periode kejadian. plot(CL, lattice=TRUE) 11.4 GLMs and Bootstrap for Loss Reserves Bagian ini membahas model regresi untuk menganalisis segitiga run-off. Ketika menganalisis data dalam segitiga run-off dengan model regresi, alat standar untuk pembangunan model, estimasi, dan prediksi tersedia. Dengan menggunakan alat-alat ini, kita dapat melampaui estimasi titik dan kesalahan standar seperti yang dijelaskan di Bagian 11.3. Secara khusus, kita membangun model linier generalisasi (GLM) untuk pembayaran inkremental \\(X_{ij}\\) dalam Gambar 11.6. Sementara metode chain-ladder bekerja dengan data kumulatif, GLM khas mengasumsikan variabel respons menjadi independen dan oleh karena itu bekerja dengan segitiga run-off inkremental. 11.4.1 Model Specification Misalkan \\(X_{ij}\\) menyatakan pembayaran inkremental dalam sel \\((i, j)\\) dari segitiga run-off. Kami mengasumsikan bahwa \\(X_{ij}\\) saling independen dengan kepadatan \\(f(x_{ij};\\theta_{ij},\\phi)\\) dari keluarga distribusi eksponensial. Kami mengidentifikasi: \\(\\mu_{ij}=E[X_{ij}]\\) sebagai nilai harapan dari sel \\(X_{ij}\\), \\(\\phi\\)sebagai parameter dispersi, dan \\(\\text{Var}[X_{ij}]=\\phi \\cdot V(\\mu_{ij})\\) , di mana \\(V(.)\\) adalah fungsi varians \\(\\eta_{ij}\\) sebagai prediktor linear sehingga \\(\\eta_{ij}=g(\\mu_{ij})\\) dengan \\(g\\) sebagai fungsi link. Distribusi dari keluarga eksponensial dan fungsi link default-nya tercantum di http://stat.ethz.ch/R-manual/R-patched/library/stats/html/family.html. Sekarang kami akan membahas tiga GLM khusus yang banyak digunakan untuk penyisihan kerugian. Pertama, model regresi Poisson diperkenalkan dalam Bagian 8.2. Dalam model ini, kami mengasumsikan bahwa Xij memiliki distribusi Poisson dengan parameter \\[\\mu_{ij} = \\pi_i \\cdot \\gamma_j,\\] struktur yang terdiri dari persilangan kelas yang mencakup efek multiplicative dari tahun kejadian \\(i\\) dan periode perkembangan \\(j\\). Struktur model yang diusulkan tidak dapat diidentifikasi tanpa batasan tambahan pada parameter, misalnya \\(\\sum_{j=0}^J \\gamma_j=1\\). Batasan ini memberikan interpretasi eksplisit terhadap \\(\\pi_i\\) (dengan \\(i=1,\\ldots,I\\)) sebagai ukuran paparan atau volume untuk tahun kejadian \\(i\\) dan \\(γ_j\\) sebagai fraksi dari total volume yang dibayarkan dengan penundaan \\(j\\). Namun, saat melakukan kalibrasi GLM di R, batasan alternatif seperti \\(\\pi_1=1\\) atau \\(\\gamma_1=1\\), atau reparametrisasi di mana \\(\\mu_{ij} = \\exp{(\\mu+\\alpha_i+\\beta_j)}\\) lebih mudah diimplementasikan. Kami melanjutkan dengan spesifikasi terakhir tersebut, termasuk \\(\\alpha_1 = \\beta_0 = 0\\), yang dikenal sebagai batasan sudut. GLM ini memperlakukan tahun kejadian dan penundaan pembayaran sebagai variabel faktor dan cocok dengan parameter per tingkat, disamping intercept \\(\\mu\\). Batasan sudut menjadikan efek tingkat pertama variabel faktor sama dengan nol. Asumsi Poisson sangat berguna untuk segitiga run-off dengan jumlah klaim yang dilaporkan, sering digunakan dalam estimasi jumlah klaim IBNR (lihat Bagian 11.2). Kedua, modifikasi menarik dari model regresi Poisson dasar adalah model regresi Poisson yang terdispersi berlebihan di mana \\(Z_{ij}\\) memiliki distribusi Poisson dengan parameter \\(\\mu_{ij}/\\phi\\) dan \\[\\begin{array}{rl} X_{ij} &amp;\\sim \\phi \\cdot Z_{ij} \\\\ \\mu_{ij} &amp;= \\exp{(\\mu + \\alpha_i + \\beta_j)}. \\end{array}\\] sebagai akibatnya, \\(X_{ij}\\) memiliki spesifikasi yang sama untuk rata-ratanya seperti dalam model regresi Poisson dasar, tetapi sekarang \\[\\text{Var}[X_{ij}] = \\phi^2 \\cdot \\text{Var}[Z_{ij}] = \\phi \\cdot \\exp{(\\mu + \\alpha_i + \\beta_j)}.\\] konstruksi ini memungkinkan untuk adanya di bawah (ketika \\(\\phi &lt;1\\)) dan over-dispersion (dengan \\(\\phi &gt;1\\)). Karena \\(X_{ij}\\) tidak lagi mengikuti distribusi yang terkenal, pendekatan ini disebut quasi-likelihood. Ini sangat berguna untuk memodelkan segitiga run-off dengan pembayaran bertambah, karena biasanya mengungkapkan over-dispersion. Ketiga, model regresi gamma relevan untuk memodelkan segitiga run-off dengan pembayaran klaim. Ingat dari Bagian 3.2.1 (lihat juga Lampiran Bab 18) bahwa distribusi gamma memiliki parameter bentuk \\(\\alpha\\) dan parameter skala \\(\\theta\\). Dari ini, kita melakukan reparameterisasi dan mendefinisikan parameter baru \\(\\mu = \\alpha \\cdot \\theta\\) sambil tetap mempertahankan parameter skala \\(\\theta\\). Selanjutnya, anggap bahwa \\(X_{ij}\\) memiliki distribusi gamma dan memperbolehkan \\(\\phi\\) bervariasi berdasarkan \\(ij\\) sehingga \\[\\mu_{ij} = \\exp{(\\mu + \\alpha_i + \\beta_j)}.\\] 11.4.2 Model Estimation and Prediction kami sekarang mengestimasi parameter regresi \\(\\phi\\), \\(\\alpha_{i}\\), dan \\(\\beta_j\\) dalam GLM yang diusulkan. Di R, fungsi glm tersedia untuk mengestimasi parameter-parameter ini melalui estimasi maximum likelihood (mle) atau estimasi quasi-likelihood (dalam kasus Poisson over-dispersed). Dengan adanya estimasi parameter \\(\\hat{\\phi}\\), \\(\\hat{\\alpha_i}\\), dan\\(\\hat{\\beta}_j\\), kita dapat menghasilkan estimasi titik untuk setiap sel dalam segitiga atas. \\[\\hat{X}_{ij} =\\hat{E[X_{ij}]} = \\exp{(\\hat{\\mu}+\\hat{\\alpha}_i+\\hat{\\beta}_j)},\\ \\text{with}\\ i+j\\leq I.\\] \\[\\hat{X}_{ij} = \\hat{E[X_{ij}]} = \\exp{(\\hat{\\mu}+\\hat{\\alpha}_i+\\hat{\\beta}_j)},\\ \\text{with}\\ i+j&gt; I.\\] "],["ChapBonusMalus.html", "Bab 12 Experience Rating using Bonus-Malus 12.1 Introduction 12.2 NCD System in Several Countries 12.3 BMS and Markov Chain Model 12.4 BMS and Stationary Distribution", " Bab 12 Experience Rating using Bonus-Malus 12.1 Introduction Bonus-malus system, which is used interchangeably as “no-fault discount”, “merit rating”, “experience rating” or “no-claim discount” in different countries, is based on penalizing insureds who are responsible for one or more claims by a premium surcharge (malus), and rewarding insureds with a premium discount (bonus) if they do not have any claims. A bonus-malus system can be perceived as a commercial implementation of the ideal credibility premium as discussed in Chapter ??, where premium payable for insureds are calculated on an individual basis. Insurers use bonus-malus systems for two main purposes: to encourage drivers to drive more carefully in a year without any claims, and to ensure insureds to pay premiums proportional to their risks based on their claims experience via an experience rating mechanism. No Claim Discount (NCD) system is an experience rating system commonly used in motor insurance. It represents an attempt to categorize insureds into homogeneous groups who pay premiums based on their claims experience. Depending on the rules in the scheme, new policyholders may be required to pay full premium initially, and obtain discounts in the future years as a result of claim-free years. An NCD system rewards policyholders for not making any claims during a year. In other words, it grants a bonus to a careful driver. This bonus principle may affect policy holders’ decisions whether to claim or not to claim, especially when involving accidents with slight damages, which is known as the hunger for bonus phenomenon. The ‘hunger for bonus’ under an NCD system may reduce insurers’ claim costs, and may be able to offset the expected decrease in premium income. 12.2 NCD System in Several Countries 12.2.1 NCD System in Malaysia Before the liberalization of Motor Tariff on 1st July 2017, the rating of motor insurance in Malaysia was governed by the Motor Tariff. Under the tariff, the rate charged should not be lower than the rates specified under the classes of risks, to ensure that the price competition among insurers will not go below the country’s economic level. The basic rating factors considered were scope of insurance, cubic capacity of vehicle and estimated value of vehicle (or sum insured, whichever is lower). Under the Motor Tariff, the final premium to be paid is adjusted by the policyholder’s claim experience, or equivalently, his NCD entitlement. Effective on 1st July 2017, the premium rates for motor insurance are liberalized, or de-tariffed. The pricing of premium is now determined by individual insurers and takaful operators, and the consumers are able to enjoy a wider choice of motor insurance products at competitive prices. Since tariff liberalization encourages innovation and competition among insurers and takaful operators, the premiums are based on broader risk factors other than the two rating factors specified in the Motor Tariff, i.e. sum insured and cubic capacity of vehicle. Other rating factors may be defined in the risk profile of an insured, such as age of vehicle, age of driver, safety and security features of vehicle, geographical location of vehicle and traffic offences of driver. As different insurers and takaful operators have different ways of defining the risk profile of an insured, the price of a policy may differ from one insurer to another. However, the NCD structure from the Motor Tariff remains ‘unchanged’ and continue to exist, and is ‘transferable’ from one insurer, or from one takaful operator, to another. The discounts in the Malaysian NCD system are divided into six classes, starting from the initial class of 0% discount, followed by classes of 25%, 30%, 38.3%, 45% and 55% discounts. Figure 12.1 provides the classes of NCD system in Malaysia. A claim-free year indicates that a policyholder is entitled to move one-step forward to the next discount class, such as from a 0% discount to a 25% discount in the renewal year. If a policyholder is already at the highest class, which is at a 55% discount, a claim-free year indicates that the policyholder remains in the same class. On the other hand, if one or more claims are made within the year, the NCD will be forfeited and the policyholder has to start at 0% discount in the renewal year. This set of transition rules can also be summarized as a rule of -1/Top, that is, a class of bonus for a claim-free year, and moving to the highest class after having one or more claims. For an illustration purpose, Figure 12.1 shows the classes and the transition diagram for the Malaysian NCD system. ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine Figure 12.1: Transition Diagram for NCD Classes (Malaysia) 12.2.2 NCD Systems in Other Countries The NCD system in Brazil are subdivided into seven classes, with the following premium levels (Lemaire &amp; Zi, 1994): 100, 90, 85, 80, 75, 70, and 65. These premium levels are also equivalent to the following discount classes: 0%, 10%, 15%, 20%, 25%, 30% and 45%. New policyholders have to start at 0% discount, or at premium level of 100, and a claim-free year indicates that a policyholder can move forward at a one-class discount. If one or more claims incurred within the year, the policyholder has to move one-class backward for each claim. Figure 12.2 shows the classes and the transition diagram for the NCD system in Brazil. This set of transition rules can also be summarized as a rule of -1/+1, that is, a class of bonus for a claim-free year, and a class of malus for each claim reported. Figure 12.2: Transition Diagram for NCD Classes (Brazil) The NCD system in Switzerland are subdivided into twenty-two classes, with the following premium levels: 270, 250, 230, 215, 200, 185, 170, 155, 140, 130, 120, 110, 100, 90, 80, 75, 70, 65, 60, 55, 50 and 45 (Lemaire &amp; Zi, 1994). These levels are also equivalent to the following loadings (malus): 170%, 150%, 130%, 115%, 100%, 85%, 70%, 55%, 40%, 30%, 20%, and 10%, and the following discounts: 0%, 10%, 20%, 25%, 30%, 35%, 40%, 45%, 50% and 55%. New policyholders have to start at 0% discount, or at premium level of 100, and a claim-free year indicates that a policyholder can move one-class forward. If one or more claims incurred within the year, the policyholder has to move four-classes backward for each claim. Table 12.1 and Figure 12.3 respectively show the classes and the transition diagram for the NCD system in Switzerland. This set of transition rule can be summarized as a rule of -1/+4. Table 12.1. Classes of NCD (Switzerland) \\[ \\small{ \\begin{array}{*{20}c} \\hline \\text{Classes} &amp; \\text{Loadings } (\\%) &amp; \\text{Classes} &amp; \\text{Discounts } (\\%)\\\\ \\hline {0} &amp; {170} &amp; {12} &amp; {0}\\\\ {1} &amp; {150} &amp; {13} &amp; {10}\\\\ {2} &amp; {130} &amp; {14} &amp; {20}\\\\ {3} &amp; {115} &amp; {15} &amp; {25}\\\\ {4} &amp; {100} &amp; {16} &amp; {30}\\\\ {5} &amp; {85} &amp; {17} &amp; {35}\\\\ {6} &amp; {70} &amp; {18} &amp; {40}\\\\ {7} &amp; {55} &amp; {19} &amp; {45}\\\\ {8} &amp; {40} &amp; {20} &amp; {50}\\\\ {9} &amp; {30} &amp; {21} &amp; {55}\\\\ {10} &amp; {20} &amp;&amp; \\\\ {11} &amp; {10} &amp;&amp; \\\\ \\hline \\end{array} } \\] Figure 12.3: Transition Diagram for NCD Classes (Switzerland) 12.3 BMS and Markov Chain Model A BMS can be represented by a discrete time Markov chain. A stochastic process is said to possess the Markov property if the evolution of the process in the future depends only on the present state but not the past. A discrete time Markov Chain is a Markov process with discrete state space. 12.3.1 Transition Probability A Markov Chain is determined by its transition probabilities. The transition probability from state \\(i\\) (at time \\(n\\)) to state \\(j\\) (at time \\(n + 1\\)) is called a one-step transition probability, and is denoted by \\(p_{ij}(n,n+1) = Pr (X_{n + 1} = j|X_n = i)\\), \\(i = 1,2,\\ldots,k\\), \\(j = 1,2,\\ldots,k\\). For general transition from time \\(m\\) to time \\(n\\), for \\(m&lt;n\\), by conditioning on \\(X_{o}\\) for \\(m\\le o\\le n\\), we have the Chapman-Kolmogorov equation of \\[\\begin{equation} p_{ij}(m,n)=\\sum_{l\\in S} p_{il}(m,o)p_{lj}(o,n). \\end{equation}\\] A time-homogeneous Markov Chain satisfies the property of \\(p_{ij}(n,n+t)=p_{ij}^{(t)}\\) for all \\(n\\). For instance, we have \\(p_{ij}(n,n+1)=p_{ij}^{(1)}\\equiv p_{ij}\\). In this case, the Chapman-Kolmogorov equation can be written as \\[\\begin{equation} p_{ij}(0,m+n)=\\sum_{l\\in S} p_{il}(0,m)p_{lj}(m,m+n)=\\sum_{l\\in S}p_{il}^{(m)}p_{lj}^{(n)}. \\end{equation}\\] In the context of BMS, the transition of the NCD classes is governed by the transition probability in a given year. The transition of the NCD classes is also a time-homogeneous Markov Chain since the set of transition rules is fixed and independent of time. We can represent the one-step transition probabilities by a \\(k \\times k\\) transition matrix \\({\\bf P}=(p_{ij})\\) that corresponds to NCD classes \\(0,1,2,\\ldots,k-1\\). \\[ \\small{ {\\bf P} = \\left[ {\\begin{array}{*{20}c} p_{00} &amp; p_{01} &amp; \\ldots &amp; &amp; &amp; p_{0k-1} \\\\ p_{10} &amp; p_{11} &amp; \\ldots &amp; &amp; &amp; p_{1k-1} \\\\ \\vdots &amp; \\ddots &amp; &amp; &amp; &amp; \\vdots \\\\ p_{k-10} &amp; p_{k-11} &amp; \\cdots &amp; &amp; &amp; p_{k-1k-1} \\end{array} } \\right] } \\] Here, its \\((i,j)\\)-th element is the transition probability from state \\(i\\) to state \\(j\\). In other words, each row of the transition matrix represents the transition of flowing out of state, whereas each column represents the transition of flowing into the state. The summation of transition probabilities of flowing out of state must equal to 1, or each row of the matrix must sum to 1, i.e. \\(\\sum_j p_{ij} = 1\\). All probabilities must also be non-negative (since they are probabilities), i.e. \\(p_{ij} \\ge 0\\). Consider the Malaysian NCD system. Let \\(\\{X_{t}:t=0,1,2,\\ldots\\}\\) be the NCD class occupied by a policyholder at time \\(t\\) with state space \\(S=\\{0,1,2,3,4,5\\}\\). Therefore, the transition probability in a no-claim year is equal to the probability of transition from state \\(i\\) to state \\(i+1\\), i.e. \\(p_{ii+1}\\). If an insured has one or more claims within the year, the probability of transitioning back to state 0 is represented by \\(p_{i0}=1-p_{ii+1}\\). Hence, the Malaysian NCD system can be represented by the following \\(6\\times 6\\) transition matrix: \\[ \\small{ {\\bf P} = \\left[ {\\begin{array}{*{20}c} p_{00}&amp;p_{01}&amp;0&amp;0&amp;0&amp;0\\\\ p_{10}&amp;0&amp;p_{12}&amp;0&amp;0&amp;0\\\\ p_{20}&amp;0&amp;0&amp;p_{23}&amp;0&amp;0\\\\ p_{30}&amp;0&amp;0&amp;0&amp;p_{34}&amp;0\\\\ p_{40}&amp;0&amp;0&amp;0&amp;0&amp;p_{45}\\\\ p_{50}&amp;0&amp;0&amp;0&amp;0&amp;p_{55} \\end{array} }\\right] = \\left[ {\\begin{array}{*{20}c} {1 - p_{01}}&amp;p_{01}&amp;0&amp;0&amp;0&amp;0\\\\ {1 - p_{12}}&amp;0&amp;p_{12}&amp;0&amp;0&amp;0\\\\ {1 - p_{23}}&amp;0&amp;0&amp;p_{23}&amp;0&amp;0\\\\ {1 - p_{34}}&amp;0&amp;0&amp;0&amp;p_{34}&amp;0\\\\ {1 - p_{45}}&amp;0&amp;0&amp;0&amp;0&amp;p_{45}\\\\ {1 - p_{55}}&amp;0&amp;0&amp;0&amp;0&amp;p_{55} \\end{array} }\\right] } \\] Example 12.3.1. Provide the transition matrix for the NCD system in Brazil. 12.3.1’,’Show Example Solution Solution Based on the NCD classes and the transition diagram shown in Figure 12.2 , the probability of a no-claim year is equal to the probability of moving one-class forward, whereas the probability of having one or more claims within the year is equal to the probability of moving one-class backward for each claim. Therefore, each row can contain two or more transition probabilities; one probability for advancing to the next state, and one or more probabilities for moving one-class backward. The transition matrix is: \\[ \\small{ {\\bf P} = \\left[ {\\begin{array}{*{20}{c}} {1 - p_{01}}&amp;p_{01}&amp;0&amp;0&amp;0&amp;0&amp;0\\\\ {1 - p_{12}}&amp;0&amp;p_{12}&amp;0&amp;0&amp;0&amp;0\\\\ {1 - \\sum_j p_{2j}}&amp;p_{21}&amp;0&amp;p_{23}&amp;0&amp;0&amp;0\\\\ {1 - \\sum_j p_{3j}}&amp;p_{31}&amp;p_{32}&amp;0&amp;p_{34}&amp;0&amp;0\\\\\\ {1 - \\sum_j p_{4j}}&amp;p_{41}&amp;p_{42}&amp;p_{43}&amp;0&amp;p_{45}&amp;0\\\\ {1 - \\sum_j p_{5j}}&amp;p_{51}&amp;p_{52}&amp;p_{53}&amp;p_{54}&amp;0&amp;p_{56}\\\\ {1 - \\sum_j p_{6j}}&amp;p_{61}&amp;p_{62}&amp;p_{63}&amp;p_{64}&amp;p_{65}&amp;p_{66} \\end{array} } \\right] } \\] Example 12.3.2. Provide the transition matrix for the NCD system in Switzerland. 12.3.2’,’Show Example Solution Solution. From Table 12.1 and Figure 12.3, the probability of a no-claim year is equal to the probability of moving one-class forward, whereas the probability of having one or more claims within the year is equal to the probability of moving four-classes backward for each claim. The transition matrix is: \\[ \\small{ {\\bf P} = \\left[ {\\begin{array}{*{12}{c}} 1 - {p_{01}} &amp; p_{01} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots\\\\ 1 - {p_{12}} &amp; 0 &amp; p_{12} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots\\\\ 1 - {p_{23}} &amp; 0 &amp; 0 &amp; p_{23} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots\\\\ 1 - {p_{34}} &amp; 0 &amp; 0 &amp; 0 &amp; p_{34} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots\\\\ 1 - {p_{45}} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; p_{45} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots\\\\ 1 - \\sum\\limits_j {p_{5j}} &amp; p_{51} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; p_{56} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots\\\\ 1 - \\sum\\limits_j {p_{6j}} &amp; 0 &amp; p_{62} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; p_{67} &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots\\\\ 1 - \\sum\\limits_j {p_{7j}} &amp; 0 &amp; 0 &amp; p_{73} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; p_{78} &amp; 0 &amp; 0 &amp; \\cdots\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots\\\\ 1 - \\sum\\limits_j {p_{19,j}} &amp; 0 &amp; 0 &amp; p_{19,3} &amp; 0 &amp; 0 &amp; 0 &amp; p_{19,7} &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots\\\\ 1 - \\sum\\limits_j {p_{20,j}} &amp; 0 &amp; 0 &amp; 0 &amp; p_{20,4} &amp; 0 &amp; 0 &amp; 0 &amp; p_{20,8} &amp; 0 &amp; 0 &amp; \\cdots\\\\ 1 - \\sum\\limits_j {p_{21,j}} &amp; p_{21,1} &amp; 0 &amp; 0 &amp; 0 &amp; p_{21,5} &amp; 0 &amp; 0 &amp; 0 &amp; p_{21,9} &amp; 0 &amp; \\cdots \\end{array} } \\right] } \\] 12.4 BMS and Stationary Distribution 12.4.1 Stationary Distribution A stationary distribution of a Markov Chain is a probability distribution that remains unchanged as time progresses into the future. It is represented by a row vector \\(\\boldsymbol \\pi =(\\pi_{1},\\pi_{2},\\ldots,\\pi_{k})\\) with the following properties: \\[\\begin{align} 0\\le \\pi_{j}\\le 1,\\notag\\\\ \\sum\\limits_{j}\\pi_{j}=1,\\notag\\\\ \\pi_{j}=\\sum\\limits_{i}\\pi_{i}p_{ij}. \\end{align}\\] The last equation can be written as \\(\\boldsymbol\\pi \\bf P=\\boldsymbol\\pi\\). The first two conditions are necessary for probability distribution whereas the last property indicates that the row vector \\(\\boldsymbol\\pi\\) is invariant (i.e. unchanged) by the one-step transition matrix. In other words, once the Markov Chain has reached the stationary state, its probability distribution will stay stationary over time. Mathematically, the stationary vector \\(\\boldsymbol\\pi\\) can also be obtained by finding the left eigenvector of the one-step transition matrix. Example 12.4.1. Find the stationary distribution for the NCD system in Malaysia assuming that the probability of a no-claim year for all NCD classes are \\({p_0}\\). 12.4.1’,’Show Example Solution Solution. The transition matrix can be re-written as: \\[ \\small{ {\\bf P} = \\left[ {\\begin{array}{*{20}c} {1 - p_{0}}&amp;p_{0}&amp;0&amp;0&amp;0&amp;0\\\\ {1 - p_{0}}&amp;0&amp;p_{0}&amp;0&amp;0&amp;0\\\\ {1 - p_{0}}&amp;0&amp;0&amp;p_{0}&amp;0&amp;0\\\\ {1 - p_{0}}&amp;0&amp;0&amp;0&amp;p_{0}&amp;0\\\\ {1 - p_{0}}&amp;0&amp;0&amp;0&amp;0&amp;p_{0}\\\\ {1 - p_{0}}&amp;0&amp;0&amp;0&amp;0&amp;p_0 \\end{array} }\\right] } \\] The stationary distribution can be calculated using \\(\\pi_{j}=\\sum\\limits_{i}\\pi_{i}p_{ij}\\). The solutions are: \\[ \\small{ \\begin{array}{lll} {\\pi _0} = \\sum\\limits_i {\\pi_i}p_{i0} = (1 - {p_0})\\sum\\limits_i {{\\pi _i}} = 1 - {p_0}\\\\ {\\pi _1} = \\sum\\limits_i {\\pi _i}p_{i1} = {\\pi_0}{p_{01}} = (1 - {p_0}){p_0}\\\\ {\\pi _2} = \\sum\\limits_i {\\pi _i}p_{i2} = {\\pi _1}{p_{12}} = (1 - {p_0}){p_0}^2\\\\ {\\pi _3} = \\sum\\limits_i {\\pi _i}p_{i3} = {\\pi _2}{p_{23}} = (1 - {p_0}){p_0}^3\\\\ {\\pi _4} = \\sum\\limits_i {\\pi _i}p_{i4} = {\\pi _3}{p_{34}} = (1 - {p_0}){p_0}^4\\\\ {\\pi _5} = \\sum\\limits_i {\\pi _i}p_{i5} = {\\pi _4}{p_{45}} + {\\pi _5}{p_{55}} = (1 - {p_0}){p_0}^5 + {\\pi _5}{p_0}\\\\ \\therefore {\\pi _5} = \\frac{(1 - {p_0}){p_0}^5}{{(1 - {p_0})}} = {p_0}^5 \\end{array} } \\] The stationary distribution shown in Example 12.4.1 represents the asymptotic distribution of the NCD system, or the distribution in the long run. As an example, assuming that the probability of a no-claim year is \\(p_0 = 0.90,\\) the stationary probabilities are: \\[ \\small{ \\begin{array}{l} {\\pi _0} = 1 - {p_0} = 0.1000\\\\ {\\pi _1} = (1 - {p_0}){p_0} = 0.0900\\\\ {\\pi _2} = (1 - {p_0}){p_0}^2 = 0.0810\\\\ {\\pi _3} = (1 - {p_0}){p_0}^3 = 0.0729\\\\ {\\pi _4} = (1 - {p_0}){p_0}^4 = 0.0656\\\\ {\\pi _5} = {p_0}^5 = 0.5905 \\end{array} } \\] In other words, \\({\\pi_0} = 0.10\\) indicates that 10% of insureds will eventually belong to class 0, \\({\\pi _1} = 0.09\\) indicates that 9% of insureds will eventually belong to class 1, and so forth, until \\({\\pi _5} = 0.59\\), which indicates that 59% of insureds will eventually belong to class 5. 12.4.2 R Code for a Stationary Distribution We can use the left eigenvector of a transition matrix to calculate a stationary distribution. The following R code can be used to calculate the left eigenvector: 1. Create a Transition Matrix #create transition matrix entries = c(0.1,0.9,0,0,0,0, 0.1,0,0.9,0,0,0, 0.1,0,0,0.9,0,0, 0.1,0,0,0,0.9,0, 0.1,0,0,0,0,0.9, 0.1,0,0,0,0,0.9) (TP &lt;- matrix(entries,nrow=6,byrow=TRUE) ) LeftEigen.1’, ’Show R Output #create transition matrix entries = c(0.1,0.9,0,0,0,0, 0.1,0,0.9,0,0,0, 0.1,0,0,0.9,0,0, 0.1,0,0,0,0.9,0, 0.1,0,0,0,0,0.9, 0.1,0,0,0,0,0.9) (TP &lt;- matrix(entries,nrow=6,byrow=TRUE) ) [,1] [,2] [,3] [,4] [,5] [,6] [1,] 0.1 0.9 0.0 0.0 0.0 0.0 [2,] 0.1 0.0 0.9 0.0 0.0 0.0 [3,] 0.1 0.0 0.0 0.9 0.0 0.0 [4,] 0.1 0.0 0.0 0.0 0.9 0.0 [5,] 0.1 0.0 0.0 0.0 0.0 0.9 [6,] 0.1 0.0 0.0 0.0 0.0 0.9 Bibliography Lemaire, J., &amp; Zi, H. (1994). A comparative analysis of 30 bonus-malus systems. ASTIN Bulletin, 24(2), 287–309. "],["aggregate-loss-models-1.html", "Bab 13 Aggregate Loss Models", " Bab 13 Aggregate Loss Models "],["dependence-modeling.html", "Bab 14 Dependence Modeling", " Bab 14 Dependence Modeling "],["appendix-a-review-of-statistical-inference.html", "Bab 15 Appendix A: Review of Statistical Inference 15.1 konsep dasar 15.2 Estimasi Titik dan Properti 15.3 Estimasi Interval 15.4 Pengujian Hipotesis", " Bab 15 Appendix A: Review of Statistical Inference 15.1 konsep dasar Inferensi statistik adalah proses membuat kesimpulan tentang karakteristik dari sekumpulan besar item/individu (yaitu, populasi), menggunakan set data yang mewakili (misalnya, sampel acak) dari daftar item atau individu dari populasi yang dapat diambil sampel. Meskipun proses ini memiliki berbagai aplikasi di berbagai bidang termasuk ilmu pengetahuan, rekayasa, kesehatan, sosial, dan ekonomi, inferensi statistik penting bagi perusahaan asuransi yang menggunakan data dari pemegang polis mereka yang ada untuk membuat inferensi tentang karakteristik (misalnya, profil risiko) dari segmen target pelanggan tertentu (yaitu, populasi) yang tidak langsung diamati oleh perusahaan asuransi tersebut. 15.1.1 Sampel Acak Dalam statistika, kesalahan sampling terjadi ketika kerangka sampling, yaitu daftar dari mana sampel diambil, tidak cukup mewakili populasi yang menjadi tujuan. Sampel harus merupakan subset yang representatif dari populasi atau universum yang menjadi tujuan. Jika sampel tidak representatif, mengambil sampel yang lebih besar tidak akan menghilangkan bias, karena kesalahan yang sama terulang berulang kali. Oleh karena itu, kita memperkenalkan konsep sampling acak yang menghasilkan sampel acak sederhana yang mewakili populasi. Kita mengasumsikan bahwa variabel acak \\(X\\) mewakili pengambilan dari populasi dengan fungsi distribusi \\(F(\\cdot)\\) dengan rata-rata \\(\\mathrm{E}[X]=\\mu\\) dan varians \\(\\mathrm{Var}[X]=\\mathrm{E}[(X-\\mu)^2]\\), di mana \\(E(\\cdot)\\) menunjukkan ekspektasi dari variabel acak. Dalam sampling acak, kita melakukan total \\(n\\) pengambilan seperti yang direpresentasikan oleh \\(X_1,...,X_n\\), yang masing-masing tidak saling berhubungan (yaitu, statistik independen). Kita mengacu pada \\(X_1,...,X_n\\) sebagai sampel acak (dengan penggantian) dari \\(F(\\cdot)\\), baik dalam bentuk parametrik maupun nonparametrik. Alternatifnya, kita dapat mengatakan bahwa \\(X_1,...,X_n\\) secara identik dan independen didistribusikan (iid) dengan fungsi distribusi \\(F(\\cdot)\\). 15.1.2 Distribusi Sampel Dengan menggunakan sampel acak \\(X_1,...,X_n\\), kita tertarik untuk membuat kesimpulan tentang atribut tertentu dari distribusi populasi \\(F(\\cdot)\\). Misalnya, kita mungkin tertarik untuk membuat inferensi tentang rata-rata populasi yang ditandai dengan \\(\\mu\\). Alami untuk mempertimbangkan rata-rata sampel, \\(\\bar{X}=\\sum_{i=1}^nX_i\\), sebagai estimasi dari rata-rata populasi \\(\\mu\\). Kita menyebut rata-rata sampel sebagai statistik yang dihitung dari sampel acak \\(X_1,...,X_n\\). Statistik ringkasan lain yang umum digunakan termasuk simpangan baku sampel dan kuantil sampel. Ketika menggunakan statistik (misalnya, rata-rata sampel \\(\\bar{X}\\)) untuk membuat inferensi statistik tentang atribut populasi (misalnya, rata-rata populasi \\(\\mu\\)), kualitas inferensi ditentukan oleh bias dan ketidakpastian dalam estimasi, karena menggunakan sampel sebagai pengganti populasi. Oleh karena itu, penting untuk mempelajari distribusi statistik yang mengukur bias dan variabilitas statistik tersebut. Secara khusus, distribusi rata-rata sampel, \\(\\bar{X}\\) (atau statistik lainnya), disebut sebagai distribusi sampling. Distribusi sampling bergantung pada proses sampling, statistik, ukuran sampel \\(n\\), dan distribusi populasi \\(F(\\cdot)\\). Teorema limit sentral memberikan distribusi sampel (sampling) dari rata-rata sampel dalam sampel besar di bawah kondisi tertentu. 15.1.3 Teorema Limit Sentral Dalam statistika, terdapat variasi dari teorema limit sentral (Central Limit Theorem, CLT) yang menjamin bahwa, dalam kondisi tertentu, rata-rata sampel akan mendekati rata-rata populasi dengan distribusi samplingnya mendekati distribusi normal saat ukuran sampel mendekati tak hingga. Kami menyebutkan teorema limit sentral Lindeberg-Levy yang menetapkan distribusi sampling asimtotik dari rata-rata sampel \\(\\bar{X}\\) yang dihitung menggunakan sampel acak dari populasi universe dengan distribusi \\(F(\\cdot)\\). Teorema limit sentral Lindeberg-Levy. Misalkan \\(X_1,...,X_n\\) adalah sampel acak dari distribusi populasi \\(F(\\cdot)\\) dengan rata-rata \\(\\mu\\) dan varians \\(\\sigma^2&lt;\\infty\\). Selisih antara rata-rata sampel \\(\\bar{X}\\) dan \\(\\mu\\), ketika dikalikan dengan akar kuadrat \\(n\\), konvergen dalam distribusi menjadi distribusi normal saat ukuran sampel mendekati tak hingga. Artinya, \\[\\begin{equation} \\sqrt{n}(\\bar{X}-\\mu)\\xrightarrow[]{d}N(0,\\sigma). \\end{equation}\\] Perlu dicatat bahwa teorema limit sentral tidak memerlukan bentuk parametrik untuk \\(F(\\cdot)\\). Berdasarkan teorema limit sentral, kita dapat melakukan inferensi statistik pada rata-rata populasi (kita menyimpulkan, bukan menyimpulkan secara deduktif). Jenis inferensi yang dapat kita lakukan meliputi estimasi populasi, pengujian hipotesis tentang kebenaran suatu pernyataan nol, dan prediksi sampel masa depan dari populasi. 15.2 Estimasi Titik dan Properti Fungsi distribusi populasi \\(F(\\cdot)\\) biasanya dapat dikarakterisasi oleh sejumlah terbatas (terbatas) parameter yang disebut parameter, dalam hal ini kita mengacu pada distribusi sebagai distribusi parametrik. Sebaliknya, dalam analisis nonparametrik , atribut-atribut distribusi sampling tidak terbatas pada sejumlah kecil parameter. Untuk memperoleh karakteristik populasi, terdapat berbagai atribut yang terkait dengan distribusi populasi \\(F(\\cdot)\\). Ukuran-ukuran tersebut meliputi rata-rata, median, persentil (misalnya persentil ke-95), dan simpangan baku. Karena ukuran-ukuran ringkasan ini tidak bergantung pada referensi parametrik tertentu, mereka adalah ukuran ringkasan nonparametrik. Di sisi lain, dalam analisis parametrik, kita dapat mengasumsikan keluarga distribusi tertentu dengan parameter-parameter tertentu. Misalnya, orang biasanya menganggap bahwa logaritma jumlah klaim memiliki distribusi normal dengan rata-rata \\(\\mu\\) dan simpangan baku \\(\\sigma\\). Dengan kata lain, kita mengasumsikan bahwa klaim memiliki distribusi lognormal dengan parameter-parameter \\(\\mu\\) dan \\(\\sigma\\). Sebagai alternatif, perusahaan asuransi umumnya mengasumsikan bahwa tingkat keparahan klaim mengikuti distribusi gamma dengan parameter bentuk \\(\\alpha\\) dan parameter skala \\(\\boldsymbol{\\theta}\\). Di sini, distribusi normal, lognormal, dan gamma adalah contoh dari distribusi parametrik. Dalam contoh di atas, besaran \\(\\mu\\), \\(sigma\\), \\(alpha\\), dan \\(\\boldsymbol{\\theta}\\) dikenal sebagai parameter. Untuk keluarga distribusi parametrik yang diberikan, distribusi secara unik ditentukan oleh nilai-nilai parameter tersebut. Seringkali, kita menggunakan \\(\\boldsymbol{\\theta}\\) untuk menyatakan atribut ringkasan dari populasi. Dalam model-parametrik, \\(\\boldsymbol{\\theta}\\) dapat berupa parameter atau fungsi parameter dari suatu distribusi, seperti parameter rata-rata dan varians normal. Dalam analisis nonparametrik, \\(\\theta\\) dapat berbentuk ukuran ringkasan nonparametrik seperti rata-rata atau simpangan baku populasi. Misalkan \\(\\hat{\\theta} =\\hat{\\theta}(X_1, \\ldots, X_n)\\) adalah fungsi dari sampel yang memberikan representasi atau estimasi dari \\(\\theta\\). Ini disebut sebagai statistik, yaitu fungsi dari sampel \\(X_1,…,X_n\\). 15.2.1 Estimasi Metode Momen Sebelum mendefinisikan estimasi metode momen, kita akan mendefinisikan konsep momen terlebih dahulu. Momen adalah atribut populasi yang menggambarkan fungsi distribusi \\(F(\\cdot)\\). Diberikan pengambilan acak \\(X\\) dari \\(F(\\cdot)\\), ekspektasi \\(\\mu_k=\\mathrm{E}[X^k]\\) disebut sebagai momen ke-\\(k\\) dari \\(X\\), dengan \\(k=1,2,3,…\\) Sebagai contoh, rata-rata populasi \\(\\mu\\) adalah momen pertama. Selain itu, ekspektasi \\(\\mathrm{E}[(X-\\mu)^k]\\) disebut momen sentral ke-\\(k\\). Oleh karena itu, varians adalah momen sentral kedua. Dengan menggunakan sampel acak \\(X_1,…,X_n\\), kita dapat membuat momen sampel yang sesuai, \\(\\hat{\\mu}_k=(1/n)\\sum_{i=1}^n X_i^k\\), untuk mengestimasi atribut populasi \\(\\mu_k\\). Sebagai contoh, kita telah menggunakan rata-rata sampel \\(\\bar{X}\\) sebagai estimator untuk rata-rata populasi \\(\\mu\\). Demikian pula, momen sentral kedua dapat diestimasi sebagai \\((1/n)\\sum_{i=1}^n(X_i-\\bar{X})^2\\). Tanpa mengasumsikan bentuk parametrik untuk \\(F(\\cdot)\\), momen sampel merupakan estimasi nonparametrik dari atribut populasi yang sesuai. Estimator seperti itu yang didasarkan pada pencocokan momen sampel dan momen populasi yang sesuai disebut sebagai estimator metode momen (mme). Meskipun mme secara alami digunakan dalam model nonparametrik, itu juga dapat digunakan untuk mengestimasi parameter ketika diasumsikan sebuah keluarga distribusi parametrik tertentu untuk \\(F(\\cdot)\\). Misalkan \\(\\boldsymbol{\\theta}=(\\theta_1,\\cdots,\\theta_m)\\) merupakan vektor parameter yang sesuai dengan distribusi parametrik \\(F(\\cdot)\\). Dalam keluarga distribusi tersebut, biasanya kita mengetahui hubungan antara parameter dan momen. Khususnya, kita mengetahui bentuk spesifik dari fungsi-fungsi \\(h_1(\\cdot),h_2(\\cdot),\\cdots,h_m(\\cdot)\\) sehingga \\(\\mu_1=h_1(\\boldsymbol{\\theta}),\\,\\mu_2=h_2(\\boldsymbol{\\theta}),\\,\\cdots,\\,\\mu_m=h_m(\\boldsymbol{\\theta})\\). Diberikan mme \\(\\hat{\\mu}_1, \\ldots, \\hat{\\mu}_m\\) dari sampel acak, mme parameter \\(\\hat{\\theta}_1,\\cdots,\\hat{\\theta}_m\\) dapat diperoleh dengan memecahkan persamaan-persamaan dari \\[\\begin{equation} \\hat{\\mu}_1=h_1(\\hat{\\theta}_1,\\cdots,\\hat{\\theta}_m); \\end{equation}\\] \\[\\begin{equation} \\hat{\\mu}_2=h_2(\\hat{\\theta}_1,\\cdots,\\hat{\\theta}_m); \\end{equation}\\] \\[\\begin{equation} \\cdots \\end{equation}\\] \\[\\begin{equation} \\hat{\\mu}_m=h_m(\\hat{\\theta}_1,\\cdots,\\hat{\\theta}_m). \\end{equation}\\] 15.2.2 Estimasi Maksimum Likelihood Ketika \\(F(\\cdot)\\) mengambil bentuk parametrik, metode estimasi likelihood maksimum (maximum likelihood estimation) banyak digunakan untuk mengestimasi parameter populasi \\(\\boldsymbol{\\theta}\\). Estimasi likelihood maksimum didasarkan pada fungsi likelihood, yang merupakan fungsi dari parameter-parameter yang diberikan sampel yang diamati. Misalkan \\(f(x_i|\\boldsymbol{\\theta})\\) adalah fungsi probabilitas dari \\(X_i\\) yang dievaluasi pada \\(X_i=x_i\\) \\((i=1,2,\\cdots,n)\\); ini adalah fungsi massa probabilitas dalam kasus \\(X\\) yang diskrit dan fungsi densitas probabilitas dalam kasus \\(X\\) yang kontinu. Dengan asumsi independensi, fungsi likelihood dari \\(\\boldsymbol{\\theta}\\) yang terkait dengan pengamatan \\((X_1,X_2,\\cdots,X_n)=(x_1,x_2,\\cdots,x_n)=\\mathbf{x}\\) dapat ditulis sebagai \\[\\begin{equation} L(\\boldsymbol{\\theta}|\\mathbf{x})=\\prod_{i=1}^nf(x_i|\\boldsymbol{\\theta}), \\end{equation}\\] dengan fungsi log-likelihood yang sesuai diberikan oleh \\[\\begin{equation} l(\\boldsymbol{\\theta}|\\mathbf{x})=\\log(L(\\boldsymbol{\\theta}|\\mathbf{x}))=\\sum_{i=1}^n\\log f(x_i|\\boldsymbol{\\theta}). \\end{equation}\\] Estimator maximum likelihood (MLE) dari \\(\\boldsymbol{\\theta}\\) adalah himpunan nilai \\(\\boldsymbol{\\theta}\\) yang memaksimalkan fungsi likelihood (fungsi log-likelihood), dengan mempertimbangkan sampel yang diamati. Dengan demikian, MLE \\(\\hat{\\boldsymbol{\\theta}}\\) dapat ditulis sebagai \\[\\begin{equation} \\hat{\\boldsymbol{\\theta}}={\\mbox{argmax}}_{\\boldsymbol{\\theta}\\in\\Theta}l(\\boldsymbol{\\theta}|\\mathbf{x}), \\end{equation}\\] di mana \\(\\theta\\) adalah ruang parameter dari \\(\\boldsymbol{\\theta}\\), dan \\({\\mbox{argmax}}_{\\boldsymbol{\\theta}\\in\\Theta}l(\\boldsymbol{\\theta}|\\mathbf{x})\\) didefinisikan sebagai nilai \\(\\boldsymbol{\\theta}\\) di mana fungsi \\(l(\\boldsymbol{\\theta}|\\mathbf{x})\\) mencapai nilai maksimumnya. Diberikan bentuk analitis dari fungsi likelihood, mle dapat diperoleh dengan mengambil turunan pertama dari fungsi log-likelihood terhadap \\(\\boldsymbol{\\theta}\\), dan menetapkan nilai-nilai turunan parsial menjadi nol. Dengan kata lain, mle adalah solusi dari persamaan-persamaan: \\[\\begin{equation} \\frac{\\partial l(\\hat{\\boldsymbol{\\theta}}|\\mathbf{x})}{\\partial\\hat{\\theta}_1}=0; \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial l(\\hat{\\boldsymbol{\\theta}}|\\mathbf{x})}{\\partial\\hat{\\theta}_2}=0; \\end{equation}\\] \\[\\begin{equation} \\cdots \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial l(\\hat{\\boldsymbol{\\theta}}|\\mathbf{x})}{\\partial\\hat{\\theta}_m}=0, \\end{equation}\\] asalkan turunan parsial kedua negatif. Untuk model parametrik, mle dari parameter-parameter dapat diperoleh secara analitis (misalnya, dalam kasus distribusi normal dan estimasi linear), atau secara numerik melalui algoritma iteratif seperti metode Newton-Raphson dan versi adaptifnya (misalnya, dalam kasus model linier umum dengan variabel respons non-normal). Distribusi normal. Anggaplah \\((X_1,X_2,⋯,X_n)\\) sebagai sampel acak dari distribusi normal \\(N(\\mu, \\sigma^2)\\). Dengan sampel yang diamati \\((X_1,X_2,\\cdots,X_n)=(x_1,x_2,\\cdots,x_n)\\) kita dapat menulis fungsi likelihood dari \\(\\mu,\\sigma^2\\) sebagai \\[\\begin{equation} L(\\mu,\\sigma^2)=\\prod_{i=1}^n\\left[\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{\\left(x_i-\\mu\\right)^2}{2\\sigma^2}}\\right], \\end{equation}\\] dengan fungsi log-likelihood yang sesuai diberikan oleh \\[\\begin{equation} l(\\mu,\\sigma^2)=-\\frac{n}{2}[\\log(2\\pi)+\\log(\\sigma^2)]-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n\\left(x_i-\\mu\\right)^2. \\end{equation}\\] Dengan memecahkan \\[\\begin{equation} \\frac{\\partial l(\\hat{\\mu},\\sigma^2)}{\\partial \\hat{\\mu}}=0, \\end{equation}\\] kita mendapatkan \\(\\hat{\\mu}=\\bar{x}=(1/n)\\sum_{i=1}^nx_i\\). Mudah untuk diverifikasi bahwa \\(\\frac{\\partial l^2(\\hat{\\mu},\\sigma^2)}{\\partial \\hat{\\mu}^2}\\left|_{\\hat{\\mu}=\\bar{x}}\\right.&lt;0\\). Karena ini berlaku untuk \\(x\\) yang sembarang, \\(\\hat{\\mu}=\\bar{X}\\) adalah mle dari \\(\\mu\\). Demikian pula, dengan memecahkan \\[\\begin{equation} \\frac{\\partial l(\\mu,\\hat{\\sigma}^2)}{\\partial \\hat{\\sigma}^2}=0, \\end{equation}\\] kita mendapatkan \\(\\hat{\\sigma}^2=(1/n)\\sum_{i=1}^n(x_i-\\mu)^2\\). Dengan menggantikan \\(\\mu\\) oleh \\(\\hat{\\mu}\\), kita mendapatkan mle dari \\(\\sigma^2\\) sebagai \\(\\hat{\\sigma}^2=(1/n)\\sum_{i=1}^n(X_i-\\bar{X})^2\\). Dengan demikian, rata-rata sampel \\(\\bar{X}\\) dan \\(\\sigma^2\\) keduanya adalah mme dan MLE untuk rata-rata \\(\\mu\\) dan varian \\(\\sigma^2\\), dalam distribusi populasi normal \\(F(\\cdot)\\). Lebih banyak detail mengenai sifat-sifat fungsi likelihood diberikan dalam Lampiran Bagian 17.1. 15.3 Estimasi Interval Setelah kita memperkenalkan mme dan mle, kita dapat melakukan jenis pertama dari inferensi statistik, yaitu estimasi interval yang mengukur ketidakpastian akibat penggunaan sampel yang terbatas. Dengan mendapatkan distribusi sampling dari mle, kita dapat mengestimasi interval (interval kepercayaan) untuk parameter tersebut. Dalam pendekatan frequentist (misalnya, yang didasarkan pada estimasi maksimum likelihood), interval kepercayaan yang dihasilkan dari kerangka sampling yang sama akan mencakup nilai sebenarnya sebagian besar waktu (misalnya, 95% dari waktu), jika kita mengulangi proses sampling dan menghitung ulang interval berulang kali. Proses tersebut membutuhkan derivasi distribusi sampling untuk mle. 15.3.1 Distribusi Tepat untuk Rata-rata Sampel Normal Karena sifat aditivitas dari distribusi normal (yaitu, jumlah dari variabel acak normal yang mengikuti distribusi normal multivariat tetap mengikuti distribusi normal) dan distribusi normal termasuk dalam keluarga lokasi–skala (yaitu, transformasi lokasi dan/atau skala dari variabel acak normal menghasilkan distribusi normal), maka rata-rata sampel \\(\\bar{X}\\) dari sampel acak dari distribusi normal \\(F(\\cdot)\\) memiliki distribusi sampling normal untuk setiap \\(n\\) yang terbatas. Diberikan \\(X_i\\sim^{iid} N(\\mu,\\sigma^2)\\), mle dari \\(\\mu\\) memiliki distribusi yang tepat \\[\\begin{equation} \\bar{X}\\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right). \\end{equation}\\] Oleh karena itu, rata-rata sampel adalah estimator tidak bias dari \\(\\mu\\). Selain itu, ketidakpastian dalam estimasi dapat diukur dengan varian \\(\\sigma^2/n\\), yang akan berkurang seiring dengan pertambahan ukuran sampel \\(n\\). Ketika ukuran sampel mendekati tak terhingga, rata-rata sampel akan mendekati titik tunggal pada nilai sebenarnya. 15.3.2 Properti Sampel Besar dari Estimasi Maksimum Likelihood Untuk mle dari parameter mean dan parameter-parameter lain dari keluarga distribusi parametrik lainnya, kita biasanya tidak dapat mendapatkan distribusi sampling yang tepat untuk sampel yang terbatas. Untungnya, ketika ukuran sampel cukup besar, mle dapat diaproksimasi dengan distribusi normal. Berkat teori maximum likelihood secara umum, mle memiliki beberapa sifat yang baik dalam sampel yang cukup besar. MLE (Maximum Likelihood Estimator) \\(\\hat{\\theta}\\) dari parameter \\(\\theta\\) merupakan estimator yang konsisten. Artinya, \\(\\hat{\\theta}\\) konvergen dalam probabilitas ke nilai sebenarnya \\(\\theta\\), ketika ukuran sampel \\(n\\) mendekati tak hingga. MLE memiliki sifat asimptotik normalitas, yang berarti bahwa estimator akan konvergen dalam distribusi menjadi distribusi normal yang berpusat di sekitar nilai sebenarnya, ketika ukuran sampel mendekati tak hingga. Secara khusus, \\[\\begin{equation} \\sqrt{n}(\\hat{\\theta}-\\theta)\\rightarrow_d N\\left(0,\\,V\\right),\\quad \\mbox{as}\\quad n\\rightarrow \\infty, \\end{equation}\\] di mana \\(V\\) adalah invers dari Informasi Fisher. Dengan demikian, mle \\(\\hat{\\theta}\\) mengikuti secara aproksimatif distribusi normal dengan mean \\(\\theta\\) dan varian \\(V/n\\), ketika ukuran sampel besar. - MLE adalah efisien, yang berarti bahwa ia memiliki varian asimptotik terkecil \\(V\\), umumnya disebut sebagai batas bawah Cramer-Rao. Secara khusus, batas bawah Cramer-Rao adalah invers dari informasi Fisher yang didefinisikan sebagai \\(\\mathcal{I}(\\theta)=-\\mathrm{E}(\\partial^2\\log f(X;\\theta)/\\partial \\theta^2)\\). Oleh karena itu, \\(\\mathrm{Var}(\\hat{\\theta})\\) dapat diestimasi berdasarkan informasi Fisher yang diamati yang dapat ditulis sebagai \\(-\\sum_{i=1}^n \\partial^2\\log f(X_i;\\theta)/\\partial \\theta^2\\). Untuk banyak distribusi parametrik, informasi Fisher dapat dihitung secara analitik untuk mle dari parameter-parameter tersebut. Untuk model parametrik yang lebih kompleks, informasi Fisher dapat dievaluasi secara numerik menggunakan integrasi numerik untuk distribusi kontinu, atau penjumlahan numerik untuk distribusi diskrit. Informasi lebih lanjut tentang estimasi maximum likelihood dapat ditemukan di Bagian Lampiran 17.2. 15.3.3 Interval Kepercayaan Diberikan bahwa mle \\(\\hat{\\theta}\\) memiliki distribusi normal yang tepat atau mendekati dengan mean \\(\\theta\\) dan varian \\(\\mathrm{Var}(\\hat{\\theta})\\), kita dapat mengambil akar kuadrat dari varian dan memasukkan estimasi untuk mendefinisikan \\(se(\\hat{\\theta}) = \\sqrt{\\mathrm{Var}(\\hat{\\theta})}\\). Standar error adalah simpangan baku yang diestimasi yang mengukur ketidakpastian dalam estimasi yang dihasilkan dari penggunaan sampel terbatas. Dalam beberapa kondisi keberaturan yang mengatur distribusi populasi, kita dapat menunjukkan bahwa statistik \\[\\begin{equation} \\frac{\\hat{\\theta}-\\theta}{se(\\hat{\\theta})} \\end{equation}\\] konvergen dalam distribusi ke distribusi Student-\\(t\\) dengan derajat kebebasan (sebuah parameter dari distribusi) \\(n−p\\), di mana \\(p\\) adalah jumlah parameter dalam model selain varians. Sebagai contoh, untuk kasus distribusi normal, kita memiliki \\(p=1\\) untuk parameter \\(\\mu\\); untuk model regresi linear dengan satu variabel independen, kita memiliki \\(p=2\\) untuk parameter intercept dan variabel independen. Misalkan \\(t_{n-p}(1-\\alpha/2)\\) adalah persentil ke-\\(100\\times(1-\\alpha/2)\\) dari distribusi Student-\\(t\\) yang memenuhi \\(\\Pr\\left[t&lt; t_{n-p}\\left(1-{\\alpha}/{2}\\right) \\right]= 1-{\\alpha}/{2}\\). Maka, kita memiliki, \\[\\begin{equation} \\Pr\\left[-t_{n-p}\\left(1-\\frac{\\alpha}{2}\\right)&lt;\\frac{\\hat{\\theta}-\\theta}{se(\\hat{\\theta})}&lt; t_{n-p}\\left(1-\\frac{\\alpha}{2}\\right) \\right]= 1-{\\alpha}, \\end{equation}\\] dari mana kita dapat menentukan interval kepercayaan untuk \\(\\theta\\). Dari persamaan di atas, kita dapat mendapatkan sepasang statistik, \\(\\hat{\\theta}_1\\) dan \\(\\hat{\\theta}_2\\), yang memberikan interval dalam bentuk \\([\\hat{\\theta}_1, \\hat{\\theta}_2]\\). Interval ini adalah interval kepercayaan \\(1-\\alpha\\) untuk \\(\\theta\\), sehingga \\(\\Pr\\left(\\hat{\\theta}_1 \\le \\theta \\le \\hat{\\theta}_2\\right) = 1-\\alpha,\\), di mana probabilitas 1−α disebut tingkat kepercayaan. Perlu diperhatikan bahwa interval kepercayaan di atas tidak valid untuk sampel kecil, kecuali untuk kasus rata-rata normal. Dalam distribusi normal. Untuk rata-rata populasi normal \\(\\mu\\), mle memiliki distribusi sampling yang tepat \\(\\bar{X}\\sim N(\\mu,\\sigma/\\sqrt{n})\\), di mana kita dapat mengestimasi \\(se(\\hat{\\theta})\\) dengan \\(\\hat{\\sigma}/\\sqrt{n}\\). Berdasarkan Teorema Cochran, statistik yang dihasilkan memiliki distribusi Student-t yang tepat dengan derajat kebebasan \\(n−1\\). Oleh karena itu, kita dapat menentukan batas bawah dan batas atas dari interval kepercayaan sebagai \\[\\begin{equation} \\hat{\\mu}_1 = \\hat{\\mu} - t_{n-1}\\left(1-\\frac{\\alpha}{2}\\right)\\frac{ \\hat{\\sigma}}{\\sqrt{n}} \\end{equation}\\] dan \\[\\begin{equation} \\hat{\\mu}_2 = \\hat{\\mu} + t_{n-1}\\left(1-\\frac{\\alpha}{2}\\right)\\frac{ \\hat{\\sigma}}{\\sqrt{n}}. \\end{equation}\\] Ketika \\(\\alpha = 0.05\\), \\(t_{n-1}(1-\\alpha/2) \\approx 1.96\\) untuk nilai n yang besar. Berdasarkan Teorema Cochran, interval kepercayaan tetap valid tidak peduli dengan ukuran sampel. 15.4 Pengujian Hipotesis Untuk parameter \\(\\boldsymbol{\\theta}\\) dari distribusi parametrik, jenis lain dari inferensi statistik adalah pengujian hipotesis yang memverifikasi apakah hipotesis mengenai parameter tersebut benar, dengan menggunakan tingkat signifikansi tertentu yang disebut level of significance \\(\\alpha\\) (misalnya, 5%). Dalam pengujian hipotesis, kita menolak hipotesis nol, yaitu pernyataan yang membatasi mengenai parameter, jika probabilitas mengamati sampel acak seekstrem sampai dengan yang diamati lebih kecil daripada \\(\\alpha\\), jika hipotesis nol benar. 15.4.1 Konsep Dasar Pada pengujian statistik, biasanya kita tertarik untuk menguji apakah pernyataan mengenai beberapa parameter, hipotesis nol (dinyatakan sebagai \\(H_0\\)), benar berdasarkan data yang diamati. Hipotesis nol dapat memiliki bentuk umum \\(H_0:\\theta\\in\\Theta_0\\), di mana \\(\\Theta_0\\) adalah subset dari ruang parameter \\(Θ\\) dari \\(θ\\) yang mungkin berisi beberapa parameter. Untuk kasus dengan satu parameter \\(θ\\), hipotesis nol biasanya memiliki bentuk \\(H_0:\\theta=\\theta_0\\) atau \\(H_0:\\theta\\leq\\theta_0\\). Kebalikan dari hipotesis nol disebut hipotesis alternatif yang dapat dituliskan sebagai \\(H_a:\\theta\\neq\\theta_0\\) atau \\(H_a:\\theta&gt;\\theta_0\\). Uji statistik pada \\(H_0:\\theta=\\theta_0\\) disebut uji dua sisi karena hipotesis alternatif berisi dua ketidaksamaan, sedangkan uji dengan \\(H_0:\\theta\\leq\\theta_0\\) atau \\(H_0:\\theta\\geq\\theta_0\\) disebut uji satu sisi. Uji statistik biasanya dibangun berdasarkan statistik \\(T\\) dan distribusinya yang eksak atau berdasarkan sampel besar. Uji tersebut umumnya menolak uji dua sisi ketika \\(T&gt;c_1\\) atau \\(T&lt;c_2\\), di mana dua konstanta \\(c_1\\) dan \\(c_2\\) diperoleh berdasarkan distribusi sampel dari \\(T\\) pada tingkat probabilitas \\(\\alpha\\) yang disebut tingkat signifikansi. Secara khusus, tingkat signifikansi \\(\\alpha\\) memenuhi \\[\\begin{equation} \\alpha=\\Pr(\\mbox{reject }H_0|H_0\\mbox{ is true}), \\end{equation}\\] Artinya, jika hipotesis nol benar, kita hanya akan menolak hipotesis nol sebanyak 5% dari jumlah pengujian yang dilakukan, jika kita mengulangi proses sampling dan pengujian berulang kali. Oleh karena itu, tingkat signifikansi adalah probabilitas terjadinya kesalahan tipe I (kesalahan jenis pertama), yaitu kesalahan menolak hipotesis nol yang sebenarnya benar. Untuk alasan ini, tingkat signifikansi \\(\\alpha\\) juga disebut sebagai tingkat kesalahan tipe I. Jenis kesalahan lain yang dapat kita buat dalam pengujian hipotesis adalah kesalahan tipe II (kesalahan jenis kedua), yaitu kesalahan menerima hipotesis nol yang sebenarnya salah. Demikian pula, kita dapat mendefinisikan tingkat kesalahan tipe II sebagai probabilitas tidak menolak (menerima) hipotesis nol yang sebenarnya salah. Dengan kata lain, tingkat kesalahan tipe II diberikan oleh \\[\\begin{equation} \\Pr(\\mbox{accept }H_0|H_0\\mbox{ is false}). \\end{equation}\\] Jumlah lain yang penting mengenai kualitas uji statistik disebut daya uji (power of the test) \\(\\beta\\), yang didefinisikan sebagai probabilitas menolak hipotesis nol yang salah. Definisi matematis dari daya uji adalah sebagai berikut: \\[\\begin{equation} \\beta=\\Pr(\\mbox{reject }H_0|H_0\\mbox{ is false}). \\end{equation}\\] Perlu diperhatikan bahwa daya uji biasanya dihitung berdasarkan nilai alternatif tertentu \\(\\theta=\\theta_a\\), dengan distribusi sampling yang spesifik dan ukuran sampel yang spesifik pula. Dalam penelitian eksperimen sebenarnya, orang biasanya menghitung ukuran sampel yang diperlukan untuk memilih ukuran sampel yang akan memastikan peluang besar mendapatkan uji statistik yang signifikan secara statistik (misalnya, dengan daya uji yang telah ditentukan sebelumnya seperti 85%). 15.4.2 Uji Student-t berdasarkan Estimasi Maksimum Likelihood (MLE) Berdasarkan hasil dari Bagian 15.3.1, kita dapat mendefinisikan uji Student-\\(t\\) untuk menguji \\(H_0:\\theta=\\theta_0\\). Secara khusus, kita mendefinisikan statistik uji sebagai \\[\\begin{equation} t\\text{-stat}=\\frac{\\hat{\\theta}-\\theta_0}{se(\\hat{\\theta})}, \\end{equation}\\] distribusi besar-sampel dari distribusi student-\\(t\\) dengan derajat kebebasan \\(n−p\\), ketika hipotesis nol benar (yaitu, ketika \\(\\theta=\\theta_0\\). Dengan tingkat signifikansi tertentu \\(α\\), misalnya 5%, kita menolak hipotesis nol jika kejadian \\(t\\text{-stat}&lt;-t_{n-p}\\left(1-{\\alpha}/{2}\\right)\\) atau \\(t\\text{-stat}&gt; t_{n-p}\\left(1-{\\alpha}/{2}\\right)\\) terjadi (daerah penolakan). Dalam hipotesis nol \\(H_0\\), kita memiliki \\[\\begin{equation} \\Pr\\left[t\\text{-stat}&lt;-t_{n-p}\\left(1-\\frac{\\alpha}{2}\\right)\\right]=\\Pr\\left[t\\text{-stat}&gt; t_{n-p}\\left(1-\\frac{\\alpha}{2}\\right) \\right]= \\frac{\\alpha}{2}. \\end{equation}\\] Selain konsep daerah penolakan, kita dapat menolak uji berdasarkan nilai \\(p\\)-value yang didefinisikan sebagai \\(2\\Pr(T&gt;|t\\text{-stat}|)\\) untuk uji dua sisi yang disebutkan sebelumnya, di mana variabel acak \\(T\\sim T_{n-p}\\). Kita menolak hipotesis nol jika nilai \\(p\\)-value lebih kecil dari atau sama dengan \\(α\\). Untuk sampel yang diberikan, nilai \\(p\\)-value didefinisikan sebagai tingkat signifikansi terkecil di mana hipotesis nol akan ditolak. Demikian pula, kita dapat membuat uji satu sisi untuk hipotesis nol \\(H_0:\\theta\\leq\\theta_0\\) (atau \\(H_0:\\theta\\geq\\theta_0\\)). Dengan menggunakan statistik uji yang sama, kita menolak hipotesis nol ketika \\(t\\text{-stat}&gt; t_{n-p}\\left(1-{\\alpha}\\right)\\) (atau \\(t\\text{-stat}&lt;- t_{n-p}\\left(1-{\\alpha}\\right)\\) untuk uji pada \\(H_0:\\theta\\geq\\theta_0\\)). Nilai \\(p\\)-value yang sesuai didefinisikan sebagai \\(\\Pr(T&gt;|t\\text{-stat}|)\\) (atau \\(\\Pr(T&lt;|t\\text{-stat}|)\\)) untuk uji pada \\(H_0:\\theta\\geq\\theta_0\\)). Perlu diperhatikan bahwa uji ini tidak valid untuk sampel kecil, kecuali untuk kasus uji pada rata-rata normal. Uji \\(t\\) Satu-Sampel untuk Rata-Rata Normal. Untuk uji pada rata-rata normal dengan bentuk \\(H_0:\\mu=\\mu_0\\), \\(H_0:\\mu\\leq\\mu_0\\), atau \\(H_0:\\mu\\geq\\mu_0\\), kita dapat mendefinisikan statistik uji sebagai \\[\\begin{equation} t\\text{-stat}=\\frac{\\bar{X}-\\mu_0}{{\\hat{\\sigma}}/{\\sqrt{n}}}, \\end{equation}\\] untuk mana kita memiliki distribusi sampling yang tepat \\(t\\text{-stat}\\sim T_{n-1}\\) berdasarkan teorema Cochran, dengan \\(T_{n-1}\\) mengindikasikan distribusi Student-\\(t\\) dengan derajat kebebasan \\(n−1\\). Menurut teorema Cochran, uji ini valid baik untuk sampel kecil maupun besar. 15.4.3 Uji Rasio Kemungkinan (Likelihood Ratio Test) Pada subbagian sebelumnya, kami telah memperkenalkan uji Student-\\(t\\) pada satu parameter, berdasarkan sifat-sifat mle. Dalam bagian ini, kami mendefinisikan uji alternatif yang disebut uji rasio kemungkinan (likelihood ratio test, LRT). LRT dapat digunakan untuk menguji beberapa parameter dari model statistik yang sama. Diberikan fungsi kemungkinan \\(L(\\theta|\\mathbf{x})\\) dan \\(\\Theta_0 \\subset \\Theta\\), statistik uji rasio kemungkinan untuk menguji \\(H_0:\\theta\\in\\Theta_0\\) terhadap \\(H_a:\\theta\\notin\\Theta_0\\) diberikan oleh \\[\\begin{equation} L=\\frac{\\sup_{\\theta\\in\\Theta_0}L(\\theta|\\mathbf{x})}{\\sup_{\\theta\\in\\Theta}L(\\theta|\\mathbf{x})}, \\end{equation}\\] dan untuk menguji \\(H_0:\\theta=\\theta_0\\) versus \\(H_a:\\theta\\neq\\theta_0\\) adalah \\[\\begin{equation} L=\\frac{L(\\theta_0|\\mathbf{x})}{\\sup_{\\theta\\in\\Theta}L(\\theta|\\mathbf{x})}. \\end{equation}\\] LRT menolak hipotesis nol ketika \\(L &lt; c\\), dengan ambang batas tergantung pada tingkat signifikansi \\(α\\), ukuran sampel \\(n\\), dan jumlah parameter dalam \\(θ\\). Berdasarkan Lembaran Neyman-Pearson, LRT adalah uji paling kuat secara seragam untuk menguji \\(H_0:\\theta=\\theta_0\\) dengan \\(H_a:\\theta=\\theta_a\\). Artinya, LRT memberikan daya paling besar \\(β\\) untuk \\(α\\) dan nilai alternatif \\(\\theta_a\\) yang diberikan. Berdasarkan Teorema Wilks, statistik uji rasio kemungkinan \\(-2\\log(L)\\) konvergen dalam distribusi ke distribusi Chi-square dengan derajat kebebasan adalah selisih antara dimensionalitas ruang parameter \\(Θ\\) dan \\(\\Theta_0\\), ketika ukuran sampel mendekati tak terhingga dan ketika model nol tertanam dalam model alternatif. Dengan kata lain, ketika model nol adalah kasus khusus dari model alternatif yang berisi ruang sampel yang terbatas, kita dapat memperkirakan \\(c\\) dengan \\(\\chi^2_{p_1 - p_2}(1-\\alpha)\\), persentil ke-\\(100\\times(1-\\alpha)\\) dari distribusi Chi-square, dengan \\(p1−p2\\) sebagai derajat kebebasan, dan \\(p1\\) dan \\(p2\\) adalah jumlah parameter dalam model alternatif dan nol, masing-masing. Perlu diperhatikan bahwa LRT juga adalah uji berukuran besar yang tidak akan valid untuk sampel kecil. 15.4.4 Kriteria Informasi Dalam aplikasi kehidupan nyata, LRT (Likelihood Ratio Test) umumnya digunakan untuk membandingkan dua model yang saling bersarang. Namun, pendekatan LRT sebagai alat seleksi model memiliki dua kelemahan utama: 1) Biasanya membutuhkan model nol yang bersarang di dalam model alternatif; 2) Model yang dipilih dari LRT cenderung menghasilkan overfitting dalam sampel, sehingga mengakibatkan prediksi di luar sampel yang buruk. Untuk mengatasi masalah ini, pemilihan model berdasarkan kriteria informasi, yang berlaku untuk model non-bersarang sambil mempertimbangkan kompleksitas model, lebih banyak digunakan untuk pemilihan model. Di sini, kami memperkenalkan dua kriteria yang paling umum digunakan, yaitu kriteria informasi Akaike dan kriteria informasi Bayes. Secara khusus, kriteria informasi Akaike (\\(AIC\\)) didefinisikan sebagai \\[\\begin{equation} AIC = -2\\log L(\\hat{\\boldsymbol \\theta}) + 2p, \\end{equation}\\] dimana \\(\\hat{\\boldsymbol \\theta}\\) merupakan MLE dari \\({\\boldsymbol \\theta}\\), dan \\(p\\) adalah jumlah parameter dalam model. Istilah tambahan \\(2p\\) mewakili penalti untuk kompleksitas model. Dengan kata lain, dengan fungsi likelihood yang maksimum yang sama, \\(AIC\\) memilih model dengan jumlah parameter yang lebih sedikit. Perlu diperhatikan bahwa \\(AIC\\) tidak mempertimbangkan pengaruh dari ukuran sampel \\(n\\). Secara alternatif, orang menggunakan kriteria informasi Bayesian (\\(BIC\\)) yang mempertimbangkan ukuran sampel. \\(BIC\\) didefinisikan sebagai \\[\\begin{equation} BIC = -2\\log L(\\hat{\\boldsymbol \\theta}) + p\\,\\log(n). \\end{equation}\\] Kita dapat mengamati bahwa \\(BIC\\) cenderung memberikan bobot yang lebih tinggi pada jumlah parameter. Dengan fungsi likelihood yang dimaksimumkan yang sama, \\(BIC\\) akan menyarankan model yang lebih parsimonius dibandingkan dengan \\(AIC\\). "],["appendix-b-iterated-expectations.html", "Bab 16 Appendix B: Iterated Expectations", " Bab 16 Appendix B: Iterated Expectations "],["appendix-c-maximum-likelihood-theory.html", "Bab 17 Appendix C: Maximum Likelihood Theory 17.1 Likelihood Function 17.2 Maximum Likelihood Estimators 17.3 Statistical Inference Based on Maximum Likelihood Estimation", " Bab 17 Appendix C: Maximum Likelihood Theory Pratinjau Bab. Lampiran Bab 15 memperkenalkan teori kemungkinan maksimum terkait estimasi parameter dari keluarga parametrik. Lampiran ini memberikan contoh yang lebih spesifik dan mengembangkan beberapa konsep. Bagian 17.1 mengulas definisi fungsi kemungkinan dan memperkenalkan propertinya. Bagian 17.2 mengulas estimasi kemungkinan maksimum, dan memperluas properti sampel besar mereka untuk kasus di mana terdapat beberapa parameter dalam model. Bagian 17.3 mengulas inferensi statistik berdasarkan estimasi kemungkinan maksimum, dengan contoh-contoh khusus pada kasus dengan beberapa parameter. 17.1 Likelihood Function 17.1.1 Likelihood and Log-likelihood Functions Di sini, kami memberikan tinjauan singkat tentang fungsi kemungkinan dan fungsi log-kemungkinan dari Lampiran Bab 15. Biarkan \\(f(\\cdot|\\boldsymbol\\theta)\\) menjadi fungsi probabilitas dari \\(X\\) , fungsi massa probabilitas(probability mass function/PMF) jika \\(X\\) diskrit atau fungsi densitas probabilitas (probability density function/PDF) jika kontinu. Kemungkinan adalah fungsi dari parameter ( \\(\\theta\\) ) yang diberikan data ( \\(x\\) ). Oleh karena itu, itu adalah fungsi parameter dengan data yang tetap, bukan fungsi data dengan parameter yang tetap. Vektor data \\(x\\) biasanya merupakan realisasi dari sampel acak seperti yang didefinisikan dalam Lampiran Bab 15. \\[L(\\boldsymbol{\\theta}|\\mathbf{x})=f(\\mathbf{x}|\\boldsymbol{\\theta})=\\prod_{i=1}^nf(x_i|\\boldsymbol{\\theta}),\\] Diberikan suatu realisasi dari sampel acak \\(\\mathbf{x}=(x_1,x_2,\\cdots,x_n)\\) dengan ukuran n , fungsi kemungkinan didefinisikan sebagai berikut: \\[l(\\boldsymbol{\\theta}|\\mathbf{x})=\\log L(\\boldsymbol{\\theta}|\\mathbf{x})=\\sum_{i=1}^n\\log f(x_i|\\boldsymbol{\\theta}),\\] \\[l(\\boldsymbol{\\theta}|\\mathbf{x})=\\log L(\\boldsymbol{\\theta}|\\mathbf{x})=\\sum_{i=1}^n\\log f(x_i|\\boldsymbol{\\theta}),\\] Di Appendix Bab 15, kita telah menggunakan distribusi normal sebagai contoh untuk menggambarkan konsep fungsi kemungkinan (likelihood function) dan fungsi log-kemungkinan (log-likelihood function). Di sini, kita akan turunkan fungsi kemungkinan dan fungsi log-kemungkinan yang sesuai ketika distribusi populasi berasal dari keluarga distribusi Pareto. 17.1.2 Properties of Likelihood Functions Dalam statistik matematika, turunan pertama dari fungsi log-kemungkinan terhadap parameter, \\(u(\\boldsymbol\\theta)=\\partial l(\\boldsymbol \\theta|\\mathbf{x})/\\partial \\boldsymbol \\theta\\), disebut sebagai fungsi skor, atau vektor skor ketika terdapat beberapa parameter dalam \\(\\theta\\). Fungsi skor atau vektor skor dapat dituliskan sebagai berikut: \\[u(\\boldsymbol\\theta)=\\frac{ \\partial}{\\partial \\boldsymbol \\theta} l(\\boldsymbol \\theta|\\mathbf{x}) =\\frac{ \\partial}{\\partial \\boldsymbol \\theta} \\log \\prod_{i=1}^n f(x_i;\\boldsymbol \\theta ) =\\sum_{i=1}^n \\frac{ \\partial}{\\partial \\boldsymbol \\theta} \\log f(x_i;\\boldsymbol \\theta ),\\] \\[\\mathrm{E}[u(\\boldsymbol\\theta)]=\\mathrm{E} \\left[ \\frac{ \\partial}{\\partial \\boldsymbol \\theta} l(\\boldsymbol \\theta|\\mathbf{x}) \\right] = \\mathbf 0 .\\] di mana \\(u(\\boldsymbol\\theta)=(u_1(\\boldsymbol\\theta),u_2(\\boldsymbol\\theta),\\cdots,u_p(\\boldsymbol\\theta))\\) ketika \\(\\boldsymbol\\theta=(\\theta_1,\\cdots,\\theta_p)\\) berisi \\(p &gt; 2\\) parameter, dengan elemen uk(θ) = ∂l(θ|x)/∂θk merupakan turunan parsial terhadap \\(\\theta_k\\) \\(k=1,2,\\cdots,p\\) Fungsi kemungkinan memiliki sifat-sifat berikut: Salah satu sifat dasar dari fungsi kemungkinan adalah bahwa harapan dari fungsi skor terhadap \\(x\\) adalah 0. Yaitu, \\[\\begin{aligned} \\mathrm{E} \\left[ \\frac{ \\partial}{\\partial \\boldsymbol \\theta} l(\\boldsymbol \\theta|\\mathbf{x}) \\right] &amp;= \\mathrm{E} \\left[ \\frac{\\frac{\\partial}{\\partial \\boldsymbol \\theta}f(\\mathbf{x};\\boldsymbol \\theta)}{f(\\mathbf{x};\\boldsymbol \\theta )} \\right] = \\int\\frac{\\partial}{\\partial \\boldsymbol \\theta} f(\\mathbf{y};\\boldsymbol \\theta ) d \\mathbf y \\\\ &amp;= \\frac{\\partial}{\\partial \\boldsymbol \\theta} \\int f(\\mathbf{y};\\boldsymbol \\theta ) d \\mathbf y = \\frac{\\partial}{\\partial \\boldsymbol \\theta} 1 = \\mathbf 0.\\end{aligned} \\] nyatakan dengan \\({ \\partial^2 l(\\boldsymbol \\theta|\\mathbf{x}) }/{\\partial \\boldsymbol \\theta\\partial \\boldsymbol \\theta^{\\prime}}={ \\partial^2 l(\\boldsymbol \\theta|\\mathbf{x}) }/{\\partial \\boldsymbol \\theta^{2}}\\) turunan kedua dari fungsi log-kemungkinan saat θ adalah parameter tunggal, atau dengan \\({ \\partial^2 l(\\boldsymbol \\theta|\\mathbf{x}) }/{\\partial \\boldsymbol \\theta\\partial \\boldsymbol \\theta^{\\prime}}=(h_{jk})=({ \\partial^2 l(\\boldsymbol \\theta|\\mathbf{x}) }/\\partial x_j\\partial x_k)\\) matriks Hessiana dari fungsi log-kemungkinan saat mengandung beberapa parameter. Nyatakan \\([{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta}][{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta&#39;}]=u^2(\\boldsymbol \\theta)\\) saat \\(\\theta\\) adalah parameter tunggal, atau biarkan \\([{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta}][{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta&#39;}]=(uu_{jk})\\) menjadi matriks p×p saat θ mengandung total \\(p\\) parameter, dengan setiap elemen \\(uu_{jk}=u_j(\\boldsymbol \\theta)u_k(\\boldsymbol \\theta)\\) merupakan elemen ke-k dari vektor skor seperti yang didefinisikan sebelumnya. Sifat dasar lain dari fungsi kemungkinan adalah bahwa jumlah dari harapan matriks Hessiana dan harapan hasil kali Kronecker dari vektor skor dan transpose-nya adalah 0. Yaitu, \\[\\mathrm{E} \\left( \\frac{ \\partial^2 }{\\partial \\boldsymbol \\theta\\partial \\boldsymbol \\theta^{\\prime}} l(\\boldsymbol \\theta|\\mathbf{x}) \\right) + \\mathrm{E} \\left( \\frac{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta} \\frac{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta^{\\prime}}\\right) = \\mathbf 0.\\] \\[\\mathcal{I}(\\boldsymbol \\theta) = \\mathrm{E} \\left( \\frac{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial \\boldsymbol \\theta} \\frac{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial \\boldsymbol \\theta^{\\prime}} \\right) = -\\mathrm{E} \\left( \\frac{ \\partial^2}{\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta^{\\prime}} l(\\boldsymbol \\theta|\\mathbf{x}) \\right).\\] Saat ukuran sampel n mendekati tak hingga, fungsi skor (vektor) akan konvergen dalam distribusi ke distribusi normal (atau distribusi normal multivariat jika \\(\\theta\\) mengandung beberapa parameter) dengan rata-rata 0 dan varian (atau matriks kovarian dalam kasus multivariat) diberikan oleh \\(\\mathcal{I}(\\boldsymbol \\theta)\\). 17.2 Maximum Likelihood Estimators Dalam statistika, estimasi maksimum likelihood adalah nilai-nilai parameter θ yang paling mungkin dihasilkan oleh data. 17.2.1 Definition and Derivation of MLE Berdasarkan definisi yang diberikan dalam Lampiran Bab 15, nilai \\(\\theta\\), katakanlah \\(\\hat{\\boldsymbol \\theta}_{MLE}\\), yang memaksimalkan fungsi likelihood, disebut sebagai estimasi maksimum likelihood (MLE) dari \\(\\theta\\). Karena fungsi logaritma \\(\\log(\\cdot)\\) adalah fungsi satu-ke-satu, kita juga dapat menentukan \\(\\hat{\\boldsymbol \\theta}_{MLE}\\)dengan memaksimalkan fungsi log-likelihood, \\(l(\\boldsymbol \\theta|\\mathbf{x})\\). Dengan kata lain, MLE didefinisikan sebagai: \\[\\hat{\\boldsymbol \\theta}_{MLE} = {\\mbox{argmax}}_{\\boldsymbol{\\theta}\\in\\Theta}~l(\\boldsymbol{\\theta}|\\mathbf{x}).\\] Diberikan bentuk analitik dari fungsi likelihood, MLE dapat diperoleh dengan mengambil turunan pertama dari fungsi log-likelihood terhadap θ, dan mengatur nilai-nilai turunan parsial menjadi nol. Dengan kata lain, MLE adalah solusi dari persamaan-persamaan berikut: \\[\\frac{\\partial l(\\hat{\\boldsymbol{\\theta}}|\\mathbf{x})}{\\partial\\hat{\\boldsymbol{\\theta}}}=\\mathbf 0.\\] 17.2.2 Asymptotic Properties of MLE Dari Appendix Chapter 15, MLE memiliki beberapa sifat yang baik dalam sampel besar, di bawah kondisi reguler tertentu. Kami menyajikan hasil-hasil tersebut untuk kasus satu parameter di Appendix Chapter 15, tetapi hasil-hasil tersebut juga berlaku untuk kasus ketika \\(\\theta\\) mengandung beberapa parameter. Secara khusus, kami memiliki hasil-hasil berikut, dalam kasus umum ketika \\(\\boldsymbol{\\theta}=(\\theta_1,\\theta_2,\\cdots,\\theta_p)\\). MLE dari suatu parameter \\(\\theta\\), \\(\\hat{\\boldsymbol \\theta}_{MLE}\\), adalah estimator yang konsisten. Artinya, MLE \\(\\hat{\\boldsymbol \\theta}_{MLE}\\) konvergen dalam probabilitas menuju nilai sebenarnya \\(\\theta\\), saat ukuran sampel n menuju tak hingga. MLE memiliki sifat asymptotic normality, yang berarti bahwa estimator akan konvergen dalam distribusi menuju distribusi normal multivariat yang berpusat pada nilai sebenarnya, saat ukuran sampel menuju tak hingga. Secara khusus, \\[\\sqrt{n}(\\hat{\\boldsymbol{\\theta}}_{MLE}-\\boldsymbol{\\theta})\\rightarrow N\\left(\\mathbf 0,\\,\\boldsymbol{V}\\right),\\quad \\mbox{as}\\quad n\\rightarrow \\infty,\\] di mana V merupakan varian asimptotik (atau matriks kovarian) dari estimator. Oleh karena itu, MLE \\(\\hat{\\boldsymbol \\theta}_{MLE}\\) memiliki distribusi normal yang hampir dengan mean \\(\\theta\\) dan varian (matriks kovarian jika \\(\\boldsymbol{V}/n\\) saat ukuran sampel besar. MLE adalah estimator yang efisien, yang berarti memiliki varian asimptotik terkecil V, yang biasa disebut sebagai batas bawah Cramer-Rao. Secara khusus, batas bawah Cramer-Rao adalah invers dari informasi Fisher (matriks) \\(\\mathcal{I}(\\boldsymbol{\\theta})\\) yang didefinisikan sebelumnya dalam lampiran ini. Oleh karena itu, \\(\\mathrm{Var}(\\hat{\\boldsymbol{\\theta}}_{MLE})\\) dapat diestimasi berdasarkan informasi Fisher yang diamati. Berdasarkan hasil-hasil di atas, kita dapat melakukan inferensi statistik berdasarkan prosedur yang ditentukan dalam Appendix Chapter 15. 17.2.3 Use of Maximum Likelihood Estimation Metode estimasi maksimum likelihood memiliki banyak keunggulan dibandingkan metode alternatif seperti metode momen yang diperkenalkan dalam Appendix Chapter 15. Ini adalah alat umum yang berfungsi dalam banyak situasi. Misalnya, kita dapat menuliskan fungsi likelihood dalam bentuk tertutup untuk data yang tercensored dan tertruncated. Estimasi maksimum likelihood dapat digunakan untuk model regresi termasuk covariate, seperti regresi survival, generalized linear models, dan mixed models, yang mungkin mencakup covariate yang bergantung pada waktu. Dari efisiensi MLE, metode ini optimal, yang terbaik, dalam arti bahwa memiliki varian terkecil di antara kelas semua estimator yang tidak bias untuk ukuran sampel besar. Dari hasil mengenai asimptotik normalitas MLE, kita dapat memperoleh distribusi untuk estimator dalam sampel besar, memungkinkan pengguna untuk menilai variabilitas dalam estimasi dan melakukan inferensi statistik pada parameter. Pendekatan ini lebih sedikit menghabiskan komputasi dibandingkan metode resampling yang membutuhkan banyak fitting model. Meskipun memiliki banyak keunggulan, MLE memiliki kekurangan dalam kasus seperti generalized linear models ketika tidak memiliki bentuk analitik tertutup. Dalam kasus seperti itu, estimator maksimum likelihood dihitung secara iteratif menggunakan metode optimisasi numerik. Misalnya, kita dapat menggunakan algoritma iteratif Newton-Raphson atau variasinya untuk mendapatkan MLE. Algoritma iteratif membutuhkan nilai awal. Untuk beberapa masalah, pemilihan nilai awal yang dekat menjadi sangat penting, terutama dalam kasus di mana fungsi likelihood memiliki minimum atau maksimum lokal. Oleh karena itu, mungkin ada masalah konvergensi ketika nilai awal jauh dari nilai maksimum. Oleh karena itu, penting untuk memulai dari nilai yang berbeda di seluruh ruang parameter, dan membandingkan likelihood atau log-likelihood yang dimaksimumkan untuk memastikan bahwa algoritma telah konvergen ke maksimum global. 17.3 Statistical Inference Based on Maximum Likelihood Estimation Di Appendix Chapter 15, kami telah memperkenalkan metode berbasis maksimum likelihood untuk inferensi statistik ketika θ mengandung satu parameter. Di sini, kami akan memperluas hasil tersebut untuk kasus di mana terdapat beberapa parameter dalam θ. 17.3.1 Hypothesis Testing Di Appendix Chapter 15, kami mendefinisikan pengujian hipotesis terkait hipotesis nol, yaitu pernyataan mengenai parameter-parameter dari distribusi atau model. Salah satu jenis inferensi yang penting adalah untuk menilai apakah estimasi parameter secara signifikan secara statistik, artinya apakah nilai parameter tersebut nol atau tidak. Sebelumnya, kami telah belajar bahwa mle \\(\\mathrm{Var}(\\hat{\\boldsymbol{\\theta}}_{MLE})\\) memiliki distribusi normal untuk ukuran sampel yang besar dengan mean θ dan matriks kovarian varian \\(\\mathcal{I}^{-1}(\\boldsymbol \\theta)\\). Berdasarkan distribusi normal multivariat, elemen ke-j dari θ^MLE, katakanlah \\(\\mathrm{Var}(\\hat{\\boldsymbol{\\theta}}_{MLE})\\),\\(\\hat{\\theta}_{MLE,j}\\), memiliki distribusi normal univariat untuk ukuran sampel yang besar. Tentukan \\(se(\\hat{\\theta}_{MLE,j})\\), yaitu kesalahan standar (deviasi standar yang diestimasi), sebagai akar kuadrat elemen diagonal ke-j dari \\(\\mathcal{I}^{-1}(\\boldsymbol \\theta)_{MLE}\\). Untuk menilai hipotesis nol bahwa \\(\\theta_j=\\theta_0\\), kami mendefinisikan statistik t atau rasio t sebagai \\(t(\\hat{\\theta}_{MLE,j})=(\\hat{\\theta}_{MLE,j}-\\theta_0)/se(\\hat{\\theta}_{MLE,j})\\). Di bawah hipotesis nol, statistik t tersebut memiliki distribusi t-Student dengan derajat kebebasan sebesar \\(n−p\\), dengan \\(p\\) adalah dimensi dari \\(\\theta\\) . Untuk sebagian besar aplikasi aktuaria, kita memiliki ukuran sampel yang besar \\(n\\), sehingga distribusi \\(t\\) sangat dekat dengan distribusi normal (standar). Dalam kasus ketika n sangat besar atau ketika kesalahan standar diketahui, statistik \\(t\\) dapat disebut sebagai statistik \\(z\\) atau skor \\(z\\). Berdasarkan hasil dari Appendix Chapter 15, jika statistik \\(t\\) \\(t(\\hat{\\theta}_{MLE,j})\\) melebihi nilai batas (dalam nilai absolut), maka pengujian untuk parameter ke-j \\(\\theta_j\\) dianggap signifikan secara statistik. Jika \\(\\theta_j\\) adalah koefisien regresi dari variabel independen ke-j, maka kita mengatakan bahwa variabel ke-j tersebut signifikan secara statistik. Sebagai contoh, jika kita menggunakan tingkat signifikansi 5%, maka nilai batasnya adalah 1.96 dengan menggunakan pendekatan distribusi normal untuk kasus dengan ukuran sampel yang besar. Secara umum, dengan menggunakan tingkat signifikansi \\(100 \\alpha \\%\\), maka nilai batasnya adalah kuantil \\(100(1-\\alpha/2)\\%\\) dari distribusi t-Student dengan derajat kebebasan \\(n−p\\). Konsep lain yang berguna dalam pengujian hipotesis adalah p-value atau probability value. Berdasarkan definisi matematis dalam Appendix Chapter 15, p-value didefinisikan sebagai tingkat signifikansi terkecil di mana hipotesis nol akan ditolak. Oleh karena itu, p-value adalah statistik ringkasan yang berguna bagi analis data untuk dilaporkan karena memungkinkan pembaca memahami kekuatan bukti statistik tentang penyimpangan dari hipotesis nol. 17.3.2 MLE and Model Validation Selain pengujian hipotesis dan estimasi interval yang diperkenalkan di Appendix Chapter 15 dan subbagian sebelumnya, jenis inferensi penting lainnya adalah pemilihan model dari dua pilihan, di mana satu pilihan merupakan kasus khusus dari yang lain dengan beberapa parameter dibatasi. Untuk dua model tersebut, di mana satu model termasuk dalam model yang lain, kami telah memperkenalkan uji rasio kemungkinan (likelihood ratio test/LRT) di Appendix Chapter 15. Di sini, kami akan secara singkat mengulas proses melakukan LRT berdasarkan contoh khusus dari dua model alternatif. Misalkan kita memiliki sebuah model (besar) di mana kita memperoleh estimasi maximum likelihood, \\(\\hat{\\boldsymbol{\\theta}}_{MLE}v\\). Sekarang diasumsikan bahwa beberapa elemen \\(p\\) dalam \\(\\theta\\) adalah nol, dan kita menentukan estimasi maximum likelihood dari himpunan yang tersisa, dengan estimasi yang dihasilkan ditunjukkan sebagai \\(\\hat{\\boldsymbol{\\theta}}_{Reduced}\\). Berdasarkan definisi di Appendix Chapter 15, statistik \\(LRT= 2 \\left( l(\\hat{\\boldsymbol{\\theta}}_{MLE}) - l(\\hat{\\boldsymbol{\\theta}}_{Reduced}) \\right)\\), disebut sebagai statistik rasio kemungkinan. Di bawah hipotesis nol bahwa model yang dibatasi adalah benar, rasio kemungkinan memiliki distribusi chi-kuadrat dengan derajat kebebasan sebanyak d, yaitu jumlah variabel yang diatur menjadi nol. Uji seperti ini memungkinkan kita menentukan model mana dari dua model yang lebih mungkin benar, dengan mempertimbangkan data yang diamati. Jika statistik LRT besar relatif terhadap nilai kritis dari distribusi chi-kuadrat, maka kita menolak model yang dibatasi dan memilih model yang lebih besar. Detail mengenai nilai kritis dan metode alternatif berdasarkan kriteria informasi dijelaskan di Appendix Chapter 15. "],["appendix-d-summary-of-distributions.html", "Bab 18 Appendix D: Summary of Distributions", " Bab 18 Appendix D: Summary of Distributions "],["appendix-e-conventions-for-notation.html", "Bab 19 Appendix E: Conventions for Notation", " Bab 19 Appendix E: Conventions for Notation "],["section.html", "", " "],["bibliography.html", "Bibliography", " Bibliography Lemaire, J., &amp; Zi, H. (1994). A comparative analysis of 30 bonus-malus systems. ASTIN Bulletin, 24(2), 287–309. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
